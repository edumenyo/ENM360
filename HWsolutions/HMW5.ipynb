{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, num_params, lr=1e-3, momentum = 0.0):\n",
    "        self.lr = lr \n",
    "        self.momentum = momentum\n",
    "        self.velocity = np.zeros(num_params)\n",
    "\n",
    "    def step(self, w, grad_w):\n",
    "        self.velocity = self.momentum * self.velocity - (1.0 - self.momentum) * grad_w\n",
    "        \n",
    "        w = w + self.lr * self.velocity\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearSystem:\n",
    "  # Initialize model class\n",
    "  def __init__(self, A, b, x0):    \n",
    "      self.A = A\n",
    "      self.b = b\n",
    "      self.x = x0\n",
    "      self.N = A.shape[0]\n",
    "      self.it_cnt = 0\n",
    "      self.loss_log = []\n",
    "      \n",
    "  def compute_loss(self, A, b):\n",
    "      diff = np.matmul(A, self.x) - b\n",
    "      loss = np.mean(np.matmul(diff.T, diff), axis = 0)\n",
    "      gradient = 2.0*np.matmul(diff.T, A)\n",
    "      return loss, gradient\n",
    "      \n",
    "  def fetch_minibtach(self, M):\n",
    "      idx = np.random.permutation(self.N)[:M]\n",
    "      A = self.A[idx,:]\n",
    "      b = self.b[idx]\n",
    "      return A,b\n",
    "      \n",
    "  def solve(self, optimizer, batch_size = 128, tol = 1e-6):\n",
    "      loss = 1.0\n",
    "      while loss > tol:\n",
    "          self.it_cnt = self.it_cnt + 1\n",
    "          # Fetch minibatch\n",
    "          A, b = self.fetch_minibtach(batch_size)\n",
    "          # Evaluate loss using current parameters\n",
    "          loss, gradient = self.compute_loss(A, b)\n",
    "          if self.it_cnt % 10 == 0:\n",
    "              self.loss_log.append(loss)\n",
    "              print(\"Iteration: %d, loss: %.3e\" % (self.it_cnt, loss))\n",
    "          if np.isinf(loss):\n",
    "              break\n",
    "          # Update parameters\n",
    "          self.x = optimizer.step(self.x.flatten(), gradient.flatten())[:,None]\n",
    "      return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10, loss: 3.579e+02\n",
      "Iteration: 20, loss: 4.120e+02\n",
      "Iteration: 30, loss: 3.186e+02\n",
      "Iteration: 40, loss: 2.512e+02\n",
      "Iteration: 50, loss: 2.085e+02\n",
      "Iteration: 60, loss: 1.762e+02\n",
      "Iteration: 70, loss: 2.806e+02\n",
      "Iteration: 80, loss: 9.203e+01\n",
      "Iteration: 90, loss: 3.847e+02\n",
      "Iteration: 100, loss: 2.094e+02\n",
      "Iteration: 110, loss: 1.976e+02\n",
      "Iteration: 120, loss: 1.905e+02\n",
      "Iteration: 130, loss: 2.755e+02\n",
      "Iteration: 140, loss: 1.883e+02\n",
      "Iteration: 150, loss: 2.726e+02\n",
      "Iteration: 160, loss: 2.740e+02\n",
      "Iteration: 170, loss: 2.432e+02\n",
      "Iteration: 180, loss: 1.485e+02\n",
      "Iteration: 190, loss: 1.206e+02\n",
      "Iteration: 200, loss: 1.872e+02\n",
      "Iteration: 210, loss: 2.312e+02\n",
      "Iteration: 220, loss: 1.218e+02\n",
      "Iteration: 230, loss: 1.110e+02\n",
      "Iteration: 240, loss: 1.269e+02\n",
      "Iteration: 250, loss: 1.214e+02\n",
      "Iteration: 260, loss: 1.679e+02\n",
      "Iteration: 270, loss: 1.769e+02\n",
      "Iteration: 280, loss: 1.370e+02\n",
      "Iteration: 290, loss: 1.033e+02\n",
      "Iteration: 300, loss: 1.481e+02\n",
      "Iteration: 310, loss: 1.456e+02\n",
      "Iteration: 320, loss: 1.772e+02\n",
      "Iteration: 330, loss: 1.152e+02\n",
      "Iteration: 340, loss: 8.960e+01\n",
      "Iteration: 350, loss: 1.421e+02\n",
      "Iteration: 360, loss: 8.696e+01\n",
      "Iteration: 370, loss: 8.883e+01\n",
      "Iteration: 380, loss: 1.075e+02\n",
      "Iteration: 390, loss: 1.127e+02\n",
      "Iteration: 400, loss: 1.009e+02\n",
      "Iteration: 410, loss: 7.067e+01\n",
      "Iteration: 420, loss: 9.776e+01\n",
      "Iteration: 430, loss: 1.121e+02\n",
      "Iteration: 440, loss: 9.942e+01\n",
      "Iteration: 450, loss: 9.140e+01\n",
      "Iteration: 460, loss: 1.087e+02\n",
      "Iteration: 470, loss: 7.821e+01\n",
      "Iteration: 480, loss: 8.512e+01\n",
      "Iteration: 490, loss: 7.449e+01\n",
      "Iteration: 500, loss: 1.154e+02\n",
      "Iteration: 510, loss: 6.028e+01\n",
      "Iteration: 520, loss: 5.218e+01\n",
      "Iteration: 530, loss: 7.902e+01\n",
      "Iteration: 540, loss: 8.137e+01\n",
      "Iteration: 550, loss: 5.324e+01\n",
      "Iteration: 560, loss: 3.792e+01\n",
      "Iteration: 570, loss: 6.615e+01\n",
      "Iteration: 580, loss: 1.092e+02\n",
      "Iteration: 590, loss: 6.011e+01\n",
      "Iteration: 600, loss: 4.337e+01\n",
      "Iteration: 610, loss: 6.052e+01\n",
      "Iteration: 620, loss: 4.570e+01\n",
      "Iteration: 630, loss: 3.139e+01\n",
      "Iteration: 640, loss: 4.386e+01\n",
      "Iteration: 650, loss: 5.287e+01\n",
      "Iteration: 660, loss: 4.109e+01\n",
      "Iteration: 670, loss: 4.882e+01\n",
      "Iteration: 680, loss: 4.159e+01\n",
      "Iteration: 690, loss: 6.418e+01\n",
      "Iteration: 700, loss: 4.593e+01\n",
      "Iteration: 710, loss: 5.811e+01\n",
      "Iteration: 720, loss: 5.226e+01\n",
      "Iteration: 730, loss: 3.831e+01\n",
      "Iteration: 740, loss: 4.611e+01\n",
      "Iteration: 750, loss: 3.877e+01\n",
      "Iteration: 760, loss: 3.191e+01\n",
      "Iteration: 770, loss: 3.339e+01\n",
      "Iteration: 780, loss: 4.519e+01\n",
      "Iteration: 790, loss: 4.027e+01\n",
      "Iteration: 800, loss: 4.037e+01\n",
      "Iteration: 810, loss: 4.088e+01\n",
      "Iteration: 820, loss: 3.785e+01\n",
      "Iteration: 830, loss: 3.765e+01\n",
      "Iteration: 840, loss: 3.014e+01\n",
      "Iteration: 850, loss: 4.948e+01\n",
      "Iteration: 860, loss: 4.570e+01\n",
      "Iteration: 870, loss: 3.066e+01\n",
      "Iteration: 880, loss: 1.806e+01\n",
      "Iteration: 890, loss: 3.849e+01\n",
      "Iteration: 900, loss: 2.450e+01\n",
      "Iteration: 910, loss: 3.798e+01\n",
      "Iteration: 920, loss: 2.922e+01\n",
      "Iteration: 930, loss: 3.078e+01\n",
      "Iteration: 940, loss: 2.314e+01\n",
      "Iteration: 950, loss: 3.159e+01\n",
      "Iteration: 960, loss: 2.861e+01\n",
      "Iteration: 970, loss: 4.121e+01\n",
      "Iteration: 980, loss: 3.970e+01\n",
      "Iteration: 990, loss: 3.332e+01\n",
      "Iteration: 1000, loss: 2.562e+01\n",
      "Iteration: 1010, loss: 2.600e+01\n",
      "Iteration: 1020, loss: 1.446e+01\n",
      "Iteration: 1030, loss: 4.083e+01\n",
      "Iteration: 1040, loss: 4.313e+01\n",
      "Iteration: 1050, loss: 1.685e+01\n",
      "Iteration: 1060, loss: 2.714e+01\n",
      "Iteration: 1070, loss: 2.398e+01\n",
      "Iteration: 1080, loss: 2.204e+01\n",
      "Iteration: 1090, loss: 2.959e+01\n",
      "Iteration: 1100, loss: 2.755e+01\n",
      "Iteration: 1110, loss: 2.222e+01\n",
      "Iteration: 1120, loss: 1.350e+01\n",
      "Iteration: 1130, loss: 2.221e+01\n",
      "Iteration: 1140, loss: 1.922e+01\n",
      "Iteration: 1150, loss: 2.631e+01\n",
      "Iteration: 1160, loss: 2.543e+01\n",
      "Iteration: 1170, loss: 1.197e+01\n",
      "Iteration: 1180, loss: 2.521e+01\n",
      "Iteration: 1190, loss: 2.821e+01\n",
      "Iteration: 1200, loss: 2.154e+01\n",
      "Iteration: 1210, loss: 2.081e+01\n",
      "Iteration: 1220, loss: 2.228e+01\n",
      "Iteration: 1230, loss: 2.025e+01\n",
      "Iteration: 1240, loss: 2.392e+01\n",
      "Iteration: 1250, loss: 3.162e+01\n",
      "Iteration: 1260, loss: 2.605e+01\n",
      "Iteration: 1270, loss: 1.966e+01\n",
      "Iteration: 1280, loss: 2.352e+01\n",
      "Iteration: 1290, loss: 1.149e+01\n",
      "Iteration: 1300, loss: 9.985e+00\n",
      "Iteration: 1310, loss: 1.860e+01\n",
      "Iteration: 1320, loss: 2.112e+01\n",
      "Iteration: 1330, loss: 1.830e+01\n",
      "Iteration: 1340, loss: 1.725e+01\n",
      "Iteration: 1350, loss: 2.165e+01\n",
      "Iteration: 1360, loss: 1.877e+01\n",
      "Iteration: 1370, loss: 1.224e+01\n",
      "Iteration: 1380, loss: 2.136e+01\n",
      "Iteration: 1390, loss: 1.168e+01\n",
      "Iteration: 1400, loss: 1.558e+01\n",
      "Iteration: 1410, loss: 1.425e+01\n",
      "Iteration: 1420, loss: 1.400e+01\n",
      "Iteration: 1430, loss: 1.291e+01\n",
      "Iteration: 1440, loss: 1.922e+01\n",
      "Iteration: 1450, loss: 1.974e+01\n",
      "Iteration: 1460, loss: 1.599e+01\n",
      "Iteration: 1470, loss: 1.680e+01\n",
      "Iteration: 1480, loss: 1.166e+01\n",
      "Iteration: 1490, loss: 1.719e+01\n",
      "Iteration: 1500, loss: 1.226e+01\n",
      "Iteration: 1510, loss: 1.500e+01\n",
      "Iteration: 1520, loss: 1.121e+01\n",
      "Iteration: 1530, loss: 1.726e+01\n",
      "Iteration: 1540, loss: 1.794e+01\n",
      "Iteration: 1550, loss: 1.774e+01\n",
      "Iteration: 1560, loss: 1.293e+01\n",
      "Iteration: 1570, loss: 1.477e+01\n",
      "Iteration: 1580, loss: 1.038e+01\n",
      "Iteration: 1590, loss: 1.037e+01\n",
      "Iteration: 1600, loss: 1.710e+01\n",
      "Iteration: 1610, loss: 1.789e+01\n",
      "Iteration: 1620, loss: 1.729e+01\n",
      "Iteration: 1630, loss: 2.065e+01\n",
      "Iteration: 1640, loss: 8.805e+00\n",
      "Iteration: 1650, loss: 1.054e+01\n",
      "Iteration: 1660, loss: 1.706e+01\n",
      "Iteration: 1670, loss: 1.196e+01\n",
      "Iteration: 1680, loss: 1.038e+01\n",
      "Iteration: 1690, loss: 1.107e+01\n",
      "Iteration: 1700, loss: 1.127e+01\n",
      "Iteration: 1710, loss: 1.543e+01\n",
      "Iteration: 1720, loss: 1.096e+01\n",
      "Iteration: 1730, loss: 1.023e+01\n",
      "Iteration: 1740, loss: 1.457e+01\n",
      "Iteration: 1750, loss: 7.932e+00\n",
      "Iteration: 1760, loss: 1.246e+01\n",
      "Iteration: 1770, loss: 1.030e+01\n",
      "Iteration: 1780, loss: 1.161e+01\n",
      "Iteration: 1790, loss: 1.526e+01\n",
      "Iteration: 1800, loss: 5.121e+00\n",
      "Iteration: 1810, loss: 1.264e+01\n",
      "Iteration: 1820, loss: 8.785e+00\n",
      "Iteration: 1830, loss: 1.363e+01\n",
      "Iteration: 1840, loss: 1.622e+01\n",
      "Iteration: 1850, loss: 1.931e+01\n",
      "Iteration: 1860, loss: 1.211e+01\n",
      "Iteration: 1870, loss: 1.034e+01\n",
      "Iteration: 1880, loss: 1.100e+01\n",
      "Iteration: 1890, loss: 7.246e+00\n",
      "Iteration: 1900, loss: 9.045e+00\n",
      "Iteration: 1910, loss: 8.904e+00\n",
      "Iteration: 1920, loss: 4.843e+00\n",
      "Iteration: 1930, loss: 9.487e+00\n",
      "Iteration: 1940, loss: 1.025e+01\n",
      "Iteration: 1950, loss: 8.741e+00\n",
      "Iteration: 1960, loss: 4.576e+00\n",
      "Iteration: 1970, loss: 1.047e+01\n",
      "Iteration: 1980, loss: 1.224e+01\n",
      "Iteration: 1990, loss: 4.850e+00\n",
      "Iteration: 2000, loss: 1.147e+01\n",
      "Iteration: 2010, loss: 1.039e+01\n",
      "Iteration: 2020, loss: 1.166e+01\n",
      "Iteration: 2030, loss: 1.058e+01\n",
      "Iteration: 2040, loss: 5.211e+00\n",
      "Iteration: 2050, loss: 9.847e+00\n",
      "Iteration: 2060, loss: 5.878e+00\n",
      "Iteration: 2070, loss: 6.164e+00\n",
      "Iteration: 2080, loss: 7.753e+00\n",
      "Iteration: 2090, loss: 6.819e+00\n",
      "Iteration: 2100, loss: 8.424e+00\n",
      "Iteration: 2110, loss: 9.387e+00\n",
      "Iteration: 2120, loss: 7.171e+00\n",
      "Iteration: 2130, loss: 8.298e+00\n",
      "Iteration: 2140, loss: 1.331e+01\n",
      "Iteration: 2150, loss: 7.240e+00\n",
      "Iteration: 2160, loss: 7.542e+00\n",
      "Iteration: 2170, loss: 5.323e+00\n",
      "Iteration: 2180, loss: 8.549e+00\n",
      "Iteration: 2190, loss: 4.770e+00\n",
      "Iteration: 2200, loss: 7.343e+00\n",
      "Iteration: 2210, loss: 6.488e+00\n",
      "Iteration: 2220, loss: 7.200e+00\n",
      "Iteration: 2230, loss: 6.646e+00\n",
      "Iteration: 2240, loss: 6.424e+00\n",
      "Iteration: 2250, loss: 4.759e+00\n",
      "Iteration: 2260, loss: 6.864e+00\n",
      "Iteration: 2270, loss: 1.001e+01\n",
      "Iteration: 2280, loss: 8.553e+00\n",
      "Iteration: 2290, loss: 8.139e+00\n",
      "Iteration: 2300, loss: 8.788e+00\n",
      "Iteration: 2310, loss: 7.724e+00\n",
      "Iteration: 2320, loss: 6.313e+00\n",
      "Iteration: 2330, loss: 7.187e+00\n",
      "Iteration: 2340, loss: 7.963e+00\n",
      "Iteration: 2350, loss: 7.715e+00\n",
      "Iteration: 2360, loss: 6.529e+00\n",
      "Iteration: 2370, loss: 4.078e+00\n",
      "Iteration: 2380, loss: 5.182e+00\n",
      "Iteration: 2390, loss: 6.957e+00\n",
      "Iteration: 2400, loss: 8.896e+00\n",
      "Iteration: 2410, loss: 9.529e+00\n",
      "Iteration: 2420, loss: 3.755e+00\n",
      "Iteration: 2430, loss: 4.759e+00\n",
      "Iteration: 2440, loss: 6.733e+00\n",
      "Iteration: 2450, loss: 6.147e+00\n",
      "Iteration: 2460, loss: 5.166e+00\n",
      "Iteration: 2470, loss: 6.220e+00\n",
      "Iteration: 2480, loss: 6.042e+00\n",
      "Iteration: 2490, loss: 5.384e+00\n",
      "Iteration: 2500, loss: 3.969e+00\n",
      "Iteration: 2510, loss: 5.509e+00\n",
      "Iteration: 2520, loss: 5.100e+00\n",
      "Iteration: 2530, loss: 5.458e+00\n",
      "Iteration: 2540, loss: 4.236e+00\n",
      "Iteration: 2550, loss: 7.093e+00\n",
      "Iteration: 2560, loss: 5.998e+00\n",
      "Iteration: 2570, loss: 4.594e+00\n",
      "Iteration: 2580, loss: 4.329e+00\n",
      "Iteration: 2590, loss: 4.717e+00\n",
      "Iteration: 2600, loss: 6.098e+00\n",
      "Iteration: 2610, loss: 5.977e+00\n",
      "Iteration: 2620, loss: 6.718e+00\n",
      "Iteration: 2630, loss: 4.668e+00\n",
      "Iteration: 2640, loss: 5.812e+00\n",
      "Iteration: 2650, loss: 4.805e+00\n",
      "Iteration: 2660, loss: 4.612e+00\n",
      "Iteration: 2670, loss: 7.303e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2680, loss: 6.013e+00\n",
      "Iteration: 2690, loss: 3.296e+00\n",
      "Iteration: 2700, loss: 3.549e+00\n",
      "Iteration: 2710, loss: 4.832e+00\n",
      "Iteration: 2720, loss: 5.354e+00\n",
      "Iteration: 2730, loss: 4.456e+00\n",
      "Iteration: 2740, loss: 6.734e+00\n",
      "Iteration: 2750, loss: 4.745e+00\n",
      "Iteration: 2760, loss: 4.485e+00\n",
      "Iteration: 2770, loss: 5.571e+00\n",
      "Iteration: 2780, loss: 5.741e+00\n",
      "Iteration: 2790, loss: 4.515e+00\n",
      "Iteration: 2800, loss: 7.242e+00\n",
      "Iteration: 2810, loss: 5.569e+00\n",
      "Iteration: 2820, loss: 4.681e+00\n",
      "Iteration: 2830, loss: 4.427e+00\n",
      "Iteration: 2840, loss: 5.071e+00\n",
      "Iteration: 2850, loss: 3.686e+00\n",
      "Iteration: 2860, loss: 5.079e+00\n",
      "Iteration: 2870, loss: 5.728e+00\n",
      "Iteration: 2880, loss: 4.992e+00\n",
      "Iteration: 2890, loss: 3.935e+00\n",
      "Iteration: 2900, loss: 4.264e+00\n",
      "Iteration: 2910, loss: 3.317e+00\n",
      "Iteration: 2920, loss: 4.648e+00\n",
      "Iteration: 2930, loss: 3.727e+00\n",
      "Iteration: 2940, loss: 4.140e+00\n",
      "Iteration: 2950, loss: 4.220e+00\n",
      "Iteration: 2960, loss: 2.705e+00\n",
      "Iteration: 2970, loss: 2.563e+00\n",
      "Iteration: 2980, loss: 6.029e+00\n",
      "Iteration: 2990, loss: 4.906e+00\n",
      "Iteration: 3000, loss: 3.964e+00\n",
      "Iteration: 3010, loss: 5.114e+00\n",
      "Iteration: 3020, loss: 5.333e+00\n",
      "Iteration: 3030, loss: 3.925e+00\n",
      "Iteration: 3040, loss: 3.419e+00\n",
      "Iteration: 3050, loss: 5.528e+00\n",
      "Iteration: 3060, loss: 3.852e+00\n",
      "Iteration: 3070, loss: 4.824e+00\n",
      "Iteration: 3080, loss: 2.671e+00\n",
      "Iteration: 3090, loss: 4.902e+00\n",
      "Iteration: 3100, loss: 5.017e+00\n",
      "Iteration: 3110, loss: 5.041e+00\n",
      "Iteration: 3120, loss: 2.312e+00\n",
      "Iteration: 3130, loss: 2.466e+00\n",
      "Iteration: 3140, loss: 3.068e+00\n",
      "Iteration: 3150, loss: 3.019e+00\n",
      "Iteration: 3160, loss: 3.844e+00\n",
      "Iteration: 3170, loss: 4.675e+00\n",
      "Iteration: 3180, loss: 3.371e+00\n",
      "Iteration: 3190, loss: 4.129e+00\n",
      "Iteration: 3200, loss: 2.552e+00\n",
      "Iteration: 3210, loss: 3.973e+00\n",
      "Iteration: 3220, loss: 4.778e+00\n",
      "Iteration: 3230, loss: 2.689e+00\n",
      "Iteration: 3240, loss: 3.345e+00\n",
      "Iteration: 3250, loss: 3.251e+00\n",
      "Iteration: 3260, loss: 4.340e+00\n",
      "Iteration: 3270, loss: 5.311e+00\n",
      "Iteration: 3280, loss: 3.822e+00\n",
      "Iteration: 3290, loss: 4.891e+00\n",
      "Iteration: 3300, loss: 4.142e+00\n",
      "Iteration: 3310, loss: 2.943e+00\n",
      "Iteration: 3320, loss: 4.095e+00\n",
      "Iteration: 3330, loss: 4.620e+00\n",
      "Iteration: 3340, loss: 3.500e+00\n",
      "Iteration: 3350, loss: 2.386e+00\n",
      "Iteration: 3360, loss: 3.651e+00\n",
      "Iteration: 3370, loss: 3.117e+00\n",
      "Iteration: 3380, loss: 3.815e+00\n",
      "Iteration: 3390, loss: 3.223e+00\n",
      "Iteration: 3400, loss: 2.112e+00\n",
      "Iteration: 3410, loss: 3.353e+00\n",
      "Iteration: 3420, loss: 3.021e+00\n",
      "Iteration: 3430, loss: 2.330e+00\n",
      "Iteration: 3440, loss: 2.315e+00\n",
      "Iteration: 3450, loss: 3.054e+00\n",
      "Iteration: 3460, loss: 2.671e+00\n",
      "Iteration: 3470, loss: 3.555e+00\n",
      "Iteration: 3480, loss: 2.632e+00\n",
      "Iteration: 3490, loss: 3.126e+00\n",
      "Iteration: 3500, loss: 3.924e+00\n",
      "Iteration: 3510, loss: 2.451e+00\n",
      "Iteration: 3520, loss: 2.233e+00\n",
      "Iteration: 3530, loss: 2.805e+00\n",
      "Iteration: 3540, loss: 2.488e+00\n",
      "Iteration: 3550, loss: 4.165e+00\n",
      "Iteration: 3560, loss: 4.219e+00\n",
      "Iteration: 3570, loss: 2.289e+00\n",
      "Iteration: 3580, loss: 2.594e+00\n",
      "Iteration: 3590, loss: 2.351e+00\n",
      "Iteration: 3600, loss: 3.149e+00\n",
      "Iteration: 3610, loss: 2.860e+00\n",
      "Iteration: 3620, loss: 4.163e+00\n",
      "Iteration: 3630, loss: 4.286e+00\n",
      "Iteration: 3640, loss: 1.294e+00\n",
      "Iteration: 3650, loss: 3.941e+00\n",
      "Iteration: 3660, loss: 2.462e+00\n",
      "Iteration: 3670, loss: 2.420e+00\n",
      "Iteration: 3680, loss: 2.768e+00\n",
      "Iteration: 3690, loss: 3.606e+00\n",
      "Iteration: 3700, loss: 3.606e+00\n",
      "Iteration: 3710, loss: 4.516e+00\n",
      "Iteration: 3720, loss: 1.979e+00\n",
      "Iteration: 3730, loss: 1.968e+00\n",
      "Iteration: 3740, loss: 2.556e+00\n",
      "Iteration: 3750, loss: 2.984e+00\n",
      "Iteration: 3760, loss: 3.179e+00\n",
      "Iteration: 3770, loss: 2.225e+00\n",
      "Iteration: 3780, loss: 3.064e+00\n",
      "Iteration: 3790, loss: 2.456e+00\n",
      "Iteration: 3800, loss: 2.694e+00\n",
      "Iteration: 3810, loss: 3.114e+00\n",
      "Iteration: 3820, loss: 3.105e+00\n",
      "Iteration: 3830, loss: 2.232e+00\n",
      "Iteration: 3840, loss: 3.574e+00\n",
      "Iteration: 3850, loss: 2.706e+00\n",
      "Iteration: 3860, loss: 3.055e+00\n",
      "Iteration: 3870, loss: 1.792e+00\n",
      "Iteration: 3880, loss: 2.743e+00\n",
      "Iteration: 3890, loss: 2.631e+00\n",
      "Iteration: 3900, loss: 1.887e+00\n",
      "Iteration: 3910, loss: 2.505e+00\n",
      "Iteration: 3920, loss: 2.314e+00\n",
      "Iteration: 3930, loss: 2.249e+00\n",
      "Iteration: 3940, loss: 1.577e+00\n",
      "Iteration: 3950, loss: 2.834e+00\n",
      "Iteration: 3960, loss: 2.128e+00\n",
      "Iteration: 3970, loss: 2.315e+00\n",
      "Iteration: 3980, loss: 2.560e+00\n",
      "Iteration: 3990, loss: 1.999e+00\n",
      "Iteration: 4000, loss: 3.270e+00\n",
      "Iteration: 4010, loss: 2.710e+00\n",
      "Iteration: 4020, loss: 2.064e+00\n",
      "Iteration: 4030, loss: 2.129e+00\n",
      "Iteration: 4040, loss: 2.975e+00\n",
      "Iteration: 4050, loss: 2.557e+00\n",
      "Iteration: 4060, loss: 2.337e+00\n",
      "Iteration: 4070, loss: 1.659e+00\n",
      "Iteration: 4080, loss: 1.762e+00\n",
      "Iteration: 4090, loss: 2.641e+00\n",
      "Iteration: 4100, loss: 2.445e+00\n",
      "Iteration: 4110, loss: 2.325e+00\n",
      "Iteration: 4120, loss: 2.203e+00\n",
      "Iteration: 4130, loss: 2.846e+00\n",
      "Iteration: 4140, loss: 2.702e+00\n",
      "Iteration: 4150, loss: 1.366e+00\n",
      "Iteration: 4160, loss: 2.303e+00\n",
      "Iteration: 4170, loss: 2.215e+00\n",
      "Iteration: 4180, loss: 2.392e+00\n",
      "Iteration: 4190, loss: 2.012e+00\n",
      "Iteration: 4200, loss: 3.118e+00\n",
      "Iteration: 4210, loss: 2.708e+00\n",
      "Iteration: 4220, loss: 2.077e+00\n",
      "Iteration: 4230, loss: 1.344e+00\n",
      "Iteration: 4240, loss: 1.255e+00\n",
      "Iteration: 4250, loss: 2.157e+00\n",
      "Iteration: 4260, loss: 2.745e+00\n",
      "Iteration: 4270, loss: 1.507e+00\n",
      "Iteration: 4280, loss: 1.730e+00\n",
      "Iteration: 4290, loss: 1.324e+00\n",
      "Iteration: 4300, loss: 1.089e+00\n",
      "Iteration: 4310, loss: 1.729e+00\n",
      "Iteration: 4320, loss: 2.543e+00\n",
      "Iteration: 4330, loss: 2.190e+00\n",
      "Iteration: 4340, loss: 1.432e+00\n",
      "Iteration: 4350, loss: 1.980e+00\n",
      "Iteration: 4360, loss: 1.284e+00\n",
      "Iteration: 4370, loss: 1.896e+00\n",
      "Iteration: 4380, loss: 1.860e+00\n",
      "Iteration: 4390, loss: 1.867e+00\n",
      "Iteration: 4400, loss: 1.738e+00\n",
      "Iteration: 4410, loss: 1.547e+00\n",
      "Iteration: 4420, loss: 1.415e+00\n",
      "Iteration: 4430, loss: 1.369e+00\n",
      "Iteration: 4440, loss: 2.202e+00\n",
      "Iteration: 4450, loss: 2.503e+00\n",
      "Iteration: 4460, loss: 2.780e+00\n",
      "Iteration: 4470, loss: 1.404e+00\n",
      "Iteration: 4480, loss: 1.276e+00\n",
      "Iteration: 4490, loss: 1.483e+00\n",
      "Iteration: 4500, loss: 2.207e+00\n",
      "Iteration: 4510, loss: 1.448e+00\n",
      "Iteration: 4520, loss: 1.829e+00\n",
      "Iteration: 4530, loss: 2.595e+00\n",
      "Iteration: 4540, loss: 1.701e+00\n",
      "Iteration: 4550, loss: 1.054e+00\n",
      "Iteration: 4560, loss: 2.060e+00\n",
      "Iteration: 4570, loss: 1.498e+00\n",
      "Iteration: 4580, loss: 1.917e+00\n",
      "Iteration: 4590, loss: 2.000e+00\n",
      "Iteration: 4600, loss: 1.826e+00\n",
      "Iteration: 4610, loss: 2.011e+00\n",
      "Iteration: 4620, loss: 1.736e+00\n",
      "Iteration: 4630, loss: 1.621e+00\n",
      "Iteration: 4640, loss: 1.881e+00\n",
      "Iteration: 4650, loss: 1.342e+00\n",
      "Iteration: 4660, loss: 1.912e+00\n",
      "Iteration: 4670, loss: 1.767e+00\n",
      "Iteration: 4680, loss: 1.314e+00\n",
      "Iteration: 4690, loss: 1.447e+00\n",
      "Iteration: 4700, loss: 1.472e+00\n",
      "Iteration: 4710, loss: 1.516e+00\n",
      "Iteration: 4720, loss: 1.351e+00\n",
      "Iteration: 4730, loss: 1.462e+00\n",
      "Iteration: 4740, loss: 2.234e+00\n",
      "Iteration: 4750, loss: 8.075e-01\n",
      "Iteration: 4760, loss: 8.321e-01\n",
      "Iteration: 4770, loss: 1.312e+00\n",
      "Iteration: 4780, loss: 1.239e+00\n",
      "Iteration: 4790, loss: 1.940e+00\n",
      "Iteration: 4800, loss: 1.849e+00\n",
      "Iteration: 4810, loss: 1.405e+00\n",
      "Iteration: 4820, loss: 1.734e+00\n",
      "Iteration: 4830, loss: 1.256e+00\n",
      "Iteration: 4840, loss: 1.865e+00\n",
      "Iteration: 4850, loss: 2.650e+00\n",
      "Iteration: 4860, loss: 1.724e+00\n",
      "Iteration: 4870, loss: 2.121e+00\n",
      "Iteration: 4880, loss: 1.614e+00\n",
      "Iteration: 4890, loss: 2.165e+00\n",
      "Iteration: 4900, loss: 2.106e+00\n",
      "Iteration: 4910, loss: 3.042e+00\n",
      "Iteration: 4920, loss: 1.606e+00\n",
      "Iteration: 4930, loss: 9.883e-01\n",
      "Iteration: 4940, loss: 1.724e+00\n",
      "Iteration: 4950, loss: 1.691e+00\n",
      "Iteration: 4960, loss: 1.357e+00\n",
      "Iteration: 4970, loss: 1.104e+00\n",
      "Iteration: 4980, loss: 1.514e+00\n",
      "Iteration: 4990, loss: 1.160e+00\n",
      "Iteration: 5000, loss: 1.356e+00\n",
      "Iteration: 5010, loss: 1.631e+00\n",
      "Iteration: 5020, loss: 1.595e+00\n",
      "Iteration: 5030, loss: 1.883e+00\n",
      "Iteration: 5040, loss: 1.148e+00\n",
      "Iteration: 5050, loss: 1.069e+00\n",
      "Iteration: 5060, loss: 1.123e+00\n",
      "Iteration: 5070, loss: 1.037e+00\n",
      "Iteration: 5080, loss: 1.443e+00\n",
      "Iteration: 5090, loss: 1.011e+00\n",
      "Iteration: 5100, loss: 1.183e+00\n",
      "Iteration: 5110, loss: 1.830e+00\n",
      "Iteration: 5120, loss: 1.451e+00\n",
      "Iteration: 5130, loss: 1.793e+00\n",
      "Iteration: 5140, loss: 8.298e-01\n",
      "Iteration: 5150, loss: 1.991e+00\n",
      "Iteration: 5160, loss: 1.208e+00\n",
      "Iteration: 5170, loss: 1.542e+00\n",
      "Iteration: 5180, loss: 8.133e-01\n",
      "Iteration: 5190, loss: 1.148e+00\n",
      "Iteration: 5200, loss: 1.736e+00\n",
      "Iteration: 5210, loss: 1.118e+00\n",
      "Iteration: 5220, loss: 1.201e+00\n",
      "Iteration: 5230, loss: 7.495e-01\n",
      "Iteration: 5240, loss: 1.534e+00\n",
      "Iteration: 5250, loss: 8.854e-01\n",
      "Iteration: 5260, loss: 8.573e-01\n",
      "Iteration: 5270, loss: 2.048e+00\n",
      "Iteration: 5280, loss: 1.021e+00\n",
      "Iteration: 5290, loss: 1.780e+00\n",
      "Iteration: 5300, loss: 1.183e+00\n",
      "Iteration: 5310, loss: 1.554e+00\n",
      "Iteration: 5320, loss: 1.501e+00\n",
      "Iteration: 5330, loss: 1.079e+00\n",
      "Iteration: 5340, loss: 1.887e+00\n",
      "Iteration: 5350, loss: 1.094e+00\n",
      "Iteration: 5360, loss: 1.594e+00\n",
      "Iteration: 5370, loss: 1.264e+00\n",
      "Iteration: 5380, loss: 1.137e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5390, loss: 1.605e+00\n",
      "Iteration: 5400, loss: 7.767e-01\n",
      "Iteration: 5410, loss: 1.020e+00\n",
      "Iteration: 5420, loss: 1.474e+00\n",
      "Iteration: 5430, loss: 1.770e+00\n",
      "Iteration: 5440, loss: 1.004e+00\n",
      "Iteration: 5450, loss: 9.849e-01\n",
      "Iteration: 5460, loss: 8.842e-01\n",
      "Iteration: 5470, loss: 9.311e-01\n",
      "Iteration: 5480, loss: 1.126e+00\n",
      "Iteration: 5490, loss: 9.504e-01\n",
      "Iteration: 5500, loss: 1.209e+00\n",
      "Iteration: 5510, loss: 7.717e-01\n",
      "Iteration: 5520, loss: 1.044e+00\n",
      "Iteration: 5530, loss: 1.263e+00\n",
      "Iteration: 5540, loss: 1.170e+00\n",
      "Iteration: 5550, loss: 1.572e+00\n",
      "Iteration: 5560, loss: 1.447e+00\n",
      "Iteration: 5570, loss: 9.771e-01\n",
      "Iteration: 5580, loss: 9.828e-01\n",
      "Iteration: 5590, loss: 1.176e+00\n",
      "Iteration: 5600, loss: 1.115e+00\n",
      "Iteration: 5610, loss: 9.988e-01\n",
      "Iteration: 5620, loss: 7.661e-01\n",
      "Iteration: 5630, loss: 8.185e-01\n",
      "Iteration: 5640, loss: 1.006e+00\n",
      "Iteration: 5650, loss: 1.188e+00\n",
      "Iteration: 5660, loss: 9.844e-01\n",
      "Iteration: 5670, loss: 8.000e-01\n",
      "Iteration: 5680, loss: 1.169e+00\n",
      "Iteration: 5690, loss: 1.239e+00\n",
      "Iteration: 5700, loss: 6.175e-01\n",
      "Iteration: 5710, loss: 9.426e-01\n",
      "Iteration: 5720, loss: 1.481e+00\n",
      "Iteration: 5730, loss: 8.469e-01\n",
      "Iteration: 5740, loss: 8.394e-01\n",
      "Iteration: 5750, loss: 1.000e+00\n",
      "Iteration: 5760, loss: 1.294e+00\n",
      "Iteration: 5770, loss: 1.225e+00\n",
      "Iteration: 5780, loss: 9.618e-01\n",
      "Iteration: 5790, loss: 9.168e-01\n",
      "Iteration: 5800, loss: 1.029e+00\n",
      "Iteration: 5810, loss: 7.942e-01\n",
      "Iteration: 5820, loss: 1.641e+00\n",
      "Iteration: 5830, loss: 9.812e-01\n",
      "Iteration: 5840, loss: 1.018e+00\n",
      "Iteration: 5850, loss: 1.455e+00\n",
      "Iteration: 5860, loss: 1.046e+00\n",
      "Iteration: 5870, loss: 1.112e+00\n",
      "Iteration: 5880, loss: 7.871e-01\n",
      "Iteration: 5890, loss: 1.029e+00\n",
      "Iteration: 5900, loss: 1.098e+00\n",
      "Iteration: 5910, loss: 1.087e+00\n",
      "Iteration: 5920, loss: 8.616e-01\n",
      "Iteration: 5930, loss: 1.225e+00\n",
      "Iteration: 5940, loss: 6.431e-01\n",
      "Iteration: 5950, loss: 1.081e+00\n",
      "Iteration: 5960, loss: 6.941e-01\n",
      "Iteration: 5970, loss: 1.001e+00\n",
      "Iteration: 5980, loss: 9.797e-01\n",
      "Iteration: 5990, loss: 6.777e-01\n",
      "Iteration: 6000, loss: 8.614e-01\n",
      "Iteration: 6010, loss: 6.324e-01\n",
      "Iteration: 6020, loss: 1.127e+00\n",
      "Iteration: 6030, loss: 1.142e+00\n",
      "Iteration: 6040, loss: 4.795e-01\n",
      "Iteration: 6050, loss: 1.423e+00\n",
      "Iteration: 6060, loss: 8.664e-01\n",
      "Iteration: 6070, loss: 8.739e-01\n",
      "Iteration: 6080, loss: 9.227e-01\n",
      "Iteration: 6090, loss: 8.913e-01\n",
      "Iteration: 6100, loss: 1.072e+00\n",
      "Iteration: 6110, loss: 5.738e-01\n",
      "Iteration: 6120, loss: 9.468e-01\n",
      "Iteration: 6130, loss: 1.003e+00\n",
      "Iteration: 6140, loss: 6.500e-01\n",
      "Iteration: 6150, loss: 9.297e-01\n",
      "Iteration: 6160, loss: 7.407e-01\n",
      "Iteration: 6170, loss: 8.620e-01\n",
      "Iteration: 6180, loss: 1.041e+00\n",
      "Iteration: 6190, loss: 9.883e-01\n",
      "Iteration: 6200, loss: 1.126e+00\n",
      "Iteration: 6210, loss: 1.549e+00\n",
      "Iteration: 6220, loss: 1.227e+00\n",
      "Iteration: 6230, loss: 6.403e-01\n",
      "Iteration: 6240, loss: 6.813e-01\n",
      "Iteration: 6250, loss: 6.255e-01\n",
      "Iteration: 6260, loss: 8.481e-01\n",
      "Iteration: 6270, loss: 5.181e-01\n",
      "Iteration: 6280, loss: 6.615e-01\n",
      "Iteration: 6290, loss: 5.179e-01\n",
      "Iteration: 6300, loss: 6.918e-01\n",
      "Iteration: 6310, loss: 8.374e-01\n",
      "Iteration: 6320, loss: 1.011e+00\n",
      "Iteration: 6330, loss: 4.774e-01\n",
      "Iteration: 6340, loss: 8.096e-01\n",
      "Iteration: 6350, loss: 9.289e-01\n",
      "Iteration: 6360, loss: 9.453e-01\n",
      "Iteration: 6370, loss: 8.167e-01\n",
      "Iteration: 6380, loss: 5.040e-01\n",
      "Iteration: 6390, loss: 7.169e-01\n",
      "Iteration: 6400, loss: 6.424e-01\n",
      "Iteration: 6410, loss: 6.544e-01\n",
      "Iteration: 6420, loss: 1.154e+00\n",
      "Iteration: 6430, loss: 9.544e-01\n",
      "Iteration: 6440, loss: 9.168e-01\n",
      "Iteration: 6450, loss: 8.225e-01\n",
      "Iteration: 6460, loss: 4.163e-01\n",
      "Iteration: 6470, loss: 6.523e-01\n",
      "Iteration: 6480, loss: 1.027e+00\n",
      "Iteration: 6490, loss: 6.584e-01\n",
      "Iteration: 6500, loss: 8.040e-01\n",
      "Iteration: 6510, loss: 6.946e-01\n",
      "Iteration: 6520, loss: 7.007e-01\n",
      "Iteration: 6530, loss: 6.581e-01\n",
      "Iteration: 6540, loss: 8.075e-01\n",
      "Iteration: 6550, loss: 7.056e-01\n",
      "Iteration: 6560, loss: 6.824e-01\n",
      "Iteration: 6570, loss: 6.127e-01\n",
      "Iteration: 6580, loss: 6.381e-01\n",
      "Iteration: 6590, loss: 7.710e-01\n",
      "Iteration: 6600, loss: 8.192e-01\n",
      "Iteration: 6610, loss: 6.545e-01\n",
      "Iteration: 6620, loss: 7.301e-01\n",
      "Iteration: 6630, loss: 5.699e-01\n",
      "Iteration: 6640, loss: 1.011e+00\n",
      "Iteration: 6650, loss: 4.979e-01\n",
      "Iteration: 6660, loss: 7.270e-01\n",
      "Iteration: 6670, loss: 9.362e-01\n",
      "Iteration: 6680, loss: 8.057e-01\n",
      "Iteration: 6690, loss: 5.219e-01\n",
      "Iteration: 6700, loss: 9.448e-01\n",
      "Iteration: 6710, loss: 5.475e-01\n",
      "Iteration: 6720, loss: 7.362e-01\n",
      "Iteration: 6730, loss: 3.709e-01\n",
      "Iteration: 6740, loss: 5.036e-01\n",
      "Iteration: 6750, loss: 1.059e+00\n",
      "Iteration: 6760, loss: 5.694e-01\n",
      "Iteration: 6770, loss: 3.656e-01\n",
      "Iteration: 6780, loss: 5.212e-01\n",
      "Iteration: 6790, loss: 9.261e-01\n",
      "Iteration: 6800, loss: 7.407e-01\n",
      "Iteration: 6810, loss: 6.120e-01\n",
      "Iteration: 6820, loss: 7.040e-01\n",
      "Iteration: 6830, loss: 7.238e-01\n",
      "Iteration: 6840, loss: 7.310e-01\n",
      "Iteration: 6850, loss: 4.103e-01\n",
      "Iteration: 6860, loss: 2.958e-01\n",
      "Iteration: 6870, loss: 4.251e-01\n",
      "Iteration: 6880, loss: 6.096e-01\n",
      "Iteration: 6890, loss: 5.911e-01\n",
      "Iteration: 6900, loss: 6.086e-01\n",
      "Iteration: 6910, loss: 5.869e-01\n",
      "Iteration: 6920, loss: 5.860e-01\n",
      "Iteration: 6930, loss: 5.510e-01\n",
      "Iteration: 6940, loss: 7.976e-01\n",
      "Iteration: 6950, loss: 7.078e-01\n",
      "Iteration: 6960, loss: 4.876e-01\n",
      "Iteration: 6970, loss: 9.032e-01\n",
      "Iteration: 6980, loss: 4.692e-01\n",
      "Iteration: 6990, loss: 6.222e-01\n",
      "Iteration: 7000, loss: 6.766e-01\n",
      "Iteration: 7010, loss: 1.039e+00\n",
      "Iteration: 7020, loss: 5.418e-01\n",
      "Iteration: 7030, loss: 7.136e-01\n",
      "Iteration: 7040, loss: 5.225e-01\n",
      "Iteration: 7050, loss: 4.914e-01\n",
      "Iteration: 7060, loss: 4.186e-01\n",
      "Iteration: 7070, loss: 3.880e-01\n",
      "Iteration: 7080, loss: 5.208e-01\n",
      "Iteration: 7090, loss: 6.457e-01\n",
      "Iteration: 7100, loss: 6.938e-01\n",
      "Iteration: 7110, loss: 5.654e-01\n",
      "Iteration: 7120, loss: 6.429e-01\n",
      "Iteration: 7130, loss: 3.712e-01\n",
      "Iteration: 7140, loss: 6.500e-01\n",
      "Iteration: 7150, loss: 6.416e-01\n",
      "Iteration: 7160, loss: 3.690e-01\n",
      "Iteration: 7170, loss: 4.141e-01\n",
      "Iteration: 7180, loss: 7.509e-01\n",
      "Iteration: 7190, loss: 4.459e-01\n",
      "Iteration: 7200, loss: 5.668e-01\n",
      "Iteration: 7210, loss: 5.197e-01\n",
      "Iteration: 7220, loss: 5.692e-01\n",
      "Iteration: 7230, loss: 7.004e-01\n",
      "Iteration: 7240, loss: 4.435e-01\n",
      "Iteration: 7250, loss: 3.647e-01\n",
      "Iteration: 7260, loss: 4.853e-01\n",
      "Iteration: 7270, loss: 6.238e-01\n",
      "Iteration: 7280, loss: 4.879e-01\n",
      "Iteration: 7290, loss: 6.314e-01\n",
      "Iteration: 7300, loss: 6.486e-01\n",
      "Iteration: 7310, loss: 5.363e-01\n",
      "Iteration: 7320, loss: 5.092e-01\n",
      "Iteration: 7330, loss: 6.832e-01\n",
      "Iteration: 7340, loss: 6.476e-01\n",
      "Iteration: 7350, loss: 6.941e-01\n",
      "Iteration: 7360, loss: 5.706e-01\n",
      "Iteration: 7370, loss: 5.030e-01\n",
      "Iteration: 7380, loss: 5.846e-01\n",
      "Iteration: 7390, loss: 5.650e-01\n",
      "Iteration: 7400, loss: 6.605e-01\n",
      "Iteration: 7410, loss: 4.247e-01\n",
      "Iteration: 7420, loss: 3.932e-01\n",
      "Iteration: 7430, loss: 5.302e-01\n",
      "Iteration: 7440, loss: 7.330e-01\n",
      "Iteration: 7450, loss: 5.628e-01\n",
      "Iteration: 7460, loss: 4.571e-01\n",
      "Iteration: 7470, loss: 4.590e-01\n",
      "Iteration: 7480, loss: 9.558e-01\n",
      "Iteration: 7490, loss: 5.000e-01\n",
      "Iteration: 7500, loss: 7.926e-01\n",
      "Iteration: 7510, loss: 6.886e-01\n",
      "Iteration: 7520, loss: 2.015e-01\n",
      "Iteration: 7530, loss: 5.404e-01\n",
      "Iteration: 7540, loss: 4.353e-01\n",
      "Iteration: 7550, loss: 3.863e-01\n",
      "Iteration: 7560, loss: 6.166e-01\n",
      "Iteration: 7570, loss: 4.879e-01\n",
      "Iteration: 7580, loss: 4.565e-01\n",
      "Iteration: 7590, loss: 4.904e-01\n",
      "Iteration: 7600, loss: 4.561e-01\n",
      "Iteration: 7610, loss: 6.314e-01\n",
      "Iteration: 7620, loss: 5.561e-01\n",
      "Iteration: 7630, loss: 5.604e-01\n",
      "Iteration: 7640, loss: 3.370e-01\n",
      "Iteration: 7650, loss: 3.741e-01\n",
      "Iteration: 7660, loss: 4.795e-01\n",
      "Iteration: 7670, loss: 3.032e-01\n",
      "Iteration: 7680, loss: 4.163e-01\n",
      "Iteration: 7690, loss: 4.225e-01\n",
      "Iteration: 7700, loss: 3.644e-01\n",
      "Iteration: 7710, loss: 5.168e-01\n",
      "Iteration: 7720, loss: 3.059e-01\n",
      "Iteration: 7730, loss: 2.636e-01\n",
      "Iteration: 7740, loss: 5.019e-01\n",
      "Iteration: 7750, loss: 4.024e-01\n",
      "Iteration: 7760, loss: 4.618e-01\n",
      "Iteration: 7770, loss: 3.788e-01\n",
      "Iteration: 7780, loss: 3.907e-01\n",
      "Iteration: 7790, loss: 3.689e-01\n",
      "Iteration: 7800, loss: 4.510e-01\n",
      "Iteration: 7810, loss: 5.554e-01\n",
      "Iteration: 7820, loss: 8.585e-01\n",
      "Iteration: 7830, loss: 5.093e-01\n",
      "Iteration: 7840, loss: 2.985e-01\n",
      "Iteration: 7850, loss: 5.338e-01\n",
      "Iteration: 7860, loss: 3.838e-01\n",
      "Iteration: 7870, loss: 6.231e-01\n",
      "Iteration: 7880, loss: 2.638e-01\n",
      "Iteration: 7890, loss: 5.699e-01\n",
      "Iteration: 7900, loss: 3.717e-01\n",
      "Iteration: 7910, loss: 4.238e-01\n",
      "Iteration: 7920, loss: 4.470e-01\n",
      "Iteration: 7930, loss: 6.115e-01\n",
      "Iteration: 7940, loss: 5.188e-01\n",
      "Iteration: 7950, loss: 5.481e-01\n",
      "Iteration: 7960, loss: 3.553e-01\n",
      "Iteration: 7970, loss: 6.034e-01\n",
      "Iteration: 7980, loss: 5.751e-01\n",
      "Iteration: 7990, loss: 2.596e-01\n",
      "Iteration: 8000, loss: 3.695e-01\n",
      "Iteration: 8010, loss: 3.634e-01\n",
      "Iteration: 8020, loss: 2.815e-01\n",
      "Iteration: 8030, loss: 5.056e-01\n",
      "Iteration: 8040, loss: 4.757e-01\n",
      "Iteration: 8050, loss: 4.375e-01\n",
      "Iteration: 8060, loss: 5.085e-01\n",
      "Iteration: 8070, loss: 3.907e-01\n",
      "Iteration: 8080, loss: 4.795e-01\n",
      "Iteration: 8090, loss: 6.358e-01\n",
      "Iteration: 8100, loss: 4.603e-01\n",
      "Iteration: 8110, loss: 3.217e-01\n",
      "Iteration: 8120, loss: 4.354e-01\n",
      "Iteration: 8130, loss: 2.774e-01\n",
      "Iteration: 8140, loss: 3.432e-01\n",
      "Iteration: 8150, loss: 4.547e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8160, loss: 3.866e-01\n",
      "Iteration: 8170, loss: 3.103e-01\n",
      "Iteration: 8180, loss: 3.149e-01\n",
      "Iteration: 8190, loss: 3.688e-01\n",
      "Iteration: 8200, loss: 3.523e-01\n",
      "Iteration: 8210, loss: 5.505e-01\n",
      "Iteration: 8220, loss: 4.443e-01\n",
      "Iteration: 8230, loss: 3.559e-01\n",
      "Iteration: 8240, loss: 3.285e-01\n",
      "Iteration: 8250, loss: 3.064e-01\n",
      "Iteration: 8260, loss: 2.892e-01\n",
      "Iteration: 8270, loss: 4.296e-01\n",
      "Iteration: 8280, loss: 2.446e-01\n",
      "Iteration: 8290, loss: 3.551e-01\n",
      "Iteration: 8300, loss: 2.681e-01\n",
      "Iteration: 8310, loss: 4.469e-01\n",
      "Iteration: 8320, loss: 2.413e-01\n",
      "Iteration: 8330, loss: 2.877e-01\n",
      "Iteration: 8340, loss: 2.034e-01\n",
      "Iteration: 8350, loss: 3.555e-01\n",
      "Iteration: 8360, loss: 3.574e-01\n",
      "Iteration: 8370, loss: 4.264e-01\n",
      "Iteration: 8380, loss: 3.820e-01\n",
      "Iteration: 8390, loss: 3.646e-01\n",
      "Iteration: 8400, loss: 4.016e-01\n",
      "Iteration: 8410, loss: 4.859e-01\n",
      "Iteration: 8420, loss: 2.783e-01\n",
      "Iteration: 8430, loss: 2.365e-01\n",
      "Iteration: 8440, loss: 4.310e-01\n",
      "Iteration: 8450, loss: 2.965e-01\n",
      "Iteration: 8460, loss: 4.149e-01\n",
      "Iteration: 8470, loss: 4.808e-01\n",
      "Iteration: 8480, loss: 2.633e-01\n",
      "Iteration: 8490, loss: 3.167e-01\n",
      "Iteration: 8500, loss: 5.945e-01\n",
      "Iteration: 8510, loss: 4.350e-01\n",
      "Iteration: 8520, loss: 3.471e-01\n",
      "Iteration: 8530, loss: 2.947e-01\n",
      "Iteration: 8540, loss: 2.956e-01\n",
      "Iteration: 8550, loss: 2.558e-01\n",
      "Iteration: 8560, loss: 2.143e-01\n",
      "Iteration: 8570, loss: 5.016e-01\n",
      "Iteration: 8580, loss: 2.354e-01\n",
      "Iteration: 8590, loss: 3.282e-01\n",
      "Iteration: 8600, loss: 2.226e-01\n",
      "Iteration: 8610, loss: 2.430e-01\n",
      "Iteration: 8620, loss: 3.046e-01\n",
      "Iteration: 8630, loss: 3.194e-01\n",
      "Iteration: 8640, loss: 3.380e-01\n",
      "Iteration: 8650, loss: 2.262e-01\n",
      "Iteration: 8660, loss: 4.123e-01\n",
      "Iteration: 8670, loss: 3.481e-01\n",
      "Iteration: 8680, loss: 5.141e-01\n",
      "Iteration: 8690, loss: 2.428e-01\n",
      "Iteration: 8700, loss: 1.559e-01\n",
      "Iteration: 8710, loss: 3.139e-01\n",
      "Iteration: 8720, loss: 1.756e-01\n",
      "Iteration: 8730, loss: 4.153e-01\n",
      "Iteration: 8740, loss: 3.159e-01\n",
      "Iteration: 8750, loss: 2.831e-01\n",
      "Iteration: 8760, loss: 2.104e-01\n",
      "Iteration: 8770, loss: 2.736e-01\n",
      "Iteration: 8780, loss: 4.287e-01\n",
      "Iteration: 8790, loss: 2.851e-01\n",
      "Iteration: 8800, loss: 4.641e-01\n",
      "Iteration: 8810, loss: 3.199e-01\n",
      "Iteration: 8820, loss: 2.752e-01\n",
      "Iteration: 8830, loss: 3.038e-01\n",
      "Iteration: 8840, loss: 2.759e-01\n",
      "Iteration: 8850, loss: 2.370e-01\n",
      "Iteration: 8860, loss: 4.188e-01\n",
      "Iteration: 8870, loss: 3.224e-01\n",
      "Iteration: 8880, loss: 2.234e-01\n",
      "Iteration: 8890, loss: 1.963e-01\n",
      "Iteration: 8900, loss: 1.618e-01\n",
      "Iteration: 8910, loss: 2.349e-01\n",
      "Iteration: 8920, loss: 2.575e-01\n",
      "Iteration: 8930, loss: 3.246e-01\n",
      "Iteration: 8940, loss: 2.570e-01\n",
      "Iteration: 8950, loss: 2.423e-01\n",
      "Iteration: 8960, loss: 3.642e-01\n",
      "Iteration: 8970, loss: 2.686e-01\n",
      "Iteration: 8980, loss: 2.216e-01\n",
      "Iteration: 8990, loss: 2.036e-01\n",
      "Iteration: 9000, loss: 3.794e-01\n",
      "Iteration: 9010, loss: 2.524e-01\n",
      "Iteration: 9020, loss: 3.142e-01\n",
      "Iteration: 9030, loss: 5.324e-01\n",
      "Iteration: 9040, loss: 2.219e-01\n",
      "Iteration: 9050, loss: 2.358e-01\n",
      "Iteration: 9060, loss: 2.268e-01\n",
      "Iteration: 9070, loss: 2.498e-01\n",
      "Iteration: 9080, loss: 3.536e-01\n",
      "Iteration: 9090, loss: 2.491e-01\n",
      "Iteration: 9100, loss: 2.294e-01\n",
      "Iteration: 9110, loss: 2.261e-01\n",
      "Iteration: 9120, loss: 1.693e-01\n",
      "Iteration: 9130, loss: 1.801e-01\n",
      "Iteration: 9140, loss: 2.207e-01\n",
      "Iteration: 9150, loss: 1.570e-01\n",
      "Iteration: 9160, loss: 2.347e-01\n",
      "Iteration: 9170, loss: 1.909e-01\n",
      "Iteration: 9180, loss: 1.564e-01\n",
      "Iteration: 9190, loss: 2.695e-01\n",
      "Iteration: 9200, loss: 4.766e-01\n",
      "Iteration: 9210, loss: 2.623e-01\n",
      "Iteration: 9220, loss: 1.706e-01\n",
      "Iteration: 9230, loss: 1.701e-01\n",
      "Iteration: 9240, loss: 2.116e-01\n",
      "Iteration: 9250, loss: 1.598e-01\n",
      "Iteration: 9260, loss: 2.365e-01\n",
      "Iteration: 9270, loss: 2.484e-01\n",
      "Iteration: 9280, loss: 2.431e-01\n",
      "Iteration: 9290, loss: 3.578e-01\n",
      "Iteration: 9300, loss: 3.421e-01\n",
      "Iteration: 9310, loss: 2.136e-01\n",
      "Iteration: 9320, loss: 1.897e-01\n",
      "Iteration: 9330, loss: 2.379e-01\n",
      "Iteration: 9340, loss: 3.826e-01\n",
      "Iteration: 9350, loss: 2.220e-01\n",
      "Iteration: 9360, loss: 1.982e-01\n",
      "Iteration: 9370, loss: 2.137e-01\n",
      "Iteration: 9380, loss: 1.700e-01\n",
      "Iteration: 9390, loss: 2.211e-01\n",
      "Iteration: 9400, loss: 1.720e-01\n",
      "Iteration: 9410, loss: 3.703e-01\n",
      "Iteration: 9420, loss: 2.615e-01\n",
      "Iteration: 9430, loss: 1.515e-01\n",
      "Iteration: 9440, loss: 2.348e-01\n",
      "Iteration: 9450, loss: 1.961e-01\n",
      "Iteration: 9460, loss: 2.038e-01\n",
      "Iteration: 9470, loss: 2.365e-01\n",
      "Iteration: 9480, loss: 1.244e-01\n",
      "Iteration: 9490, loss: 1.942e-01\n",
      "Iteration: 9500, loss: 2.444e-01\n",
      "Iteration: 9510, loss: 3.641e-01\n",
      "Iteration: 9520, loss: 1.560e-01\n",
      "Iteration: 9530, loss: 3.377e-01\n",
      "Iteration: 9540, loss: 1.749e-01\n",
      "Iteration: 9550, loss: 2.211e-01\n",
      "Iteration: 9560, loss: 2.784e-01\n",
      "Iteration: 9570, loss: 1.684e-01\n",
      "Iteration: 9580, loss: 2.443e-01\n",
      "Iteration: 9590, loss: 2.215e-01\n",
      "Iteration: 9600, loss: 2.551e-01\n",
      "Iteration: 9610, loss: 2.144e-01\n",
      "Iteration: 9620, loss: 2.344e-01\n",
      "Iteration: 9630, loss: 3.565e-01\n",
      "Iteration: 9640, loss: 2.100e-01\n",
      "Iteration: 9650, loss: 2.406e-01\n",
      "Iteration: 9660, loss: 1.937e-01\n",
      "Iteration: 9670, loss: 2.813e-01\n",
      "Iteration: 9680, loss: 2.713e-01\n",
      "Iteration: 9690, loss: 1.772e-01\n",
      "Iteration: 9700, loss: 1.801e-01\n",
      "Iteration: 9710, loss: 2.198e-01\n",
      "Iteration: 9720, loss: 9.633e-02\n",
      "Iteration: 9730, loss: 2.279e-01\n",
      "Iteration: 9740, loss: 1.208e-01\n",
      "Iteration: 9750, loss: 2.105e-01\n",
      "Iteration: 9760, loss: 1.181e-01\n",
      "Iteration: 9770, loss: 1.535e-01\n",
      "Iteration: 9780, loss: 1.916e-01\n",
      "Iteration: 9790, loss: 2.088e-01\n",
      "Iteration: 9800, loss: 2.476e-01\n",
      "Iteration: 9810, loss: 1.697e-01\n",
      "Iteration: 9820, loss: 2.546e-01\n",
      "Iteration: 9830, loss: 1.804e-01\n",
      "Iteration: 9840, loss: 1.802e-01\n",
      "Iteration: 9850, loss: 1.404e-01\n",
      "Iteration: 9860, loss: 1.212e-01\n",
      "Iteration: 9870, loss: 2.106e-01\n",
      "Iteration: 9880, loss: 1.667e-01\n",
      "Iteration: 9890, loss: 1.955e-01\n",
      "Iteration: 9900, loss: 1.784e-01\n",
      "Iteration: 9910, loss: 1.977e-01\n",
      "Iteration: 9920, loss: 1.596e-01\n",
      "Iteration: 9930, loss: 2.816e-01\n",
      "Iteration: 9940, loss: 1.603e-01\n",
      "Iteration: 9950, loss: 8.189e-02\n",
      "Iteration: 9960, loss: 2.007e-01\n",
      "Iteration: 9970, loss: 1.563e-01\n",
      "Iteration: 9980, loss: 1.940e-01\n",
      "Iteration: 9990, loss: 1.792e-01\n",
      "Iteration: 10000, loss: 9.250e-02\n",
      "Iteration: 10010, loss: 1.802e-01\n",
      "Iteration: 10020, loss: 1.317e-01\n",
      "Iteration: 10030, loss: 2.655e-01\n",
      "Iteration: 10040, loss: 3.271e-01\n",
      "Iteration: 10050, loss: 2.795e-01\n",
      "Iteration: 10060, loss: 1.460e-01\n",
      "Iteration: 10070, loss: 1.959e-01\n",
      "Iteration: 10080, loss: 1.545e-01\n",
      "Iteration: 10090, loss: 2.576e-01\n",
      "Iteration: 10100, loss: 2.036e-01\n",
      "Iteration: 10110, loss: 1.908e-01\n",
      "Iteration: 10120, loss: 2.104e-01\n",
      "Iteration: 10130, loss: 2.553e-01\n",
      "Iteration: 10140, loss: 1.248e-01\n",
      "Iteration: 10150, loss: 1.622e-01\n",
      "Iteration: 10160, loss: 2.864e-01\n",
      "Iteration: 10170, loss: 1.669e-01\n",
      "Iteration: 10180, loss: 2.467e-01\n",
      "Iteration: 10190, loss: 1.186e-01\n",
      "Iteration: 10200, loss: 2.572e-01\n",
      "Iteration: 10210, loss: 1.982e-01\n",
      "Iteration: 10220, loss: 1.449e-01\n",
      "Iteration: 10230, loss: 2.442e-01\n",
      "Iteration: 10240, loss: 6.753e-02\n",
      "Iteration: 10250, loss: 1.223e-01\n",
      "Iteration: 10260, loss: 2.512e-01\n",
      "Iteration: 10270, loss: 1.965e-01\n",
      "Iteration: 10280, loss: 1.892e-01\n",
      "Iteration: 10290, loss: 1.821e-01\n",
      "Iteration: 10300, loss: 1.727e-01\n",
      "Iteration: 10310, loss: 1.809e-01\n",
      "Iteration: 10320, loss: 1.809e-01\n",
      "Iteration: 10330, loss: 1.615e-01\n",
      "Iteration: 10340, loss: 1.726e-01\n",
      "Iteration: 10350, loss: 1.422e-01\n",
      "Iteration: 10360, loss: 1.593e-01\n",
      "Iteration: 10370, loss: 2.218e-01\n",
      "Iteration: 10380, loss: 1.731e-01\n",
      "Iteration: 10390, loss: 1.042e-01\n",
      "Iteration: 10400, loss: 1.544e-01\n",
      "Iteration: 10410, loss: 2.102e-01\n",
      "Iteration: 10420, loss: 1.936e-01\n",
      "Iteration: 10430, loss: 2.438e-01\n",
      "Iteration: 10440, loss: 1.579e-01\n",
      "Iteration: 10450, loss: 2.690e-01\n",
      "Iteration: 10460, loss: 1.268e-01\n",
      "Iteration: 10470, loss: 1.705e-01\n",
      "Iteration: 10480, loss: 1.407e-01\n",
      "Iteration: 10490, loss: 1.196e-01\n",
      "Iteration: 10500, loss: 8.073e-02\n",
      "Iteration: 10510, loss: 1.187e-01\n",
      "Iteration: 10520, loss: 1.404e-01\n",
      "Iteration: 10530, loss: 1.962e-01\n",
      "Iteration: 10540, loss: 1.943e-01\n",
      "Iteration: 10550, loss: 1.754e-01\n",
      "Iteration: 10560, loss: 1.576e-01\n",
      "Iteration: 10570, loss: 2.863e-01\n",
      "Iteration: 10580, loss: 1.294e-01\n",
      "Iteration: 10590, loss: 8.463e-02\n",
      "Iteration: 10600, loss: 1.836e-01\n",
      "Iteration: 10610, loss: 2.357e-01\n",
      "Iteration: 10620, loss: 1.303e-01\n",
      "Iteration: 10630, loss: 1.231e-01\n",
      "Iteration: 10640, loss: 1.340e-01\n",
      "Iteration: 10650, loss: 1.511e-01\n",
      "Iteration: 10660, loss: 1.856e-01\n",
      "Iteration: 10670, loss: 1.187e-01\n",
      "Iteration: 10680, loss: 1.141e-01\n",
      "Iteration: 10690, loss: 1.438e-01\n",
      "Iteration: 10700, loss: 1.663e-01\n",
      "Iteration: 10710, loss: 9.886e-02\n",
      "Iteration: 10720, loss: 1.730e-01\n",
      "Iteration: 10730, loss: 1.104e-01\n",
      "Iteration: 10740, loss: 2.136e-01\n",
      "Iteration: 10750, loss: 1.940e-01\n",
      "Iteration: 10760, loss: 2.083e-01\n",
      "Iteration: 10770, loss: 1.552e-01\n",
      "Iteration: 10780, loss: 1.589e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10790, loss: 2.244e-01\n",
      "Iteration: 10800, loss: 1.934e-01\n",
      "Iteration: 10810, loss: 1.188e-01\n",
      "Iteration: 10820, loss: 1.020e-01\n",
      "Iteration: 10830, loss: 1.624e-01\n",
      "Iteration: 10840, loss: 1.733e-01\n",
      "Iteration: 10850, loss: 1.622e-01\n",
      "Iteration: 10860, loss: 1.666e-01\n",
      "Iteration: 10870, loss: 1.623e-01\n",
      "Iteration: 10880, loss: 1.116e-01\n",
      "Iteration: 10890, loss: 9.079e-02\n",
      "Iteration: 10900, loss: 1.152e-01\n",
      "Iteration: 10910, loss: 1.424e-01\n",
      "Iteration: 10920, loss: 1.053e-01\n",
      "Iteration: 10930, loss: 2.196e-01\n",
      "Iteration: 10940, loss: 1.385e-01\n",
      "Iteration: 10950, loss: 1.550e-01\n",
      "Iteration: 10960, loss: 1.435e-01\n",
      "Iteration: 10970, loss: 1.285e-01\n",
      "Iteration: 10980, loss: 2.072e-01\n",
      "Iteration: 10990, loss: 2.106e-01\n",
      "Iteration: 11000, loss: 1.596e-01\n",
      "Iteration: 11010, loss: 1.054e-01\n",
      "Iteration: 11020, loss: 1.274e-01\n",
      "Iteration: 11030, loss: 9.914e-02\n",
      "Iteration: 11040, loss: 1.637e-01\n",
      "Iteration: 11050, loss: 1.401e-01\n",
      "Iteration: 11060, loss: 1.182e-01\n",
      "Iteration: 11070, loss: 1.606e-01\n",
      "Iteration: 11080, loss: 1.322e-01\n",
      "Iteration: 11090, loss: 9.791e-02\n",
      "Iteration: 11100, loss: 8.818e-02\n",
      "Iteration: 11110, loss: 1.327e-01\n",
      "Iteration: 11120, loss: 1.196e-01\n",
      "Iteration: 11130, loss: 1.286e-01\n",
      "Iteration: 11140, loss: 1.333e-01\n",
      "Iteration: 11150, loss: 1.189e-01\n",
      "Iteration: 11160, loss: 2.102e-01\n",
      "Iteration: 11170, loss: 8.748e-02\n",
      "Iteration: 11180, loss: 1.616e-01\n",
      "Iteration: 11190, loss: 1.240e-01\n",
      "Iteration: 11200, loss: 1.323e-01\n",
      "Iteration: 11210, loss: 1.510e-01\n",
      "Iteration: 11220, loss: 1.062e-01\n",
      "Iteration: 11230, loss: 9.748e-02\n",
      "Iteration: 11240, loss: 1.258e-01\n",
      "Iteration: 11250, loss: 1.247e-01\n",
      "Iteration: 11260, loss: 1.159e-01\n",
      "Iteration: 11270, loss: 1.591e-01\n",
      "Iteration: 11280, loss: 6.275e-02\n",
      "Iteration: 11290, loss: 6.494e-02\n",
      "Iteration: 11300, loss: 9.251e-02\n",
      "Iteration: 11310, loss: 1.432e-01\n",
      "Iteration: 11320, loss: 1.016e-01\n",
      "Iteration: 11330, loss: 7.865e-02\n",
      "Iteration: 11340, loss: 1.254e-01\n",
      "Iteration: 11350, loss: 9.994e-02\n",
      "Iteration: 11360, loss: 1.150e-01\n",
      "Iteration: 11370, loss: 1.585e-01\n",
      "Iteration: 11380, loss: 1.210e-01\n",
      "Iteration: 11390, loss: 1.014e-01\n",
      "Iteration: 11400, loss: 1.577e-01\n",
      "Iteration: 11410, loss: 1.097e-01\n",
      "Iteration: 11420, loss: 1.021e-01\n",
      "Iteration: 11430, loss: 1.481e-01\n",
      "Iteration: 11440, loss: 7.354e-02\n",
      "Iteration: 11450, loss: 1.227e-01\n",
      "Iteration: 11460, loss: 1.488e-01\n",
      "Iteration: 11470, loss: 8.311e-02\n",
      "Iteration: 11480, loss: 9.773e-02\n",
      "Iteration: 11490, loss: 8.950e-02\n",
      "Iteration: 11500, loss: 1.133e-01\n",
      "Iteration: 11510, loss: 5.640e-02\n",
      "Iteration: 11520, loss: 8.871e-02\n",
      "Iteration: 11530, loss: 1.191e-01\n",
      "Iteration: 11540, loss: 6.037e-02\n",
      "Iteration: 11550, loss: 1.012e-01\n",
      "Iteration: 11560, loss: 9.322e-02\n",
      "Iteration: 11570, loss: 1.126e-01\n",
      "Iteration: 11580, loss: 8.003e-02\n",
      "Iteration: 11590, loss: 1.289e-01\n",
      "Iteration: 11600, loss: 1.591e-01\n",
      "Iteration: 11610, loss: 7.431e-02\n",
      "Iteration: 11620, loss: 1.311e-01\n",
      "Iteration: 11630, loss: 1.152e-01\n",
      "Iteration: 11640, loss: 1.114e-01\n",
      "Iteration: 11650, loss: 5.233e-02\n",
      "Iteration: 11660, loss: 1.297e-01\n",
      "Iteration: 11670, loss: 1.085e-01\n",
      "Iteration: 11680, loss: 1.523e-01\n",
      "Iteration: 11690, loss: 7.973e-02\n",
      "Iteration: 11700, loss: 1.347e-01\n",
      "Iteration: 11710, loss: 1.135e-01\n",
      "Iteration: 11720, loss: 8.878e-02\n",
      "Iteration: 11730, loss: 7.329e-02\n",
      "Iteration: 11740, loss: 5.975e-02\n",
      "Iteration: 11750, loss: 8.254e-02\n",
      "Iteration: 11760, loss: 1.039e-01\n",
      "Iteration: 11770, loss: 1.560e-01\n",
      "Iteration: 11780, loss: 9.520e-02\n",
      "Iteration: 11790, loss: 8.537e-02\n",
      "Iteration: 11800, loss: 1.089e-01\n",
      "Iteration: 11810, loss: 5.441e-02\n",
      "Iteration: 11820, loss: 1.317e-01\n",
      "Iteration: 11830, loss: 8.006e-02\n",
      "Iteration: 11840, loss: 5.802e-02\n",
      "Iteration: 11850, loss: 4.884e-02\n",
      "Iteration: 11860, loss: 7.210e-02\n",
      "Iteration: 11870, loss: 1.217e-01\n",
      "Iteration: 11880, loss: 8.672e-02\n",
      "Iteration: 11890, loss: 8.314e-02\n",
      "Iteration: 11900, loss: 1.574e-01\n",
      "Iteration: 11910, loss: 8.432e-02\n",
      "Iteration: 11920, loss: 1.575e-01\n",
      "Iteration: 11930, loss: 1.212e-01\n",
      "Iteration: 11940, loss: 8.205e-02\n",
      "Iteration: 11950, loss: 1.100e-01\n",
      "Iteration: 11960, loss: 7.729e-02\n",
      "Iteration: 11970, loss: 1.169e-01\n",
      "Iteration: 11980, loss: 1.140e-01\n",
      "Iteration: 11990, loss: 1.041e-01\n",
      "Iteration: 12000, loss: 1.090e-01\n",
      "Iteration: 12010, loss: 1.347e-01\n",
      "Iteration: 12020, loss: 7.599e-02\n",
      "Iteration: 12030, loss: 1.108e-01\n",
      "Iteration: 12040, loss: 7.172e-02\n",
      "Iteration: 12050, loss: 9.592e-02\n",
      "Iteration: 12060, loss: 8.845e-02\n",
      "Iteration: 12070, loss: 1.055e-01\n",
      "Iteration: 12080, loss: 1.182e-01\n",
      "Iteration: 12090, loss: 8.166e-02\n",
      "Iteration: 12100, loss: 1.185e-01\n",
      "Iteration: 12110, loss: 8.595e-02\n",
      "Iteration: 12120, loss: 7.943e-02\n",
      "Iteration: 12130, loss: 7.668e-02\n",
      "Iteration: 12140, loss: 9.051e-02\n",
      "Iteration: 12150, loss: 1.037e-01\n",
      "Iteration: 12160, loss: 9.999e-02\n",
      "Iteration: 12170, loss: 1.002e-01\n",
      "Iteration: 12180, loss: 7.061e-02\n",
      "Iteration: 12190, loss: 7.741e-02\n",
      "Iteration: 12200, loss: 7.135e-02\n",
      "Iteration: 12210, loss: 1.154e-01\n",
      "Iteration: 12220, loss: 4.977e-02\n",
      "Iteration: 12230, loss: 1.122e-01\n",
      "Iteration: 12240, loss: 9.366e-02\n",
      "Iteration: 12250, loss: 7.109e-02\n",
      "Iteration: 12260, loss: 4.833e-02\n",
      "Iteration: 12270, loss: 9.167e-02\n",
      "Iteration: 12280, loss: 1.434e-01\n",
      "Iteration: 12290, loss: 7.308e-02\n",
      "Iteration: 12300, loss: 7.513e-02\n",
      "Iteration: 12310, loss: 9.904e-02\n",
      "Iteration: 12320, loss: 8.291e-02\n",
      "Iteration: 12330, loss: 1.038e-01\n",
      "Iteration: 12340, loss: 9.985e-02\n",
      "Iteration: 12350, loss: 6.382e-02\n",
      "Iteration: 12360, loss: 9.435e-02\n",
      "Iteration: 12370, loss: 9.060e-02\n",
      "Iteration: 12380, loss: 1.039e-01\n",
      "Iteration: 12390, loss: 8.050e-02\n",
      "Iteration: 12400, loss: 5.355e-02\n",
      "Iteration: 12410, loss: 8.219e-02\n",
      "Iteration: 12420, loss: 6.894e-02\n",
      "Iteration: 12430, loss: 9.801e-02\n",
      "Iteration: 12440, loss: 6.251e-02\n",
      "Iteration: 12450, loss: 5.453e-02\n",
      "Iteration: 12460, loss: 7.137e-02\n",
      "Iteration: 12470, loss: 8.071e-02\n",
      "Iteration: 12480, loss: 5.832e-02\n",
      "Iteration: 12490, loss: 9.849e-02\n",
      "Iteration: 12500, loss: 2.690e-02\n",
      "Iteration: 12510, loss: 4.367e-02\n",
      "Iteration: 12520, loss: 8.860e-02\n",
      "Iteration: 12530, loss: 6.917e-02\n",
      "Iteration: 12540, loss: 6.059e-02\n",
      "Iteration: 12550, loss: 8.708e-02\n",
      "Iteration: 12560, loss: 8.125e-02\n",
      "Iteration: 12570, loss: 9.010e-02\n",
      "Iteration: 12580, loss: 6.091e-02\n",
      "Iteration: 12590, loss: 8.607e-02\n",
      "Iteration: 12600, loss: 1.008e-01\n",
      "Iteration: 12610, loss: 7.627e-02\n",
      "Iteration: 12620, loss: 4.945e-02\n",
      "Iteration: 12630, loss: 6.575e-02\n",
      "Iteration: 12640, loss: 9.653e-02\n",
      "Iteration: 12650, loss: 8.087e-02\n",
      "Iteration: 12660, loss: 5.621e-02\n",
      "Iteration: 12670, loss: 1.081e-01\n",
      "Iteration: 12680, loss: 1.011e-01\n",
      "Iteration: 12690, loss: 1.082e-01\n",
      "Iteration: 12700, loss: 4.474e-02\n",
      "Iteration: 12710, loss: 5.120e-02\n",
      "Iteration: 12720, loss: 9.560e-02\n",
      "Iteration: 12730, loss: 7.731e-02\n",
      "Iteration: 12740, loss: 7.416e-02\n",
      "Iteration: 12750, loss: 6.618e-02\n",
      "Iteration: 12760, loss: 3.284e-02\n",
      "Iteration: 12770, loss: 6.433e-02\n",
      "Iteration: 12780, loss: 6.129e-02\n",
      "Iteration: 12790, loss: 9.022e-02\n",
      "Iteration: 12800, loss: 7.920e-02\n",
      "Iteration: 12810, loss: 4.804e-02\n",
      "Iteration: 12820, loss: 6.180e-02\n",
      "Iteration: 12830, loss: 1.378e-01\n",
      "Iteration: 12840, loss: 5.848e-02\n",
      "Iteration: 12850, loss: 1.073e-01\n",
      "Iteration: 12860, loss: 9.405e-02\n",
      "Iteration: 12870, loss: 7.423e-02\n",
      "Iteration: 12880, loss: 7.865e-02\n",
      "Iteration: 12890, loss: 6.547e-02\n",
      "Iteration: 12900, loss: 7.098e-02\n",
      "Iteration: 12910, loss: 6.459e-02\n",
      "Iteration: 12920, loss: 8.208e-02\n",
      "Iteration: 12930, loss: 4.551e-02\n",
      "Iteration: 12940, loss: 5.178e-02\n",
      "Iteration: 12950, loss: 7.361e-02\n",
      "Iteration: 12960, loss: 9.783e-02\n",
      "Iteration: 12970, loss: 5.406e-02\n",
      "Iteration: 12980, loss: 2.144e-02\n",
      "Iteration: 12990, loss: 8.043e-02\n",
      "Iteration: 13000, loss: 7.558e-02\n",
      "Iteration: 13010, loss: 1.015e-01\n",
      "Iteration: 13020, loss: 5.640e-02\n",
      "Iteration: 13030, loss: 3.749e-02\n",
      "Iteration: 13040, loss: 9.687e-02\n",
      "Iteration: 13050, loss: 4.298e-02\n",
      "Iteration: 13060, loss: 6.270e-02\n",
      "Iteration: 13070, loss: 7.528e-02\n",
      "Iteration: 13080, loss: 7.575e-02\n",
      "Iteration: 13090, loss: 4.269e-02\n",
      "Iteration: 13100, loss: 6.560e-02\n",
      "Iteration: 13110, loss: 6.572e-02\n",
      "Iteration: 13120, loss: 5.934e-02\n",
      "Iteration: 13130, loss: 4.280e-02\n",
      "Iteration: 13140, loss: 6.898e-02\n",
      "Iteration: 13150, loss: 8.222e-02\n",
      "Iteration: 13160, loss: 6.362e-02\n",
      "Iteration: 13170, loss: 6.493e-02\n",
      "Iteration: 13180, loss: 4.989e-02\n",
      "Iteration: 13190, loss: 6.964e-02\n",
      "Iteration: 13200, loss: 6.101e-02\n",
      "Iteration: 13210, loss: 4.813e-02\n",
      "Iteration: 13220, loss: 5.972e-02\n",
      "Iteration: 13230, loss: 7.146e-02\n",
      "Iteration: 13240, loss: 8.473e-02\n",
      "Iteration: 13250, loss: 6.774e-02\n",
      "Iteration: 13260, loss: 6.146e-02\n",
      "Iteration: 13270, loss: 5.921e-02\n",
      "Iteration: 13280, loss: 7.690e-02\n",
      "Iteration: 13290, loss: 4.064e-02\n",
      "Iteration: 13300, loss: 6.873e-02\n",
      "Iteration: 13310, loss: 7.858e-02\n",
      "Iteration: 13320, loss: 5.594e-02\n",
      "Iteration: 13330, loss: 5.802e-02\n",
      "Iteration: 13340, loss: 6.756e-02\n",
      "Iteration: 13350, loss: 5.824e-02\n",
      "Iteration: 13360, loss: 2.696e-02\n",
      "Iteration: 13370, loss: 8.670e-02\n",
      "Iteration: 13380, loss: 7.879e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13390, loss: 6.401e-02\n",
      "Iteration: 13400, loss: 6.132e-02\n",
      "Iteration: 13410, loss: 8.627e-02\n",
      "Iteration: 13420, loss: 5.641e-02\n",
      "Iteration: 13430, loss: 6.116e-02\n",
      "Iteration: 13440, loss: 1.048e-01\n",
      "Iteration: 13450, loss: 6.284e-02\n",
      "Iteration: 13460, loss: 4.286e-02\n",
      "Iteration: 13470, loss: 5.822e-02\n",
      "Iteration: 13480, loss: 3.592e-02\n",
      "Iteration: 13490, loss: 5.265e-02\n",
      "Iteration: 13500, loss: 4.429e-02\n",
      "Iteration: 13510, loss: 3.549e-02\n",
      "Iteration: 13520, loss: 6.036e-02\n",
      "Iteration: 13530, loss: 7.347e-02\n",
      "Iteration: 13540, loss: 4.743e-02\n",
      "Iteration: 13550, loss: 4.564e-02\n",
      "Iteration: 13560, loss: 3.816e-02\n",
      "Iteration: 13570, loss: 2.599e-02\n",
      "Iteration: 13580, loss: 5.375e-02\n",
      "Iteration: 13590, loss: 6.938e-02\n",
      "Iteration: 13600, loss: 6.540e-02\n",
      "Iteration: 13610, loss: 7.774e-02\n",
      "Iteration: 13620, loss: 5.204e-02\n",
      "Iteration: 13630, loss: 4.922e-02\n",
      "Iteration: 13640, loss: 5.482e-02\n",
      "Iteration: 13650, loss: 8.950e-02\n",
      "Iteration: 13660, loss: 3.728e-02\n",
      "Iteration: 13670, loss: 5.110e-02\n",
      "Iteration: 13680, loss: 5.625e-02\n",
      "Iteration: 13690, loss: 3.482e-02\n",
      "Iteration: 13700, loss: 4.611e-02\n",
      "Iteration: 13710, loss: 3.953e-02\n",
      "Iteration: 13720, loss: 9.253e-02\n",
      "Iteration: 13730, loss: 7.906e-02\n",
      "Iteration: 13740, loss: 6.048e-02\n",
      "Iteration: 13750, loss: 5.275e-02\n",
      "Iteration: 13760, loss: 4.719e-02\n",
      "Iteration: 13770, loss: 4.590e-02\n",
      "Iteration: 13780, loss: 5.543e-02\n",
      "Iteration: 13790, loss: 5.288e-02\n",
      "Iteration: 13800, loss: 2.187e-02\n",
      "Iteration: 13810, loss: 6.469e-02\n",
      "Iteration: 13820, loss: 3.000e-02\n",
      "Iteration: 13830, loss: 2.693e-02\n",
      "Iteration: 13840, loss: 3.345e-02\n",
      "Iteration: 13850, loss: 6.451e-02\n",
      "Iteration: 13860, loss: 4.067e-02\n",
      "Iteration: 13870, loss: 4.202e-02\n",
      "Iteration: 13880, loss: 4.347e-02\n",
      "Iteration: 13890, loss: 6.054e-02\n",
      "Iteration: 13900, loss: 6.269e-02\n",
      "Iteration: 13910, loss: 3.521e-02\n",
      "Iteration: 13920, loss: 5.621e-02\n",
      "Iteration: 13930, loss: 5.169e-02\n",
      "Iteration: 13940, loss: 4.581e-02\n",
      "Iteration: 13950, loss: 6.054e-02\n",
      "Iteration: 13960, loss: 3.940e-02\n",
      "Iteration: 13970, loss: 4.574e-02\n",
      "Iteration: 13980, loss: 4.629e-02\n",
      "Iteration: 13990, loss: 3.750e-02\n",
      "Iteration: 14000, loss: 4.268e-02\n",
      "Iteration: 14010, loss: 5.227e-02\n",
      "Iteration: 14020, loss: 3.777e-02\n",
      "Iteration: 14030, loss: 4.247e-02\n",
      "Iteration: 14040, loss: 4.878e-02\n",
      "Iteration: 14050, loss: 6.220e-02\n",
      "Iteration: 14060, loss: 5.631e-02\n",
      "Iteration: 14070, loss: 5.978e-02\n",
      "Iteration: 14080, loss: 5.385e-02\n",
      "Iteration: 14090, loss: 4.262e-02\n",
      "Iteration: 14100, loss: 3.741e-02\n",
      "Iteration: 14110, loss: 4.139e-02\n",
      "Iteration: 14120, loss: 5.623e-02\n",
      "Iteration: 14130, loss: 3.401e-02\n",
      "Iteration: 14140, loss: 6.188e-02\n",
      "Iteration: 14150, loss: 5.868e-02\n",
      "Iteration: 14160, loss: 3.244e-02\n",
      "Iteration: 14170, loss: 4.978e-02\n",
      "Iteration: 14180, loss: 6.525e-02\n",
      "Iteration: 14190, loss: 5.502e-02\n",
      "Iteration: 14200, loss: 5.043e-02\n",
      "Iteration: 14210, loss: 4.426e-02\n",
      "Iteration: 14220, loss: 4.864e-02\n",
      "Iteration: 14230, loss: 6.280e-02\n",
      "Iteration: 14240, loss: 7.022e-02\n",
      "Iteration: 14250, loss: 3.640e-02\n",
      "Iteration: 14260, loss: 4.294e-02\n",
      "Iteration: 14270, loss: 4.906e-02\n",
      "Iteration: 14280, loss: 5.071e-02\n",
      "Iteration: 14290, loss: 4.257e-02\n",
      "Iteration: 14300, loss: 4.001e-02\n",
      "Iteration: 14310, loss: 4.288e-02\n",
      "Iteration: 14320, loss: 5.627e-02\n",
      "Iteration: 14330, loss: 3.592e-02\n",
      "Iteration: 14340, loss: 4.502e-02\n",
      "Iteration: 14350, loss: 3.599e-02\n",
      "Iteration: 14360, loss: 4.571e-02\n",
      "Iteration: 14370, loss: 4.124e-02\n",
      "Iteration: 14380, loss: 1.931e-02\n",
      "Iteration: 14390, loss: 6.357e-02\n",
      "Iteration: 14400, loss: 4.682e-02\n",
      "Iteration: 14410, loss: 3.165e-02\n",
      "Iteration: 14420, loss: 3.103e-02\n",
      "Iteration: 14430, loss: 3.675e-02\n",
      "Iteration: 14440, loss: 3.209e-02\n",
      "Iteration: 14450, loss: 5.654e-02\n",
      "Iteration: 14460, loss: 4.963e-02\n",
      "Iteration: 14470, loss: 5.597e-02\n",
      "Iteration: 14480, loss: 6.389e-02\n",
      "Iteration: 14490, loss: 5.149e-02\n",
      "Iteration: 14500, loss: 3.594e-02\n",
      "Iteration: 14510, loss: 3.766e-02\n",
      "Iteration: 14520, loss: 3.467e-02\n",
      "Iteration: 14530, loss: 4.469e-02\n",
      "Iteration: 14540, loss: 3.720e-02\n",
      "Iteration: 14550, loss: 5.316e-02\n",
      "Iteration: 14560, loss: 3.824e-02\n",
      "Iteration: 14570, loss: 2.761e-02\n",
      "Iteration: 14580, loss: 3.660e-02\n",
      "Iteration: 14590, loss: 3.044e-02\n",
      "Iteration: 14600, loss: 3.841e-02\n",
      "Iteration: 14610, loss: 4.681e-02\n",
      "Iteration: 14620, loss: 4.474e-02\n",
      "Iteration: 14630, loss: 3.217e-02\n",
      "Iteration: 14640, loss: 5.000e-02\n",
      "Iteration: 14650, loss: 4.720e-02\n",
      "Iteration: 14660, loss: 3.660e-02\n",
      "Iteration: 14670, loss: 3.633e-02\n",
      "Iteration: 14680, loss: 3.891e-02\n",
      "Iteration: 14690, loss: 3.104e-02\n",
      "Iteration: 14700, loss: 4.778e-02\n",
      "Iteration: 14710, loss: 4.295e-02\n",
      "Iteration: 14720, loss: 3.187e-02\n",
      "Iteration: 14730, loss: 2.427e-02\n",
      "Iteration: 14740, loss: 3.077e-02\n",
      "Iteration: 14750, loss: 3.356e-02\n",
      "Iteration: 14760, loss: 4.303e-02\n",
      "Iteration: 14770, loss: 5.824e-02\n",
      "Iteration: 14780, loss: 4.185e-02\n",
      "Iteration: 14790, loss: 5.223e-02\n",
      "Iteration: 14800, loss: 2.947e-02\n",
      "Iteration: 14810, loss: 3.540e-02\n",
      "Iteration: 14820, loss: 3.056e-02\n",
      "Iteration: 14830, loss: 2.310e-02\n",
      "Iteration: 14840, loss: 5.302e-02\n",
      "Iteration: 14850, loss: 2.902e-02\n",
      "Iteration: 14860, loss: 2.122e-02\n",
      "Iteration: 14870, loss: 2.962e-02\n",
      "Iteration: 14880, loss: 5.305e-02\n",
      "Iteration: 14890, loss: 4.412e-02\n",
      "Iteration: 14900, loss: 5.152e-02\n",
      "Iteration: 14910, loss: 4.722e-02\n",
      "Iteration: 14920, loss: 3.827e-02\n",
      "Iteration: 14930, loss: 4.543e-02\n",
      "Iteration: 14940, loss: 3.618e-02\n",
      "Iteration: 14950, loss: 3.426e-02\n",
      "Iteration: 14960, loss: 2.633e-02\n",
      "Iteration: 14970, loss: 3.639e-02\n",
      "Iteration: 14980, loss: 4.991e-02\n",
      "Iteration: 14990, loss: 3.578e-02\n",
      "Iteration: 15000, loss: 3.115e-02\n",
      "Iteration: 15010, loss: 3.279e-02\n",
      "Iteration: 15020, loss: 2.773e-02\n",
      "Iteration: 15030, loss: 3.171e-02\n",
      "Iteration: 15040, loss: 4.692e-02\n",
      "Iteration: 15050, loss: 3.089e-02\n",
      "Iteration: 15060, loss: 3.215e-02\n",
      "Iteration: 15070, loss: 3.246e-02\n",
      "Iteration: 15080, loss: 2.759e-02\n",
      "Iteration: 15090, loss: 3.443e-02\n",
      "Iteration: 15100, loss: 3.166e-02\n",
      "Iteration: 15110, loss: 4.365e-02\n",
      "Iteration: 15120, loss: 2.911e-02\n",
      "Iteration: 15130, loss: 3.532e-02\n",
      "Iteration: 15140, loss: 5.204e-02\n",
      "Iteration: 15150, loss: 2.405e-02\n",
      "Iteration: 15160, loss: 3.075e-02\n",
      "Iteration: 15170, loss: 3.191e-02\n",
      "Iteration: 15180, loss: 3.194e-02\n",
      "Iteration: 15190, loss: 1.708e-02\n",
      "Iteration: 15200, loss: 3.732e-02\n",
      "Iteration: 15210, loss: 3.099e-02\n",
      "Iteration: 15220, loss: 3.919e-02\n",
      "Iteration: 15230, loss: 3.833e-02\n",
      "Iteration: 15240, loss: 3.709e-02\n",
      "Iteration: 15250, loss: 2.662e-02\n",
      "Iteration: 15260, loss: 3.198e-02\n",
      "Iteration: 15270, loss: 3.337e-02\n",
      "Iteration: 15280, loss: 2.543e-02\n",
      "Iteration: 15290, loss: 4.149e-02\n",
      "Iteration: 15300, loss: 2.362e-02\n",
      "Iteration: 15310, loss: 3.461e-02\n",
      "Iteration: 15320, loss: 3.643e-02\n",
      "Iteration: 15330, loss: 2.111e-02\n",
      "Iteration: 15340, loss: 3.543e-02\n",
      "Iteration: 15350, loss: 1.728e-02\n",
      "Iteration: 15360, loss: 2.649e-02\n",
      "Iteration: 15370, loss: 3.428e-02\n",
      "Iteration: 15380, loss: 2.423e-02\n",
      "Iteration: 15390, loss: 1.837e-02\n",
      "Iteration: 15400, loss: 4.077e-02\n",
      "Iteration: 15410, loss: 2.972e-02\n",
      "Iteration: 15420, loss: 3.616e-02\n",
      "Iteration: 15430, loss: 4.684e-02\n",
      "Iteration: 15440, loss: 2.807e-02\n",
      "Iteration: 15450, loss: 3.836e-02\n",
      "Iteration: 15460, loss: 3.942e-02\n",
      "Iteration: 15470, loss: 3.194e-02\n",
      "Iteration: 15480, loss: 4.145e-02\n",
      "Iteration: 15490, loss: 2.492e-02\n",
      "Iteration: 15500, loss: 4.199e-02\n",
      "Iteration: 15510, loss: 3.052e-02\n",
      "Iteration: 15520, loss: 2.593e-02\n",
      "Iteration: 15530, loss: 3.483e-02\n",
      "Iteration: 15540, loss: 4.072e-02\n",
      "Iteration: 15550, loss: 3.105e-02\n",
      "Iteration: 15560, loss: 2.277e-02\n",
      "Iteration: 15570, loss: 4.069e-02\n",
      "Iteration: 15580, loss: 2.061e-02\n",
      "Iteration: 15590, loss: 3.778e-02\n",
      "Iteration: 15600, loss: 1.948e-02\n",
      "Iteration: 15610, loss: 2.817e-02\n",
      "Iteration: 15620, loss: 3.219e-02\n",
      "Iteration: 15630, loss: 3.005e-02\n",
      "Iteration: 15640, loss: 3.026e-02\n",
      "Iteration: 15650, loss: 2.497e-02\n",
      "Iteration: 15660, loss: 2.994e-02\n",
      "Iteration: 15670, loss: 2.340e-02\n",
      "Iteration: 15680, loss: 2.934e-02\n",
      "Iteration: 15690, loss: 3.281e-02\n",
      "Iteration: 15700, loss: 1.687e-02\n",
      "Iteration: 15710, loss: 1.815e-02\n",
      "Iteration: 15720, loss: 2.890e-02\n",
      "Iteration: 15730, loss: 3.782e-02\n",
      "Iteration: 15740, loss: 1.836e-02\n",
      "Iteration: 15750, loss: 2.534e-02\n",
      "Iteration: 15760, loss: 2.569e-02\n",
      "Iteration: 15770, loss: 3.797e-02\n",
      "Iteration: 15780, loss: 1.598e-02\n",
      "Iteration: 15790, loss: 3.868e-02\n",
      "Iteration: 15800, loss: 1.780e-02\n",
      "Iteration: 15810, loss: 2.867e-02\n",
      "Iteration: 15820, loss: 2.838e-02\n",
      "Iteration: 15830, loss: 2.614e-02\n",
      "Iteration: 15840, loss: 3.009e-02\n",
      "Iteration: 15850, loss: 2.870e-02\n",
      "Iteration: 15860, loss: 2.480e-02\n",
      "Iteration: 15870, loss: 1.689e-02\n",
      "Iteration: 15880, loss: 3.185e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15890, loss: 2.660e-02\n",
      "Iteration: 15900, loss: 1.882e-02\n",
      "Iteration: 15910, loss: 2.480e-02\n",
      "Iteration: 15920, loss: 3.147e-02\n",
      "Iteration: 15930, loss: 2.014e-02\n",
      "Iteration: 15940, loss: 2.682e-02\n",
      "Iteration: 15950, loss: 3.502e-02\n",
      "Iteration: 15960, loss: 3.944e-02\n",
      "Iteration: 15970, loss: 4.407e-02\n",
      "Iteration: 15980, loss: 3.265e-02\n",
      "Iteration: 15990, loss: 2.954e-02\n",
      "Iteration: 16000, loss: 1.990e-02\n",
      "Iteration: 16010, loss: 2.353e-02\n",
      "Iteration: 16020, loss: 3.006e-02\n",
      "Iteration: 16030, loss: 2.515e-02\n",
      "Iteration: 16040, loss: 1.801e-02\n",
      "Iteration: 16050, loss: 2.284e-02\n",
      "Iteration: 16060, loss: 2.835e-02\n",
      "Iteration: 16070, loss: 2.045e-02\n",
      "Iteration: 16080, loss: 2.456e-02\n",
      "Iteration: 16090, loss: 2.290e-02\n",
      "Iteration: 16100, loss: 2.838e-02\n",
      "Iteration: 16110, loss: 3.831e-02\n",
      "Iteration: 16120, loss: 2.470e-02\n",
      "Iteration: 16130, loss: 2.601e-02\n",
      "Iteration: 16140, loss: 1.601e-02\n",
      "Iteration: 16150, loss: 2.177e-02\n",
      "Iteration: 16160, loss: 3.438e-02\n",
      "Iteration: 16170, loss: 2.862e-02\n",
      "Iteration: 16180, loss: 2.513e-02\n",
      "Iteration: 16190, loss: 2.182e-02\n",
      "Iteration: 16200, loss: 1.834e-02\n",
      "Iteration: 16210, loss: 1.558e-02\n",
      "Iteration: 16220, loss: 3.417e-02\n",
      "Iteration: 16230, loss: 2.618e-02\n",
      "Relative error: 5.637100e-02\n",
      "Iteration: 10, loss: 1.463e+03\n",
      "Iteration: 20, loss: 1.281e+03\n",
      "Iteration: 30, loss: 1.334e+03\n",
      "Iteration: 40, loss: 8.360e+02\n",
      "Iteration: 50, loss: 5.442e+02\n",
      "Iteration: 60, loss: 4.028e+02\n",
      "Iteration: 70, loss: 3.805e+02\n",
      "Iteration: 80, loss: 4.804e+02\n",
      "Iteration: 90, loss: 4.201e+02\n",
      "Iteration: 100, loss: 3.953e+02\n",
      "Iteration: 110, loss: 4.013e+02\n",
      "Iteration: 120, loss: 2.484e+02\n",
      "Iteration: 130, loss: 2.467e+02\n",
      "Iteration: 140, loss: 1.560e+02\n",
      "Iteration: 150, loss: 1.481e+02\n",
      "Iteration: 160, loss: 2.101e+02\n",
      "Iteration: 170, loss: 1.451e+02\n",
      "Iteration: 180, loss: 1.421e+02\n",
      "Iteration: 190, loss: 1.429e+02\n",
      "Iteration: 200, loss: 1.597e+02\n",
      "Iteration: 210, loss: 1.161e+02\n",
      "Iteration: 220, loss: 1.027e+02\n",
      "Iteration: 230, loss: 8.255e+01\n",
      "Iteration: 240, loss: 5.232e+01\n",
      "Iteration: 250, loss: 5.871e+01\n",
      "Iteration: 260, loss: 7.475e+01\n",
      "Iteration: 270, loss: 8.272e+01\n",
      "Iteration: 280, loss: 5.604e+01\n",
      "Iteration: 290, loss: 1.089e+02\n",
      "Iteration: 300, loss: 5.701e+01\n",
      "Iteration: 310, loss: 5.631e+01\n",
      "Iteration: 320, loss: 2.519e+01\n",
      "Iteration: 330, loss: 5.562e+01\n",
      "Iteration: 340, loss: 5.218e+01\n",
      "Iteration: 350, loss: 3.468e+01\n",
      "Iteration: 360, loss: 3.690e+01\n",
      "Iteration: 370, loss: 4.566e+01\n",
      "Iteration: 380, loss: 6.701e+01\n",
      "Iteration: 390, loss: 3.335e+01\n",
      "Iteration: 400, loss: 5.006e+01\n",
      "Iteration: 410, loss: 2.495e+01\n",
      "Iteration: 420, loss: 4.172e+01\n",
      "Iteration: 430, loss: 2.521e+01\n",
      "Iteration: 440, loss: 3.650e+01\n",
      "Iteration: 450, loss: 4.596e+01\n",
      "Iteration: 460, loss: 5.890e+01\n",
      "Iteration: 470, loss: 2.558e+01\n",
      "Iteration: 480, loss: 2.337e+01\n",
      "Iteration: 490, loss: 4.148e+01\n",
      "Iteration: 500, loss: 2.994e+01\n",
      "Iteration: 510, loss: 2.832e+01\n",
      "Iteration: 520, loss: 1.987e+01\n",
      "Iteration: 530, loss: 2.598e+01\n",
      "Iteration: 540, loss: 3.374e+01\n",
      "Iteration: 550, loss: 2.102e+01\n",
      "Iteration: 560, loss: 2.716e+01\n",
      "Iteration: 570, loss: 1.353e+01\n",
      "Iteration: 580, loss: 2.003e+01\n",
      "Iteration: 590, loss: 2.543e+01\n",
      "Iteration: 600, loss: 1.842e+01\n",
      "Iteration: 610, loss: 1.437e+01\n",
      "Iteration: 620, loss: 1.700e+01\n",
      "Iteration: 630, loss: 2.497e+01\n",
      "Iteration: 640, loss: 1.741e+01\n",
      "Iteration: 650, loss: 1.072e+01\n",
      "Iteration: 660, loss: 1.326e+01\n",
      "Iteration: 670, loss: 1.995e+01\n",
      "Iteration: 680, loss: 1.826e+01\n",
      "Iteration: 690, loss: 3.032e+01\n",
      "Iteration: 700, loss: 1.865e+01\n",
      "Iteration: 710, loss: 2.195e+01\n",
      "Iteration: 720, loss: 1.405e+01\n",
      "Iteration: 730, loss: 1.622e+01\n",
      "Iteration: 740, loss: 1.502e+01\n",
      "Iteration: 750, loss: 1.780e+01\n",
      "Iteration: 760, loss: 1.010e+01\n",
      "Iteration: 770, loss: 2.489e+01\n",
      "Iteration: 780, loss: 1.614e+01\n",
      "Iteration: 790, loss: 1.283e+01\n",
      "Iteration: 800, loss: 8.695e+00\n",
      "Iteration: 810, loss: 1.419e+01\n",
      "Iteration: 820, loss: 1.835e+01\n",
      "Iteration: 830, loss: 1.700e+01\n",
      "Iteration: 840, loss: 1.128e+01\n",
      "Iteration: 850, loss: 1.658e+01\n",
      "Iteration: 860, loss: 1.492e+01\n",
      "Iteration: 870, loss: 9.817e+00\n",
      "Iteration: 880, loss: 1.030e+01\n",
      "Iteration: 890, loss: 1.245e+01\n",
      "Iteration: 900, loss: 7.928e+00\n",
      "Iteration: 910, loss: 9.388e+00\n",
      "Iteration: 920, loss: 1.069e+01\n",
      "Iteration: 930, loss: 8.859e+00\n",
      "Iteration: 940, loss: 8.733e+00\n",
      "Iteration: 950, loss: 1.096e+01\n",
      "Iteration: 960, loss: 1.025e+01\n",
      "Iteration: 970, loss: 1.141e+01\n",
      "Iteration: 980, loss: 7.433e+00\n",
      "Iteration: 990, loss: 1.308e+01\n",
      "Iteration: 1000, loss: 7.174e+00\n",
      "Iteration: 1010, loss: 8.009e+00\n",
      "Iteration: 1020, loss: 7.684e+00\n",
      "Iteration: 1030, loss: 9.543e+00\n",
      "Iteration: 1040, loss: 6.204e+00\n",
      "Iteration: 1050, loss: 1.724e+01\n",
      "Iteration: 1060, loss: 1.197e+01\n",
      "Iteration: 1070, loss: 1.771e+01\n",
      "Iteration: 1080, loss: 8.654e+00\n",
      "Iteration: 1090, loss: 1.085e+01\n",
      "Iteration: 1100, loss: 9.703e+00\n",
      "Iteration: 1110, loss: 4.619e+00\n",
      "Iteration: 1120, loss: 9.496e+00\n",
      "Iteration: 1130, loss: 7.712e+00\n",
      "Iteration: 1140, loss: 7.708e+00\n",
      "Iteration: 1150, loss: 6.178e+00\n",
      "Iteration: 1160, loss: 1.194e+01\n",
      "Iteration: 1170, loss: 6.083e+00\n",
      "Iteration: 1180, loss: 7.708e+00\n",
      "Iteration: 1190, loss: 3.760e+00\n",
      "Iteration: 1200, loss: 8.487e+00\n",
      "Iteration: 1210, loss: 1.109e+01\n",
      "Iteration: 1220, loss: 8.602e+00\n",
      "Iteration: 1230, loss: 8.357e+00\n",
      "Iteration: 1240, loss: 7.874e+00\n",
      "Iteration: 1250, loss: 6.698e+00\n",
      "Iteration: 1260, loss: 6.185e+00\n",
      "Iteration: 1270, loss: 5.241e+00\n",
      "Iteration: 1280, loss: 6.730e+00\n",
      "Iteration: 1290, loss: 8.085e+00\n",
      "Iteration: 1300, loss: 4.580e+00\n",
      "Iteration: 1310, loss: 6.462e+00\n",
      "Iteration: 1320, loss: 8.407e+00\n",
      "Iteration: 1330, loss: 5.956e+00\n",
      "Iteration: 1340, loss: 7.502e+00\n",
      "Iteration: 1350, loss: 6.028e+00\n",
      "Iteration: 1360, loss: 9.498e+00\n",
      "Iteration: 1370, loss: 1.201e+01\n",
      "Iteration: 1380, loss: 8.083e+00\n",
      "Iteration: 1390, loss: 3.684e+00\n",
      "Iteration: 1400, loss: 7.901e+00\n",
      "Iteration: 1410, loss: 4.853e+00\n",
      "Iteration: 1420, loss: 1.152e+01\n",
      "Iteration: 1430, loss: 6.220e+00\n",
      "Iteration: 1440, loss: 6.574e+00\n",
      "Iteration: 1450, loss: 4.600e+00\n",
      "Iteration: 1460, loss: 7.251e+00\n",
      "Iteration: 1470, loss: 4.372e+00\n",
      "Iteration: 1480, loss: 4.592e+00\n",
      "Iteration: 1490, loss: 3.856e+00\n",
      "Iteration: 1500, loss: 6.813e+00\n",
      "Iteration: 1510, loss: 7.083e+00\n",
      "Iteration: 1520, loss: 7.607e+00\n",
      "Iteration: 1530, loss: 2.555e+00\n",
      "Iteration: 1540, loss: 3.391e+00\n",
      "Iteration: 1550, loss: 6.300e+00\n",
      "Iteration: 1560, loss: 4.307e+00\n",
      "Iteration: 1570, loss: 4.090e+00\n",
      "Iteration: 1580, loss: 4.674e+00\n",
      "Iteration: 1590, loss: 4.666e+00\n",
      "Iteration: 1600, loss: 3.587e+00\n",
      "Iteration: 1610, loss: 2.728e+00\n",
      "Iteration: 1620, loss: 5.081e+00\n",
      "Iteration: 1630, loss: 7.755e+00\n",
      "Iteration: 1640, loss: 4.531e+00\n",
      "Iteration: 1650, loss: 5.025e+00\n",
      "Iteration: 1660, loss: 3.609e+00\n",
      "Iteration: 1670, loss: 3.304e+00\n",
      "Iteration: 1680, loss: 4.118e+00\n",
      "Iteration: 1690, loss: 4.728e+00\n",
      "Iteration: 1700, loss: 5.801e+00\n",
      "Iteration: 1710, loss: 4.391e+00\n",
      "Iteration: 1720, loss: 5.259e+00\n",
      "Iteration: 1730, loss: 3.226e+00\n",
      "Iteration: 1740, loss: 5.129e+00\n",
      "Iteration: 1750, loss: 6.203e+00\n",
      "Iteration: 1760, loss: 5.778e+00\n",
      "Iteration: 1770, loss: 5.164e+00\n",
      "Iteration: 1780, loss: 5.187e+00\n",
      "Iteration: 1790, loss: 1.972e+00\n",
      "Iteration: 1800, loss: 4.584e+00\n",
      "Iteration: 1810, loss: 4.671e+00\n",
      "Iteration: 1820, loss: 4.911e+00\n",
      "Iteration: 1830, loss: 4.078e+00\n",
      "Iteration: 1840, loss: 5.123e+00\n",
      "Iteration: 1850, loss: 5.188e+00\n",
      "Iteration: 1860, loss: 4.031e+00\n",
      "Iteration: 1870, loss: 2.316e+00\n",
      "Iteration: 1880, loss: 3.691e+00\n",
      "Iteration: 1890, loss: 3.170e+00\n",
      "Iteration: 1900, loss: 2.657e+00\n",
      "Iteration: 1910, loss: 3.309e+00\n",
      "Iteration: 1920, loss: 4.885e+00\n",
      "Iteration: 1930, loss: 3.231e+00\n",
      "Iteration: 1940, loss: 5.171e+00\n",
      "Iteration: 1950, loss: 3.327e+00\n",
      "Iteration: 1960, loss: 5.251e+00\n",
      "Iteration: 1970, loss: 4.063e+00\n",
      "Iteration: 1980, loss: 3.161e+00\n",
      "Iteration: 1990, loss: 3.161e+00\n",
      "Iteration: 2000, loss: 3.089e+00\n",
      "Iteration: 2010, loss: 4.575e+00\n",
      "Iteration: 2020, loss: 3.403e+00\n",
      "Iteration: 2030, loss: 4.103e+00\n",
      "Iteration: 2040, loss: 3.517e+00\n",
      "Iteration: 2050, loss: 5.566e+00\n",
      "Iteration: 2060, loss: 4.237e+00\n",
      "Iteration: 2070, loss: 5.433e+00\n",
      "Iteration: 2080, loss: 3.953e+00\n",
      "Iteration: 2090, loss: 3.883e+00\n",
      "Iteration: 2100, loss: 3.645e+00\n",
      "Iteration: 2110, loss: 1.960e+00\n",
      "Iteration: 2120, loss: 3.150e+00\n",
      "Iteration: 2130, loss: 6.574e+00\n",
      "Iteration: 2140, loss: 2.837e+00\n",
      "Iteration: 2150, loss: 6.193e+00\n",
      "Iteration: 2160, loss: 3.011e+00\n",
      "Iteration: 2170, loss: 3.233e+00\n",
      "Iteration: 2180, loss: 2.798e+00\n",
      "Iteration: 2190, loss: 1.929e+00\n",
      "Iteration: 2200, loss: 3.247e+00\n",
      "Iteration: 2210, loss: 2.147e+00\n",
      "Iteration: 2220, loss: 2.614e+00\n",
      "Iteration: 2230, loss: 2.763e+00\n",
      "Iteration: 2240, loss: 3.514e+00\n",
      "Iteration: 2250, loss: 3.294e+00\n",
      "Iteration: 2260, loss: 2.172e+00\n",
      "Iteration: 2270, loss: 3.296e+00\n",
      "Iteration: 2280, loss: 3.258e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2290, loss: 3.321e+00\n",
      "Iteration: 2300, loss: 3.160e+00\n",
      "Iteration: 2310, loss: 3.628e+00\n",
      "Iteration: 2320, loss: 2.758e+00\n",
      "Iteration: 2330, loss: 3.227e+00\n",
      "Iteration: 2340, loss: 3.153e+00\n",
      "Iteration: 2350, loss: 2.074e+00\n",
      "Iteration: 2360, loss: 3.566e+00\n",
      "Iteration: 2370, loss: 3.664e+00\n",
      "Iteration: 2380, loss: 2.804e+00\n",
      "Iteration: 2390, loss: 4.092e+00\n",
      "Iteration: 2400, loss: 2.877e+00\n",
      "Iteration: 2410, loss: 2.837e+00\n",
      "Iteration: 2420, loss: 2.517e+00\n",
      "Iteration: 2430, loss: 1.394e+00\n",
      "Iteration: 2440, loss: 1.649e+00\n",
      "Iteration: 2450, loss: 2.922e+00\n",
      "Iteration: 2460, loss: 2.810e+00\n",
      "Iteration: 2470, loss: 2.151e+00\n",
      "Iteration: 2480, loss: 3.748e+00\n",
      "Iteration: 2490, loss: 3.286e+00\n",
      "Iteration: 2500, loss: 3.584e+00\n",
      "Iteration: 2510, loss: 1.349e+00\n",
      "Iteration: 2520, loss: 2.313e+00\n",
      "Iteration: 2530, loss: 2.401e+00\n",
      "Iteration: 2540, loss: 2.337e+00\n",
      "Iteration: 2550, loss: 4.481e+00\n",
      "Iteration: 2560, loss: 1.840e+00\n",
      "Iteration: 2570, loss: 3.759e+00\n",
      "Iteration: 2580, loss: 2.135e+00\n",
      "Iteration: 2590, loss: 1.723e+00\n",
      "Iteration: 2600, loss: 3.066e+00\n",
      "Iteration: 2610, loss: 2.318e+00\n",
      "Iteration: 2620, loss: 2.090e+00\n",
      "Iteration: 2630, loss: 1.591e+00\n",
      "Iteration: 2640, loss: 1.855e+00\n",
      "Iteration: 2650, loss: 2.427e+00\n",
      "Iteration: 2660, loss: 2.773e+00\n",
      "Iteration: 2670, loss: 2.493e+00\n",
      "Iteration: 2680, loss: 2.379e+00\n",
      "Iteration: 2690, loss: 1.430e+00\n",
      "Iteration: 2700, loss: 3.088e+00\n",
      "Iteration: 2710, loss: 2.960e+00\n",
      "Iteration: 2720, loss: 2.232e+00\n",
      "Iteration: 2730, loss: 1.838e+00\n",
      "Iteration: 2740, loss: 3.299e+00\n",
      "Iteration: 2750, loss: 1.355e+00\n",
      "Iteration: 2760, loss: 2.477e+00\n",
      "Iteration: 2770, loss: 2.472e+00\n",
      "Iteration: 2780, loss: 1.598e+00\n",
      "Iteration: 2790, loss: 1.659e+00\n",
      "Iteration: 2800, loss: 3.041e+00\n",
      "Iteration: 2810, loss: 1.803e+00\n",
      "Iteration: 2820, loss: 1.983e+00\n",
      "Iteration: 2830, loss: 1.206e+00\n",
      "Iteration: 2840, loss: 1.578e+00\n",
      "Iteration: 2850, loss: 1.579e+00\n",
      "Iteration: 2860, loss: 2.238e+00\n",
      "Iteration: 2870, loss: 1.120e+00\n",
      "Iteration: 2880, loss: 2.301e+00\n",
      "Iteration: 2890, loss: 2.914e+00\n",
      "Iteration: 2900, loss: 2.032e+00\n",
      "Iteration: 2910, loss: 1.284e+00\n",
      "Iteration: 2920, loss: 1.647e+00\n",
      "Iteration: 2930, loss: 2.715e+00\n",
      "Iteration: 2940, loss: 2.699e+00\n",
      "Iteration: 2950, loss: 1.220e+00\n",
      "Iteration: 2960, loss: 1.189e+00\n",
      "Iteration: 2970, loss: 1.139e+00\n",
      "Iteration: 2980, loss: 1.317e+00\n",
      "Iteration: 2990, loss: 2.120e+00\n",
      "Iteration: 3000, loss: 2.019e+00\n",
      "Iteration: 3010, loss: 2.308e+00\n",
      "Iteration: 3020, loss: 1.490e+00\n",
      "Iteration: 3030, loss: 2.628e+00\n",
      "Iteration: 3040, loss: 1.336e+00\n",
      "Iteration: 3050, loss: 2.533e+00\n",
      "Iteration: 3060, loss: 1.442e+00\n",
      "Iteration: 3070, loss: 1.110e+00\n",
      "Iteration: 3080, loss: 1.215e+00\n",
      "Iteration: 3090, loss: 1.433e+00\n",
      "Iteration: 3100, loss: 2.165e+00\n",
      "Iteration: 3110, loss: 1.573e+00\n",
      "Iteration: 3120, loss: 1.497e+00\n",
      "Iteration: 3130, loss: 1.164e+00\n",
      "Iteration: 3140, loss: 2.762e+00\n",
      "Iteration: 3150, loss: 1.520e+00\n",
      "Iteration: 3160, loss: 1.570e+00\n",
      "Iteration: 3170, loss: 1.792e+00\n",
      "Iteration: 3180, loss: 2.041e+00\n",
      "Iteration: 3190, loss: 1.351e+00\n",
      "Iteration: 3200, loss: 1.660e+00\n",
      "Iteration: 3210, loss: 1.696e+00\n",
      "Iteration: 3220, loss: 2.150e+00\n",
      "Iteration: 3230, loss: 2.514e+00\n",
      "Iteration: 3240, loss: 2.128e+00\n",
      "Iteration: 3250, loss: 1.265e+00\n",
      "Iteration: 3260, loss: 1.658e+00\n",
      "Iteration: 3270, loss: 1.243e+00\n",
      "Iteration: 3280, loss: 1.016e+00\n",
      "Iteration: 3290, loss: 1.249e+00\n",
      "Iteration: 3300, loss: 2.185e+00\n",
      "Iteration: 3310, loss: 1.760e+00\n",
      "Iteration: 3320, loss: 1.044e+00\n",
      "Iteration: 3330, loss: 7.330e-01\n",
      "Iteration: 3340, loss: 1.411e+00\n",
      "Iteration: 3350, loss: 2.311e+00\n",
      "Iteration: 3360, loss: 9.016e-01\n",
      "Iteration: 3370, loss: 1.201e+00\n",
      "Iteration: 3380, loss: 1.339e+00\n",
      "Iteration: 3390, loss: 1.599e+00\n",
      "Iteration: 3400, loss: 1.772e+00\n",
      "Iteration: 3410, loss: 1.036e+00\n",
      "Iteration: 3420, loss: 1.827e+00\n",
      "Iteration: 3430, loss: 1.131e+00\n",
      "Iteration: 3440, loss: 1.358e+00\n",
      "Iteration: 3450, loss: 1.741e+00\n",
      "Iteration: 3460, loss: 1.711e+00\n",
      "Iteration: 3470, loss: 2.227e+00\n",
      "Iteration: 3480, loss: 1.543e+00\n",
      "Iteration: 3490, loss: 1.179e+00\n",
      "Iteration: 3500, loss: 2.071e+00\n",
      "Iteration: 3510, loss: 2.320e+00\n",
      "Iteration: 3520, loss: 1.362e+00\n",
      "Iteration: 3530, loss: 1.154e+00\n",
      "Iteration: 3540, loss: 1.334e+00\n",
      "Iteration: 3550, loss: 8.288e-01\n",
      "Iteration: 3560, loss: 1.872e+00\n",
      "Iteration: 3570, loss: 1.164e+00\n",
      "Iteration: 3580, loss: 1.577e+00\n",
      "Iteration: 3590, loss: 1.246e+00\n",
      "Iteration: 3600, loss: 1.165e+00\n",
      "Iteration: 3610, loss: 1.055e+00\n",
      "Iteration: 3620, loss: 1.418e+00\n",
      "Iteration: 3630, loss: 1.309e+00\n",
      "Iteration: 3640, loss: 1.373e+00\n",
      "Iteration: 3650, loss: 1.886e+00\n",
      "Iteration: 3660, loss: 1.573e+00\n",
      "Iteration: 3670, loss: 8.261e-01\n",
      "Iteration: 3680, loss: 1.018e+00\n",
      "Iteration: 3690, loss: 1.452e+00\n",
      "Iteration: 3700, loss: 1.553e+00\n",
      "Iteration: 3710, loss: 1.469e+00\n",
      "Iteration: 3720, loss: 1.887e+00\n",
      "Iteration: 3730, loss: 1.463e+00\n",
      "Iteration: 3740, loss: 1.291e+00\n",
      "Iteration: 3750, loss: 1.274e+00\n",
      "Iteration: 3760, loss: 6.923e-01\n",
      "Iteration: 3770, loss: 1.051e+00\n",
      "Iteration: 3780, loss: 1.355e+00\n",
      "Iteration: 3790, loss: 8.724e-01\n",
      "Iteration: 3800, loss: 1.705e+00\n",
      "Iteration: 3810, loss: 8.793e-01\n",
      "Iteration: 3820, loss: 1.664e+00\n",
      "Iteration: 3830, loss: 1.013e+00\n",
      "Iteration: 3840, loss: 1.050e+00\n",
      "Iteration: 3850, loss: 7.964e-01\n",
      "Iteration: 3860, loss: 7.836e-01\n",
      "Iteration: 3870, loss: 5.865e-01\n",
      "Iteration: 3880, loss: 1.018e+00\n",
      "Iteration: 3890, loss: 1.652e+00\n",
      "Iteration: 3900, loss: 7.033e-01\n",
      "Iteration: 3910, loss: 9.040e-01\n",
      "Iteration: 3920, loss: 9.147e-01\n",
      "Iteration: 3930, loss: 6.650e-01\n",
      "Iteration: 3940, loss: 1.622e+00\n",
      "Iteration: 3950, loss: 8.865e-01\n",
      "Iteration: 3960, loss: 1.061e+00\n",
      "Iteration: 3970, loss: 1.331e+00\n",
      "Iteration: 3980, loss: 1.616e+00\n",
      "Iteration: 3990, loss: 9.008e-01\n",
      "Iteration: 4000, loss: 1.452e+00\n",
      "Iteration: 4010, loss: 1.230e+00\n",
      "Iteration: 4020, loss: 9.287e-01\n",
      "Iteration: 4030, loss: 7.769e-01\n",
      "Iteration: 4040, loss: 1.559e+00\n",
      "Iteration: 4050, loss: 1.088e+00\n",
      "Iteration: 4060, loss: 1.165e+00\n",
      "Iteration: 4070, loss: 1.126e+00\n",
      "Iteration: 4080, loss: 1.156e+00\n",
      "Iteration: 4090, loss: 8.202e-01\n",
      "Iteration: 4100, loss: 8.107e-01\n",
      "Iteration: 4110, loss: 1.078e+00\n",
      "Iteration: 4120, loss: 1.298e+00\n",
      "Iteration: 4130, loss: 1.484e+00\n",
      "Iteration: 4140, loss: 7.211e-01\n",
      "Iteration: 4150, loss: 1.272e+00\n",
      "Iteration: 4160, loss: 4.556e-01\n",
      "Iteration: 4170, loss: 1.352e+00\n",
      "Iteration: 4180, loss: 1.168e+00\n",
      "Iteration: 4190, loss: 7.680e-01\n",
      "Iteration: 4200, loss: 9.311e-01\n",
      "Iteration: 4210, loss: 8.534e-01\n",
      "Iteration: 4220, loss: 8.842e-01\n",
      "Iteration: 4230, loss: 7.777e-01\n",
      "Iteration: 4240, loss: 1.020e+00\n",
      "Iteration: 4250, loss: 6.014e-01\n",
      "Iteration: 4260, loss: 7.451e-01\n",
      "Iteration: 4270, loss: 6.433e-01\n",
      "Iteration: 4280, loss: 6.646e-01\n",
      "Iteration: 4290, loss: 1.401e+00\n",
      "Iteration: 4300, loss: 9.458e-01\n",
      "Iteration: 4310, loss: 6.419e-01\n",
      "Iteration: 4320, loss: 1.136e+00\n",
      "Iteration: 4330, loss: 1.108e+00\n",
      "Iteration: 4340, loss: 5.975e-01\n",
      "Iteration: 4350, loss: 7.676e-01\n",
      "Iteration: 4360, loss: 8.829e-01\n",
      "Iteration: 4370, loss: 1.091e+00\n",
      "Iteration: 4380, loss: 8.261e-01\n",
      "Iteration: 4390, loss: 1.020e+00\n",
      "Iteration: 4400, loss: 1.135e+00\n",
      "Iteration: 4410, loss: 1.297e+00\n",
      "Iteration: 4420, loss: 1.186e+00\n",
      "Iteration: 4430, loss: 7.883e-01\n",
      "Iteration: 4440, loss: 9.379e-01\n",
      "Iteration: 4450, loss: 7.301e-01\n",
      "Iteration: 4460, loss: 1.591e+00\n",
      "Iteration: 4470, loss: 1.025e+00\n",
      "Iteration: 4480, loss: 1.074e+00\n",
      "Iteration: 4490, loss: 1.411e+00\n",
      "Iteration: 4500, loss: 9.489e-01\n",
      "Iteration: 4510, loss: 9.791e-01\n",
      "Iteration: 4520, loss: 1.010e+00\n",
      "Iteration: 4530, loss: 1.187e+00\n",
      "Iteration: 4540, loss: 6.766e-01\n",
      "Iteration: 4550, loss: 7.944e-01\n",
      "Iteration: 4560, loss: 1.243e+00\n",
      "Iteration: 4570, loss: 9.824e-01\n",
      "Iteration: 4580, loss: 9.522e-01\n",
      "Iteration: 4590, loss: 1.238e+00\n",
      "Iteration: 4600, loss: 8.961e-01\n",
      "Iteration: 4610, loss: 1.112e+00\n",
      "Iteration: 4620, loss: 6.274e-01\n",
      "Iteration: 4630, loss: 9.302e-01\n",
      "Iteration: 4640, loss: 1.207e+00\n",
      "Iteration: 4650, loss: 1.260e+00\n",
      "Iteration: 4660, loss: 6.660e-01\n",
      "Iteration: 4670, loss: 1.424e+00\n",
      "Iteration: 4680, loss: 6.900e-01\n",
      "Iteration: 4690, loss: 3.838e-01\n",
      "Iteration: 4700, loss: 8.344e-01\n",
      "Iteration: 4710, loss: 7.383e-01\n",
      "Iteration: 4720, loss: 1.126e+00\n",
      "Iteration: 4730, loss: 5.786e-01\n",
      "Iteration: 4740, loss: 5.547e-01\n",
      "Iteration: 4750, loss: 7.596e-01\n",
      "Iteration: 4760, loss: 4.602e-01\n",
      "Iteration: 4770, loss: 7.219e-01\n",
      "Iteration: 4780, loss: 8.559e-01\n",
      "Iteration: 4790, loss: 5.666e-01\n",
      "Iteration: 4800, loss: 6.362e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4810, loss: 6.503e-01\n",
      "Iteration: 4820, loss: 7.091e-01\n",
      "Iteration: 4830, loss: 5.253e-01\n",
      "Iteration: 4840, loss: 1.024e+00\n",
      "Iteration: 4850, loss: 9.346e-01\n",
      "Iteration: 4860, loss: 7.102e-01\n",
      "Iteration: 4870, loss: 8.422e-01\n",
      "Iteration: 4880, loss: 8.194e-01\n",
      "Iteration: 4890, loss: 1.038e+00\n",
      "Iteration: 4900, loss: 9.654e-01\n",
      "Iteration: 4910, loss: 6.635e-01\n",
      "Iteration: 4920, loss: 9.437e-01\n",
      "Iteration: 4930, loss: 1.086e+00\n",
      "Iteration: 4940, loss: 6.493e-01\n",
      "Iteration: 4950, loss: 6.714e-01\n",
      "Iteration: 4960, loss: 6.052e-01\n",
      "Iteration: 4970, loss: 6.093e-01\n",
      "Iteration: 4980, loss: 7.091e-01\n",
      "Iteration: 4990, loss: 7.508e-01\n",
      "Iteration: 5000, loss: 8.290e-01\n",
      "Iteration: 5010, loss: 6.409e-01\n",
      "Iteration: 5020, loss: 7.857e-01\n",
      "Iteration: 5030, loss: 6.341e-01\n",
      "Iteration: 5040, loss: 5.820e-01\n",
      "Iteration: 5050, loss: 1.013e+00\n",
      "Iteration: 5060, loss: 7.522e-01\n",
      "Iteration: 5070, loss: 1.256e+00\n",
      "Iteration: 5080, loss: 6.513e-01\n",
      "Iteration: 5090, loss: 6.319e-01\n",
      "Iteration: 5100, loss: 5.261e-01\n",
      "Iteration: 5110, loss: 7.158e-01\n",
      "Iteration: 5120, loss: 4.964e-01\n",
      "Iteration: 5130, loss: 7.321e-01\n",
      "Iteration: 5140, loss: 7.267e-01\n",
      "Iteration: 5150, loss: 6.607e-01\n",
      "Iteration: 5160, loss: 4.155e-01\n",
      "Iteration: 5170, loss: 8.288e-01\n",
      "Iteration: 5180, loss: 6.905e-01\n",
      "Iteration: 5190, loss: 4.597e-01\n",
      "Iteration: 5200, loss: 1.023e+00\n",
      "Iteration: 5210, loss: 8.288e-01\n",
      "Iteration: 5220, loss: 5.235e-01\n",
      "Iteration: 5230, loss: 6.287e-01\n",
      "Iteration: 5240, loss: 5.173e-01\n",
      "Iteration: 5250, loss: 6.735e-01\n",
      "Iteration: 5260, loss: 5.697e-01\n",
      "Iteration: 5270, loss: 9.722e-01\n",
      "Iteration: 5280, loss: 5.241e-01\n",
      "Iteration: 5290, loss: 4.750e-01\n",
      "Iteration: 5300, loss: 9.180e-01\n",
      "Iteration: 5310, loss: 7.415e-01\n",
      "Iteration: 5320, loss: 9.156e-01\n",
      "Iteration: 5330, loss: 5.652e-01\n",
      "Iteration: 5340, loss: 6.227e-01\n",
      "Iteration: 5350, loss: 7.375e-01\n",
      "Iteration: 5360, loss: 5.510e-01\n",
      "Iteration: 5370, loss: 6.924e-01\n",
      "Iteration: 5380, loss: 8.653e-01\n",
      "Iteration: 5390, loss: 5.360e-01\n",
      "Iteration: 5400, loss: 6.234e-01\n",
      "Iteration: 5410, loss: 7.411e-01\n",
      "Iteration: 5420, loss: 4.793e-01\n",
      "Iteration: 5430, loss: 6.962e-01\n",
      "Iteration: 5440, loss: 7.277e-01\n",
      "Iteration: 5450, loss: 4.130e-01\n",
      "Iteration: 5460, loss: 6.273e-01\n",
      "Iteration: 5470, loss: 7.364e-01\n",
      "Iteration: 5480, loss: 7.482e-01\n",
      "Iteration: 5490, loss: 7.929e-01\n",
      "Iteration: 5500, loss: 4.735e-01\n",
      "Iteration: 5510, loss: 7.045e-01\n",
      "Iteration: 5520, loss: 4.579e-01\n",
      "Iteration: 5530, loss: 6.465e-01\n",
      "Iteration: 5540, loss: 5.606e-01\n",
      "Iteration: 5550, loss: 4.748e-01\n",
      "Iteration: 5560, loss: 4.714e-01\n",
      "Iteration: 5570, loss: 3.524e-01\n",
      "Iteration: 5580, loss: 6.341e-01\n",
      "Iteration: 5590, loss: 4.083e-01\n",
      "Iteration: 5600, loss: 5.242e-01\n",
      "Iteration: 5610, loss: 4.947e-01\n",
      "Iteration: 5620, loss: 3.857e-01\n",
      "Iteration: 5630, loss: 5.884e-01\n",
      "Iteration: 5640, loss: 6.607e-01\n",
      "Iteration: 5650, loss: 4.485e-01\n",
      "Iteration: 5660, loss: 6.515e-01\n",
      "Iteration: 5670, loss: 4.762e-01\n",
      "Iteration: 5680, loss: 7.642e-01\n",
      "Iteration: 5690, loss: 6.164e-01\n",
      "Iteration: 5700, loss: 3.090e-01\n",
      "Iteration: 5710, loss: 4.922e-01\n",
      "Iteration: 5720, loss: 3.768e-01\n",
      "Iteration: 5730, loss: 4.413e-01\n",
      "Iteration: 5740, loss: 8.439e-01\n",
      "Iteration: 5750, loss: 6.227e-01\n",
      "Iteration: 5760, loss: 3.846e-01\n",
      "Iteration: 5770, loss: 7.147e-01\n",
      "Iteration: 5780, loss: 4.322e-01\n",
      "Iteration: 5790, loss: 4.121e-01\n",
      "Iteration: 5800, loss: 7.413e-01\n",
      "Iteration: 5810, loss: 6.858e-01\n",
      "Iteration: 5820, loss: 5.537e-01\n",
      "Iteration: 5830, loss: 4.767e-01\n",
      "Iteration: 5840, loss: 5.478e-01\n",
      "Iteration: 5850, loss: 6.278e-01\n",
      "Iteration: 5860, loss: 4.954e-01\n",
      "Iteration: 5870, loss: 5.459e-01\n",
      "Iteration: 5880, loss: 6.487e-01\n",
      "Iteration: 5890, loss: 3.647e-01\n",
      "Iteration: 5900, loss: 4.911e-01\n",
      "Iteration: 5910, loss: 4.529e-01\n",
      "Iteration: 5920, loss: 4.032e-01\n",
      "Iteration: 5930, loss: 3.595e-01\n",
      "Iteration: 5940, loss: 6.442e-01\n",
      "Iteration: 5950, loss: 6.723e-01\n",
      "Iteration: 5960, loss: 4.624e-01\n",
      "Iteration: 5970, loss: 5.586e-01\n",
      "Iteration: 5980, loss: 4.939e-01\n",
      "Iteration: 5990, loss: 4.719e-01\n",
      "Iteration: 6000, loss: 5.610e-01\n",
      "Iteration: 6010, loss: 3.729e-01\n",
      "Iteration: 6020, loss: 6.857e-01\n",
      "Iteration: 6030, loss: 8.614e-01\n",
      "Iteration: 6040, loss: 4.357e-01\n",
      "Iteration: 6050, loss: 4.669e-01\n",
      "Iteration: 6060, loss: 3.727e-01\n",
      "Iteration: 6070, loss: 5.594e-01\n",
      "Iteration: 6080, loss: 4.817e-01\n",
      "Iteration: 6090, loss: 6.587e-01\n",
      "Iteration: 6100, loss: 3.951e-01\n",
      "Iteration: 6110, loss: 4.920e-01\n",
      "Iteration: 6120, loss: 5.571e-01\n",
      "Iteration: 6130, loss: 3.012e-01\n",
      "Iteration: 6140, loss: 8.197e-01\n",
      "Iteration: 6150, loss: 4.996e-01\n",
      "Iteration: 6160, loss: 4.132e-01\n",
      "Iteration: 6170, loss: 3.503e-01\n",
      "Iteration: 6180, loss: 4.780e-01\n",
      "Iteration: 6190, loss: 4.611e-01\n",
      "Iteration: 6200, loss: 4.055e-01\n",
      "Iteration: 6210, loss: 3.827e-01\n",
      "Iteration: 6220, loss: 7.031e-01\n",
      "Iteration: 6230, loss: 5.174e-01\n",
      "Iteration: 6240, loss: 4.177e-01\n",
      "Iteration: 6250, loss: 5.213e-01\n",
      "Iteration: 6260, loss: 4.683e-01\n",
      "Iteration: 6270, loss: 3.806e-01\n",
      "Iteration: 6280, loss: 4.736e-01\n",
      "Iteration: 6290, loss: 4.820e-01\n",
      "Iteration: 6300, loss: 3.666e-01\n",
      "Iteration: 6310, loss: 4.222e-01\n",
      "Iteration: 6320, loss: 6.203e-01\n",
      "Iteration: 6330, loss: 3.710e-01\n",
      "Iteration: 6340, loss: 3.843e-01\n",
      "Iteration: 6350, loss: 5.219e-01\n",
      "Iteration: 6360, loss: 4.081e-01\n",
      "Iteration: 6370, loss: 4.089e-01\n",
      "Iteration: 6380, loss: 5.620e-01\n",
      "Iteration: 6390, loss: 5.168e-01\n",
      "Iteration: 6400, loss: 4.070e-01\n",
      "Iteration: 6410, loss: 7.716e-01\n",
      "Iteration: 6420, loss: 3.022e-01\n",
      "Iteration: 6430, loss: 5.393e-01\n",
      "Iteration: 6440, loss: 5.238e-01\n",
      "Iteration: 6450, loss: 4.302e-01\n",
      "Iteration: 6460, loss: 3.529e-01\n",
      "Iteration: 6470, loss: 3.163e-01\n",
      "Iteration: 6480, loss: 3.969e-01\n",
      "Iteration: 6490, loss: 6.449e-01\n",
      "Iteration: 6500, loss: 3.968e-01\n",
      "Iteration: 6510, loss: 4.664e-01\n",
      "Iteration: 6520, loss: 4.179e-01\n",
      "Iteration: 6530, loss: 4.625e-01\n",
      "Iteration: 6540, loss: 3.389e-01\n",
      "Iteration: 6550, loss: 4.992e-01\n",
      "Iteration: 6560, loss: 3.058e-01\n",
      "Iteration: 6570, loss: 6.194e-01\n",
      "Iteration: 6580, loss: 3.368e-01\n",
      "Iteration: 6590, loss: 2.315e-01\n",
      "Iteration: 6600, loss: 3.022e-01\n",
      "Iteration: 6610, loss: 3.378e-01\n",
      "Iteration: 6620, loss: 5.222e-01\n",
      "Iteration: 6630, loss: 3.723e-01\n",
      "Iteration: 6640, loss: 2.958e-01\n",
      "Iteration: 6650, loss: 3.639e-01\n",
      "Iteration: 6660, loss: 4.408e-01\n",
      "Iteration: 6670, loss: 5.917e-01\n",
      "Iteration: 6680, loss: 4.582e-01\n",
      "Iteration: 6690, loss: 3.323e-01\n",
      "Iteration: 6700, loss: 2.340e-01\n",
      "Iteration: 6710, loss: 4.030e-01\n",
      "Iteration: 6720, loss: 3.901e-01\n",
      "Iteration: 6730, loss: 2.977e-01\n",
      "Iteration: 6740, loss: 3.084e-01\n",
      "Iteration: 6750, loss: 3.138e-01\n",
      "Iteration: 6760, loss: 4.717e-01\n",
      "Iteration: 6770, loss: 4.145e-01\n",
      "Iteration: 6780, loss: 1.905e-01\n",
      "Iteration: 6790, loss: 2.166e-01\n",
      "Iteration: 6800, loss: 4.180e-01\n",
      "Iteration: 6810, loss: 2.031e-01\n",
      "Iteration: 6820, loss: 4.155e-01\n",
      "Iteration: 6830, loss: 5.640e-01\n",
      "Iteration: 6840, loss: 4.045e-01\n",
      "Iteration: 6850, loss: 3.837e-01\n",
      "Iteration: 6860, loss: 4.664e-01\n",
      "Iteration: 6870, loss: 5.408e-01\n",
      "Iteration: 6880, loss: 5.843e-01\n",
      "Iteration: 6890, loss: 3.124e-01\n",
      "Iteration: 6900, loss: 2.795e-01\n",
      "Iteration: 6910, loss: 2.857e-01\n",
      "Iteration: 6920, loss: 2.881e-01\n",
      "Iteration: 6930, loss: 4.125e-01\n",
      "Iteration: 6940, loss: 3.654e-01\n",
      "Iteration: 6950, loss: 3.035e-01\n",
      "Iteration: 6960, loss: 2.409e-01\n",
      "Iteration: 6970, loss: 4.669e-01\n",
      "Iteration: 6980, loss: 2.862e-01\n",
      "Iteration: 6990, loss: 3.202e-01\n",
      "Iteration: 7000, loss: 3.282e-01\n",
      "Iteration: 7010, loss: 4.318e-01\n",
      "Iteration: 7020, loss: 3.374e-01\n",
      "Iteration: 7030, loss: 2.096e-01\n",
      "Iteration: 7040, loss: 1.657e-01\n",
      "Iteration: 7050, loss: 2.986e-01\n",
      "Iteration: 7060, loss: 4.084e-01\n",
      "Iteration: 7070, loss: 2.891e-01\n",
      "Iteration: 7080, loss: 2.616e-01\n",
      "Iteration: 7090, loss: 3.651e-01\n",
      "Iteration: 7100, loss: 3.855e-01\n",
      "Iteration: 7110, loss: 3.210e-01\n",
      "Iteration: 7120, loss: 2.537e-01\n",
      "Iteration: 7130, loss: 4.163e-01\n",
      "Iteration: 7140, loss: 2.330e-01\n",
      "Iteration: 7150, loss: 3.526e-01\n",
      "Iteration: 7160, loss: 2.530e-01\n",
      "Iteration: 7170, loss: 3.061e-01\n",
      "Iteration: 7180, loss: 3.665e-01\n",
      "Iteration: 7190, loss: 2.873e-01\n",
      "Iteration: 7200, loss: 2.478e-01\n",
      "Iteration: 7210, loss: 1.917e-01\n",
      "Iteration: 7220, loss: 2.766e-01\n",
      "Iteration: 7230, loss: 4.655e-01\n",
      "Iteration: 7240, loss: 1.724e-01\n",
      "Iteration: 7250, loss: 3.127e-01\n",
      "Iteration: 7260, loss: 2.276e-01\n",
      "Iteration: 7270, loss: 3.334e-01\n",
      "Iteration: 7280, loss: 4.025e-01\n",
      "Iteration: 7290, loss: 2.839e-01\n",
      "Iteration: 7300, loss: 2.883e-01\n",
      "Iteration: 7310, loss: 3.473e-01\n",
      "Iteration: 7320, loss: 3.048e-01\n",
      "Iteration: 7330, loss: 4.344e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7340, loss: 2.746e-01\n",
      "Iteration: 7350, loss: 2.052e-01\n",
      "Iteration: 7360, loss: 2.313e-01\n",
      "Iteration: 7370, loss: 4.549e-01\n",
      "Iteration: 7380, loss: 2.945e-01\n",
      "Iteration: 7390, loss: 2.577e-01\n",
      "Iteration: 7400, loss: 2.711e-01\n",
      "Iteration: 7410, loss: 4.081e-01\n",
      "Iteration: 7420, loss: 2.143e-01\n",
      "Iteration: 7430, loss: 3.488e-01\n",
      "Iteration: 7440, loss: 2.362e-01\n",
      "Iteration: 7450, loss: 3.911e-01\n",
      "Iteration: 7460, loss: 3.407e-01\n",
      "Iteration: 7470, loss: 1.761e-01\n",
      "Iteration: 7480, loss: 3.644e-01\n",
      "Iteration: 7490, loss: 3.160e-01\n",
      "Iteration: 7500, loss: 2.857e-01\n",
      "Iteration: 7510, loss: 3.414e-01\n",
      "Iteration: 7520, loss: 2.080e-01\n",
      "Iteration: 7530, loss: 3.106e-01\n",
      "Iteration: 7540, loss: 2.218e-01\n",
      "Iteration: 7550, loss: 1.747e-01\n",
      "Iteration: 7560, loss: 1.698e-01\n",
      "Iteration: 7570, loss: 3.018e-01\n",
      "Iteration: 7580, loss: 2.479e-01\n",
      "Iteration: 7590, loss: 2.857e-01\n",
      "Iteration: 7600, loss: 3.981e-01\n",
      "Iteration: 7610, loss: 4.133e-01\n",
      "Iteration: 7620, loss: 2.749e-01\n",
      "Iteration: 7630, loss: 4.032e-01\n",
      "Iteration: 7640, loss: 2.525e-01\n",
      "Iteration: 7650, loss: 2.276e-01\n",
      "Iteration: 7660, loss: 3.062e-01\n",
      "Iteration: 7670, loss: 3.875e-01\n",
      "Iteration: 7680, loss: 1.965e-01\n",
      "Iteration: 7690, loss: 2.151e-01\n",
      "Iteration: 7700, loss: 2.573e-01\n",
      "Iteration: 7710, loss: 1.755e-01\n",
      "Iteration: 7720, loss: 3.001e-01\n",
      "Iteration: 7730, loss: 4.046e-01\n",
      "Iteration: 7740, loss: 2.343e-01\n",
      "Iteration: 7750, loss: 3.086e-01\n",
      "Iteration: 7760, loss: 3.103e-01\n",
      "Iteration: 7770, loss: 2.705e-01\n",
      "Iteration: 7780, loss: 2.565e-01\n",
      "Iteration: 7790, loss: 3.259e-01\n",
      "Iteration: 7800, loss: 3.092e-01\n",
      "Iteration: 7810, loss: 2.527e-01\n",
      "Iteration: 7820, loss: 2.900e-01\n",
      "Iteration: 7830, loss: 2.222e-01\n",
      "Iteration: 7840, loss: 2.820e-01\n",
      "Iteration: 7850, loss: 3.217e-01\n",
      "Iteration: 7860, loss: 2.701e-01\n",
      "Iteration: 7870, loss: 3.118e-01\n",
      "Iteration: 7880, loss: 3.363e-01\n",
      "Iteration: 7890, loss: 2.420e-01\n",
      "Iteration: 7900, loss: 1.112e-01\n",
      "Iteration: 7910, loss: 2.738e-01\n",
      "Iteration: 7920, loss: 3.490e-01\n",
      "Iteration: 7930, loss: 1.564e-01\n",
      "Iteration: 7940, loss: 3.000e-01\n",
      "Iteration: 7950, loss: 2.268e-01\n",
      "Iteration: 7960, loss: 1.785e-01\n",
      "Iteration: 7970, loss: 1.653e-01\n",
      "Iteration: 7980, loss: 2.184e-01\n",
      "Iteration: 7990, loss: 2.575e-01\n",
      "Iteration: 8000, loss: 1.334e-01\n",
      "Iteration: 8010, loss: 1.177e-01\n",
      "Iteration: 8020, loss: 3.106e-01\n",
      "Iteration: 8030, loss: 1.516e-01\n",
      "Iteration: 8040, loss: 1.911e-01\n",
      "Iteration: 8050, loss: 1.451e-01\n",
      "Iteration: 8060, loss: 2.783e-01\n",
      "Iteration: 8070, loss: 2.778e-01\n",
      "Iteration: 8080, loss: 1.908e-01\n",
      "Iteration: 8090, loss: 1.158e-01\n",
      "Iteration: 8100, loss: 2.077e-01\n",
      "Iteration: 8110, loss: 2.228e-01\n",
      "Iteration: 8120, loss: 2.252e-01\n",
      "Iteration: 8130, loss: 1.917e-01\n",
      "Iteration: 8140, loss: 1.765e-01\n",
      "Iteration: 8150, loss: 1.828e-01\n",
      "Iteration: 8160, loss: 2.296e-01\n",
      "Iteration: 8170, loss: 2.286e-01\n",
      "Iteration: 8180, loss: 2.547e-01\n",
      "Iteration: 8190, loss: 1.789e-01\n",
      "Iteration: 8200, loss: 3.044e-01\n",
      "Iteration: 8210, loss: 1.505e-01\n",
      "Iteration: 8220, loss: 2.146e-01\n",
      "Iteration: 8230, loss: 2.347e-01\n",
      "Iteration: 8240, loss: 1.910e-01\n",
      "Iteration: 8250, loss: 2.059e-01\n",
      "Iteration: 8260, loss: 1.966e-01\n",
      "Iteration: 8270, loss: 2.711e-01\n",
      "Iteration: 8280, loss: 2.739e-01\n",
      "Iteration: 8290, loss: 2.010e-01\n",
      "Iteration: 8300, loss: 2.996e-01\n",
      "Iteration: 8310, loss: 2.186e-01\n",
      "Iteration: 8320, loss: 3.599e-01\n",
      "Iteration: 8330, loss: 3.107e-01\n",
      "Iteration: 8340, loss: 1.776e-01\n",
      "Iteration: 8350, loss: 2.532e-01\n",
      "Iteration: 8360, loss: 2.828e-01\n",
      "Iteration: 8370, loss: 1.667e-01\n",
      "Iteration: 8380, loss: 3.616e-01\n",
      "Iteration: 8390, loss: 2.674e-01\n",
      "Iteration: 8400, loss: 2.603e-01\n",
      "Iteration: 8410, loss: 2.916e-01\n",
      "Iteration: 8420, loss: 1.800e-01\n",
      "Iteration: 8430, loss: 2.996e-01\n",
      "Iteration: 8440, loss: 1.921e-01\n",
      "Iteration: 8450, loss: 2.293e-01\n",
      "Iteration: 8460, loss: 2.156e-01\n",
      "Iteration: 8470, loss: 1.793e-01\n",
      "Iteration: 8480, loss: 1.925e-01\n",
      "Iteration: 8490, loss: 1.264e-01\n",
      "Iteration: 8500, loss: 2.731e-01\n",
      "Iteration: 8510, loss: 1.672e-01\n",
      "Iteration: 8520, loss: 2.021e-01\n",
      "Iteration: 8530, loss: 2.291e-01\n",
      "Iteration: 8540, loss: 3.496e-01\n",
      "Iteration: 8550, loss: 1.594e-01\n",
      "Iteration: 8560, loss: 1.695e-01\n",
      "Iteration: 8570, loss: 1.688e-01\n",
      "Iteration: 8580, loss: 7.835e-02\n",
      "Iteration: 8590, loss: 1.335e-01\n",
      "Iteration: 8600, loss: 2.764e-01\n",
      "Iteration: 8610, loss: 2.024e-01\n",
      "Iteration: 8620, loss: 2.005e-01\n",
      "Iteration: 8630, loss: 1.386e-01\n",
      "Iteration: 8640, loss: 1.069e-01\n",
      "Iteration: 8650, loss: 2.482e-01\n",
      "Iteration: 8660, loss: 2.565e-01\n",
      "Iteration: 8670, loss: 2.042e-01\n",
      "Iteration: 8680, loss: 3.270e-01\n",
      "Iteration: 8690, loss: 1.586e-01\n",
      "Iteration: 8700, loss: 1.423e-01\n",
      "Iteration: 8710, loss: 2.112e-01\n",
      "Iteration: 8720, loss: 2.024e-01\n",
      "Iteration: 8730, loss: 2.147e-01\n",
      "Iteration: 8740, loss: 1.191e-01\n",
      "Iteration: 8750, loss: 1.426e-01\n",
      "Iteration: 8760, loss: 1.860e-01\n",
      "Iteration: 8770, loss: 2.549e-01\n",
      "Iteration: 8780, loss: 1.056e-01\n",
      "Iteration: 8790, loss: 8.558e-02\n",
      "Iteration: 8800, loss: 1.695e-01\n",
      "Iteration: 8810, loss: 2.031e-01\n",
      "Iteration: 8820, loss: 2.018e-01\n",
      "Iteration: 8830, loss: 1.575e-01\n",
      "Iteration: 8840, loss: 2.237e-01\n",
      "Iteration: 8850, loss: 2.054e-01\n",
      "Iteration: 8860, loss: 2.209e-01\n",
      "Iteration: 8870, loss: 2.230e-01\n",
      "Iteration: 8880, loss: 1.824e-01\n",
      "Iteration: 8890, loss: 2.257e-01\n",
      "Iteration: 8900, loss: 1.581e-01\n",
      "Iteration: 8910, loss: 1.736e-01\n",
      "Iteration: 8920, loss: 2.756e-01\n",
      "Iteration: 8930, loss: 1.122e-01\n",
      "Iteration: 8940, loss: 2.301e-01\n",
      "Iteration: 8950, loss: 2.769e-01\n",
      "Iteration: 8960, loss: 1.597e-01\n",
      "Iteration: 8970, loss: 1.894e-01\n",
      "Iteration: 8980, loss: 1.240e-01\n",
      "Iteration: 8990, loss: 1.392e-01\n",
      "Iteration: 9000, loss: 1.514e-01\n",
      "Iteration: 9010, loss: 1.017e-01\n",
      "Iteration: 9020, loss: 1.983e-01\n",
      "Iteration: 9030, loss: 1.280e-01\n",
      "Iteration: 9040, loss: 1.823e-01\n",
      "Iteration: 9050, loss: 1.847e-01\n",
      "Iteration: 9060, loss: 2.433e-01\n",
      "Iteration: 9070, loss: 1.602e-01\n",
      "Iteration: 9080, loss: 1.969e-01\n",
      "Iteration: 9090, loss: 2.203e-01\n",
      "Iteration: 9100, loss: 1.911e-01\n",
      "Iteration: 9110, loss: 1.851e-01\n",
      "Iteration: 9120, loss: 1.419e-01\n",
      "Iteration: 9130, loss: 1.614e-01\n",
      "Iteration: 9140, loss: 2.140e-01\n",
      "Iteration: 9150, loss: 1.357e-01\n",
      "Iteration: 9160, loss: 1.824e-01\n",
      "Iteration: 9170, loss: 1.736e-01\n",
      "Iteration: 9180, loss: 1.496e-01\n",
      "Iteration: 9190, loss: 1.942e-01\n",
      "Iteration: 9200, loss: 1.779e-01\n",
      "Iteration: 9210, loss: 1.081e-01\n",
      "Iteration: 9220, loss: 1.739e-01\n",
      "Iteration: 9230, loss: 1.285e-01\n",
      "Iteration: 9240, loss: 1.329e-01\n",
      "Iteration: 9250, loss: 1.808e-01\n",
      "Iteration: 9260, loss: 1.667e-01\n",
      "Iteration: 9270, loss: 1.371e-01\n",
      "Iteration: 9280, loss: 1.071e-01\n",
      "Iteration: 9290, loss: 2.127e-01\n",
      "Iteration: 9300, loss: 1.456e-01\n",
      "Iteration: 9310, loss: 1.237e-01\n",
      "Iteration: 9320, loss: 1.317e-01\n",
      "Iteration: 9330, loss: 1.928e-01\n",
      "Iteration: 9340, loss: 8.553e-02\n",
      "Iteration: 9350, loss: 1.794e-01\n",
      "Iteration: 9360, loss: 1.384e-01\n",
      "Iteration: 9370, loss: 2.149e-01\n",
      "Iteration: 9380, loss: 1.267e-01\n",
      "Iteration: 9390, loss: 2.367e-01\n",
      "Iteration: 9400, loss: 1.721e-01\n",
      "Iteration: 9410, loss: 1.652e-01\n",
      "Iteration: 9420, loss: 1.480e-01\n",
      "Iteration: 9430, loss: 1.764e-01\n",
      "Iteration: 9440, loss: 1.607e-01\n",
      "Iteration: 9450, loss: 1.919e-01\n",
      "Iteration: 9460, loss: 1.430e-01\n",
      "Iteration: 9470, loss: 1.428e-01\n",
      "Iteration: 9480, loss: 2.514e-01\n",
      "Iteration: 9490, loss: 1.345e-01\n",
      "Iteration: 9500, loss: 1.327e-01\n",
      "Iteration: 9510, loss: 8.566e-02\n",
      "Iteration: 9520, loss: 2.467e-01\n",
      "Iteration: 9530, loss: 1.044e-01\n",
      "Iteration: 9540, loss: 1.634e-01\n",
      "Iteration: 9550, loss: 1.284e-01\n",
      "Iteration: 9560, loss: 1.060e-01\n",
      "Iteration: 9570, loss: 5.298e-02\n",
      "Iteration: 9580, loss: 2.170e-01\n",
      "Iteration: 9590, loss: 9.255e-02\n",
      "Iteration: 9600, loss: 1.324e-01\n",
      "Iteration: 9610, loss: 1.180e-01\n",
      "Iteration: 9620, loss: 1.373e-01\n",
      "Iteration: 9630, loss: 1.242e-01\n",
      "Iteration: 9640, loss: 1.443e-01\n",
      "Iteration: 9650, loss: 1.498e-01\n",
      "Iteration: 9660, loss: 7.719e-02\n",
      "Iteration: 9670, loss: 9.483e-02\n",
      "Iteration: 9680, loss: 1.118e-01\n",
      "Iteration: 9690, loss: 2.566e-01\n",
      "Iteration: 9700, loss: 1.009e-01\n",
      "Iteration: 9710, loss: 1.050e-01\n",
      "Iteration: 9720, loss: 1.658e-01\n",
      "Iteration: 9730, loss: 1.431e-01\n",
      "Iteration: 9740, loss: 1.614e-01\n",
      "Iteration: 9750, loss: 7.104e-02\n",
      "Iteration: 9760, loss: 1.399e-01\n",
      "Iteration: 9770, loss: 1.335e-01\n",
      "Iteration: 9780, loss: 1.126e-01\n",
      "Iteration: 9790, loss: 2.052e-01\n",
      "Iteration: 9800, loss: 8.161e-02\n",
      "Iteration: 9810, loss: 1.243e-01\n",
      "Iteration: 9820, loss: 1.694e-01\n",
      "Iteration: 9830, loss: 1.084e-01\n",
      "Iteration: 9840, loss: 1.849e-01\n",
      "Iteration: 9850, loss: 1.172e-01\n",
      "Iteration: 9860, loss: 8.286e-02\n",
      "Iteration: 9870, loss: 1.130e-01\n",
      "Iteration: 9880, loss: 8.491e-02\n",
      "Iteration: 9890, loss: 1.241e-01\n",
      "Iteration: 9900, loss: 1.245e-01\n",
      "Iteration: 9910, loss: 1.656e-01\n",
      "Iteration: 9920, loss: 1.837e-01\n",
      "Iteration: 9930, loss: 1.184e-01\n",
      "Iteration: 9940, loss: 1.600e-01\n",
      "Iteration: 9950, loss: 1.352e-01\n",
      "Iteration: 9960, loss: 1.498e-01\n",
      "Iteration: 9970, loss: 9.331e-02\n",
      "Iteration: 9980, loss: 1.324e-01\n",
      "Iteration: 9990, loss: 9.831e-02\n",
      "Iteration: 10000, loss: 9.013e-02\n",
      "Iteration: 10010, loss: 1.347e-01\n",
      "Iteration: 10020, loss: 1.095e-01\n",
      "Iteration: 10030, loss: 7.874e-02\n",
      "Iteration: 10040, loss: 8.238e-02\n",
      "Iteration: 10050, loss: 1.584e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10060, loss: 1.343e-01\n",
      "Iteration: 10070, loss: 1.588e-01\n",
      "Iteration: 10080, loss: 1.612e-01\n",
      "Iteration: 10090, loss: 9.342e-02\n",
      "Iteration: 10100, loss: 8.801e-02\n",
      "Iteration: 10110, loss: 1.347e-01\n",
      "Iteration: 10120, loss: 1.662e-01\n",
      "Iteration: 10130, loss: 9.018e-02\n",
      "Iteration: 10140, loss: 1.762e-01\n",
      "Iteration: 10150, loss: 1.387e-01\n",
      "Iteration: 10160, loss: 1.037e-01\n",
      "Iteration: 10170, loss: 1.365e-01\n",
      "Iteration: 10180, loss: 9.678e-02\n",
      "Iteration: 10190, loss: 1.352e-01\n",
      "Iteration: 10200, loss: 1.833e-01\n",
      "Iteration: 10210, loss: 1.237e-01\n",
      "Iteration: 10220, loss: 1.888e-01\n",
      "Iteration: 10230, loss: 1.184e-01\n",
      "Iteration: 10240, loss: 9.253e-02\n",
      "Iteration: 10250, loss: 1.506e-01\n",
      "Iteration: 10260, loss: 1.046e-01\n",
      "Iteration: 10270, loss: 1.486e-01\n",
      "Iteration: 10280, loss: 1.606e-01\n",
      "Iteration: 10290, loss: 1.279e-01\n",
      "Iteration: 10300, loss: 1.198e-01\n",
      "Iteration: 10310, loss: 8.054e-02\n",
      "Iteration: 10320, loss: 1.607e-01\n",
      "Iteration: 10330, loss: 1.863e-01\n",
      "Iteration: 10340, loss: 1.078e-01\n",
      "Iteration: 10350, loss: 1.194e-01\n",
      "Iteration: 10360, loss: 1.522e-01\n",
      "Iteration: 10370, loss: 8.817e-02\n",
      "Iteration: 10380, loss: 1.280e-01\n",
      "Iteration: 10390, loss: 1.296e-01\n",
      "Iteration: 10400, loss: 7.628e-02\n",
      "Iteration: 10410, loss: 1.248e-01\n",
      "Iteration: 10420, loss: 9.698e-02\n",
      "Iteration: 10430, loss: 1.321e-01\n",
      "Iteration: 10440, loss: 1.379e-01\n",
      "Iteration: 10450, loss: 6.337e-02\n",
      "Iteration: 10460, loss: 1.464e-01\n",
      "Iteration: 10470, loss: 1.373e-01\n",
      "Iteration: 10480, loss: 1.014e-01\n",
      "Iteration: 10490, loss: 6.817e-02\n",
      "Iteration: 10500, loss: 1.240e-01\n",
      "Iteration: 10510, loss: 1.523e-01\n",
      "Iteration: 10520, loss: 7.592e-02\n",
      "Iteration: 10530, loss: 1.017e-01\n",
      "Iteration: 10540, loss: 1.147e-01\n",
      "Iteration: 10550, loss: 1.218e-01\n",
      "Iteration: 10560, loss: 1.462e-01\n",
      "Iteration: 10570, loss: 1.272e-01\n",
      "Iteration: 10580, loss: 1.175e-01\n",
      "Iteration: 10590, loss: 1.355e-01\n",
      "Iteration: 10600, loss: 1.194e-01\n",
      "Iteration: 10610, loss: 9.704e-02\n",
      "Iteration: 10620, loss: 9.146e-02\n",
      "Iteration: 10630, loss: 1.383e-01\n",
      "Iteration: 10640, loss: 1.088e-01\n",
      "Iteration: 10650, loss: 1.518e-01\n",
      "Iteration: 10660, loss: 1.139e-01\n",
      "Iteration: 10670, loss: 9.784e-02\n",
      "Iteration: 10680, loss: 8.428e-02\n",
      "Iteration: 10690, loss: 1.150e-01\n",
      "Iteration: 10700, loss: 1.083e-01\n",
      "Iteration: 10710, loss: 7.316e-02\n",
      "Iteration: 10720, loss: 8.131e-02\n",
      "Iteration: 10730, loss: 5.252e-02\n",
      "Iteration: 10740, loss: 1.020e-01\n",
      "Iteration: 10750, loss: 4.943e-02\n",
      "Iteration: 10760, loss: 1.080e-01\n",
      "Iteration: 10770, loss: 7.901e-02\n",
      "Iteration: 10780, loss: 1.204e-01\n",
      "Iteration: 10790, loss: 1.313e-01\n",
      "Iteration: 10800, loss: 8.181e-02\n",
      "Iteration: 10810, loss: 8.227e-02\n",
      "Iteration: 10820, loss: 8.925e-02\n",
      "Iteration: 10830, loss: 9.407e-02\n",
      "Iteration: 10840, loss: 1.427e-01\n",
      "Iteration: 10850, loss: 4.499e-02\n",
      "Iteration: 10860, loss: 8.759e-02\n",
      "Iteration: 10870, loss: 1.542e-01\n",
      "Iteration: 10880, loss: 1.371e-01\n",
      "Iteration: 10890, loss: 9.889e-02\n",
      "Iteration: 10900, loss: 4.121e-02\n",
      "Iteration: 10910, loss: 8.839e-02\n",
      "Iteration: 10920, loss: 6.486e-02\n",
      "Iteration: 10930, loss: 1.086e-01\n",
      "Iteration: 10940, loss: 9.893e-02\n",
      "Iteration: 10950, loss: 1.095e-01\n",
      "Iteration: 10960, loss: 8.180e-02\n",
      "Iteration: 10970, loss: 8.970e-02\n",
      "Iteration: 10980, loss: 7.348e-02\n",
      "Iteration: 10990, loss: 8.318e-02\n",
      "Iteration: 11000, loss: 5.909e-02\n",
      "Iteration: 11010, loss: 1.331e-01\n",
      "Iteration: 11020, loss: 7.114e-02\n",
      "Iteration: 11030, loss: 8.256e-02\n",
      "Iteration: 11040, loss: 9.300e-02\n",
      "Iteration: 11050, loss: 1.114e-01\n",
      "Iteration: 11060, loss: 5.787e-02\n",
      "Iteration: 11070, loss: 1.364e-01\n",
      "Iteration: 11080, loss: 1.083e-01\n",
      "Iteration: 11090, loss: 1.457e-01\n",
      "Iteration: 11100, loss: 7.602e-02\n",
      "Iteration: 11110, loss: 6.426e-02\n",
      "Iteration: 11120, loss: 9.816e-02\n",
      "Iteration: 11130, loss: 1.017e-01\n",
      "Iteration: 11140, loss: 1.228e-01\n",
      "Iteration: 11150, loss: 8.284e-02\n",
      "Iteration: 11160, loss: 4.833e-02\n",
      "Iteration: 11170, loss: 8.634e-02\n",
      "Iteration: 11180, loss: 9.790e-02\n",
      "Iteration: 11190, loss: 8.882e-02\n",
      "Iteration: 11200, loss: 1.022e-01\n",
      "Iteration: 11210, loss: 9.371e-02\n",
      "Iteration: 11220, loss: 8.356e-02\n",
      "Iteration: 11230, loss: 5.451e-02\n",
      "Iteration: 11240, loss: 1.079e-01\n",
      "Iteration: 11250, loss: 9.449e-02\n",
      "Iteration: 11260, loss: 9.939e-02\n",
      "Iteration: 11270, loss: 8.096e-02\n",
      "Iteration: 11280, loss: 5.863e-02\n",
      "Iteration: 11290, loss: 6.364e-02\n",
      "Iteration: 11300, loss: 7.616e-02\n",
      "Iteration: 11310, loss: 1.104e-01\n",
      "Iteration: 11320, loss: 1.167e-01\n",
      "Iteration: 11330, loss: 9.259e-02\n",
      "Iteration: 11340, loss: 7.480e-02\n",
      "Iteration: 11350, loss: 7.217e-02\n",
      "Iteration: 11360, loss: 6.130e-02\n",
      "Iteration: 11370, loss: 9.899e-02\n",
      "Iteration: 11380, loss: 9.897e-02\n",
      "Iteration: 11390, loss: 5.911e-02\n",
      "Iteration: 11400, loss: 1.440e-01\n",
      "Iteration: 11410, loss: 5.700e-02\n",
      "Iteration: 11420, loss: 6.737e-02\n",
      "Iteration: 11430, loss: 6.528e-02\n",
      "Iteration: 11440, loss: 8.468e-02\n",
      "Iteration: 11450, loss: 7.484e-02\n",
      "Iteration: 11460, loss: 5.358e-02\n",
      "Iteration: 11470, loss: 5.968e-02\n",
      "Iteration: 11480, loss: 7.567e-02\n",
      "Iteration: 11490, loss: 8.106e-02\n",
      "Iteration: 11500, loss: 4.178e-02\n",
      "Iteration: 11510, loss: 1.256e-01\n",
      "Iteration: 11520, loss: 6.078e-02\n",
      "Iteration: 11530, loss: 7.910e-02\n",
      "Iteration: 11540, loss: 5.886e-02\n",
      "Iteration: 11550, loss: 6.953e-02\n",
      "Iteration: 11560, loss: 6.504e-02\n",
      "Iteration: 11570, loss: 9.205e-02\n",
      "Iteration: 11580, loss: 8.496e-02\n",
      "Iteration: 11590, loss: 8.974e-02\n",
      "Iteration: 11600, loss: 1.023e-01\n",
      "Iteration: 11610, loss: 7.199e-02\n",
      "Iteration: 11620, loss: 7.998e-02\n",
      "Iteration: 11630, loss: 8.484e-02\n",
      "Iteration: 11640, loss: 9.075e-02\n",
      "Iteration: 11650, loss: 5.949e-02\n",
      "Iteration: 11660, loss: 5.301e-02\n",
      "Iteration: 11670, loss: 1.043e-01\n",
      "Iteration: 11680, loss: 6.193e-02\n",
      "Iteration: 11690, loss: 5.636e-02\n",
      "Iteration: 11700, loss: 7.529e-02\n",
      "Iteration: 11710, loss: 9.465e-02\n",
      "Iteration: 11720, loss: 9.528e-02\n",
      "Iteration: 11730, loss: 7.303e-02\n",
      "Iteration: 11740, loss: 6.285e-02\n",
      "Iteration: 11750, loss: 6.155e-02\n",
      "Iteration: 11760, loss: 7.794e-02\n",
      "Iteration: 11770, loss: 9.388e-02\n",
      "Iteration: 11780, loss: 3.448e-02\n",
      "Iteration: 11790, loss: 8.899e-02\n",
      "Iteration: 11800, loss: 5.691e-02\n",
      "Iteration: 11810, loss: 6.634e-02\n",
      "Iteration: 11820, loss: 8.903e-02\n",
      "Iteration: 11830, loss: 6.916e-02\n",
      "Iteration: 11840, loss: 6.422e-02\n",
      "Iteration: 11850, loss: 6.031e-02\n",
      "Iteration: 11860, loss: 6.539e-02\n",
      "Iteration: 11870, loss: 6.097e-02\n",
      "Iteration: 11880, loss: 5.359e-02\n",
      "Iteration: 11890, loss: 6.680e-02\n",
      "Iteration: 11900, loss: 8.785e-02\n",
      "Iteration: 11910, loss: 6.580e-02\n",
      "Iteration: 11920, loss: 7.308e-02\n",
      "Iteration: 11930, loss: 4.307e-02\n",
      "Iteration: 11940, loss: 6.151e-02\n",
      "Iteration: 11950, loss: 5.050e-02\n",
      "Iteration: 11960, loss: 5.181e-02\n",
      "Iteration: 11970, loss: 6.668e-02\n",
      "Iteration: 11980, loss: 8.089e-02\n",
      "Iteration: 11990, loss: 6.570e-02\n",
      "Iteration: 12000, loss: 7.211e-02\n",
      "Iteration: 12010, loss: 6.279e-02\n",
      "Iteration: 12020, loss: 4.701e-02\n",
      "Iteration: 12030, loss: 5.569e-02\n",
      "Iteration: 12040, loss: 6.758e-02\n",
      "Iteration: 12050, loss: 7.285e-02\n",
      "Iteration: 12060, loss: 7.119e-02\n",
      "Iteration: 12070, loss: 5.728e-02\n",
      "Iteration: 12080, loss: 8.936e-02\n",
      "Iteration: 12090, loss: 8.756e-02\n",
      "Iteration: 12100, loss: 7.904e-02\n",
      "Iteration: 12110, loss: 7.502e-02\n",
      "Iteration: 12120, loss: 6.378e-02\n",
      "Iteration: 12130, loss: 4.083e-02\n",
      "Iteration: 12140, loss: 6.495e-02\n",
      "Iteration: 12150, loss: 7.362e-02\n",
      "Iteration: 12160, loss: 6.730e-02\n",
      "Iteration: 12170, loss: 6.602e-02\n",
      "Iteration: 12180, loss: 7.609e-02\n",
      "Iteration: 12190, loss: 4.921e-02\n",
      "Iteration: 12200, loss: 1.001e-01\n",
      "Iteration: 12210, loss: 8.657e-02\n",
      "Iteration: 12220, loss: 1.153e-01\n",
      "Iteration: 12230, loss: 4.388e-02\n",
      "Iteration: 12240, loss: 6.088e-02\n",
      "Iteration: 12250, loss: 7.139e-02\n",
      "Iteration: 12260, loss: 6.967e-02\n",
      "Iteration: 12270, loss: 7.135e-02\n",
      "Iteration: 12280, loss: 6.293e-02\n",
      "Iteration: 12290, loss: 7.840e-02\n",
      "Iteration: 12300, loss: 7.676e-02\n",
      "Iteration: 12310, loss: 7.102e-02\n",
      "Iteration: 12320, loss: 5.535e-02\n",
      "Iteration: 12330, loss: 6.988e-02\n",
      "Iteration: 12340, loss: 9.199e-02\n",
      "Iteration: 12350, loss: 6.792e-02\n",
      "Iteration: 12360, loss: 9.015e-02\n",
      "Iteration: 12370, loss: 9.490e-02\n",
      "Iteration: 12380, loss: 6.642e-02\n",
      "Iteration: 12390, loss: 4.470e-02\n",
      "Iteration: 12400, loss: 6.498e-02\n",
      "Iteration: 12410, loss: 6.371e-02\n",
      "Iteration: 12420, loss: 4.994e-02\n",
      "Iteration: 12430, loss: 8.045e-02\n",
      "Iteration: 12440, loss: 3.964e-02\n",
      "Iteration: 12450, loss: 7.705e-02\n",
      "Iteration: 12460, loss: 7.723e-02\n",
      "Iteration: 12470, loss: 7.334e-02\n",
      "Iteration: 12480, loss: 7.993e-02\n",
      "Iteration: 12490, loss: 5.047e-02\n",
      "Iteration: 12500, loss: 4.823e-02\n",
      "Iteration: 12510, loss: 4.427e-02\n",
      "Iteration: 12520, loss: 5.685e-02\n",
      "Iteration: 12530, loss: 7.331e-02\n",
      "Iteration: 12540, loss: 3.977e-02\n",
      "Iteration: 12550, loss: 8.275e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12560, loss: 5.397e-02\n",
      "Iteration: 12570, loss: 3.526e-02\n",
      "Iteration: 12580, loss: 4.651e-02\n",
      "Iteration: 12590, loss: 3.695e-02\n",
      "Iteration: 12600, loss: 5.499e-02\n",
      "Iteration: 12610, loss: 4.998e-02\n",
      "Iteration: 12620, loss: 4.019e-02\n",
      "Iteration: 12630, loss: 6.729e-02\n",
      "Iteration: 12640, loss: 4.216e-02\n",
      "Iteration: 12650, loss: 7.228e-02\n",
      "Iteration: 12660, loss: 7.537e-02\n",
      "Iteration: 12670, loss: 9.332e-02\n",
      "Iteration: 12680, loss: 6.661e-02\n",
      "Iteration: 12690, loss: 2.633e-02\n",
      "Iteration: 12700, loss: 5.249e-02\n",
      "Iteration: 12710, loss: 5.444e-02\n",
      "Iteration: 12720, loss: 6.862e-02\n",
      "Iteration: 12730, loss: 6.556e-02\n",
      "Iteration: 12740, loss: 4.330e-02\n",
      "Iteration: 12750, loss: 3.663e-02\n",
      "Iteration: 12760, loss: 5.493e-02\n",
      "Iteration: 12770, loss: 5.915e-02\n",
      "Iteration: 12780, loss: 6.842e-02\n",
      "Iteration: 12790, loss: 9.440e-02\n",
      "Iteration: 12800, loss: 7.732e-02\n",
      "Iteration: 12810, loss: 3.420e-02\n",
      "Iteration: 12820, loss: 8.919e-02\n",
      "Iteration: 12830, loss: 5.961e-02\n",
      "Iteration: 12840, loss: 4.677e-02\n",
      "Iteration: 12850, loss: 8.223e-02\n",
      "Iteration: 12860, loss: 7.199e-02\n",
      "Iteration: 12870, loss: 6.430e-02\n",
      "Iteration: 12880, loss: 4.807e-02\n",
      "Iteration: 12890, loss: 4.130e-02\n",
      "Iteration: 12900, loss: 5.694e-02\n",
      "Iteration: 12910, loss: 7.747e-02\n",
      "Iteration: 12920, loss: 5.680e-02\n",
      "Iteration: 12930, loss: 6.368e-02\n",
      "Iteration: 12940, loss: 5.408e-02\n",
      "Iteration: 12950, loss: 4.636e-02\n",
      "Iteration: 12960, loss: 3.011e-02\n",
      "Iteration: 12970, loss: 6.416e-02\n",
      "Iteration: 12980, loss: 6.508e-02\n",
      "Iteration: 12990, loss: 4.863e-02\n",
      "Iteration: 13000, loss: 6.569e-02\n",
      "Iteration: 13010, loss: 4.330e-02\n",
      "Iteration: 13020, loss: 7.557e-02\n",
      "Iteration: 13030, loss: 4.272e-02\n",
      "Iteration: 13040, loss: 5.756e-02\n",
      "Iteration: 13050, loss: 2.195e-02\n",
      "Iteration: 13060, loss: 3.885e-02\n",
      "Iteration: 13070, loss: 7.219e-02\n",
      "Iteration: 13080, loss: 5.589e-02\n",
      "Iteration: 13090, loss: 6.037e-02\n",
      "Iteration: 13100, loss: 3.287e-02\n",
      "Iteration: 13110, loss: 3.756e-02\n",
      "Iteration: 13120, loss: 8.022e-02\n",
      "Iteration: 13130, loss: 5.529e-02\n",
      "Iteration: 13140, loss: 5.744e-02\n",
      "Iteration: 13150, loss: 4.836e-02\n",
      "Iteration: 13160, loss: 5.169e-02\n",
      "Iteration: 13170, loss: 4.899e-02\n",
      "Iteration: 13180, loss: 5.506e-02\n",
      "Iteration: 13190, loss: 5.667e-02\n",
      "Iteration: 13200, loss: 6.159e-02\n",
      "Iteration: 13210, loss: 4.572e-02\n",
      "Iteration: 13220, loss: 4.835e-02\n",
      "Iteration: 13230, loss: 4.450e-02\n",
      "Iteration: 13240, loss: 3.357e-02\n",
      "Iteration: 13250, loss: 3.663e-02\n",
      "Iteration: 13260, loss: 3.507e-02\n",
      "Iteration: 13270, loss: 6.386e-02\n",
      "Iteration: 13280, loss: 5.036e-02\n",
      "Iteration: 13290, loss: 8.346e-02\n",
      "Iteration: 13300, loss: 6.324e-02\n",
      "Iteration: 13310, loss: 3.750e-02\n",
      "Iteration: 13320, loss: 5.684e-02\n",
      "Iteration: 13330, loss: 5.757e-02\n",
      "Iteration: 13340, loss: 5.811e-02\n",
      "Iteration: 13350, loss: 5.357e-02\n",
      "Iteration: 13360, loss: 2.462e-02\n",
      "Iteration: 13370, loss: 5.389e-02\n",
      "Iteration: 13380, loss: 8.012e-02\n",
      "Iteration: 13390, loss: 4.108e-02\n",
      "Iteration: 13400, loss: 3.136e-02\n",
      "Iteration: 13410, loss: 5.792e-02\n",
      "Iteration: 13420, loss: 6.064e-02\n",
      "Iteration: 13430, loss: 4.894e-02\n",
      "Iteration: 13440, loss: 4.914e-02\n",
      "Iteration: 13450, loss: 5.926e-02\n",
      "Iteration: 13460, loss: 5.845e-02\n",
      "Iteration: 13470, loss: 4.948e-02\n",
      "Iteration: 13480, loss: 4.354e-02\n",
      "Iteration: 13490, loss: 3.370e-02\n",
      "Iteration: 13500, loss: 2.983e-02\n",
      "Iteration: 13510, loss: 3.132e-02\n",
      "Iteration: 13520, loss: 4.602e-02\n",
      "Iteration: 13530, loss: 5.560e-02\n",
      "Iteration: 13540, loss: 1.543e-02\n",
      "Iteration: 13550, loss: 4.004e-02\n",
      "Iteration: 13560, loss: 4.021e-02\n",
      "Iteration: 13570, loss: 3.741e-02\n",
      "Iteration: 13580, loss: 9.081e-02\n",
      "Iteration: 13590, loss: 4.129e-02\n",
      "Iteration: 13600, loss: 2.219e-02\n",
      "Iteration: 13610, loss: 4.342e-02\n",
      "Iteration: 13620, loss: 8.993e-02\n",
      "Iteration: 13630, loss: 6.248e-02\n",
      "Iteration: 13640, loss: 2.924e-02\n",
      "Iteration: 13650, loss: 5.835e-02\n",
      "Iteration: 13660, loss: 4.389e-02\n",
      "Iteration: 13670, loss: 4.265e-02\n",
      "Iteration: 13680, loss: 4.645e-02\n",
      "Iteration: 13690, loss: 3.878e-02\n",
      "Iteration: 13700, loss: 4.175e-02\n",
      "Iteration: 13710, loss: 3.233e-02\n",
      "Iteration: 13720, loss: 3.223e-02\n",
      "Iteration: 13730, loss: 4.929e-02\n",
      "Iteration: 13740, loss: 5.577e-02\n",
      "Iteration: 13750, loss: 3.916e-02\n",
      "Iteration: 13760, loss: 4.026e-02\n",
      "Iteration: 13770, loss: 2.391e-02\n",
      "Iteration: 13780, loss: 2.483e-02\n",
      "Iteration: 13790, loss: 3.843e-02\n",
      "Iteration: 13800, loss: 5.490e-02\n",
      "Iteration: 13810, loss: 3.291e-02\n",
      "Iteration: 13820, loss: 2.909e-02\n",
      "Iteration: 13830, loss: 3.670e-02\n",
      "Iteration: 13840, loss: 3.820e-02\n",
      "Iteration: 13850, loss: 4.792e-02\n",
      "Iteration: 13860, loss: 4.737e-02\n",
      "Iteration: 13870, loss: 7.042e-02\n",
      "Iteration: 13880, loss: 6.149e-02\n",
      "Iteration: 13890, loss: 2.697e-02\n",
      "Iteration: 13900, loss: 4.829e-02\n",
      "Iteration: 13910, loss: 3.815e-02\n",
      "Iteration: 13920, loss: 4.628e-02\n",
      "Iteration: 13930, loss: 4.796e-02\n",
      "Iteration: 13940, loss: 2.817e-02\n",
      "Iteration: 13950, loss: 3.686e-02\n",
      "Iteration: 13960, loss: 3.837e-02\n",
      "Iteration: 13970, loss: 5.224e-02\n",
      "Iteration: 13980, loss: 3.941e-02\n",
      "Iteration: 13990, loss: 3.899e-02\n",
      "Iteration: 14000, loss: 4.385e-02\n",
      "Iteration: 14010, loss: 4.382e-02\n",
      "Iteration: 14020, loss: 2.860e-02\n",
      "Iteration: 14030, loss: 3.909e-02\n",
      "Iteration: 14040, loss: 4.257e-02\n",
      "Iteration: 14050, loss: 4.499e-02\n",
      "Iteration: 14060, loss: 3.078e-02\n",
      "Iteration: 14070, loss: 4.331e-02\n",
      "Iteration: 14080, loss: 5.288e-02\n",
      "Iteration: 14090, loss: 4.836e-02\n",
      "Iteration: 14100, loss: 3.660e-02\n",
      "Iteration: 14110, loss: 2.930e-02\n",
      "Iteration: 14120, loss: 3.897e-02\n",
      "Iteration: 14130, loss: 2.797e-02\n",
      "Iteration: 14140, loss: 4.176e-02\n",
      "Iteration: 14150, loss: 3.950e-02\n",
      "Iteration: 14160, loss: 4.235e-02\n",
      "Iteration: 14170, loss: 2.869e-02\n",
      "Iteration: 14180, loss: 3.589e-02\n",
      "Iteration: 14190, loss: 2.505e-02\n",
      "Iteration: 14200, loss: 2.930e-02\n",
      "Iteration: 14210, loss: 2.279e-02\n",
      "Iteration: 14220, loss: 4.029e-02\n",
      "Iteration: 14230, loss: 3.028e-02\n",
      "Iteration: 14240, loss: 4.675e-02\n",
      "Iteration: 14250, loss: 4.849e-02\n",
      "Iteration: 14260, loss: 2.973e-02\n",
      "Iteration: 14270, loss: 2.308e-02\n",
      "Iteration: 14280, loss: 2.574e-02\n",
      "Iteration: 14290, loss: 5.387e-02\n",
      "Iteration: 14300, loss: 4.093e-02\n",
      "Iteration: 14310, loss: 3.305e-02\n",
      "Iteration: 14320, loss: 3.725e-02\n",
      "Iteration: 14330, loss: 4.081e-02\n",
      "Iteration: 14340, loss: 3.482e-02\n",
      "Iteration: 14350, loss: 4.790e-02\n",
      "Iteration: 14360, loss: 4.284e-02\n",
      "Iteration: 14370, loss: 1.951e-02\n",
      "Iteration: 14380, loss: 4.454e-02\n",
      "Iteration: 14390, loss: 2.651e-02\n",
      "Iteration: 14400, loss: 2.560e-02\n",
      "Iteration: 14410, loss: 2.170e-02\n",
      "Iteration: 14420, loss: 1.371e-02\n",
      "Iteration: 14430, loss: 2.974e-02\n",
      "Iteration: 14440, loss: 3.359e-02\n",
      "Iteration: 14450, loss: 4.030e-02\n",
      "Iteration: 14460, loss: 3.044e-02\n",
      "Iteration: 14470, loss: 2.659e-02\n",
      "Iteration: 14480, loss: 3.636e-02\n",
      "Iteration: 14490, loss: 3.777e-02\n",
      "Iteration: 14500, loss: 3.322e-02\n",
      "Iteration: 14510, loss: 3.520e-02\n",
      "Iteration: 14520, loss: 4.185e-02\n",
      "Iteration: 14530, loss: 2.506e-02\n",
      "Iteration: 14540, loss: 2.223e-02\n",
      "Iteration: 14550, loss: 2.618e-02\n",
      "Iteration: 14560, loss: 2.901e-02\n",
      "Iteration: 14570, loss: 3.423e-02\n",
      "Iteration: 14580, loss: 3.396e-02\n",
      "Iteration: 14590, loss: 1.887e-02\n",
      "Iteration: 14600, loss: 2.969e-02\n",
      "Iteration: 14610, loss: 2.603e-02\n",
      "Iteration: 14620, loss: 2.183e-02\n",
      "Iteration: 14630, loss: 2.276e-02\n",
      "Iteration: 14640, loss: 2.705e-02\n",
      "Iteration: 14650, loss: 3.220e-02\n",
      "Iteration: 14660, loss: 3.000e-02\n",
      "Iteration: 14670, loss: 2.307e-02\n",
      "Iteration: 14680, loss: 4.109e-02\n",
      "Iteration: 14690, loss: 4.497e-02\n",
      "Iteration: 14700, loss: 3.405e-02\n",
      "Iteration: 14710, loss: 2.153e-02\n",
      "Iteration: 14720, loss: 2.932e-02\n",
      "Iteration: 14730, loss: 5.140e-02\n",
      "Iteration: 14740, loss: 3.038e-02\n",
      "Iteration: 14750, loss: 2.240e-02\n",
      "Iteration: 14760, loss: 3.142e-02\n",
      "Iteration: 14770, loss: 2.687e-02\n",
      "Iteration: 14780, loss: 3.130e-02\n",
      "Iteration: 14790, loss: 4.181e-02\n",
      "Iteration: 14800, loss: 4.359e-02\n",
      "Iteration: 14810, loss: 3.307e-02\n",
      "Iteration: 14820, loss: 2.756e-02\n",
      "Iteration: 14830, loss: 3.742e-02\n",
      "Iteration: 14840, loss: 2.948e-02\n",
      "Iteration: 14850, loss: 1.663e-02\n",
      "Iteration: 14860, loss: 2.396e-02\n",
      "Iteration: 14870, loss: 2.193e-02\n",
      "Iteration: 14880, loss: 3.396e-02\n",
      "Iteration: 14890, loss: 2.773e-02\n",
      "Iteration: 14900, loss: 2.765e-02\n",
      "Iteration: 14910, loss: 4.465e-02\n",
      "Iteration: 14920, loss: 2.506e-02\n",
      "Iteration: 14930, loss: 3.099e-02\n",
      "Iteration: 14940, loss: 2.946e-02\n",
      "Iteration: 14950, loss: 2.664e-02\n",
      "Iteration: 14960, loss: 2.730e-02\n",
      "Iteration: 14970, loss: 3.742e-02\n",
      "Iteration: 14980, loss: 1.959e-02\n",
      "Iteration: 14990, loss: 4.285e-02\n",
      "Iteration: 15000, loss: 2.132e-02\n",
      "Iteration: 15010, loss: 3.026e-02\n",
      "Iteration: 15020, loss: 4.457e-02\n",
      "Iteration: 15030, loss: 1.875e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15040, loss: 3.034e-02\n",
      "Iteration: 15050, loss: 2.356e-02\n",
      "Iteration: 15060, loss: 2.626e-02\n",
      "Iteration: 15070, loss: 3.142e-02\n",
      "Iteration: 15080, loss: 2.794e-02\n",
      "Iteration: 15090, loss: 1.743e-02\n",
      "Iteration: 15100, loss: 2.971e-02\n",
      "Iteration: 15110, loss: 3.316e-02\n",
      "Iteration: 15120, loss: 1.470e-02\n",
      "Iteration: 15130, loss: 4.323e-02\n",
      "Iteration: 15140, loss: 2.468e-02\n",
      "Iteration: 15150, loss: 1.803e-02\n",
      "Iteration: 15160, loss: 1.332e-02\n",
      "Iteration: 15170, loss: 2.982e-02\n",
      "Iteration: 15180, loss: 1.964e-02\n",
      "Iteration: 15190, loss: 2.148e-02\n",
      "Iteration: 15200, loss: 3.986e-02\n",
      "Iteration: 15210, loss: 3.541e-02\n",
      "Iteration: 15220, loss: 2.629e-02\n",
      "Iteration: 15230, loss: 3.554e-02\n",
      "Iteration: 15240, loss: 2.193e-02\n",
      "Iteration: 15250, loss: 3.020e-02\n",
      "Iteration: 15260, loss: 2.665e-02\n",
      "Iteration: 15270, loss: 2.189e-02\n",
      "Iteration: 15280, loss: 2.486e-02\n",
      "Iteration: 15290, loss: 3.625e-02\n",
      "Iteration: 15300, loss: 3.072e-02\n",
      "Iteration: 15310, loss: 3.417e-02\n",
      "Iteration: 15320, loss: 2.850e-02\n",
      "Iteration: 15330, loss: 2.416e-02\n",
      "Iteration: 15340, loss: 3.147e-02\n",
      "Iteration: 15350, loss: 2.284e-02\n",
      "Iteration: 15360, loss: 2.660e-02\n",
      "Iteration: 15370, loss: 2.680e-02\n",
      "Iteration: 15380, loss: 3.387e-02\n",
      "Iteration: 15390, loss: 3.387e-02\n",
      "Iteration: 15400, loss: 2.885e-02\n",
      "Iteration: 15410, loss: 2.026e-02\n",
      "Iteration: 15420, loss: 1.780e-02\n",
      "Iteration: 15430, loss: 2.788e-02\n",
      "Iteration: 15440, loss: 2.916e-02\n",
      "Iteration: 15450, loss: 3.929e-02\n",
      "Iteration: 15460, loss: 1.554e-02\n",
      "Iteration: 15470, loss: 1.924e-02\n",
      "Iteration: 15480, loss: 2.165e-02\n",
      "Iteration: 15490, loss: 2.327e-02\n",
      "Iteration: 15500, loss: 3.921e-02\n",
      "Iteration: 15510, loss: 3.425e-02\n",
      "Iteration: 15520, loss: 2.763e-02\n",
      "Iteration: 15530, loss: 3.099e-02\n",
      "Iteration: 15540, loss: 1.545e-02\n",
      "Iteration: 15550, loss: 1.332e-02\n",
      "Relative error: 1.078941e-01\n",
      "Iteration: 10, loss: 2.891e+03\n",
      "Iteration: 20, loss: 2.720e+03\n",
      "Iteration: 30, loss: 8.038e+02\n",
      "Iteration: 40, loss: 8.880e+02\n",
      "Iteration: 50, loss: 6.938e+02\n",
      "Iteration: 60, loss: 5.271e+02\n",
      "Iteration: 70, loss: 4.565e+02\n",
      "Iteration: 80, loss: 2.910e+02\n",
      "Iteration: 90, loss: 3.915e+02\n",
      "Iteration: 100, loss: 3.305e+02\n",
      "Iteration: 110, loss: 2.294e+02\n",
      "Iteration: 120, loss: 1.079e+02\n",
      "Iteration: 130, loss: 2.925e+02\n",
      "Iteration: 140, loss: 1.268e+02\n",
      "Iteration: 150, loss: 8.329e+01\n",
      "Iteration: 160, loss: 9.030e+01\n",
      "Iteration: 170, loss: 7.156e+01\n",
      "Iteration: 180, loss: 1.587e+02\n",
      "Iteration: 190, loss: 9.506e+01\n",
      "Iteration: 200, loss: 1.063e+02\n",
      "Iteration: 210, loss: 7.239e+01\n",
      "Iteration: 220, loss: 4.901e+01\n",
      "Iteration: 230, loss: 4.598e+01\n",
      "Iteration: 240, loss: 5.949e+01\n",
      "Iteration: 250, loss: 4.348e+01\n",
      "Iteration: 260, loss: 6.053e+01\n",
      "Iteration: 270, loss: 5.528e+01\n",
      "Iteration: 280, loss: 3.272e+01\n",
      "Iteration: 290, loss: 5.009e+01\n",
      "Iteration: 300, loss: 3.859e+01\n",
      "Iteration: 310, loss: 4.274e+01\n",
      "Iteration: 320, loss: 3.567e+01\n",
      "Iteration: 330, loss: 4.152e+01\n",
      "Iteration: 340, loss: 5.309e+01\n",
      "Iteration: 350, loss: 3.163e+01\n",
      "Iteration: 360, loss: 2.778e+01\n",
      "Iteration: 370, loss: 3.826e+01\n",
      "Iteration: 380, loss: 2.093e+01\n",
      "Iteration: 390, loss: 2.865e+01\n",
      "Iteration: 400, loss: 4.687e+01\n",
      "Iteration: 410, loss: 1.813e+01\n",
      "Iteration: 420, loss: 3.150e+01\n",
      "Iteration: 430, loss: 2.265e+01\n",
      "Iteration: 440, loss: 2.218e+01\n",
      "Iteration: 450, loss: 2.738e+01\n",
      "Iteration: 460, loss: 3.063e+01\n",
      "Iteration: 470, loss: 1.786e+01\n",
      "Iteration: 480, loss: 2.343e+01\n",
      "Iteration: 490, loss: 1.447e+01\n",
      "Iteration: 500, loss: 2.285e+01\n",
      "Iteration: 510, loss: 1.636e+01\n",
      "Iteration: 520, loss: 1.014e+01\n",
      "Iteration: 530, loss: 1.526e+01\n",
      "Iteration: 540, loss: 1.405e+01\n",
      "Iteration: 550, loss: 9.730e+00\n",
      "Iteration: 560, loss: 1.515e+01\n",
      "Iteration: 570, loss: 1.215e+01\n",
      "Iteration: 580, loss: 1.076e+01\n",
      "Iteration: 590, loss: 1.638e+01\n",
      "Iteration: 600, loss: 1.325e+01\n",
      "Iteration: 610, loss: 8.363e+00\n",
      "Iteration: 620, loss: 1.651e+01\n",
      "Iteration: 630, loss: 4.573e+00\n",
      "Iteration: 640, loss: 1.802e+01\n",
      "Iteration: 650, loss: 8.810e+00\n",
      "Iteration: 660, loss: 9.226e+00\n",
      "Iteration: 670, loss: 7.626e+00\n",
      "Iteration: 680, loss: 7.329e+00\n",
      "Iteration: 690, loss: 1.369e+01\n",
      "Iteration: 700, loss: 6.693e+00\n",
      "Iteration: 710, loss: 1.058e+01\n",
      "Iteration: 720, loss: 7.495e+00\n",
      "Iteration: 730, loss: 1.030e+01\n",
      "Iteration: 740, loss: 9.077e+00\n",
      "Iteration: 750, loss: 8.253e+00\n",
      "Iteration: 760, loss: 1.452e+01\n",
      "Iteration: 770, loss: 7.565e+00\n",
      "Iteration: 780, loss: 4.081e+00\n",
      "Iteration: 790, loss: 7.636e+00\n",
      "Iteration: 800, loss: 5.789e+00\n",
      "Iteration: 810, loss: 1.150e+01\n",
      "Iteration: 820, loss: 5.920e+00\n",
      "Iteration: 830, loss: 1.191e+01\n",
      "Iteration: 840, loss: 7.804e+00\n",
      "Iteration: 850, loss: 3.259e+00\n",
      "Iteration: 860, loss: 7.358e+00\n",
      "Iteration: 870, loss: 6.481e+00\n",
      "Iteration: 880, loss: 6.015e+00\n",
      "Iteration: 890, loss: 9.971e+00\n",
      "Iteration: 900, loss: 6.981e+00\n",
      "Iteration: 910, loss: 5.794e+00\n",
      "Iteration: 920, loss: 5.818e+00\n",
      "Iteration: 930, loss: 4.034e+00\n",
      "Iteration: 940, loss: 4.438e+00\n",
      "Iteration: 950, loss: 4.337e+00\n",
      "Iteration: 960, loss: 1.011e+01\n",
      "Iteration: 970, loss: 3.133e+00\n",
      "Iteration: 980, loss: 4.618e+00\n",
      "Iteration: 990, loss: 5.335e+00\n",
      "Iteration: 1000, loss: 4.650e+00\n",
      "Iteration: 1010, loss: 4.742e+00\n",
      "Iteration: 1020, loss: 4.548e+00\n",
      "Iteration: 1030, loss: 1.636e+00\n",
      "Iteration: 1040, loss: 2.690e+00\n",
      "Iteration: 1050, loss: 4.204e+00\n",
      "Iteration: 1060, loss: 3.633e+00\n",
      "Iteration: 1070, loss: 4.245e+00\n",
      "Iteration: 1080, loss: 5.905e+00\n",
      "Iteration: 1090, loss: 6.310e+00\n",
      "Iteration: 1100, loss: 5.763e+00\n",
      "Iteration: 1110, loss: 5.923e+00\n",
      "Iteration: 1120, loss: 3.064e+00\n",
      "Iteration: 1130, loss: 3.017e+00\n",
      "Iteration: 1140, loss: 4.492e+00\n",
      "Iteration: 1150, loss: 5.217e+00\n",
      "Iteration: 1160, loss: 3.693e+00\n",
      "Iteration: 1170, loss: 2.801e+00\n",
      "Iteration: 1180, loss: 2.896e+00\n",
      "Iteration: 1190, loss: 3.364e+00\n",
      "Iteration: 1200, loss: 4.368e+00\n",
      "Iteration: 1210, loss: 6.174e+00\n",
      "Iteration: 1220, loss: 3.518e+00\n",
      "Iteration: 1230, loss: 2.499e+00\n",
      "Iteration: 1240, loss: 7.411e+00\n",
      "Iteration: 1250, loss: 2.658e+00\n",
      "Iteration: 1260, loss: 3.566e+00\n",
      "Iteration: 1270, loss: 1.432e+00\n",
      "Iteration: 1280, loss: 3.021e+00\n",
      "Iteration: 1290, loss: 5.064e+00\n",
      "Iteration: 1300, loss: 4.780e+00\n",
      "Iteration: 1310, loss: 3.330e+00\n",
      "Iteration: 1320, loss: 3.856e+00\n",
      "Iteration: 1330, loss: 4.239e+00\n",
      "Iteration: 1340, loss: 4.401e+00\n",
      "Iteration: 1350, loss: 2.546e+00\n",
      "Iteration: 1360, loss: 2.541e+00\n",
      "Iteration: 1370, loss: 4.617e+00\n",
      "Iteration: 1380, loss: 4.083e+00\n",
      "Iteration: 1390, loss: 3.161e+00\n",
      "Iteration: 1400, loss: 5.425e+00\n",
      "Iteration: 1410, loss: 3.224e+00\n",
      "Iteration: 1420, loss: 2.060e+00\n",
      "Iteration: 1430, loss: 2.132e+00\n",
      "Iteration: 1440, loss: 3.689e+00\n",
      "Iteration: 1450, loss: 4.853e+00\n",
      "Iteration: 1460, loss: 1.706e+00\n",
      "Iteration: 1470, loss: 4.344e+00\n",
      "Iteration: 1480, loss: 2.505e+00\n",
      "Iteration: 1490, loss: 4.674e+00\n",
      "Iteration: 1500, loss: 2.663e+00\n",
      "Iteration: 1510, loss: 1.785e+00\n",
      "Iteration: 1520, loss: 2.539e+00\n",
      "Iteration: 1530, loss: 3.086e+00\n",
      "Iteration: 1540, loss: 3.837e+00\n",
      "Iteration: 1550, loss: 2.699e+00\n",
      "Iteration: 1560, loss: 2.431e+00\n",
      "Iteration: 1570, loss: 2.745e+00\n",
      "Iteration: 1580, loss: 2.182e+00\n",
      "Iteration: 1590, loss: 1.299e+00\n",
      "Iteration: 1600, loss: 1.307e+00\n",
      "Iteration: 1610, loss: 2.482e+00\n",
      "Iteration: 1620, loss: 1.229e+00\n",
      "Iteration: 1630, loss: 5.791e-01\n",
      "Iteration: 1640, loss: 9.025e-01\n",
      "Iteration: 1650, loss: 1.520e+00\n",
      "Iteration: 1660, loss: 2.551e+00\n",
      "Iteration: 1670, loss: 9.693e-01\n",
      "Iteration: 1680, loss: 1.517e+00\n",
      "Iteration: 1690, loss: 3.092e+00\n",
      "Iteration: 1700, loss: 2.329e+00\n",
      "Iteration: 1710, loss: 1.474e+00\n",
      "Iteration: 1720, loss: 1.542e+00\n",
      "Iteration: 1730, loss: 1.365e+00\n",
      "Iteration: 1740, loss: 4.093e+00\n",
      "Iteration: 1750, loss: 1.349e+00\n",
      "Iteration: 1760, loss: 1.474e+00\n",
      "Iteration: 1770, loss: 2.511e+00\n",
      "Iteration: 1780, loss: 1.299e+00\n",
      "Iteration: 1790, loss: 2.357e+00\n",
      "Iteration: 1800, loss: 1.713e+00\n",
      "Iteration: 1810, loss: 1.224e+00\n",
      "Iteration: 1820, loss: 1.607e+00\n",
      "Iteration: 1830, loss: 1.798e+00\n",
      "Iteration: 1840, loss: 1.770e+00\n",
      "Iteration: 1850, loss: 2.292e+00\n",
      "Iteration: 1860, loss: 2.197e+00\n",
      "Iteration: 1870, loss: 1.912e+00\n",
      "Iteration: 1880, loss: 2.072e+00\n",
      "Iteration: 1890, loss: 1.942e+00\n",
      "Iteration: 1900, loss: 2.329e+00\n",
      "Iteration: 1910, loss: 1.561e+00\n",
      "Iteration: 1920, loss: 1.483e+00\n",
      "Iteration: 1930, loss: 1.569e+00\n",
      "Iteration: 1940, loss: 2.598e+00\n",
      "Iteration: 1950, loss: 2.382e+00\n",
      "Iteration: 1960, loss: 1.884e+00\n",
      "Iteration: 1970, loss: 1.903e+00\n",
      "Iteration: 1980, loss: 1.722e+00\n",
      "Iteration: 1990, loss: 2.258e+00\n",
      "Iteration: 2000, loss: 1.730e+00\n",
      "Iteration: 2010, loss: 8.469e-01\n",
      "Iteration: 2020, loss: 1.868e+00\n",
      "Iteration: 2030, loss: 2.145e+00\n",
      "Iteration: 2040, loss: 1.379e+00\n",
      "Iteration: 2050, loss: 9.478e-01\n",
      "Iteration: 2060, loss: 1.655e+00\n",
      "Iteration: 2070, loss: 1.745e+00\n",
      "Iteration: 2080, loss: 1.859e+00\n",
      "Iteration: 2090, loss: 1.380e+00\n",
      "Iteration: 2100, loss: 1.362e+00\n",
      "Iteration: 2110, loss: 1.403e+00\n",
      "Iteration: 2120, loss: 2.286e+00\n",
      "Iteration: 2130, loss: 1.100e+00\n",
      "Iteration: 2140, loss: 2.027e+00\n",
      "Iteration: 2150, loss: 9.005e-01\n",
      "Iteration: 2160, loss: 1.551e+00\n",
      "Iteration: 2170, loss: 9.677e-01\n",
      "Iteration: 2180, loss: 1.741e+00\n",
      "Iteration: 2190, loss: 1.274e+00\n",
      "Iteration: 2200, loss: 1.683e+00\n",
      "Iteration: 2210, loss: 1.054e+00\n",
      "Iteration: 2220, loss: 2.163e+00\n",
      "Iteration: 2230, loss: 1.392e+00\n",
      "Iteration: 2240, loss: 1.610e+00\n",
      "Iteration: 2250, loss: 8.911e-01\n",
      "Iteration: 2260, loss: 2.185e+00\n",
      "Iteration: 2270, loss: 1.700e+00\n",
      "Iteration: 2280, loss: 1.504e+00\n",
      "Iteration: 2290, loss: 9.814e-01\n",
      "Iteration: 2300, loss: 1.636e+00\n",
      "Iteration: 2310, loss: 1.123e+00\n",
      "Iteration: 2320, loss: 1.095e+00\n",
      "Iteration: 2330, loss: 1.044e+00\n",
      "Iteration: 2340, loss: 1.358e+00\n",
      "Iteration: 2350, loss: 7.463e-01\n",
      "Iteration: 2360, loss: 1.225e+00\n",
      "Iteration: 2370, loss: 1.209e+00\n",
      "Iteration: 2380, loss: 1.122e+00\n",
      "Iteration: 2390, loss: 1.956e+00\n",
      "Iteration: 2400, loss: 1.082e+00\n",
      "Iteration: 2410, loss: 1.722e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2420, loss: 1.351e+00\n",
      "Iteration: 2430, loss: 8.954e-01\n",
      "Iteration: 2440, loss: 1.682e+00\n",
      "Iteration: 2450, loss: 8.569e-01\n",
      "Iteration: 2460, loss: 1.121e+00\n",
      "Iteration: 2470, loss: 1.554e+00\n",
      "Iteration: 2480, loss: 1.042e+00\n",
      "Iteration: 2490, loss: 1.339e+00\n",
      "Iteration: 2500, loss: 1.080e+00\n",
      "Iteration: 2510, loss: 1.228e+00\n",
      "Iteration: 2520, loss: 1.022e+00\n",
      "Iteration: 2530, loss: 8.184e-01\n",
      "Iteration: 2540, loss: 1.140e+00\n",
      "Iteration: 2550, loss: 6.735e-01\n",
      "Iteration: 2560, loss: 9.199e-01\n",
      "Iteration: 2570, loss: 9.022e-01\n",
      "Iteration: 2580, loss: 8.284e-01\n",
      "Iteration: 2590, loss: 7.760e-01\n",
      "Iteration: 2600, loss: 7.272e-01\n",
      "Iteration: 2610, loss: 1.001e+00\n",
      "Iteration: 2620, loss: 6.810e-01\n",
      "Iteration: 2630, loss: 1.129e+00\n",
      "Iteration: 2640, loss: 1.416e+00\n",
      "Iteration: 2650, loss: 9.746e-01\n",
      "Iteration: 2660, loss: 6.452e-01\n",
      "Iteration: 2670, loss: 2.622e+00\n",
      "Iteration: 2680, loss: 7.618e-01\n",
      "Iteration: 2690, loss: 1.637e+00\n",
      "Iteration: 2700, loss: 9.741e-01\n",
      "Iteration: 2710, loss: 9.464e-01\n",
      "Iteration: 2720, loss: 9.419e-01\n",
      "Iteration: 2730, loss: 9.778e-01\n",
      "Iteration: 2740, loss: 8.726e-01\n",
      "Iteration: 2750, loss: 1.778e+00\n",
      "Iteration: 2760, loss: 7.831e-01\n",
      "Iteration: 2770, loss: 7.355e-01\n",
      "Iteration: 2780, loss: 1.026e+00\n",
      "Iteration: 2790, loss: 6.567e-01\n",
      "Iteration: 2800, loss: 7.952e-01\n",
      "Iteration: 2810, loss: 1.015e+00\n",
      "Iteration: 2820, loss: 1.332e+00\n",
      "Iteration: 2830, loss: 7.219e-01\n",
      "Iteration: 2840, loss: 5.289e-01\n",
      "Iteration: 2850, loss: 1.025e+00\n",
      "Iteration: 2860, loss: 8.478e-01\n",
      "Iteration: 2870, loss: 9.074e-01\n",
      "Iteration: 2880, loss: 8.980e-01\n",
      "Iteration: 2890, loss: 5.021e-01\n",
      "Iteration: 2900, loss: 5.890e-01\n",
      "Iteration: 2910, loss: 9.064e-01\n",
      "Iteration: 2920, loss: 1.328e+00\n",
      "Iteration: 2930, loss: 7.233e-01\n",
      "Iteration: 2940, loss: 1.005e+00\n",
      "Iteration: 2950, loss: 7.035e-01\n",
      "Iteration: 2960, loss: 8.244e-01\n",
      "Iteration: 2970, loss: 7.277e-01\n",
      "Iteration: 2980, loss: 1.050e+00\n",
      "Iteration: 2990, loss: 9.983e-01\n",
      "Iteration: 3000, loss: 6.910e-01\n",
      "Iteration: 3010, loss: 9.272e-01\n",
      "Iteration: 3020, loss: 1.012e+00\n",
      "Iteration: 3030, loss: 1.237e+00\n",
      "Iteration: 3040, loss: 5.595e-01\n",
      "Iteration: 3050, loss: 1.226e+00\n",
      "Iteration: 3060, loss: 6.185e-01\n",
      "Iteration: 3070, loss: 6.621e-01\n",
      "Iteration: 3080, loss: 8.393e-01\n",
      "Iteration: 3090, loss: 8.275e-01\n",
      "Iteration: 3100, loss: 6.246e-01\n",
      "Iteration: 3110, loss: 1.184e+00\n",
      "Iteration: 3120, loss: 6.990e-01\n",
      "Iteration: 3130, loss: 7.225e-01\n",
      "Iteration: 3140, loss: 9.799e-01\n",
      "Iteration: 3150, loss: 7.643e-01\n",
      "Iteration: 3160, loss: 7.312e-01\n",
      "Iteration: 3170, loss: 2.983e-01\n",
      "Iteration: 3180, loss: 9.887e-01\n",
      "Iteration: 3190, loss: 6.529e-01\n",
      "Iteration: 3200, loss: 6.592e-01\n",
      "Iteration: 3210, loss: 6.906e-01\n",
      "Iteration: 3220, loss: 4.586e-01\n",
      "Iteration: 3230, loss: 7.222e-01\n",
      "Iteration: 3240, loss: 7.338e-01\n",
      "Iteration: 3250, loss: 5.069e-01\n",
      "Iteration: 3260, loss: 5.056e-01\n",
      "Iteration: 3270, loss: 6.815e-01\n",
      "Iteration: 3280, loss: 1.165e+00\n",
      "Iteration: 3290, loss: 7.772e-01\n",
      "Iteration: 3300, loss: 8.223e-01\n",
      "Iteration: 3310, loss: 7.717e-01\n",
      "Iteration: 3320, loss: 4.402e-01\n",
      "Iteration: 3330, loss: 8.474e-01\n",
      "Iteration: 3340, loss: 4.480e-01\n",
      "Iteration: 3350, loss: 7.036e-01\n",
      "Iteration: 3360, loss: 4.916e-01\n",
      "Iteration: 3370, loss: 5.240e-01\n",
      "Iteration: 3380, loss: 5.490e-01\n",
      "Iteration: 3390, loss: 6.473e-01\n",
      "Iteration: 3400, loss: 7.440e-01\n",
      "Iteration: 3410, loss: 6.845e-01\n",
      "Iteration: 3420, loss: 8.226e-01\n",
      "Iteration: 3430, loss: 6.380e-01\n",
      "Iteration: 3440, loss: 6.855e-01\n",
      "Iteration: 3450, loss: 5.279e-01\n",
      "Iteration: 3460, loss: 2.493e-01\n",
      "Iteration: 3470, loss: 3.578e-01\n",
      "Iteration: 3480, loss: 7.200e-01\n",
      "Iteration: 3490, loss: 6.373e-01\n",
      "Iteration: 3500, loss: 7.076e-01\n",
      "Iteration: 3510, loss: 4.549e-01\n",
      "Iteration: 3520, loss: 6.344e-01\n",
      "Iteration: 3530, loss: 6.543e-01\n",
      "Iteration: 3540, loss: 5.410e-01\n",
      "Iteration: 3550, loss: 6.386e-01\n",
      "Iteration: 3560, loss: 3.737e-01\n",
      "Iteration: 3570, loss: 3.273e-01\n",
      "Iteration: 3580, loss: 5.875e-01\n",
      "Iteration: 3590, loss: 5.360e-01\n",
      "Iteration: 3600, loss: 4.350e-01\n",
      "Iteration: 3610, loss: 6.067e-01\n",
      "Iteration: 3620, loss: 4.847e-01\n",
      "Iteration: 3630, loss: 6.493e-01\n",
      "Iteration: 3640, loss: 6.794e-01\n",
      "Iteration: 3650, loss: 7.234e-01\n",
      "Iteration: 3660, loss: 4.861e-01\n",
      "Iteration: 3670, loss: 1.119e+00\n",
      "Iteration: 3680, loss: 5.147e-01\n",
      "Iteration: 3690, loss: 4.016e-01\n",
      "Iteration: 3700, loss: 1.696e-01\n",
      "Iteration: 3710, loss: 6.417e-01\n",
      "Iteration: 3720, loss: 5.283e-01\n",
      "Iteration: 3730, loss: 7.766e-01\n",
      "Iteration: 3740, loss: 7.870e-01\n",
      "Iteration: 3750, loss: 5.895e-01\n",
      "Iteration: 3760, loss: 2.557e-01\n",
      "Iteration: 3770, loss: 3.243e-01\n",
      "Iteration: 3780, loss: 4.582e-01\n",
      "Iteration: 3790, loss: 5.663e-01\n",
      "Iteration: 3800, loss: 5.153e-01\n",
      "Iteration: 3810, loss: 7.036e-01\n",
      "Iteration: 3820, loss: 1.822e-01\n",
      "Iteration: 3830, loss: 4.959e-01\n",
      "Iteration: 3840, loss: 5.037e-01\n",
      "Iteration: 3850, loss: 4.795e-01\n",
      "Iteration: 3860, loss: 5.441e-01\n",
      "Iteration: 3870, loss: 3.718e-01\n",
      "Iteration: 3880, loss: 7.668e-01\n",
      "Iteration: 3890, loss: 4.077e-01\n",
      "Iteration: 3900, loss: 4.179e-01\n",
      "Iteration: 3910, loss: 3.024e-01\n",
      "Iteration: 3920, loss: 3.768e-01\n",
      "Iteration: 3930, loss: 4.978e-01\n",
      "Iteration: 3940, loss: 4.267e-01\n",
      "Iteration: 3950, loss: 7.260e-01\n",
      "Iteration: 3960, loss: 6.439e-01\n",
      "Iteration: 3970, loss: 4.923e-01\n",
      "Iteration: 3980, loss: 6.064e-01\n",
      "Iteration: 3990, loss: 5.208e-01\n",
      "Iteration: 4000, loss: 3.074e-01\n",
      "Iteration: 4010, loss: 4.466e-01\n",
      "Iteration: 4020, loss: 6.994e-01\n",
      "Iteration: 4030, loss: 4.710e-01\n",
      "Iteration: 4040, loss: 5.614e-01\n",
      "Iteration: 4050, loss: 8.015e-01\n",
      "Iteration: 4060, loss: 4.445e-01\n",
      "Iteration: 4070, loss: 2.366e-01\n",
      "Iteration: 4080, loss: 2.201e-01\n",
      "Iteration: 4090, loss: 2.719e-01\n",
      "Iteration: 4100, loss: 4.183e-01\n",
      "Iteration: 4110, loss: 3.471e-01\n",
      "Iteration: 4120, loss: 4.777e-01\n",
      "Iteration: 4130, loss: 3.851e-01\n",
      "Iteration: 4140, loss: 3.459e-01\n",
      "Iteration: 4150, loss: 3.669e-01\n",
      "Iteration: 4160, loss: 4.922e-01\n",
      "Iteration: 4170, loss: 4.127e-01\n",
      "Iteration: 4180, loss: 3.617e-01\n",
      "Iteration: 4190, loss: 2.866e-01\n",
      "Iteration: 4200, loss: 6.978e-01\n",
      "Iteration: 4210, loss: 3.100e-01\n",
      "Iteration: 4220, loss: 4.441e-01\n",
      "Iteration: 4230, loss: 5.141e-01\n",
      "Iteration: 4240, loss: 2.844e-01\n",
      "Iteration: 4250, loss: 4.854e-01\n",
      "Iteration: 4260, loss: 1.185e-01\n",
      "Iteration: 4270, loss: 3.715e-01\n",
      "Iteration: 4280, loss: 4.200e-01\n",
      "Iteration: 4290, loss: 5.957e-01\n",
      "Iteration: 4300, loss: 4.336e-01\n",
      "Iteration: 4310, loss: 3.514e-01\n",
      "Iteration: 4320, loss: 1.730e-01\n",
      "Iteration: 4330, loss: 4.151e-01\n",
      "Iteration: 4340, loss: 2.739e-01\n",
      "Iteration: 4350, loss: 4.249e-01\n",
      "Iteration: 4360, loss: 3.376e-01\n",
      "Iteration: 4370, loss: 5.587e-01\n",
      "Iteration: 4380, loss: 6.078e-01\n",
      "Iteration: 4390, loss: 3.597e-01\n",
      "Iteration: 4400, loss: 3.876e-01\n",
      "Iteration: 4410, loss: 5.496e-01\n",
      "Iteration: 4420, loss: 5.274e-01\n",
      "Iteration: 4430, loss: 4.157e-01\n",
      "Iteration: 4440, loss: 4.773e-01\n",
      "Iteration: 4450, loss: 4.021e-01\n",
      "Iteration: 4460, loss: 5.325e-01\n",
      "Iteration: 4470, loss: 2.460e-01\n",
      "Iteration: 4480, loss: 2.497e-01\n",
      "Iteration: 4490, loss: 3.809e-01\n",
      "Iteration: 4500, loss: 4.450e-01\n",
      "Iteration: 4510, loss: 4.227e-01\n",
      "Iteration: 4520, loss: 3.519e-01\n",
      "Iteration: 4530, loss: 4.067e-01\n",
      "Iteration: 4540, loss: 6.217e-01\n",
      "Iteration: 4550, loss: 5.509e-01\n",
      "Iteration: 4560, loss: 2.171e-01\n",
      "Iteration: 4570, loss: 4.318e-01\n",
      "Iteration: 4580, loss: 1.784e-01\n",
      "Iteration: 4590, loss: 4.596e-01\n",
      "Iteration: 4600, loss: 2.876e-01\n",
      "Iteration: 4610, loss: 3.723e-01\n",
      "Iteration: 4620, loss: 1.788e-01\n",
      "Iteration: 4630, loss: 4.685e-01\n",
      "Iteration: 4640, loss: 2.930e-01\n",
      "Iteration: 4650, loss: 2.148e-01\n",
      "Iteration: 4660, loss: 4.298e-01\n",
      "Iteration: 4670, loss: 3.230e-01\n",
      "Iteration: 4680, loss: 4.709e-01\n",
      "Iteration: 4690, loss: 4.651e-01\n",
      "Iteration: 4700, loss: 3.860e-01\n",
      "Iteration: 4710, loss: 2.437e-01\n",
      "Iteration: 4720, loss: 4.243e-01\n",
      "Iteration: 4730, loss: 6.171e-01\n",
      "Iteration: 4740, loss: 3.973e-01\n",
      "Iteration: 4750, loss: 3.270e-01\n",
      "Iteration: 4760, loss: 4.634e-01\n",
      "Iteration: 4770, loss: 5.359e-01\n",
      "Iteration: 4780, loss: 3.465e-01\n",
      "Iteration: 4790, loss: 3.040e-01\n",
      "Iteration: 4800, loss: 3.432e-01\n",
      "Iteration: 4810, loss: 3.226e-01\n",
      "Iteration: 4820, loss: 3.922e-01\n",
      "Iteration: 4830, loss: 3.890e-01\n",
      "Iteration: 4840, loss: 3.922e-01\n",
      "Iteration: 4850, loss: 3.793e-01\n",
      "Iteration: 4860, loss: 2.884e-01\n",
      "Iteration: 4870, loss: 4.254e-01\n",
      "Iteration: 4880, loss: 4.424e-01\n",
      "Iteration: 4890, loss: 3.046e-01\n",
      "Iteration: 4900, loss: 2.481e-01\n",
      "Iteration: 4910, loss: 3.013e-01\n",
      "Iteration: 4920, loss: 4.364e-01\n",
      "Iteration: 4930, loss: 2.109e-01\n",
      "Iteration: 4940, loss: 3.017e-01\n",
      "Iteration: 4950, loss: 4.807e-01\n",
      "Iteration: 4960, loss: 2.706e-01\n",
      "Iteration: 4970, loss: 4.144e-01\n",
      "Iteration: 4980, loss: 4.874e-01\n",
      "Iteration: 4990, loss: 2.646e-01\n",
      "Iteration: 5000, loss: 3.468e-01\n",
      "Iteration: 5010, loss: 3.326e-01\n",
      "Iteration: 5020, loss: 5.137e-01\n",
      "Iteration: 5030, loss: 3.292e-01\n",
      "Iteration: 5040, loss: 2.188e-01\n",
      "Iteration: 5050, loss: 3.509e-01\n",
      "Iteration: 5060, loss: 2.352e-01\n",
      "Iteration: 5070, loss: 4.904e-01\n",
      "Iteration: 5080, loss: 3.779e-01\n",
      "Iteration: 5090, loss: 2.858e-01\n",
      "Iteration: 5100, loss: 2.780e-01\n",
      "Iteration: 5110, loss: 2.327e-01\n",
      "Iteration: 5120, loss: 6.552e-01\n",
      "Iteration: 5130, loss: 4.891e-01\n",
      "Iteration: 5140, loss: 2.672e-01\n",
      "Iteration: 5150, loss: 4.948e-01\n",
      "Iteration: 5160, loss: 2.477e-01\n",
      "Iteration: 5170, loss: 2.853e-01\n",
      "Iteration: 5180, loss: 3.176e-01\n",
      "Iteration: 5190, loss: 2.407e-01\n",
      "Iteration: 5200, loss: 3.545e-01\n",
      "Iteration: 5210, loss: 2.993e-01\n",
      "Iteration: 5220, loss: 4.798e-01\n",
      "Iteration: 5230, loss: 2.605e-01\n",
      "Iteration: 5240, loss: 2.616e-01\n",
      "Iteration: 5250, loss: 2.155e-01\n",
      "Iteration: 5260, loss: 1.841e-01\n",
      "Iteration: 5270, loss: 2.979e-01\n",
      "Iteration: 5280, loss: 2.132e-01\n",
      "Iteration: 5290, loss: 1.426e-01\n",
      "Iteration: 5300, loss: 3.489e-01\n",
      "Iteration: 5310, loss: 4.900e-01\n",
      "Iteration: 5320, loss: 2.081e-01\n",
      "Iteration: 5330, loss: 2.614e-01\n",
      "Iteration: 5340, loss: 1.729e-01\n",
      "Iteration: 5350, loss: 5.544e-01\n",
      "Iteration: 5360, loss: 2.942e-01\n",
      "Iteration: 5370, loss: 2.496e-01\n",
      "Iteration: 5380, loss: 1.373e-01\n",
      "Iteration: 5390, loss: 2.903e-01\n",
      "Iteration: 5400, loss: 3.593e-01\n",
      "Iteration: 5410, loss: 3.857e-01\n",
      "Iteration: 5420, loss: 2.594e-01\n",
      "Iteration: 5430, loss: 1.737e-01\n",
      "Iteration: 5440, loss: 3.625e-01\n",
      "Iteration: 5450, loss: 2.372e-01\n",
      "Iteration: 5460, loss: 2.946e-01\n",
      "Iteration: 5470, loss: 2.027e-01\n",
      "Iteration: 5480, loss: 2.943e-01\n",
      "Iteration: 5490, loss: 3.186e-01\n",
      "Iteration: 5500, loss: 3.352e-01\n",
      "Iteration: 5510, loss: 3.601e-01\n",
      "Iteration: 5520, loss: 9.319e-02\n",
      "Iteration: 5530, loss: 1.824e-01\n",
      "Iteration: 5540, loss: 1.649e-01\n",
      "Iteration: 5550, loss: 2.199e-01\n",
      "Iteration: 5560, loss: 4.306e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5570, loss: 3.756e-01\n",
      "Iteration: 5580, loss: 1.707e-01\n",
      "Iteration: 5590, loss: 1.827e-01\n",
      "Iteration: 5600, loss: 1.661e-01\n",
      "Iteration: 5610, loss: 2.729e-01\n",
      "Iteration: 5620, loss: 2.455e-01\n",
      "Iteration: 5630, loss: 2.387e-01\n",
      "Iteration: 5640, loss: 2.559e-01\n",
      "Iteration: 5650, loss: 1.386e-01\n",
      "Iteration: 5660, loss: 1.089e-01\n",
      "Iteration: 5670, loss: 1.997e-01\n",
      "Iteration: 5680, loss: 1.577e-01\n",
      "Iteration: 5690, loss: 2.348e-01\n",
      "Iteration: 5700, loss: 1.829e-01\n",
      "Iteration: 5710, loss: 3.070e-01\n",
      "Iteration: 5720, loss: 2.147e-01\n",
      "Iteration: 5730, loss: 2.477e-01\n",
      "Iteration: 5740, loss: 2.035e-01\n",
      "Iteration: 5750, loss: 2.424e-01\n",
      "Iteration: 5760, loss: 3.120e-01\n",
      "Iteration: 5770, loss: 2.333e-01\n",
      "Iteration: 5780, loss: 2.238e-01\n",
      "Iteration: 5790, loss: 1.813e-01\n",
      "Iteration: 5800, loss: 1.981e-01\n",
      "Iteration: 5810, loss: 1.351e-01\n",
      "Iteration: 5820, loss: 2.183e-01\n",
      "Iteration: 5830, loss: 3.658e-01\n",
      "Iteration: 5840, loss: 1.479e-01\n",
      "Iteration: 5850, loss: 2.794e-01\n",
      "Iteration: 5860, loss: 2.141e-01\n",
      "Iteration: 5870, loss: 2.583e-01\n",
      "Iteration: 5880, loss: 3.013e-01\n",
      "Iteration: 5890, loss: 2.097e-01\n",
      "Iteration: 5900, loss: 1.271e-01\n",
      "Iteration: 5910, loss: 2.692e-01\n",
      "Iteration: 5920, loss: 2.067e-01\n",
      "Iteration: 5930, loss: 2.569e-01\n",
      "Iteration: 5940, loss: 1.813e-01\n",
      "Iteration: 5950, loss: 2.446e-01\n",
      "Iteration: 5960, loss: 1.020e-01\n",
      "Iteration: 5970, loss: 7.130e-02\n",
      "Iteration: 5980, loss: 1.536e-01\n",
      "Iteration: 5990, loss: 2.322e-01\n",
      "Iteration: 6000, loss: 2.311e-01\n",
      "Iteration: 6010, loss: 1.482e-01\n",
      "Iteration: 6020, loss: 1.969e-01\n",
      "Iteration: 6030, loss: 1.739e-01\n",
      "Iteration: 6040, loss: 1.984e-01\n",
      "Iteration: 6050, loss: 2.464e-01\n",
      "Iteration: 6060, loss: 2.249e-01\n",
      "Iteration: 6070, loss: 2.295e-01\n",
      "Iteration: 6080, loss: 1.896e-01\n",
      "Iteration: 6090, loss: 2.627e-01\n",
      "Iteration: 6100, loss: 1.161e-01\n",
      "Iteration: 6110, loss: 1.599e-01\n",
      "Iteration: 6120, loss: 1.083e-01\n",
      "Iteration: 6130, loss: 2.511e-01\n",
      "Iteration: 6140, loss: 2.078e-01\n",
      "Iteration: 6150, loss: 8.638e-02\n",
      "Iteration: 6160, loss: 1.399e-01\n",
      "Iteration: 6170, loss: 1.699e-01\n",
      "Iteration: 6180, loss: 3.210e-01\n",
      "Iteration: 6190, loss: 1.120e-01\n",
      "Iteration: 6200, loss: 1.049e-01\n",
      "Iteration: 6210, loss: 1.998e-01\n",
      "Iteration: 6220, loss: 1.909e-01\n",
      "Iteration: 6230, loss: 1.562e-01\n",
      "Iteration: 6240, loss: 1.319e-01\n",
      "Iteration: 6250, loss: 3.001e-01\n",
      "Iteration: 6260, loss: 1.378e-01\n",
      "Iteration: 6270, loss: 1.191e-01\n",
      "Iteration: 6280, loss: 1.754e-01\n",
      "Iteration: 6290, loss: 2.849e-01\n",
      "Iteration: 6300, loss: 1.895e-01\n",
      "Iteration: 6310, loss: 1.552e-01\n",
      "Iteration: 6320, loss: 1.082e-01\n",
      "Iteration: 6330, loss: 1.209e-01\n",
      "Iteration: 6340, loss: 1.674e-01\n",
      "Iteration: 6350, loss: 3.140e-01\n",
      "Iteration: 6360, loss: 1.886e-01\n",
      "Iteration: 6370, loss: 2.363e-01\n",
      "Iteration: 6380, loss: 1.823e-01\n",
      "Iteration: 6390, loss: 1.416e-01\n",
      "Iteration: 6400, loss: 3.148e-01\n",
      "Iteration: 6410, loss: 3.033e-01\n",
      "Iteration: 6420, loss: 8.165e-02\n",
      "Iteration: 6430, loss: 1.866e-01\n",
      "Iteration: 6440, loss: 8.166e-02\n",
      "Iteration: 6450, loss: 1.329e-01\n",
      "Iteration: 6460, loss: 9.646e-02\n",
      "Iteration: 6470, loss: 2.680e-01\n",
      "Iteration: 6480, loss: 1.123e-01\n",
      "Iteration: 6490, loss: 2.233e-01\n",
      "Iteration: 6500, loss: 8.125e-02\n",
      "Iteration: 6510, loss: 1.571e-01\n",
      "Iteration: 6520, loss: 1.586e-01\n",
      "Iteration: 6530, loss: 2.485e-01\n",
      "Iteration: 6540, loss: 1.781e-01\n",
      "Iteration: 6550, loss: 3.307e-01\n",
      "Iteration: 6560, loss: 1.187e-01\n",
      "Iteration: 6570, loss: 2.253e-01\n",
      "Iteration: 6580, loss: 9.829e-02\n",
      "Iteration: 6590, loss: 1.556e-01\n",
      "Iteration: 6600, loss: 1.897e-01\n",
      "Iteration: 6610, loss: 2.822e-01\n",
      "Iteration: 6620, loss: 8.546e-02\n",
      "Iteration: 6630, loss: 1.421e-01\n",
      "Iteration: 6640, loss: 1.142e-01\n",
      "Iteration: 6650, loss: 2.110e-01\n",
      "Iteration: 6660, loss: 2.221e-01\n",
      "Iteration: 6670, loss: 1.769e-01\n",
      "Iteration: 6680, loss: 2.029e-01\n",
      "Iteration: 6690, loss: 1.443e-01\n",
      "Iteration: 6700, loss: 1.248e-01\n",
      "Iteration: 6710, loss: 1.681e-01\n",
      "Iteration: 6720, loss: 1.621e-01\n",
      "Iteration: 6730, loss: 1.605e-01\n",
      "Iteration: 6740, loss: 1.851e-01\n",
      "Iteration: 6750, loss: 1.542e-01\n",
      "Iteration: 6760, loss: 8.494e-02\n",
      "Iteration: 6770, loss: 1.605e-01\n",
      "Iteration: 6780, loss: 1.306e-01\n",
      "Iteration: 6790, loss: 5.086e-02\n",
      "Iteration: 6800, loss: 1.506e-01\n",
      "Iteration: 6810, loss: 7.721e-02\n",
      "Iteration: 6820, loss: 1.596e-01\n",
      "Iteration: 6830, loss: 1.464e-01\n",
      "Iteration: 6840, loss: 1.438e-01\n",
      "Iteration: 6850, loss: 1.495e-01\n",
      "Iteration: 6860, loss: 1.078e-01\n",
      "Iteration: 6870, loss: 1.987e-01\n",
      "Iteration: 6880, loss: 2.093e-01\n",
      "Iteration: 6890, loss: 1.556e-01\n",
      "Iteration: 6900, loss: 1.077e-01\n",
      "Iteration: 6910, loss: 1.341e-01\n",
      "Iteration: 6920, loss: 1.022e-01\n",
      "Iteration: 6930, loss: 2.334e-01\n",
      "Iteration: 6940, loss: 1.172e-01\n",
      "Iteration: 6950, loss: 1.284e-01\n",
      "Iteration: 6960, loss: 1.927e-01\n",
      "Iteration: 6970, loss: 1.445e-01\n",
      "Iteration: 6980, loss: 1.244e-01\n",
      "Iteration: 6990, loss: 1.112e-01\n",
      "Iteration: 7000, loss: 1.580e-01\n",
      "Iteration: 7010, loss: 1.598e-01\n",
      "Iteration: 7020, loss: 1.793e-01\n",
      "Iteration: 7030, loss: 1.076e-01\n",
      "Iteration: 7040, loss: 1.897e-01\n",
      "Iteration: 7050, loss: 1.136e-01\n",
      "Iteration: 7060, loss: 1.907e-01\n",
      "Iteration: 7070, loss: 9.045e-02\n",
      "Iteration: 7080, loss: 1.251e-01\n",
      "Iteration: 7090, loss: 1.835e-01\n",
      "Iteration: 7100, loss: 1.616e-01\n",
      "Iteration: 7110, loss: 1.246e-01\n",
      "Iteration: 7120, loss: 1.306e-01\n",
      "Iteration: 7130, loss: 1.936e-01\n",
      "Iteration: 7140, loss: 2.524e-01\n",
      "Iteration: 7150, loss: 1.972e-01\n",
      "Iteration: 7160, loss: 1.182e-01\n",
      "Iteration: 7170, loss: 1.043e-01\n",
      "Iteration: 7180, loss: 1.865e-01\n",
      "Iteration: 7190, loss: 1.121e-01\n",
      "Iteration: 7200, loss: 7.945e-02\n",
      "Iteration: 7210, loss: 1.407e-01\n",
      "Iteration: 7220, loss: 9.783e-02\n",
      "Iteration: 7230, loss: 1.901e-01\n",
      "Iteration: 7240, loss: 1.046e-01\n",
      "Iteration: 7250, loss: 9.028e-02\n",
      "Iteration: 7260, loss: 2.182e-01\n",
      "Iteration: 7270, loss: 1.237e-01\n",
      "Iteration: 7280, loss: 1.124e-01\n",
      "Iteration: 7290, loss: 9.706e-02\n",
      "Iteration: 7300, loss: 6.210e-02\n",
      "Iteration: 7310, loss: 7.906e-02\n",
      "Iteration: 7320, loss: 1.566e-01\n",
      "Iteration: 7330, loss: 8.932e-02\n",
      "Iteration: 7340, loss: 9.954e-02\n",
      "Iteration: 7350, loss: 3.183e-01\n",
      "Iteration: 7360, loss: 1.013e-01\n",
      "Iteration: 7370, loss: 1.450e-01\n",
      "Iteration: 7380, loss: 1.170e-01\n",
      "Iteration: 7390, loss: 1.382e-01\n",
      "Iteration: 7400, loss: 1.937e-01\n",
      "Iteration: 7410, loss: 2.478e-01\n",
      "Iteration: 7420, loss: 1.892e-01\n",
      "Iteration: 7430, loss: 1.226e-01\n",
      "Iteration: 7440, loss: 1.064e-01\n",
      "Iteration: 7450, loss: 9.511e-02\n",
      "Iteration: 7460, loss: 1.081e-01\n",
      "Iteration: 7470, loss: 1.496e-01\n",
      "Iteration: 7480, loss: 1.446e-01\n",
      "Iteration: 7490, loss: 1.559e-01\n",
      "Iteration: 7500, loss: 7.441e-02\n",
      "Iteration: 7510, loss: 1.002e-01\n",
      "Iteration: 7520, loss: 9.490e-02\n",
      "Iteration: 7530, loss: 1.257e-01\n",
      "Iteration: 7540, loss: 9.189e-02\n",
      "Iteration: 7550, loss: 1.456e-01\n",
      "Iteration: 7560, loss: 6.884e-02\n",
      "Iteration: 7570, loss: 1.070e-01\n",
      "Iteration: 7580, loss: 1.042e-01\n",
      "Iteration: 7590, loss: 9.100e-02\n",
      "Iteration: 7600, loss: 9.634e-02\n",
      "Iteration: 7610, loss: 1.390e-01\n",
      "Iteration: 7620, loss: 1.047e-01\n",
      "Iteration: 7630, loss: 2.311e-01\n",
      "Iteration: 7640, loss: 7.772e-02\n",
      "Iteration: 7650, loss: 1.545e-01\n",
      "Iteration: 7660, loss: 8.252e-02\n",
      "Iteration: 7670, loss: 1.011e-01\n",
      "Iteration: 7680, loss: 9.421e-02\n",
      "Iteration: 7690, loss: 7.182e-02\n",
      "Iteration: 7700, loss: 1.382e-01\n",
      "Iteration: 7710, loss: 7.808e-02\n",
      "Iteration: 7720, loss: 1.089e-01\n",
      "Iteration: 7730, loss: 1.318e-01\n",
      "Iteration: 7740, loss: 1.352e-01\n",
      "Iteration: 7750, loss: 1.317e-01\n",
      "Iteration: 7760, loss: 1.293e-01\n",
      "Iteration: 7770, loss: 1.125e-01\n",
      "Iteration: 7780, loss: 9.787e-02\n",
      "Iteration: 7790, loss: 7.565e-02\n",
      "Iteration: 7800, loss: 6.899e-02\n",
      "Iteration: 7810, loss: 6.194e-02\n",
      "Iteration: 7820, loss: 1.522e-01\n",
      "Iteration: 7830, loss: 1.458e-01\n",
      "Iteration: 7840, loss: 1.425e-01\n",
      "Iteration: 7850, loss: 9.303e-02\n",
      "Iteration: 7860, loss: 2.138e-01\n",
      "Iteration: 7870, loss: 8.851e-02\n",
      "Iteration: 7880, loss: 1.054e-01\n",
      "Iteration: 7890, loss: 1.164e-01\n",
      "Iteration: 7900, loss: 1.436e-01\n",
      "Iteration: 7910, loss: 7.457e-02\n",
      "Iteration: 7920, loss: 9.330e-02\n",
      "Iteration: 7930, loss: 4.803e-02\n",
      "Iteration: 7940, loss: 1.661e-01\n",
      "Iteration: 7950, loss: 9.526e-02\n",
      "Iteration: 7960, loss: 1.211e-01\n",
      "Iteration: 7970, loss: 1.501e-01\n",
      "Iteration: 7980, loss: 8.648e-02\n",
      "Iteration: 7990, loss: 9.424e-02\n",
      "Iteration: 8000, loss: 1.239e-01\n",
      "Iteration: 8010, loss: 1.093e-01\n",
      "Iteration: 8020, loss: 1.577e-01\n",
      "Iteration: 8030, loss: 9.995e-02\n",
      "Iteration: 8040, loss: 1.325e-01\n",
      "Iteration: 8050, loss: 1.629e-01\n",
      "Iteration: 8060, loss: 7.791e-02\n",
      "Iteration: 8070, loss: 4.252e-02\n",
      "Iteration: 8080, loss: 1.075e-01\n",
      "Iteration: 8090, loss: 1.530e-01\n",
      "Iteration: 8100, loss: 1.675e-01\n",
      "Iteration: 8110, loss: 5.660e-02\n",
      "Iteration: 8120, loss: 9.051e-02\n",
      "Iteration: 8130, loss: 9.299e-02\n",
      "Iteration: 8140, loss: 9.633e-02\n",
      "Iteration: 8150, loss: 9.405e-02\n",
      "Iteration: 8160, loss: 1.093e-01\n",
      "Iteration: 8170, loss: 7.713e-02\n",
      "Iteration: 8180, loss: 1.244e-01\n",
      "Iteration: 8190, loss: 5.685e-02\n",
      "Iteration: 8200, loss: 4.195e-02\n",
      "Iteration: 8210, loss: 6.867e-02\n",
      "Iteration: 8220, loss: 1.310e-01\n",
      "Iteration: 8230, loss: 4.681e-02\n",
      "Iteration: 8240, loss: 1.434e-01\n",
      "Iteration: 8250, loss: 6.457e-02\n",
      "Iteration: 8260, loss: 9.048e-02\n",
      "Iteration: 8270, loss: 6.702e-02\n",
      "Iteration: 8280, loss: 1.095e-01\n",
      "Iteration: 8290, loss: 7.505e-02\n",
      "Iteration: 8300, loss: 1.004e-01\n",
      "Iteration: 8310, loss: 9.697e-02\n",
      "Iteration: 8320, loss: 7.592e-02\n",
      "Iteration: 8330, loss: 5.715e-02\n",
      "Iteration: 8340, loss: 7.866e-02\n",
      "Iteration: 8350, loss: 7.401e-02\n",
      "Iteration: 8360, loss: 1.242e-01\n",
      "Iteration: 8370, loss: 7.534e-02\n",
      "Iteration: 8380, loss: 9.432e-02\n",
      "Iteration: 8390, loss: 6.639e-02\n",
      "Iteration: 8400, loss: 6.219e-02\n",
      "Iteration: 8410, loss: 8.021e-02\n",
      "Iteration: 8420, loss: 7.474e-02\n",
      "Iteration: 8430, loss: 7.774e-02\n",
      "Iteration: 8440, loss: 9.477e-02\n",
      "Iteration: 8450, loss: 7.195e-02\n",
      "Iteration: 8460, loss: 1.479e-01\n",
      "Iteration: 8470, loss: 9.326e-02\n",
      "Iteration: 8480, loss: 9.293e-02\n",
      "Iteration: 8490, loss: 8.927e-02\n",
      "Iteration: 8500, loss: 1.019e-01\n",
      "Iteration: 8510, loss: 7.971e-02\n",
      "Iteration: 8520, loss: 7.517e-02\n",
      "Iteration: 8530, loss: 8.317e-02\n",
      "Iteration: 8540, loss: 6.957e-02\n",
      "Iteration: 8550, loss: 8.237e-02\n",
      "Iteration: 8560, loss: 8.143e-02\n",
      "Iteration: 8570, loss: 2.890e-02\n",
      "Iteration: 8580, loss: 8.531e-02\n",
      "Iteration: 8590, loss: 6.701e-02\n",
      "Iteration: 8600, loss: 1.298e-01\n",
      "Iteration: 8610, loss: 1.062e-01\n",
      "Iteration: 8620, loss: 9.476e-02\n",
      "Iteration: 8630, loss: 9.783e-02\n",
      "Iteration: 8640, loss: 8.584e-02\n",
      "Iteration: 8650, loss: 6.808e-02\n",
      "Iteration: 8660, loss: 7.920e-02\n",
      "Iteration: 8670, loss: 1.261e-01\n",
      "Iteration: 8680, loss: 7.572e-02\n",
      "Iteration: 8690, loss: 5.164e-02\n",
      "Iteration: 8700, loss: 1.156e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8710, loss: 6.259e-02\n",
      "Iteration: 8720, loss: 5.748e-02\n",
      "Iteration: 8730, loss: 4.358e-02\n",
      "Iteration: 8740, loss: 8.274e-02\n",
      "Iteration: 8750, loss: 5.046e-02\n",
      "Iteration: 8760, loss: 1.072e-01\n",
      "Iteration: 8770, loss: 6.191e-02\n",
      "Iteration: 8780, loss: 1.002e-01\n",
      "Iteration: 8790, loss: 5.300e-02\n",
      "Iteration: 8800, loss: 4.815e-02\n",
      "Iteration: 8810, loss: 5.747e-02\n",
      "Iteration: 8820, loss: 1.051e-01\n",
      "Iteration: 8830, loss: 4.073e-02\n",
      "Iteration: 8840, loss: 1.047e-01\n",
      "Iteration: 8850, loss: 9.645e-02\n",
      "Iteration: 8860, loss: 9.174e-02\n",
      "Iteration: 8870, loss: 4.353e-02\n",
      "Iteration: 8880, loss: 1.387e-01\n",
      "Iteration: 8890, loss: 8.869e-02\n",
      "Iteration: 8900, loss: 2.678e-02\n",
      "Iteration: 8910, loss: 6.853e-02\n",
      "Iteration: 8920, loss: 8.855e-02\n",
      "Iteration: 8930, loss: 5.335e-02\n",
      "Iteration: 8940, loss: 1.464e-01\n",
      "Iteration: 8950, loss: 3.969e-02\n",
      "Iteration: 8960, loss: 5.528e-02\n",
      "Iteration: 8970, loss: 5.901e-02\n",
      "Iteration: 8980, loss: 4.673e-02\n",
      "Iteration: 8990, loss: 1.331e-01\n",
      "Iteration: 9000, loss: 7.250e-02\n",
      "Iteration: 9010, loss: 7.005e-02\n",
      "Iteration: 9020, loss: 5.973e-02\n",
      "Iteration: 9030, loss: 7.363e-02\n",
      "Iteration: 9040, loss: 1.080e-01\n",
      "Iteration: 9050, loss: 5.699e-02\n",
      "Iteration: 9060, loss: 5.563e-02\n",
      "Iteration: 9070, loss: 5.473e-02\n",
      "Iteration: 9080, loss: 7.341e-02\n",
      "Iteration: 9090, loss: 4.986e-02\n",
      "Iteration: 9100, loss: 7.018e-02\n",
      "Iteration: 9110, loss: 3.548e-02\n",
      "Iteration: 9120, loss: 4.066e-02\n",
      "Iteration: 9130, loss: 4.858e-02\n",
      "Iteration: 9140, loss: 3.714e-02\n",
      "Iteration: 9150, loss: 1.499e-01\n",
      "Iteration: 9160, loss: 9.579e-02\n",
      "Iteration: 9170, loss: 6.928e-02\n",
      "Iteration: 9180, loss: 6.947e-02\n",
      "Iteration: 9190, loss: 4.947e-02\n",
      "Iteration: 9200, loss: 5.492e-02\n",
      "Iteration: 9210, loss: 7.262e-02\n",
      "Iteration: 9220, loss: 2.661e-02\n",
      "Iteration: 9230, loss: 5.680e-02\n",
      "Iteration: 9240, loss: 8.273e-02\n",
      "Iteration: 9250, loss: 5.855e-02\n",
      "Iteration: 9260, loss: 6.110e-02\n",
      "Iteration: 9270, loss: 6.472e-02\n",
      "Iteration: 9280, loss: 6.287e-02\n",
      "Iteration: 9290, loss: 8.821e-02\n",
      "Iteration: 9300, loss: 3.309e-02\n",
      "Iteration: 9310, loss: 1.172e-01\n",
      "Iteration: 9320, loss: 5.274e-02\n",
      "Iteration: 9330, loss: 4.627e-02\n",
      "Iteration: 9340, loss: 5.644e-02\n",
      "Iteration: 9350, loss: 6.817e-02\n",
      "Iteration: 9360, loss: 4.461e-02\n",
      "Iteration: 9370, loss: 3.769e-02\n",
      "Iteration: 9380, loss: 9.584e-02\n",
      "Iteration: 9390, loss: 7.689e-02\n",
      "Iteration: 9400, loss: 5.421e-02\n",
      "Iteration: 9410, loss: 6.294e-02\n",
      "Iteration: 9420, loss: 6.488e-02\n",
      "Iteration: 9430, loss: 6.709e-02\n",
      "Iteration: 9440, loss: 5.340e-02\n",
      "Iteration: 9450, loss: 1.201e-01\n",
      "Iteration: 9460, loss: 9.683e-02\n",
      "Iteration: 9470, loss: 4.304e-02\n",
      "Iteration: 9480, loss: 7.077e-02\n",
      "Iteration: 9490, loss: 1.018e-01\n",
      "Iteration: 9500, loss: 6.196e-02\n",
      "Iteration: 9510, loss: 5.089e-02\n",
      "Iteration: 9520, loss: 5.924e-02\n",
      "Iteration: 9530, loss: 6.280e-02\n",
      "Iteration: 9540, loss: 4.559e-02\n",
      "Iteration: 9550, loss: 7.016e-02\n",
      "Iteration: 9560, loss: 4.381e-02\n",
      "Iteration: 9570, loss: 4.865e-02\n",
      "Iteration: 9580, loss: 8.102e-02\n",
      "Iteration: 9590, loss: 4.883e-02\n",
      "Iteration: 9600, loss: 7.458e-02\n",
      "Iteration: 9610, loss: 5.057e-02\n",
      "Iteration: 9620, loss: 3.565e-02\n",
      "Iteration: 9630, loss: 5.179e-02\n",
      "Iteration: 9640, loss: 5.512e-02\n",
      "Iteration: 9650, loss: 6.111e-02\n",
      "Iteration: 9660, loss: 3.931e-02\n",
      "Iteration: 9670, loss: 4.842e-02\n",
      "Iteration: 9680, loss: 8.675e-02\n",
      "Iteration: 9690, loss: 5.607e-02\n",
      "Iteration: 9700, loss: 7.567e-02\n",
      "Iteration: 9710, loss: 6.741e-02\n",
      "Iteration: 9720, loss: 5.010e-02\n",
      "Iteration: 9730, loss: 3.629e-02\n",
      "Iteration: 9740, loss: 2.310e-02\n",
      "Iteration: 9750, loss: 5.723e-02\n",
      "Iteration: 9760, loss: 9.289e-02\n",
      "Iteration: 9770, loss: 4.043e-02\n",
      "Iteration: 9780, loss: 5.343e-02\n",
      "Iteration: 9790, loss: 6.894e-02\n",
      "Iteration: 9800, loss: 4.689e-02\n",
      "Iteration: 9810, loss: 4.107e-02\n",
      "Iteration: 9820, loss: 3.540e-02\n",
      "Iteration: 9830, loss: 9.274e-02\n",
      "Iteration: 9840, loss: 5.060e-02\n",
      "Iteration: 9850, loss: 3.571e-02\n",
      "Iteration: 9860, loss: 5.284e-02\n",
      "Iteration: 9870, loss: 3.854e-02\n",
      "Iteration: 9880, loss: 6.541e-02\n",
      "Iteration: 9890, loss: 1.000e-01\n",
      "Iteration: 9900, loss: 5.722e-02\n",
      "Iteration: 9910, loss: 4.988e-02\n",
      "Iteration: 9920, loss: 3.205e-02\n",
      "Iteration: 9930, loss: 4.861e-02\n",
      "Iteration: 9940, loss: 5.018e-02\n",
      "Iteration: 9950, loss: 4.835e-02\n",
      "Iteration: 9960, loss: 3.429e-02\n",
      "Iteration: 9970, loss: 4.003e-02\n",
      "Iteration: 9980, loss: 6.275e-02\n",
      "Iteration: 9990, loss: 5.197e-02\n",
      "Iteration: 10000, loss: 5.669e-02\n",
      "Iteration: 10010, loss: 5.257e-02\n",
      "Iteration: 10020, loss: 4.230e-02\n",
      "Iteration: 10030, loss: 4.797e-02\n",
      "Iteration: 10040, loss: 4.220e-02\n",
      "Iteration: 10050, loss: 4.188e-02\n",
      "Iteration: 10060, loss: 4.888e-02\n",
      "Iteration: 10070, loss: 5.371e-02\n",
      "Iteration: 10080, loss: 4.787e-02\n",
      "Iteration: 10090, loss: 8.002e-02\n",
      "Iteration: 10100, loss: 5.006e-02\n",
      "Iteration: 10110, loss: 3.742e-02\n",
      "Iteration: 10120, loss: 4.991e-02\n",
      "Iteration: 10130, loss: 5.015e-02\n",
      "Iteration: 10140, loss: 5.862e-02\n",
      "Iteration: 10150, loss: 5.154e-02\n",
      "Iteration: 10160, loss: 5.848e-02\n",
      "Iteration: 10170, loss: 5.702e-02\n",
      "Iteration: 10180, loss: 5.564e-02\n",
      "Iteration: 10190, loss: 2.782e-02\n",
      "Iteration: 10200, loss: 5.822e-02\n",
      "Iteration: 10210, loss: 4.968e-02\n",
      "Iteration: 10220, loss: 4.963e-02\n",
      "Iteration: 10230, loss: 8.245e-02\n",
      "Iteration: 10240, loss: 5.256e-02\n",
      "Iteration: 10250, loss: 4.455e-02\n",
      "Iteration: 10260, loss: 7.242e-02\n",
      "Iteration: 10270, loss: 5.957e-02\n",
      "Iteration: 10280, loss: 4.535e-02\n",
      "Iteration: 10290, loss: 5.142e-02\n",
      "Iteration: 10300, loss: 3.739e-02\n",
      "Iteration: 10310, loss: 3.666e-02\n",
      "Iteration: 10320, loss: 2.920e-02\n",
      "Iteration: 10330, loss: 3.383e-02\n",
      "Iteration: 10340, loss: 3.860e-02\n",
      "Iteration: 10350, loss: 3.950e-02\n",
      "Iteration: 10360, loss: 2.971e-02\n",
      "Iteration: 10370, loss: 4.820e-02\n",
      "Iteration: 10380, loss: 4.802e-02\n",
      "Iteration: 10390, loss: 2.480e-02\n",
      "Iteration: 10400, loss: 4.822e-02\n",
      "Iteration: 10410, loss: 6.951e-02\n",
      "Iteration: 10420, loss: 6.502e-02\n",
      "Iteration: 10430, loss: 4.766e-02\n",
      "Iteration: 10440, loss: 2.908e-02\n",
      "Iteration: 10450, loss: 3.910e-02\n",
      "Iteration: 10460, loss: 4.254e-02\n",
      "Iteration: 10470, loss: 3.014e-02\n",
      "Iteration: 10480, loss: 3.510e-02\n",
      "Iteration: 10490, loss: 4.989e-02\n",
      "Iteration: 10500, loss: 4.580e-02\n",
      "Iteration: 10510, loss: 2.985e-02\n",
      "Iteration: 10520, loss: 3.557e-02\n",
      "Iteration: 10530, loss: 2.417e-02\n",
      "Iteration: 10540, loss: 7.263e-02\n",
      "Iteration: 10550, loss: 6.259e-02\n",
      "Iteration: 10560, loss: 3.280e-02\n",
      "Iteration: 10570, loss: 4.097e-02\n",
      "Iteration: 10580, loss: 3.045e-02\n",
      "Iteration: 10590, loss: 4.818e-02\n",
      "Iteration: 10600, loss: 5.038e-02\n",
      "Iteration: 10610, loss: 2.273e-02\n",
      "Iteration: 10620, loss: 7.436e-02\n",
      "Iteration: 10630, loss: 4.898e-02\n",
      "Iteration: 10640, loss: 6.293e-02\n",
      "Iteration: 10650, loss: 2.300e-02\n",
      "Iteration: 10660, loss: 6.739e-02\n",
      "Iteration: 10670, loss: 3.902e-02\n",
      "Iteration: 10680, loss: 3.683e-02\n",
      "Iteration: 10690, loss: 2.861e-02\n",
      "Iteration: 10700, loss: 5.277e-02\n",
      "Iteration: 10710, loss: 2.661e-02\n",
      "Iteration: 10720, loss: 2.026e-02\n",
      "Iteration: 10730, loss: 4.088e-02\n",
      "Iteration: 10740, loss: 2.427e-02\n",
      "Iteration: 10750, loss: 3.257e-02\n",
      "Iteration: 10760, loss: 5.797e-02\n",
      "Iteration: 10770, loss: 2.844e-02\n",
      "Iteration: 10780, loss: 1.895e-02\n",
      "Iteration: 10790, loss: 2.602e-02\n",
      "Iteration: 10800, loss: 2.214e-02\n",
      "Iteration: 10810, loss: 4.256e-02\n",
      "Iteration: 10820, loss: 4.825e-02\n",
      "Iteration: 10830, loss: 2.338e-02\n",
      "Iteration: 10840, loss: 3.787e-02\n",
      "Iteration: 10850, loss: 2.567e-02\n",
      "Iteration: 10860, loss: 1.647e-01\n",
      "Iteration: 10870, loss: 2.691e-02\n",
      "Iteration: 10880, loss: 5.566e-02\n",
      "Iteration: 10890, loss: 4.953e-02\n",
      "Iteration: 10900, loss: 6.087e-02\n",
      "Iteration: 10910, loss: 6.037e-02\n",
      "Iteration: 10920, loss: 2.954e-02\n",
      "Iteration: 10930, loss: 3.241e-02\n",
      "Iteration: 10940, loss: 3.960e-02\n",
      "Iteration: 10950, loss: 2.549e-02\n",
      "Iteration: 10960, loss: 2.707e-02\n",
      "Iteration: 10970, loss: 5.212e-02\n",
      "Iteration: 10980, loss: 4.450e-02\n",
      "Iteration: 10990, loss: 2.788e-02\n",
      "Iteration: 11000, loss: 2.275e-02\n",
      "Iteration: 11010, loss: 2.427e-02\n",
      "Iteration: 11020, loss: 3.580e-02\n",
      "Iteration: 11030, loss: 2.409e-02\n",
      "Iteration: 11040, loss: 3.865e-02\n",
      "Iteration: 11050, loss: 2.388e-02\n",
      "Iteration: 11060, loss: 2.584e-02\n",
      "Iteration: 11070, loss: 4.443e-02\n",
      "Iteration: 11080, loss: 5.580e-02\n",
      "Iteration: 11090, loss: 2.314e-02\n",
      "Iteration: 11100, loss: 2.752e-02\n",
      "Iteration: 11110, loss: 2.525e-02\n",
      "Iteration: 11120, loss: 2.227e-02\n",
      "Iteration: 11130, loss: 4.815e-02\n",
      "Iteration: 11140, loss: 3.238e-02\n",
      "Iteration: 11150, loss: 6.231e-02\n",
      "Iteration: 11160, loss: 2.372e-02\n",
      "Iteration: 11170, loss: 9.892e-02\n",
      "Iteration: 11180, loss: 2.810e-02\n",
      "Iteration: 11190, loss: 3.732e-02\n",
      "Iteration: 11200, loss: 2.060e-02\n",
      "Iteration: 11210, loss: 3.653e-02\n",
      "Iteration: 11220, loss: 2.557e-02\n",
      "Iteration: 11230, loss: 2.116e-02\n",
      "Relative error: 1.269421e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10, loss: 5.282e+03\n",
      "Iteration: 20, loss: 3.658e+03\n",
      "Iteration: 30, loss: 3.281e+03\n",
      "Iteration: 40, loss: 1.642e+03\n",
      "Iteration: 50, loss: 1.014e+03\n",
      "Iteration: 60, loss: 9.706e+02\n",
      "Iteration: 70, loss: 5.663e+02\n",
      "Iteration: 80, loss: 3.533e+02\n",
      "Iteration: 90, loss: 3.702e+02\n",
      "Iteration: 100, loss: 5.560e+02\n",
      "Iteration: 110, loss: 3.361e+02\n",
      "Iteration: 120, loss: 3.250e+02\n",
      "Iteration: 130, loss: 1.457e+02\n",
      "Iteration: 140, loss: 1.593e+02\n",
      "Iteration: 150, loss: 1.266e+02\n",
      "Iteration: 160, loss: 1.922e+02\n",
      "Iteration: 170, loss: 9.910e+01\n",
      "Iteration: 180, loss: 2.024e+02\n",
      "Iteration: 190, loss: 1.499e+02\n",
      "Iteration: 200, loss: 8.792e+01\n",
      "Iteration: 210, loss: 1.121e+02\n",
      "Iteration: 220, loss: 6.643e+01\n",
      "Iteration: 230, loss: 1.001e+02\n",
      "Iteration: 240, loss: 8.794e+01\n",
      "Iteration: 250, loss: 6.298e+01\n",
      "Iteration: 260, loss: 5.850e+01\n",
      "Iteration: 270, loss: 4.402e+01\n",
      "Iteration: 280, loss: 6.972e+01\n",
      "Iteration: 290, loss: 3.315e+01\n",
      "Iteration: 300, loss: 5.519e+01\n",
      "Iteration: 310, loss: 7.625e+01\n",
      "Iteration: 320, loss: 7.959e+01\n",
      "Iteration: 330, loss: 3.547e+01\n",
      "Iteration: 340, loss: 7.945e+01\n",
      "Iteration: 350, loss: 3.964e+01\n",
      "Iteration: 360, loss: 3.784e+01\n",
      "Iteration: 370, loss: 4.365e+01\n",
      "Iteration: 380, loss: 2.320e+01\n",
      "Iteration: 390, loss: 3.362e+01\n",
      "Iteration: 400, loss: 3.587e+01\n",
      "Iteration: 410, loss: 3.247e+01\n",
      "Iteration: 420, loss: 3.111e+01\n",
      "Iteration: 430, loss: 6.364e+01\n",
      "Iteration: 440, loss: 2.526e+01\n",
      "Iteration: 450, loss: 2.617e+01\n",
      "Iteration: 460, loss: 3.811e+01\n",
      "Iteration: 470, loss: 2.760e+01\n",
      "Iteration: 480, loss: 2.114e+01\n",
      "Iteration: 490, loss: 4.004e+01\n",
      "Iteration: 500, loss: 2.192e+01\n",
      "Iteration: 510, loss: 2.279e+01\n",
      "Iteration: 520, loss: 1.553e+01\n",
      "Iteration: 530, loss: 1.422e+01\n",
      "Iteration: 540, loss: 2.379e+01\n",
      "Iteration: 550, loss: 2.395e+01\n",
      "Iteration: 560, loss: 2.550e+01\n",
      "Iteration: 570, loss: 3.185e+01\n",
      "Iteration: 580, loss: 1.764e+01\n",
      "Iteration: 590, loss: 1.011e+01\n",
      "Iteration: 600, loss: 1.279e+01\n",
      "Iteration: 610, loss: 1.518e+01\n",
      "Iteration: 620, loss: 1.602e+01\n",
      "Iteration: 630, loss: 2.772e+01\n",
      "Iteration: 640, loss: 1.647e+01\n",
      "Iteration: 650, loss: 5.540e+00\n",
      "Iteration: 660, loss: 1.956e+01\n",
      "Iteration: 670, loss: 2.940e+01\n",
      "Iteration: 680, loss: 2.154e+01\n",
      "Iteration: 690, loss: 7.346e+00\n",
      "Iteration: 700, loss: 1.120e+01\n",
      "Iteration: 710, loss: 1.432e+01\n",
      "Iteration: 720, loss: 2.279e+01\n",
      "Iteration: 730, loss: 9.389e+00\n",
      "Iteration: 740, loss: 1.616e+01\n",
      "Iteration: 750, loss: 1.377e+01\n",
      "Iteration: 760, loss: 1.608e+01\n",
      "Iteration: 770, loss: 1.388e+01\n",
      "Iteration: 780, loss: 8.337e+00\n",
      "Iteration: 790, loss: 8.618e+00\n",
      "Iteration: 800, loss: 8.106e+00\n",
      "Iteration: 810, loss: 7.047e+00\n",
      "Iteration: 820, loss: 1.259e+01\n",
      "Iteration: 830, loss: 4.665e+00\n",
      "Iteration: 840, loss: 1.122e+01\n",
      "Iteration: 850, loss: 1.505e+01\n",
      "Iteration: 860, loss: 8.185e+00\n",
      "Iteration: 870, loss: 1.010e+01\n",
      "Iteration: 880, loss: 1.049e+01\n",
      "Iteration: 890, loss: 8.361e+00\n",
      "Iteration: 900, loss: 7.670e+00\n",
      "Iteration: 910, loss: 6.883e+00\n",
      "Iteration: 920, loss: 7.070e+00\n",
      "Iteration: 930, loss: 9.122e+00\n",
      "Iteration: 940, loss: 7.373e+00\n",
      "Iteration: 950, loss: 1.094e+01\n",
      "Iteration: 960, loss: 1.195e+01\n",
      "Iteration: 970, loss: 4.317e+00\n",
      "Iteration: 980, loss: 6.465e+00\n",
      "Iteration: 990, loss: 1.409e+01\n",
      "Iteration: 1000, loss: 9.838e+00\n",
      "Iteration: 1010, loss: 8.853e+00\n",
      "Iteration: 1020, loss: 7.811e+00\n",
      "Iteration: 1030, loss: 9.657e+00\n",
      "Iteration: 1040, loss: 7.763e+00\n",
      "Iteration: 1050, loss: 8.276e+00\n",
      "Iteration: 1060, loss: 5.990e+00\n",
      "Iteration: 1070, loss: 5.075e+00\n",
      "Iteration: 1080, loss: 6.450e+00\n",
      "Iteration: 1090, loss: 5.403e+00\n",
      "Iteration: 1100, loss: 6.736e+00\n",
      "Iteration: 1110, loss: 4.675e+00\n",
      "Iteration: 1120, loss: 4.850e+00\n",
      "Iteration: 1130, loss: 4.922e+00\n",
      "Iteration: 1140, loss: 6.958e+00\n",
      "Iteration: 1150, loss: 8.227e+00\n",
      "Iteration: 1160, loss: 6.121e+00\n",
      "Iteration: 1170, loss: 5.136e+00\n",
      "Iteration: 1180, loss: 3.068e+00\n",
      "Iteration: 1190, loss: 6.952e+00\n",
      "Iteration: 1200, loss: 8.804e+00\n",
      "Iteration: 1210, loss: 5.070e+00\n",
      "Iteration: 1220, loss: 4.399e+00\n",
      "Iteration: 1230, loss: 3.856e+00\n",
      "Iteration: 1240, loss: 4.590e+00\n",
      "Iteration: 1250, loss: 5.849e+00\n",
      "Iteration: 1260, loss: 3.909e+00\n",
      "Iteration: 1270, loss: 3.683e+00\n",
      "Iteration: 1280, loss: 3.340e+00\n",
      "Iteration: 1290, loss: 2.738e+00\n",
      "Iteration: 1300, loss: 3.756e+00\n",
      "Iteration: 1310, loss: 3.747e+00\n",
      "Iteration: 1320, loss: 6.703e+00\n",
      "Iteration: 1330, loss: 6.807e+00\n",
      "Iteration: 1340, loss: 8.375e+00\n",
      "Iteration: 1350, loss: 2.288e+00\n",
      "Iteration: 1360, loss: 4.848e+00\n",
      "Iteration: 1370, loss: 3.760e+00\n",
      "Iteration: 1380, loss: 3.969e+00\n",
      "Iteration: 1390, loss: 1.900e+00\n",
      "Iteration: 1400, loss: 5.085e+00\n",
      "Iteration: 1410, loss: 4.753e+00\n",
      "Iteration: 1420, loss: 3.809e+00\n",
      "Iteration: 1430, loss: 4.074e+00\n",
      "Iteration: 1440, loss: 3.765e+00\n",
      "Iteration: 1450, loss: 5.787e+00\n",
      "Iteration: 1460, loss: 4.548e+00\n",
      "Iteration: 1470, loss: 1.118e+00\n",
      "Iteration: 1480, loss: 4.627e+00\n",
      "Iteration: 1490, loss: 2.906e+00\n",
      "Iteration: 1500, loss: 2.845e+00\n",
      "Iteration: 1510, loss: 3.998e+00\n",
      "Iteration: 1520, loss: 6.173e+00\n",
      "Iteration: 1530, loss: 6.523e+00\n",
      "Iteration: 1540, loss: 2.947e+00\n",
      "Iteration: 1550, loss: 5.291e+00\n",
      "Iteration: 1560, loss: 3.631e+00\n",
      "Iteration: 1570, loss: 2.574e+00\n",
      "Iteration: 1580, loss: 2.701e+00\n",
      "Iteration: 1590, loss: 1.644e+00\n",
      "Iteration: 1600, loss: 1.799e+00\n",
      "Iteration: 1610, loss: 3.075e+00\n",
      "Iteration: 1620, loss: 1.902e+00\n",
      "Iteration: 1630, loss: 2.863e+00\n",
      "Iteration: 1640, loss: 2.255e+00\n",
      "Iteration: 1650, loss: 1.691e+00\n",
      "Iteration: 1660, loss: 1.542e+00\n",
      "Iteration: 1670, loss: 3.846e+00\n",
      "Iteration: 1680, loss: 3.863e+00\n",
      "Iteration: 1690, loss: 2.243e+00\n",
      "Iteration: 1700, loss: 2.246e+00\n",
      "Iteration: 1710, loss: 6.848e+00\n",
      "Iteration: 1720, loss: 2.233e+00\n",
      "Iteration: 1730, loss: 3.759e+00\n",
      "Iteration: 1740, loss: 1.573e+00\n",
      "Iteration: 1750, loss: 3.283e+00\n",
      "Iteration: 1760, loss: 2.758e+00\n",
      "Iteration: 1770, loss: 3.412e+00\n",
      "Iteration: 1780, loss: 4.604e+00\n",
      "Iteration: 1790, loss: 2.096e+00\n",
      "Iteration: 1800, loss: 3.396e+00\n",
      "Iteration: 1810, loss: 3.577e+00\n",
      "Iteration: 1820, loss: 1.905e+00\n",
      "Iteration: 1830, loss: 3.156e+00\n",
      "Iteration: 1840, loss: 1.800e+00\n",
      "Iteration: 1850, loss: 2.693e+00\n",
      "Iteration: 1860, loss: 4.459e+00\n",
      "Iteration: 1870, loss: 1.994e+00\n",
      "Iteration: 1880, loss: 2.001e+00\n",
      "Iteration: 1890, loss: 2.233e+00\n",
      "Iteration: 1900, loss: 2.741e+00\n",
      "Iteration: 1910, loss: 3.165e+00\n",
      "Iteration: 1920, loss: 2.130e+00\n",
      "Iteration: 1930, loss: 1.636e+00\n",
      "Iteration: 1940, loss: 3.755e+00\n",
      "Iteration: 1950, loss: 3.033e+00\n",
      "Iteration: 1960, loss: 2.309e+00\n",
      "Iteration: 1970, loss: 2.523e+00\n",
      "Iteration: 1980, loss: 1.837e+00\n",
      "Iteration: 1990, loss: 3.573e+00\n",
      "Iteration: 2000, loss: 1.121e+00\n",
      "Iteration: 2010, loss: 1.944e+00\n",
      "Iteration: 2020, loss: 2.387e+00\n",
      "Iteration: 2030, loss: 9.399e-01\n",
      "Iteration: 2040, loss: 2.197e+00\n",
      "Iteration: 2050, loss: 2.585e+00\n",
      "Iteration: 2060, loss: 2.437e+00\n",
      "Iteration: 2070, loss: 2.980e+00\n",
      "Iteration: 2080, loss: 1.468e+00\n",
      "Iteration: 2090, loss: 1.528e+00\n",
      "Iteration: 2100, loss: 1.358e+00\n",
      "Iteration: 2110, loss: 3.048e+00\n",
      "Iteration: 2120, loss: 1.828e+00\n",
      "Iteration: 2130, loss: 9.896e-01\n",
      "Iteration: 2140, loss: 1.745e+00\n",
      "Iteration: 2150, loss: 3.656e+00\n",
      "Iteration: 2160, loss: 1.902e+00\n",
      "Iteration: 2170, loss: 1.588e+00\n",
      "Iteration: 2180, loss: 1.296e+00\n",
      "Iteration: 2190, loss: 4.166e+00\n",
      "Iteration: 2200, loss: 5.939e-01\n",
      "Iteration: 2210, loss: 1.979e+00\n",
      "Iteration: 2220, loss: 1.824e+00\n",
      "Iteration: 2230, loss: 3.167e+00\n",
      "Iteration: 2240, loss: 2.207e+00\n",
      "Iteration: 2250, loss: 5.168e+00\n",
      "Iteration: 2260, loss: 8.453e-01\n",
      "Iteration: 2270, loss: 1.453e+00\n",
      "Iteration: 2280, loss: 2.104e+00\n",
      "Iteration: 2290, loss: 2.543e+00\n",
      "Iteration: 2300, loss: 1.438e+00\n",
      "Iteration: 2310, loss: 9.876e-01\n",
      "Iteration: 2320, loss: 1.359e+00\n",
      "Iteration: 2330, loss: 1.382e+00\n",
      "Iteration: 2340, loss: 1.605e+00\n",
      "Iteration: 2350, loss: 8.895e-01\n",
      "Iteration: 2360, loss: 1.332e+00\n",
      "Iteration: 2370, loss: 2.810e+00\n",
      "Iteration: 2380, loss: 1.989e+00\n",
      "Iteration: 2390, loss: 1.148e+00\n",
      "Iteration: 2400, loss: 1.415e+00\n",
      "Iteration: 2410, loss: 7.291e-01\n",
      "Iteration: 2420, loss: 1.509e+00\n",
      "Iteration: 2430, loss: 1.180e+00\n",
      "Iteration: 2440, loss: 1.576e+00\n",
      "Iteration: 2450, loss: 9.086e-01\n",
      "Iteration: 2460, loss: 1.332e+00\n",
      "Iteration: 2470, loss: 5.851e-01\n",
      "Iteration: 2480, loss: 1.138e+00\n",
      "Iteration: 2490, loss: 1.131e+00\n",
      "Iteration: 2500, loss: 9.776e-01\n",
      "Iteration: 2510, loss: 1.121e+00\n",
      "Iteration: 2520, loss: 9.694e-01\n",
      "Iteration: 2530, loss: 3.159e+00\n",
      "Iteration: 2540, loss: 8.798e-01\n",
      "Iteration: 2550, loss: 1.426e+00\n",
      "Iteration: 2560, loss: 1.195e+00\n",
      "Iteration: 2570, loss: 1.463e+00\n",
      "Iteration: 2580, loss: 6.632e-01\n",
      "Iteration: 2590, loss: 1.129e+00\n",
      "Iteration: 2600, loss: 1.059e+00\n",
      "Iteration: 2610, loss: 1.158e+00\n",
      "Iteration: 2620, loss: 1.928e+00\n",
      "Iteration: 2630, loss: 1.589e+00\n",
      "Iteration: 2640, loss: 1.190e+00\n",
      "Iteration: 2650, loss: 1.871e+00\n",
      "Iteration: 2660, loss: 1.089e+00\n",
      "Iteration: 2670, loss: 1.561e+00\n",
      "Iteration: 2680, loss: 1.029e+00\n",
      "Iteration: 2690, loss: 1.881e+00\n",
      "Iteration: 2700, loss: 1.323e+00\n",
      "Iteration: 2710, loss: 7.938e-01\n",
      "Iteration: 2720, loss: 1.648e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2730, loss: 2.199e+00\n",
      "Iteration: 2740, loss: 6.927e-01\n",
      "Iteration: 2750, loss: 6.811e-01\n",
      "Iteration: 2760, loss: 9.679e-01\n",
      "Iteration: 2770, loss: 1.098e+00\n",
      "Iteration: 2780, loss: 1.205e+00\n",
      "Iteration: 2790, loss: 7.131e-01\n",
      "Iteration: 2800, loss: 7.109e-01\n",
      "Iteration: 2810, loss: 1.403e+00\n",
      "Iteration: 2820, loss: 1.631e+00\n",
      "Iteration: 2830, loss: 9.640e-01\n",
      "Iteration: 2840, loss: 8.377e-01\n",
      "Iteration: 2850, loss: 7.333e-01\n",
      "Iteration: 2860, loss: 4.584e-01\n",
      "Iteration: 2870, loss: 3.992e-01\n",
      "Iteration: 2880, loss: 6.229e-01\n",
      "Iteration: 2890, loss: 1.440e+00\n",
      "Iteration: 2900, loss: 1.188e+00\n",
      "Iteration: 2910, loss: 8.371e-01\n",
      "Iteration: 2920, loss: 1.385e+00\n",
      "Iteration: 2930, loss: 1.054e+00\n",
      "Iteration: 2940, loss: 1.463e+00\n",
      "Iteration: 2950, loss: 1.313e+00\n",
      "Iteration: 2960, loss: 1.160e+00\n",
      "Iteration: 2970, loss: 1.626e+00\n",
      "Iteration: 2980, loss: 2.673e+00\n",
      "Iteration: 2990, loss: 8.804e-01\n",
      "Iteration: 3000, loss: 6.923e-01\n",
      "Iteration: 3010, loss: 8.491e-01\n",
      "Iteration: 3020, loss: 4.099e-01\n",
      "Iteration: 3030, loss: 1.083e+00\n",
      "Iteration: 3040, loss: 7.617e-01\n",
      "Iteration: 3050, loss: 2.513e+00\n",
      "Iteration: 3060, loss: 5.015e-01\n",
      "Iteration: 3070, loss: 8.723e-01\n",
      "Iteration: 3080, loss: 3.047e-01\n",
      "Iteration: 3090, loss: 1.013e+00\n",
      "Iteration: 3100, loss: 8.483e-01\n",
      "Iteration: 3110, loss: 1.138e+00\n",
      "Iteration: 3120, loss: 6.685e-01\n",
      "Iteration: 3130, loss: 1.322e+00\n",
      "Iteration: 3140, loss: 6.904e-01\n",
      "Iteration: 3150, loss: 4.204e-01\n",
      "Iteration: 3160, loss: 1.296e+00\n",
      "Iteration: 3170, loss: 1.195e+00\n",
      "Iteration: 3180, loss: 1.274e+00\n",
      "Iteration: 3190, loss: 1.384e+00\n",
      "Iteration: 3200, loss: 5.338e-01\n",
      "Iteration: 3210, loss: 7.136e-01\n",
      "Iteration: 3220, loss: 7.703e-01\n",
      "Iteration: 3230, loss: 4.541e-01\n",
      "Iteration: 3240, loss: 9.820e-01\n",
      "Iteration: 3250, loss: 8.731e-01\n",
      "Iteration: 3260, loss: 7.391e-01\n",
      "Iteration: 3270, loss: 7.597e-01\n",
      "Iteration: 3280, loss: 6.345e-01\n",
      "Iteration: 3290, loss: 4.911e-01\n",
      "Iteration: 3300, loss: 6.075e-01\n",
      "Iteration: 3310, loss: 5.467e-01\n",
      "Iteration: 3320, loss: 1.589e+00\n",
      "Iteration: 3330, loss: 2.600e-01\n",
      "Iteration: 3340, loss: 1.212e+00\n",
      "Iteration: 3350, loss: 8.227e-01\n",
      "Iteration: 3360, loss: 1.370e+00\n",
      "Iteration: 3370, loss: 1.030e+00\n",
      "Iteration: 3380, loss: 7.552e-01\n",
      "Iteration: 3390, loss: 7.070e-01\n",
      "Iteration: 3400, loss: 8.023e-01\n",
      "Iteration: 3410, loss: 7.766e-01\n",
      "Iteration: 3420, loss: 5.912e-01\n",
      "Iteration: 3430, loss: 1.176e+00\n",
      "Iteration: 3440, loss: 5.983e-01\n",
      "Iteration: 3450, loss: 5.464e-01\n",
      "Iteration: 3460, loss: 8.002e-01\n",
      "Iteration: 3470, loss: 4.380e-01\n",
      "Iteration: 3480, loss: 7.177e-01\n",
      "Iteration: 3490, loss: 5.194e-01\n",
      "Iteration: 3500, loss: 1.979e-01\n",
      "Iteration: 3510, loss: 6.884e-01\n",
      "Iteration: 3520, loss: 1.037e+00\n",
      "Iteration: 3530, loss: 6.951e-01\n",
      "Iteration: 3540, loss: 1.025e+00\n",
      "Iteration: 3550, loss: 8.812e-01\n",
      "Iteration: 3560, loss: 3.024e-01\n",
      "Iteration: 3570, loss: 7.440e-01\n",
      "Iteration: 3580, loss: 4.283e-01\n",
      "Iteration: 3590, loss: 3.551e-01\n",
      "Iteration: 3600, loss: 5.401e-01\n",
      "Iteration: 3610, loss: 7.182e-01\n",
      "Iteration: 3620, loss: 5.962e-01\n",
      "Iteration: 3630, loss: 4.620e-01\n",
      "Iteration: 3640, loss: 6.282e-01\n",
      "Iteration: 3650, loss: 6.559e-01\n",
      "Iteration: 3660, loss: 1.465e+00\n",
      "Iteration: 3670, loss: 1.044e+00\n",
      "Iteration: 3680, loss: 4.324e-01\n",
      "Iteration: 3690, loss: 1.023e+00\n",
      "Iteration: 3700, loss: 6.937e-01\n",
      "Iteration: 3710, loss: 2.370e-01\n",
      "Iteration: 3720, loss: 3.841e-01\n",
      "Iteration: 3730, loss: 3.782e-01\n",
      "Iteration: 3740, loss: 6.915e-01\n",
      "Iteration: 3750, loss: 7.313e-01\n",
      "Iteration: 3760, loss: 6.021e-01\n",
      "Iteration: 3770, loss: 8.362e-01\n",
      "Iteration: 3780, loss: 7.200e-01\n",
      "Iteration: 3790, loss: 6.764e-01\n",
      "Iteration: 3800, loss: 5.799e-01\n",
      "Iteration: 3810, loss: 6.816e-01\n",
      "Iteration: 3820, loss: 3.466e-01\n",
      "Iteration: 3830, loss: 8.126e-01\n",
      "Iteration: 3840, loss: 4.859e-01\n",
      "Iteration: 3850, loss: 4.421e-01\n",
      "Iteration: 3860, loss: 4.546e-01\n",
      "Iteration: 3870, loss: 5.967e-01\n",
      "Iteration: 3880, loss: 1.127e+00\n",
      "Iteration: 3890, loss: 3.864e-01\n",
      "Iteration: 3900, loss: 3.168e-01\n",
      "Iteration: 3910, loss: 7.291e-01\n",
      "Iteration: 3920, loss: 4.308e-01\n",
      "Iteration: 3930, loss: 2.688e-01\n",
      "Iteration: 3940, loss: 1.019e+00\n",
      "Iteration: 3950, loss: 3.684e-01\n",
      "Iteration: 3960, loss: 7.767e-01\n",
      "Iteration: 3970, loss: 2.044e-01\n",
      "Iteration: 3980, loss: 2.036e-01\n",
      "Iteration: 3990, loss: 9.331e-01\n",
      "Iteration: 4000, loss: 1.822e-01\n",
      "Iteration: 4010, loss: 7.696e-01\n",
      "Iteration: 4020, loss: 1.957e-01\n",
      "Iteration: 4030, loss: 4.704e-01\n",
      "Iteration: 4040, loss: 2.405e-01\n",
      "Iteration: 4050, loss: 4.195e-01\n",
      "Iteration: 4060, loss: 5.517e-01\n",
      "Iteration: 4070, loss: 5.661e-01\n",
      "Iteration: 4080, loss: 3.599e-01\n",
      "Iteration: 4090, loss: 3.453e-01\n",
      "Iteration: 4100, loss: 5.887e-01\n",
      "Iteration: 4110, loss: 4.562e-01\n",
      "Iteration: 4120, loss: 3.789e-01\n",
      "Iteration: 4130, loss: 4.728e-01\n",
      "Iteration: 4140, loss: 8.014e-01\n",
      "Iteration: 4150, loss: 6.129e-01\n",
      "Iteration: 4160, loss: 3.359e-01\n",
      "Iteration: 4170, loss: 4.742e-01\n",
      "Iteration: 4180, loss: 3.221e-01\n",
      "Iteration: 4190, loss: 2.213e-01\n",
      "Iteration: 4200, loss: 3.528e-01\n",
      "Iteration: 4210, loss: 1.787e-01\n",
      "Iteration: 4220, loss: 2.858e-01\n",
      "Iteration: 4230, loss: 4.350e-01\n",
      "Iteration: 4240, loss: 4.978e-01\n",
      "Iteration: 4250, loss: 3.710e-01\n",
      "Iteration: 4260, loss: 2.824e-01\n",
      "Iteration: 4270, loss: 4.563e-01\n",
      "Iteration: 4280, loss: 9.435e-01\n",
      "Iteration: 4290, loss: 5.300e-01\n",
      "Iteration: 4300, loss: 3.813e-01\n",
      "Iteration: 4310, loss: 3.146e-01\n",
      "Iteration: 4320, loss: 4.954e-01\n",
      "Iteration: 4330, loss: 2.477e-01\n",
      "Iteration: 4340, loss: 4.572e-01\n",
      "Iteration: 4350, loss: 1.626e-01\n",
      "Iteration: 4360, loss: 2.207e-01\n",
      "Iteration: 4370, loss: 3.173e-01\n",
      "Iteration: 4380, loss: 4.846e-01\n",
      "Iteration: 4390, loss: 4.433e-01\n",
      "Iteration: 4400, loss: 3.915e-01\n",
      "Iteration: 4410, loss: 2.581e-01\n",
      "Iteration: 4420, loss: 1.780e-01\n",
      "Iteration: 4430, loss: 3.622e-01\n",
      "Iteration: 4440, loss: 2.163e-01\n",
      "Iteration: 4450, loss: 5.804e-01\n",
      "Iteration: 4460, loss: 5.300e-01\n",
      "Iteration: 4470, loss: 4.492e-01\n",
      "Iteration: 4480, loss: 2.833e-01\n",
      "Iteration: 4490, loss: 4.453e-01\n",
      "Iteration: 4500, loss: 3.118e-01\n",
      "Iteration: 4510, loss: 3.282e-01\n",
      "Iteration: 4520, loss: 2.473e-01\n",
      "Iteration: 4530, loss: 2.122e-01\n",
      "Iteration: 4540, loss: 3.945e-01\n",
      "Iteration: 4550, loss: 5.839e-01\n",
      "Iteration: 4560, loss: 3.070e-01\n",
      "Iteration: 4570, loss: 4.068e-01\n",
      "Iteration: 4580, loss: 3.010e-01\n",
      "Iteration: 4590, loss: 4.658e-01\n",
      "Iteration: 4600, loss: 1.863e-01\n",
      "Iteration: 4610, loss: 2.637e-01\n",
      "Iteration: 4620, loss: 1.532e-01\n",
      "Iteration: 4630, loss: 3.333e-01\n",
      "Iteration: 4640, loss: 3.249e-01\n",
      "Iteration: 4650, loss: 2.024e-01\n",
      "Iteration: 4660, loss: 4.312e-01\n",
      "Iteration: 4670, loss: 4.285e-01\n",
      "Iteration: 4680, loss: 2.427e-01\n",
      "Iteration: 4690, loss: 5.593e-01\n",
      "Iteration: 4700, loss: 4.467e-01\n",
      "Iteration: 4710, loss: 2.028e-01\n",
      "Iteration: 4720, loss: 4.782e-01\n",
      "Iteration: 4730, loss: 4.116e-01\n",
      "Iteration: 4740, loss: 3.772e-01\n",
      "Iteration: 4750, loss: 1.980e-01\n",
      "Iteration: 4760, loss: 2.738e-01\n",
      "Iteration: 4770, loss: 4.956e-01\n",
      "Iteration: 4780, loss: 4.921e-01\n",
      "Iteration: 4790, loss: 5.371e-01\n",
      "Iteration: 4800, loss: 3.519e-01\n",
      "Iteration: 4810, loss: 3.036e-01\n",
      "Iteration: 4820, loss: 6.218e-01\n",
      "Iteration: 4830, loss: 2.341e-01\n",
      "Iteration: 4840, loss: 4.404e-01\n",
      "Iteration: 4850, loss: 1.841e-01\n",
      "Iteration: 4860, loss: 5.504e-01\n",
      "Iteration: 4870, loss: 2.916e-01\n",
      "Iteration: 4880, loss: 2.612e-01\n",
      "Iteration: 4890, loss: 2.844e-01\n",
      "Iteration: 4900, loss: 4.448e-01\n",
      "Iteration: 4910, loss: 3.824e-01\n",
      "Iteration: 4920, loss: 3.597e-01\n",
      "Iteration: 4930, loss: 2.256e-01\n",
      "Iteration: 4940, loss: 4.005e-01\n",
      "Iteration: 4950, loss: 3.116e-01\n",
      "Iteration: 4960, loss: 2.607e-01\n",
      "Iteration: 4970, loss: 2.387e-01\n",
      "Iteration: 4980, loss: 1.838e-01\n",
      "Iteration: 4990, loss: 3.248e-01\n",
      "Iteration: 5000, loss: 2.099e-01\n",
      "Iteration: 5010, loss: 1.976e-01\n",
      "Iteration: 5020, loss: 1.301e-01\n",
      "Iteration: 5030, loss: 2.919e-01\n",
      "Iteration: 5040, loss: 3.774e-01\n",
      "Iteration: 5050, loss: 3.623e-01\n",
      "Iteration: 5060, loss: 2.598e-01\n",
      "Iteration: 5070, loss: 2.142e-01\n",
      "Iteration: 5080, loss: 2.864e-01\n",
      "Iteration: 5090, loss: 3.181e-01\n",
      "Iteration: 5100, loss: 2.593e-01\n",
      "Iteration: 5110, loss: 5.940e-01\n",
      "Iteration: 5120, loss: 1.600e-01\n",
      "Iteration: 5130, loss: 1.658e-01\n",
      "Iteration: 5140, loss: 1.763e-01\n",
      "Iteration: 5150, loss: 3.955e-01\n",
      "Iteration: 5160, loss: 1.901e-01\n",
      "Iteration: 5170, loss: 1.968e-01\n",
      "Iteration: 5180, loss: 3.080e-01\n",
      "Iteration: 5190, loss: 3.442e-01\n",
      "Iteration: 5200, loss: 1.918e-01\n",
      "Iteration: 5210, loss: 1.513e-01\n",
      "Iteration: 5220, loss: 1.895e-01\n",
      "Iteration: 5230, loss: 6.617e-02\n",
      "Iteration: 5240, loss: 2.469e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5250, loss: 2.928e-01\n",
      "Iteration: 5260, loss: 1.632e-01\n",
      "Iteration: 5270, loss: 2.760e-01\n",
      "Iteration: 5280, loss: 2.595e-01\n",
      "Iteration: 5290, loss: 2.091e-01\n",
      "Iteration: 5300, loss: 2.459e-01\n",
      "Iteration: 5310, loss: 2.097e-01\n",
      "Iteration: 5320, loss: 2.050e-01\n",
      "Iteration: 5330, loss: 3.194e-01\n",
      "Iteration: 5340, loss: 2.320e-01\n",
      "Iteration: 5350, loss: 1.912e-01\n",
      "Iteration: 5360, loss: 1.388e-01\n",
      "Iteration: 5370, loss: 2.355e-01\n",
      "Iteration: 5380, loss: 3.746e-01\n",
      "Iteration: 5390, loss: 1.461e-01\n",
      "Iteration: 5400, loss: 1.364e-01\n",
      "Iteration: 5410, loss: 2.210e-01\n",
      "Iteration: 5420, loss: 2.656e-01\n",
      "Iteration: 5430, loss: 1.355e-01\n",
      "Iteration: 5440, loss: 1.630e-01\n",
      "Iteration: 5450, loss: 3.828e-01\n",
      "Iteration: 5460, loss: 3.068e-01\n",
      "Iteration: 5470, loss: 1.860e-01\n",
      "Iteration: 5480, loss: 2.268e-01\n",
      "Iteration: 5490, loss: 2.176e-01\n",
      "Iteration: 5500, loss: 2.183e-01\n",
      "Iteration: 5510, loss: 1.385e-01\n",
      "Iteration: 5520, loss: 3.417e-01\n",
      "Iteration: 5530, loss: 1.833e-01\n",
      "Iteration: 5540, loss: 1.659e-01\n",
      "Iteration: 5550, loss: 2.613e-01\n",
      "Iteration: 5560, loss: 1.026e-01\n",
      "Iteration: 5570, loss: 2.325e-01\n",
      "Iteration: 5580, loss: 3.978e-01\n",
      "Iteration: 5590, loss: 2.257e-01\n",
      "Iteration: 5600, loss: 1.710e-01\n",
      "Iteration: 5610, loss: 2.615e-01\n",
      "Iteration: 5620, loss: 1.391e-01\n",
      "Iteration: 5630, loss: 1.275e-01\n",
      "Iteration: 5640, loss: 1.881e-01\n",
      "Iteration: 5650, loss: 2.390e-01\n",
      "Iteration: 5660, loss: 1.337e-01\n",
      "Iteration: 5670, loss: 1.100e-01\n",
      "Iteration: 5680, loss: 2.496e-01\n",
      "Iteration: 5690, loss: 1.538e-01\n",
      "Iteration: 5700, loss: 2.566e-01\n",
      "Iteration: 5710, loss: 2.015e-01\n",
      "Iteration: 5720, loss: 1.739e-01\n",
      "Iteration: 5730, loss: 1.300e-01\n",
      "Iteration: 5740, loss: 2.468e-01\n",
      "Iteration: 5750, loss: 9.485e-02\n",
      "Iteration: 5760, loss: 1.375e-01\n",
      "Iteration: 5770, loss: 2.155e-01\n",
      "Iteration: 5780, loss: 1.704e-01\n",
      "Iteration: 5790, loss: 2.525e-01\n",
      "Iteration: 5800, loss: 2.976e-01\n",
      "Iteration: 5810, loss: 3.418e-01\n",
      "Iteration: 5820, loss: 1.236e-01\n",
      "Iteration: 5830, loss: 1.250e-01\n",
      "Iteration: 5840, loss: 2.762e-01\n",
      "Iteration: 5850, loss: 1.357e-01\n",
      "Iteration: 5860, loss: 1.861e-01\n",
      "Iteration: 5870, loss: 2.114e-01\n",
      "Iteration: 5880, loss: 2.722e-01\n",
      "Iteration: 5890, loss: 1.832e-01\n",
      "Iteration: 5900, loss: 1.191e-01\n",
      "Iteration: 5910, loss: 1.192e-01\n",
      "Iteration: 5920, loss: 4.210e-01\n",
      "Iteration: 5930, loss: 2.517e-01\n",
      "Iteration: 5940, loss: 1.283e-01\n",
      "Iteration: 5950, loss: 1.313e-01\n",
      "Iteration: 5960, loss: 1.566e-01\n",
      "Iteration: 5970, loss: 2.409e-01\n",
      "Iteration: 5980, loss: 1.761e-01\n",
      "Iteration: 5990, loss: 1.830e-01\n",
      "Iteration: 6000, loss: 1.228e-01\n",
      "Iteration: 6010, loss: 2.138e-01\n",
      "Iteration: 6020, loss: 1.550e-01\n",
      "Iteration: 6030, loss: 1.481e-01\n",
      "Iteration: 6040, loss: 2.004e-01\n",
      "Iteration: 6050, loss: 1.353e-01\n",
      "Iteration: 6060, loss: 1.501e-01\n",
      "Iteration: 6070, loss: 2.056e-01\n",
      "Iteration: 6080, loss: 1.449e-01\n",
      "Iteration: 6090, loss: 1.291e-01\n",
      "Iteration: 6100, loss: 2.899e-01\n",
      "Iteration: 6110, loss: 1.743e-01\n",
      "Iteration: 6120, loss: 1.339e-01\n",
      "Iteration: 6130, loss: 1.113e-01\n",
      "Iteration: 6140, loss: 1.393e-01\n",
      "Iteration: 6150, loss: 1.206e-01\n",
      "Iteration: 6160, loss: 9.329e-02\n",
      "Iteration: 6170, loss: 6.417e-02\n",
      "Iteration: 6180, loss: 1.801e-01\n",
      "Iteration: 6190, loss: 1.478e-01\n",
      "Iteration: 6200, loss: 2.017e-01\n",
      "Iteration: 6210, loss: 8.910e-02\n",
      "Iteration: 6220, loss: 8.186e-02\n",
      "Iteration: 6230, loss: 1.083e-01\n",
      "Iteration: 6240, loss: 1.472e-01\n",
      "Iteration: 6250, loss: 1.561e-01\n",
      "Iteration: 6260, loss: 1.408e-01\n",
      "Iteration: 6270, loss: 1.189e-01\n",
      "Iteration: 6280, loss: 8.369e-02\n",
      "Iteration: 6290, loss: 5.641e-02\n",
      "Iteration: 6300, loss: 2.444e-01\n",
      "Iteration: 6310, loss: 1.142e-01\n",
      "Iteration: 6320, loss: 9.278e-02\n",
      "Iteration: 6330, loss: 1.650e-01\n",
      "Iteration: 6340, loss: 8.799e-02\n",
      "Iteration: 6350, loss: 9.009e-02\n",
      "Iteration: 6360, loss: 3.534e-01\n",
      "Iteration: 6370, loss: 1.052e-01\n",
      "Iteration: 6380, loss: 2.458e-01\n",
      "Iteration: 6390, loss: 1.604e-01\n",
      "Iteration: 6400, loss: 5.356e-02\n",
      "Iteration: 6410, loss: 6.931e-02\n",
      "Iteration: 6420, loss: 1.202e-01\n",
      "Iteration: 6430, loss: 1.791e-01\n",
      "Iteration: 6440, loss: 1.400e-01\n",
      "Iteration: 6450, loss: 1.025e-01\n",
      "Iteration: 6460, loss: 1.050e-01\n",
      "Iteration: 6470, loss: 1.070e-01\n",
      "Iteration: 6480, loss: 1.541e-01\n",
      "Iteration: 6490, loss: 1.673e-01\n",
      "Iteration: 6500, loss: 1.053e-01\n",
      "Iteration: 6510, loss: 1.174e-01\n",
      "Iteration: 6520, loss: 1.337e-01\n",
      "Iteration: 6530, loss: 1.024e-01\n",
      "Iteration: 6540, loss: 5.266e-02\n",
      "Iteration: 6550, loss: 1.590e-01\n",
      "Iteration: 6560, loss: 7.588e-02\n",
      "Iteration: 6570, loss: 4.113e-02\n",
      "Iteration: 6580, loss: 7.553e-02\n",
      "Iteration: 6590, loss: 7.432e-02\n",
      "Iteration: 6600, loss: 8.814e-02\n",
      "Iteration: 6610, loss: 1.365e-01\n",
      "Iteration: 6620, loss: 8.932e-02\n",
      "Iteration: 6630, loss: 1.724e-01\n",
      "Iteration: 6640, loss: 2.017e-01\n",
      "Iteration: 6650, loss: 2.511e-01\n",
      "Iteration: 6660, loss: 1.349e-01\n",
      "Iteration: 6670, loss: 2.007e-01\n",
      "Iteration: 6680, loss: 1.363e-01\n",
      "Iteration: 6690, loss: 1.054e-01\n",
      "Iteration: 6700, loss: 1.252e-01\n",
      "Iteration: 6710, loss: 1.468e-01\n",
      "Iteration: 6720, loss: 1.275e-01\n",
      "Iteration: 6730, loss: 1.552e-01\n",
      "Iteration: 6740, loss: 1.774e-01\n",
      "Iteration: 6750, loss: 1.342e-01\n",
      "Iteration: 6760, loss: 7.441e-02\n",
      "Iteration: 6770, loss: 2.472e-01\n",
      "Iteration: 6780, loss: 1.356e-01\n",
      "Iteration: 6790, loss: 1.305e-01\n",
      "Iteration: 6800, loss: 2.217e-01\n",
      "Iteration: 6810, loss: 5.938e-02\n",
      "Iteration: 6820, loss: 7.035e-02\n",
      "Iteration: 6830, loss: 1.309e-01\n",
      "Iteration: 6840, loss: 8.273e-02\n",
      "Iteration: 6850, loss: 9.568e-02\n",
      "Iteration: 6860, loss: 1.024e-01\n",
      "Iteration: 6870, loss: 1.306e-01\n",
      "Iteration: 6880, loss: 5.614e-02\n",
      "Iteration: 6890, loss: 1.929e-01\n",
      "Iteration: 6900, loss: 1.074e-01\n",
      "Iteration: 6910, loss: 1.489e-01\n",
      "Iteration: 6920, loss: 7.330e-02\n",
      "Iteration: 6930, loss: 1.288e-01\n",
      "Iteration: 6940, loss: 3.604e-02\n",
      "Iteration: 6950, loss: 2.042e-01\n",
      "Iteration: 6960, loss: 7.381e-02\n",
      "Iteration: 6970, loss: 6.298e-02\n",
      "Iteration: 6980, loss: 8.728e-02\n",
      "Iteration: 6990, loss: 1.196e-01\n",
      "Iteration: 7000, loss: 1.048e-01\n",
      "Iteration: 7010, loss: 7.213e-02\n",
      "Iteration: 7020, loss: 1.447e-01\n",
      "Iteration: 7030, loss: 6.885e-02\n",
      "Iteration: 7040, loss: 7.331e-02\n",
      "Iteration: 7050, loss: 1.457e-01\n",
      "Iteration: 7060, loss: 6.191e-02\n",
      "Iteration: 7070, loss: 1.190e-01\n",
      "Iteration: 7080, loss: 1.551e-01\n",
      "Iteration: 7090, loss: 9.204e-02\n",
      "Iteration: 7100, loss: 1.330e-01\n",
      "Iteration: 7110, loss: 1.130e-01\n",
      "Iteration: 7120, loss: 9.764e-02\n",
      "Iteration: 7130, loss: 1.475e-01\n",
      "Iteration: 7140, loss: 1.466e-01\n",
      "Iteration: 7150, loss: 8.411e-02\n",
      "Iteration: 7160, loss: 1.671e-01\n",
      "Iteration: 7170, loss: 8.749e-02\n",
      "Iteration: 7180, loss: 1.339e-01\n",
      "Iteration: 7190, loss: 9.985e-02\n",
      "Iteration: 7200, loss: 6.610e-02\n",
      "Iteration: 7210, loss: 7.355e-02\n",
      "Iteration: 7220, loss: 7.719e-02\n",
      "Iteration: 7230, loss: 8.655e-02\n",
      "Iteration: 7240, loss: 1.380e-01\n",
      "Iteration: 7250, loss: 4.777e-02\n",
      "Iteration: 7260, loss: 8.605e-02\n",
      "Iteration: 7270, loss: 7.271e-02\n",
      "Iteration: 7280, loss: 6.889e-02\n",
      "Iteration: 7290, loss: 1.597e-01\n",
      "Iteration: 7300, loss: 4.642e-02\n",
      "Iteration: 7310, loss: 8.168e-02\n",
      "Iteration: 7320, loss: 8.664e-02\n",
      "Iteration: 7330, loss: 9.191e-02\n",
      "Iteration: 7340, loss: 1.019e-01\n",
      "Iteration: 7350, loss: 9.845e-02\n",
      "Iteration: 7360, loss: 4.829e-02\n",
      "Iteration: 7370, loss: 3.230e-02\n",
      "Iteration: 7380, loss: 1.309e-01\n",
      "Iteration: 7390, loss: 8.591e-02\n",
      "Iteration: 7400, loss: 1.355e-01\n",
      "Iteration: 7410, loss: 1.083e-01\n",
      "Iteration: 7420, loss: 6.983e-02\n",
      "Iteration: 7430, loss: 5.871e-02\n",
      "Iteration: 7440, loss: 8.787e-02\n",
      "Iteration: 7450, loss: 7.881e-02\n",
      "Iteration: 7460, loss: 1.840e-01\n",
      "Iteration: 7470, loss: 7.528e-02\n",
      "Iteration: 7480, loss: 9.613e-02\n",
      "Iteration: 7490, loss: 9.023e-02\n",
      "Iteration: 7500, loss: 1.159e-01\n",
      "Iteration: 7510, loss: 1.051e-01\n",
      "Iteration: 7520, loss: 1.180e-01\n",
      "Iteration: 7530, loss: 4.315e-02\n",
      "Iteration: 7540, loss: 1.018e-01\n",
      "Iteration: 7550, loss: 5.309e-02\n",
      "Iteration: 7560, loss: 1.096e-01\n",
      "Iteration: 7570, loss: 5.478e-02\n",
      "Iteration: 7580, loss: 6.682e-02\n",
      "Iteration: 7590, loss: 8.921e-02\n",
      "Iteration: 7600, loss: 9.805e-02\n",
      "Iteration: 7610, loss: 5.323e-02\n",
      "Iteration: 7620, loss: 1.505e-01\n",
      "Iteration: 7630, loss: 1.341e-01\n",
      "Iteration: 7640, loss: 7.383e-02\n",
      "Iteration: 7650, loss: 5.497e-02\n",
      "Iteration: 7660, loss: 9.187e-02\n",
      "Iteration: 7670, loss: 5.172e-02\n",
      "Iteration: 7680, loss: 6.786e-02\n",
      "Iteration: 7690, loss: 4.664e-02\n",
      "Iteration: 7700, loss: 6.041e-02\n",
      "Iteration: 7710, loss: 6.151e-02\n",
      "Iteration: 7720, loss: 7.575e-02\n",
      "Iteration: 7730, loss: 7.644e-02\n",
      "Iteration: 7740, loss: 2.141e-01\n",
      "Iteration: 7750, loss: 3.259e-02\n",
      "Iteration: 7760, loss: 1.002e-01\n",
      "Iteration: 7770, loss: 7.728e-02\n",
      "Iteration: 7780, loss: 1.439e-02\n",
      "Iteration: 7790, loss: 5.640e-02\n",
      "Iteration: 7800, loss: 1.347e-01\n",
      "Iteration: 7810, loss: 1.592e-01\n",
      "Iteration: 7820, loss: 5.169e-02\n",
      "Iteration: 7830, loss: 5.500e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7840, loss: 1.025e-01\n",
      "Iteration: 7850, loss: 1.156e-01\n",
      "Iteration: 7860, loss: 4.996e-02\n",
      "Iteration: 7870, loss: 8.751e-02\n",
      "Iteration: 7880, loss: 5.216e-02\n",
      "Iteration: 7890, loss: 8.058e-02\n",
      "Iteration: 7900, loss: 5.517e-02\n",
      "Iteration: 7910, loss: 8.769e-02\n",
      "Iteration: 7920, loss: 3.086e-02\n",
      "Iteration: 7930, loss: 3.711e-02\n",
      "Iteration: 7940, loss: 5.348e-02\n",
      "Iteration: 7950, loss: 5.074e-02\n",
      "Iteration: 7960, loss: 7.451e-02\n",
      "Iteration: 7970, loss: 7.735e-02\n",
      "Iteration: 7980, loss: 4.243e-02\n",
      "Iteration: 7990, loss: 9.346e-02\n",
      "Iteration: 8000, loss: 4.622e-02\n",
      "Iteration: 8010, loss: 4.857e-02\n",
      "Iteration: 8020, loss: 6.303e-02\n",
      "Iteration: 8030, loss: 1.494e-01\n",
      "Iteration: 8040, loss: 6.530e-02\n",
      "Iteration: 8050, loss: 4.746e-02\n",
      "Iteration: 8060, loss: 5.299e-02\n",
      "Iteration: 8070, loss: 6.080e-02\n",
      "Iteration: 8080, loss: 6.263e-02\n",
      "Iteration: 8090, loss: 3.463e-02\n",
      "Iteration: 8100, loss: 5.193e-02\n",
      "Iteration: 8110, loss: 5.129e-02\n",
      "Iteration: 8120, loss: 4.268e-02\n",
      "Iteration: 8130, loss: 8.652e-02\n",
      "Iteration: 8140, loss: 3.795e-02\n",
      "Iteration: 8150, loss: 7.868e-02\n",
      "Iteration: 8160, loss: 5.417e-02\n",
      "Iteration: 8170, loss: 4.547e-02\n",
      "Iteration: 8180, loss: 2.912e-02\n",
      "Iteration: 8190, loss: 2.671e-02\n",
      "Iteration: 8200, loss: 5.461e-02\n",
      "Iteration: 8210, loss: 1.035e-01\n",
      "Iteration: 8220, loss: 7.048e-02\n",
      "Iteration: 8230, loss: 3.443e-02\n",
      "Iteration: 8240, loss: 9.721e-02\n",
      "Iteration: 8250, loss: 7.174e-02\n",
      "Iteration: 8260, loss: 7.593e-02\n",
      "Iteration: 8270, loss: 6.222e-02\n",
      "Iteration: 8280, loss: 8.412e-02\n",
      "Iteration: 8290, loss: 2.972e-02\n",
      "Iteration: 8300, loss: 4.438e-02\n",
      "Iteration: 8310, loss: 1.116e-01\n",
      "Iteration: 8320, loss: 6.393e-02\n",
      "Iteration: 8330, loss: 7.346e-02\n",
      "Iteration: 8340, loss: 4.608e-02\n",
      "Iteration: 8350, loss: 6.187e-02\n",
      "Iteration: 8360, loss: 7.935e-02\n",
      "Iteration: 8370, loss: 5.778e-02\n",
      "Iteration: 8380, loss: 4.457e-02\n",
      "Iteration: 8390, loss: 2.553e-02\n",
      "Iteration: 8400, loss: 4.408e-02\n",
      "Iteration: 8410, loss: 2.947e-02\n",
      "Iteration: 8420, loss: 3.942e-02\n",
      "Iteration: 8430, loss: 7.318e-02\n",
      "Iteration: 8440, loss: 6.212e-02\n",
      "Iteration: 8450, loss: 6.921e-02\n",
      "Iteration: 8460, loss: 9.200e-02\n",
      "Iteration: 8470, loss: 6.033e-02\n",
      "Iteration: 8480, loss: 5.131e-02\n",
      "Iteration: 8490, loss: 8.882e-02\n",
      "Iteration: 8500, loss: 6.452e-02\n",
      "Iteration: 8510, loss: 5.154e-02\n",
      "Iteration: 8520, loss: 5.583e-02\n",
      "Iteration: 8530, loss: 5.159e-02\n",
      "Iteration: 8540, loss: 3.598e-02\n",
      "Iteration: 8550, loss: 6.408e-02\n",
      "Iteration: 8560, loss: 6.486e-02\n",
      "Iteration: 8570, loss: 7.643e-02\n",
      "Iteration: 8580, loss: 3.113e-02\n",
      "Iteration: 8590, loss: 6.880e-02\n",
      "Iteration: 8600, loss: 4.837e-02\n",
      "Iteration: 8610, loss: 6.556e-02\n",
      "Iteration: 8620, loss: 7.682e-02\n",
      "Iteration: 8630, loss: 4.193e-02\n",
      "Iteration: 8640, loss: 7.951e-02\n",
      "Iteration: 8650, loss: 4.681e-02\n",
      "Iteration: 8660, loss: 2.934e-02\n",
      "Iteration: 8670, loss: 2.635e-02\n",
      "Iteration: 8680, loss: 4.178e-02\n",
      "Iteration: 8690, loss: 6.997e-02\n",
      "Iteration: 8700, loss: 4.974e-02\n",
      "Iteration: 8710, loss: 5.728e-02\n",
      "Iteration: 8720, loss: 4.120e-02\n",
      "Iteration: 8730, loss: 2.643e-02\n",
      "Iteration: 8740, loss: 3.805e-02\n",
      "Iteration: 8750, loss: 5.630e-02\n",
      "Iteration: 8760, loss: 5.911e-02\n",
      "Iteration: 8770, loss: 1.214e-01\n",
      "Iteration: 8780, loss: 5.803e-02\n",
      "Iteration: 8790, loss: 2.255e-02\n",
      "Iteration: 8800, loss: 4.573e-02\n",
      "Iteration: 8810, loss: 2.473e-02\n",
      "Iteration: 8820, loss: 4.820e-02\n",
      "Iteration: 8830, loss: 1.713e-02\n",
      "Iteration: 8840, loss: 4.271e-02\n",
      "Iteration: 8850, loss: 3.750e-02\n",
      "Iteration: 8860, loss: 2.674e-02\n",
      "Iteration: 8870, loss: 5.884e-02\n",
      "Iteration: 8880, loss: 4.378e-02\n",
      "Iteration: 8890, loss: 3.812e-02\n",
      "Iteration: 8900, loss: 4.171e-02\n",
      "Iteration: 8910, loss: 4.189e-02\n",
      "Iteration: 8920, loss: 3.501e-02\n",
      "Iteration: 8930, loss: 4.853e-02\n",
      "Iteration: 8940, loss: 2.572e-02\n",
      "Iteration: 8950, loss: 3.632e-02\n",
      "Iteration: 8960, loss: 3.220e-02\n",
      "Iteration: 8970, loss: 2.048e-02\n",
      "Iteration: 8980, loss: 6.127e-02\n",
      "Iteration: 8990, loss: 2.849e-02\n",
      "Iteration: 9000, loss: 3.733e-02\n",
      "Iteration: 9010, loss: 3.789e-02\n",
      "Iteration: 9020, loss: 3.191e-02\n",
      "Iteration: 9030, loss: 5.939e-02\n",
      "Iteration: 9040, loss: 4.998e-02\n",
      "Iteration: 9050, loss: 4.211e-02\n",
      "Iteration: 9060, loss: 3.634e-02\n",
      "Iteration: 9070, loss: 4.410e-02\n",
      "Iteration: 9080, loss: 3.900e-02\n",
      "Iteration: 9090, loss: 3.801e-02\n",
      "Iteration: 9100, loss: 4.424e-02\n",
      "Iteration: 9110, loss: 6.415e-02\n",
      "Iteration: 9120, loss: 3.502e-02\n",
      "Iteration: 9130, loss: 4.487e-02\n",
      "Iteration: 9140, loss: 3.041e-02\n",
      "Iteration: 9150, loss: 4.985e-02\n",
      "Iteration: 9160, loss: 3.187e-02\n",
      "Iteration: 9170, loss: 2.861e-02\n",
      "Iteration: 9180, loss: 3.626e-02\n",
      "Iteration: 9190, loss: 2.933e-02\n",
      "Iteration: 9200, loss: 4.442e-02\n",
      "Iteration: 9210, loss: 2.227e-02\n",
      "Iteration: 9220, loss: 2.336e-02\n",
      "Iteration: 9230, loss: 3.414e-02\n",
      "Iteration: 9240, loss: 6.233e-02\n",
      "Iteration: 9250, loss: 2.598e-02\n",
      "Iteration: 9260, loss: 6.751e-02\n",
      "Iteration: 9270, loss: 2.644e-02\n",
      "Iteration: 9280, loss: 4.458e-02\n",
      "Iteration: 9290, loss: 4.076e-02\n",
      "Iteration: 9300, loss: 2.683e-02\n",
      "Iteration: 9310, loss: 8.253e-02\n",
      "Iteration: 9320, loss: 6.397e-02\n",
      "Iteration: 9330, loss: 3.599e-02\n",
      "Iteration: 9340, loss: 2.988e-02\n",
      "Iteration: 9350, loss: 3.066e-02\n",
      "Iteration: 9360, loss: 6.064e-02\n",
      "Iteration: 9370, loss: 4.388e-02\n",
      "Iteration: 9380, loss: 2.355e-02\n",
      "Iteration: 9390, loss: 4.209e-02\n",
      "Iteration: 9400, loss: 3.117e-02\n",
      "Iteration: 9410, loss: 3.898e-02\n",
      "Iteration: 9420, loss: 2.086e-02\n",
      "Iteration: 9430, loss: 3.024e-02\n",
      "Iteration: 9440, loss: 3.025e-02\n",
      "Iteration: 9450, loss: 5.572e-02\n",
      "Iteration: 9460, loss: 7.915e-02\n",
      "Iteration: 9470, loss: 2.446e-02\n",
      "Iteration: 9480, loss: 3.131e-02\n",
      "Iteration: 9490, loss: 4.514e-02\n",
      "Iteration: 9500, loss: 2.495e-02\n",
      "Iteration: 9510, loss: 4.531e-02\n",
      "Iteration: 9520, loss: 4.210e-02\n",
      "Iteration: 9530, loss: 3.685e-02\n",
      "Iteration: 9540, loss: 2.515e-02\n",
      "Iteration: 9550, loss: 2.813e-02\n",
      "Iteration: 9560, loss: 2.494e-02\n",
      "Iteration: 9570, loss: 3.201e-02\n",
      "Iteration: 9580, loss: 3.821e-02\n",
      "Iteration: 9590, loss: 2.225e-02\n",
      "Iteration: 9600, loss: 2.209e-02\n",
      "Iteration: 9610, loss: 2.833e-02\n",
      "Iteration: 9620, loss: 6.387e-02\n",
      "Iteration: 9630, loss: 3.042e-02\n",
      "Iteration: 9640, loss: 4.020e-02\n",
      "Iteration: 9650, loss: 2.054e-02\n",
      "Iteration: 9660, loss: 2.398e-02\n",
      "Iteration: 9670, loss: 4.614e-02\n",
      "Iteration: 9680, loss: 1.787e-02\n",
      "Iteration: 9690, loss: 2.425e-02\n",
      "Iteration: 9700, loss: 4.293e-02\n",
      "Iteration: 9710, loss: 4.683e-02\n",
      "Iteration: 9720, loss: 3.682e-02\n",
      "Iteration: 9730, loss: 2.674e-02\n",
      "Iteration: 9740, loss: 4.315e-02\n",
      "Iteration: 9750, loss: 2.738e-02\n",
      "Iteration: 9760, loss: 3.729e-02\n",
      "Iteration: 9770, loss: 3.967e-02\n",
      "Iteration: 9780, loss: 4.681e-02\n",
      "Iteration: 9790, loss: 4.008e-02\n",
      "Iteration: 9800, loss: 6.023e-02\n",
      "Iteration: 9810, loss: 2.292e-02\n",
      "Iteration: 9820, loss: 2.203e-02\n",
      "Iteration: 9830, loss: 6.404e-02\n",
      "Iteration: 9840, loss: 1.087e-02\n",
      "Iteration: 9850, loss: 2.437e-02\n",
      "Iteration: 9860, loss: 4.453e-02\n",
      "Iteration: 9870, loss: 2.500e-02\n",
      "Iteration: 9880, loss: 3.672e-02\n",
      "Iteration: 9890, loss: 2.001e-02\n",
      "Iteration: 9900, loss: 2.101e-02\n",
      "Iteration: 9910, loss: 2.454e-02\n",
      "Iteration: 9920, loss: 2.649e-02\n",
      "Iteration: 9930, loss: 2.912e-02\n",
      "Iteration: 9940, loss: 2.210e-02\n",
      "Iteration: 9950, loss: 1.893e-02\n",
      "Iteration: 9960, loss: 2.851e-02\n",
      "Iteration: 9970, loss: 3.731e-02\n",
      "Iteration: 9980, loss: 1.450e-02\n",
      "Iteration: 9990, loss: 3.092e-02\n",
      "Iteration: 10000, loss: 2.460e-02\n",
      "Relative error: 1.214574e-01\n",
      "Iteration: 10, loss: 1.823e+04\n",
      "Iteration: 20, loss: 9.450e+03\n",
      "Iteration: 30, loss: 8.777e+03\n",
      "Iteration: 40, loss: 1.041e+04\n",
      "Iteration: 50, loss: 7.524e+03\n",
      "Iteration: 60, loss: 7.657e+03\n",
      "Iteration: 70, loss: 3.687e+03\n",
      "Iteration: 80, loss: 2.766e+03\n",
      "Iteration: 90, loss: 2.336e+03\n",
      "Iteration: 100, loss: 1.039e+03\n",
      "Iteration: 110, loss: 2.146e+03\n",
      "Iteration: 120, loss: 2.300e+03\n",
      "Iteration: 130, loss: 1.623e+03\n",
      "Iteration: 140, loss: 8.573e+02\n",
      "Iteration: 150, loss: 8.357e+02\n",
      "Iteration: 160, loss: 6.727e+02\n",
      "Iteration: 170, loss: 5.109e+02\n",
      "Iteration: 180, loss: 1.066e+03\n",
      "Iteration: 190, loss: 6.195e+02\n",
      "Iteration: 200, loss: 4.953e+02\n",
      "Iteration: 210, loss: 3.588e+02\n",
      "Iteration: 220, loss: 5.558e+02\n",
      "Iteration: 230, loss: 5.115e+02\n",
      "Iteration: 240, loss: 3.646e+02\n",
      "Iteration: 250, loss: 2.666e+02\n",
      "Iteration: 260, loss: 2.552e+02\n",
      "Iteration: 270, loss: 3.306e+02\n",
      "Iteration: 280, loss: 1.686e+02\n",
      "Iteration: 290, loss: 3.291e+02\n",
      "Iteration: 300, loss: 1.396e+02\n",
      "Iteration: 310, loss: 1.158e+02\n",
      "Iteration: 320, loss: 1.334e+02\n",
      "Iteration: 330, loss: 3.258e+02\n",
      "Iteration: 340, loss: 1.400e+02\n",
      "Iteration: 350, loss: 1.693e+02\n",
      "Iteration: 360, loss: 1.511e+02\n",
      "Iteration: 370, loss: 1.026e+02\n",
      "Iteration: 380, loss: 1.302e+02\n",
      "Iteration: 390, loss: 1.501e+02\n",
      "Iteration: 400, loss: 9.211e+01\n",
      "Iteration: 410, loss: 1.010e+02\n",
      "Iteration: 420, loss: 1.112e+02\n",
      "Iteration: 430, loss: 1.511e+02\n",
      "Iteration: 440, loss: 1.213e+02\n",
      "Iteration: 450, loss: 1.449e+02\n",
      "Iteration: 460, loss: 1.271e+02\n",
      "Iteration: 470, loss: 1.218e+02\n",
      "Iteration: 480, loss: 8.545e+01\n",
      "Iteration: 490, loss: 1.007e+02\n",
      "Iteration: 500, loss: 1.406e+02\n",
      "Iteration: 510, loss: 1.167e+02\n",
      "Iteration: 520, loss: 6.324e+01\n",
      "Iteration: 530, loss: 1.136e+02\n",
      "Iteration: 540, loss: 4.390e+01\n",
      "Iteration: 550, loss: 7.763e+01\n",
      "Iteration: 560, loss: 1.075e+02\n",
      "Iteration: 570, loss: 6.702e+01\n",
      "Iteration: 580, loss: 1.417e+02\n",
      "Iteration: 590, loss: 9.610e+01\n",
      "Iteration: 600, loss: 7.674e+01\n",
      "Iteration: 610, loss: 7.076e+01\n",
      "Iteration: 620, loss: 9.250e+01\n",
      "Iteration: 630, loss: 9.710e+01\n",
      "Iteration: 640, loss: 8.031e+01\n",
      "Iteration: 650, loss: 6.410e+01\n",
      "Iteration: 660, loss: 6.017e+01\n",
      "Iteration: 670, loss: 4.947e+01\n",
      "Iteration: 680, loss: 6.349e+01\n",
      "Iteration: 690, loss: 6.334e+01\n",
      "Iteration: 700, loss: 7.226e+01\n",
      "Iteration: 710, loss: 5.757e+01\n",
      "Iteration: 720, loss: 6.138e+01\n",
      "Iteration: 730, loss: 5.525e+01\n",
      "Iteration: 740, loss: 7.067e+01\n",
      "Iteration: 750, loss: 6.837e+01\n",
      "Iteration: 760, loss: 6.809e+01\n",
      "Iteration: 770, loss: 4.868e+01\n",
      "Iteration: 780, loss: 5.428e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 790, loss: 4.928e+01\n",
      "Iteration: 800, loss: 3.491e+01\n",
      "Iteration: 810, loss: 7.304e+01\n",
      "Iteration: 820, loss: 3.963e+01\n",
      "Iteration: 830, loss: 5.750e+01\n",
      "Iteration: 840, loss: 5.580e+01\n",
      "Iteration: 850, loss: 3.587e+01\n",
      "Iteration: 860, loss: 4.571e+01\n",
      "Iteration: 870, loss: 7.821e+01\n",
      "Iteration: 880, loss: 4.325e+01\n",
      "Iteration: 890, loss: 5.945e+01\n",
      "Iteration: 900, loss: 3.306e+01\n",
      "Iteration: 910, loss: 4.587e+01\n",
      "Iteration: 920, loss: 5.755e+01\n",
      "Iteration: 930, loss: 3.526e+01\n",
      "Iteration: 940, loss: 5.443e+01\n",
      "Iteration: 950, loss: 3.555e+01\n",
      "Iteration: 960, loss: 1.789e+01\n",
      "Iteration: 970, loss: 4.854e+01\n",
      "Iteration: 980, loss: 5.596e+01\n",
      "Iteration: 990, loss: 2.832e+01\n",
      "Iteration: 1000, loss: 4.384e+01\n",
      "Iteration: 1010, loss: 2.612e+01\n",
      "Iteration: 1020, loss: 4.700e+01\n",
      "Iteration: 1030, loss: 4.237e+01\n",
      "Iteration: 1040, loss: 4.315e+01\n",
      "Iteration: 1050, loss: 2.907e+01\n",
      "Iteration: 1060, loss: 3.157e+01\n",
      "Iteration: 1070, loss: 4.018e+01\n",
      "Iteration: 1080, loss: 2.534e+01\n",
      "Iteration: 1090, loss: 2.711e+01\n",
      "Iteration: 1100, loss: 3.276e+01\n",
      "Iteration: 1110, loss: 2.585e+01\n",
      "Iteration: 1120, loss: 3.347e+01\n",
      "Iteration: 1130, loss: 3.300e+01\n",
      "Iteration: 1140, loss: 1.927e+01\n",
      "Iteration: 1150, loss: 3.517e+01\n",
      "Iteration: 1160, loss: 5.582e+01\n",
      "Iteration: 1170, loss: 2.686e+01\n",
      "Iteration: 1180, loss: 1.755e+01\n",
      "Iteration: 1190, loss: 3.381e+01\n",
      "Iteration: 1200, loss: 3.323e+01\n",
      "Iteration: 1210, loss: 3.472e+01\n",
      "Iteration: 1220, loss: 3.212e+01\n",
      "Iteration: 1230, loss: 2.908e+01\n",
      "Iteration: 1240, loss: 2.586e+01\n",
      "Iteration: 1250, loss: 1.965e+01\n",
      "Iteration: 1260, loss: 3.135e+01\n",
      "Iteration: 1270, loss: 3.962e+01\n",
      "Iteration: 1280, loss: 3.505e+01\n",
      "Iteration: 1290, loss: 2.140e+01\n",
      "Iteration: 1300, loss: 1.816e+01\n",
      "Iteration: 1310, loss: 2.310e+01\n",
      "Iteration: 1320, loss: 3.416e+01\n",
      "Iteration: 1330, loss: 2.872e+01\n",
      "Iteration: 1340, loss: 1.786e+01\n",
      "Iteration: 1350, loss: 2.029e+01\n",
      "Iteration: 1360, loss: 2.412e+01\n",
      "Iteration: 1370, loss: 2.713e+01\n",
      "Iteration: 1380, loss: 2.238e+01\n",
      "Iteration: 1390, loss: 2.292e+01\n",
      "Iteration: 1400, loss: 1.888e+01\n",
      "Iteration: 1410, loss: 2.192e+01\n",
      "Iteration: 1420, loss: 1.805e+01\n",
      "Iteration: 1430, loss: 2.338e+01\n",
      "Iteration: 1440, loss: 2.049e+01\n",
      "Iteration: 1450, loss: 2.182e+01\n",
      "Iteration: 1460, loss: 1.367e+01\n",
      "Iteration: 1470, loss: 1.517e+01\n",
      "Iteration: 1480, loss: 1.521e+01\n",
      "Iteration: 1490, loss: 1.929e+01\n",
      "Iteration: 1500, loss: 1.980e+01\n",
      "Iteration: 1510, loss: 1.764e+01\n",
      "Iteration: 1520, loss: 2.219e+01\n",
      "Iteration: 1530, loss: 3.206e+01\n",
      "Iteration: 1540, loss: 9.522e+00\n",
      "Iteration: 1550, loss: 2.030e+01\n",
      "Iteration: 1560, loss: 2.289e+01\n",
      "Iteration: 1570, loss: 1.696e+01\n",
      "Iteration: 1580, loss: 2.141e+01\n",
      "Iteration: 1590, loss: 1.717e+01\n",
      "Iteration: 1600, loss: 1.344e+01\n",
      "Iteration: 1610, loss: 1.483e+01\n",
      "Iteration: 1620, loss: 1.947e+01\n",
      "Iteration: 1630, loss: 2.080e+01\n",
      "Iteration: 1640, loss: 1.816e+01\n",
      "Iteration: 1650, loss: 2.817e+01\n",
      "Iteration: 1660, loss: 2.863e+01\n",
      "Iteration: 1670, loss: 1.968e+01\n",
      "Iteration: 1680, loss: 1.540e+01\n",
      "Iteration: 1690, loss: 1.463e+01\n",
      "Iteration: 1700, loss: 3.120e+01\n",
      "Iteration: 1710, loss: 2.592e+01\n",
      "Iteration: 1720, loss: 2.505e+01\n",
      "Iteration: 1730, loss: 2.234e+01\n",
      "Iteration: 1740, loss: 2.144e+01\n",
      "Iteration: 1750, loss: 1.420e+01\n",
      "Iteration: 1760, loss: 1.286e+01\n",
      "Iteration: 1770, loss: 1.610e+01\n",
      "Iteration: 1780, loss: 1.833e+01\n",
      "Iteration: 1790, loss: 1.349e+01\n",
      "Iteration: 1800, loss: 8.668e+00\n",
      "Iteration: 1810, loss: 9.918e+00\n",
      "Iteration: 1820, loss: 1.372e+01\n",
      "Iteration: 1830, loss: 1.454e+01\n",
      "Iteration: 1840, loss: 1.368e+01\n",
      "Iteration: 1850, loss: 2.042e+01\n",
      "Iteration: 1860, loss: 9.904e+00\n",
      "Iteration: 1870, loss: 2.044e+01\n",
      "Iteration: 1880, loss: 1.560e+01\n",
      "Iteration: 1890, loss: 1.159e+01\n",
      "Iteration: 1900, loss: 1.644e+01\n",
      "Iteration: 1910, loss: 1.539e+01\n",
      "Iteration: 1920, loss: 1.012e+01\n",
      "Iteration: 1930, loss: 1.087e+01\n",
      "Iteration: 1940, loss: 1.406e+01\n",
      "Iteration: 1950, loss: 1.298e+01\n",
      "Iteration: 1960, loss: 8.877e+00\n",
      "Iteration: 1970, loss: 1.736e+01\n",
      "Iteration: 1980, loss: 1.164e+01\n",
      "Iteration: 1990, loss: 1.149e+01\n",
      "Iteration: 2000, loss: 1.932e+01\n",
      "Iteration: 2010, loss: 1.321e+01\n",
      "Iteration: 2020, loss: 1.552e+01\n",
      "Iteration: 2030, loss: 1.090e+01\n",
      "Iteration: 2040, loss: 1.296e+01\n",
      "Iteration: 2050, loss: 1.492e+01\n",
      "Iteration: 2060, loss: 1.175e+01\n",
      "Iteration: 2070, loss: 8.250e+00\n",
      "Iteration: 2080, loss: 1.185e+01\n",
      "Iteration: 2090, loss: 1.041e+01\n",
      "Iteration: 2100, loss: 1.086e+01\n",
      "Iteration: 2110, loss: 1.384e+01\n",
      "Iteration: 2120, loss: 1.230e+01\n",
      "Iteration: 2130, loss: 1.064e+01\n",
      "Iteration: 2140, loss: 9.356e+00\n",
      "Iteration: 2150, loss: 9.410e+00\n",
      "Iteration: 2160, loss: 9.024e+00\n",
      "Iteration: 2170, loss: 6.164e+00\n",
      "Iteration: 2180, loss: 1.354e+01\n",
      "Iteration: 2190, loss: 1.697e+01\n",
      "Iteration: 2200, loss: 1.122e+01\n",
      "Iteration: 2210, loss: 8.041e+00\n",
      "Iteration: 2220, loss: 7.820e+00\n",
      "Iteration: 2230, loss: 9.141e+00\n",
      "Iteration: 2240, loss: 1.296e+01\n",
      "Iteration: 2250, loss: 9.534e+00\n",
      "Iteration: 2260, loss: 8.511e+00\n",
      "Iteration: 2270, loss: 6.076e+00\n",
      "Iteration: 2280, loss: 6.630e+00\n",
      "Iteration: 2290, loss: 1.356e+01\n",
      "Iteration: 2300, loss: 1.175e+01\n",
      "Iteration: 2310, loss: 1.134e+01\n",
      "Iteration: 2320, loss: 1.087e+01\n",
      "Iteration: 2330, loss: 1.057e+01\n",
      "Iteration: 2340, loss: 9.466e+00\n",
      "Iteration: 2350, loss: 1.410e+01\n",
      "Iteration: 2360, loss: 8.432e+00\n",
      "Iteration: 2370, loss: 6.364e+00\n",
      "Iteration: 2380, loss: 1.178e+01\n",
      "Iteration: 2390, loss: 7.456e+00\n",
      "Iteration: 2400, loss: 1.425e+01\n",
      "Iteration: 2410, loss: 1.175e+01\n",
      "Iteration: 2420, loss: 5.946e+00\n",
      "Iteration: 2430, loss: 3.630e+00\n",
      "Iteration: 2440, loss: 6.424e+00\n",
      "Iteration: 2450, loss: 6.121e+00\n",
      "Iteration: 2460, loss: 7.948e+00\n",
      "Iteration: 2470, loss: 8.491e+00\n",
      "Iteration: 2480, loss: 1.077e+01\n",
      "Iteration: 2490, loss: 1.111e+01\n",
      "Iteration: 2500, loss: 7.642e+00\n",
      "Iteration: 2510, loss: 6.739e+00\n",
      "Iteration: 2520, loss: 6.343e+00\n",
      "Iteration: 2530, loss: 7.956e+00\n",
      "Iteration: 2540, loss: 6.909e+00\n",
      "Iteration: 2550, loss: 9.432e+00\n",
      "Iteration: 2560, loss: 1.023e+01\n",
      "Iteration: 2570, loss: 5.126e+00\n",
      "Iteration: 2580, loss: 6.518e+00\n",
      "Iteration: 2590, loss: 7.510e+00\n",
      "Iteration: 2600, loss: 7.208e+00\n",
      "Iteration: 2610, loss: 7.242e+00\n",
      "Iteration: 2620, loss: 1.137e+01\n",
      "Iteration: 2630, loss: 5.681e+00\n",
      "Iteration: 2640, loss: 7.621e+00\n",
      "Iteration: 2650, loss: 7.310e+00\n",
      "Iteration: 2660, loss: 9.274e+00\n",
      "Iteration: 2670, loss: 7.721e+00\n",
      "Iteration: 2680, loss: 5.463e+00\n",
      "Iteration: 2690, loss: 9.262e+00\n",
      "Iteration: 2700, loss: 5.420e+00\n",
      "Iteration: 2710, loss: 9.594e+00\n",
      "Iteration: 2720, loss: 7.829e+00\n",
      "Iteration: 2730, loss: 7.373e+00\n",
      "Iteration: 2740, loss: 7.278e+00\n",
      "Iteration: 2750, loss: 3.909e+00\n",
      "Iteration: 2760, loss: 8.137e+00\n",
      "Iteration: 2770, loss: 5.313e+00\n",
      "Iteration: 2780, loss: 7.521e+00\n",
      "Iteration: 2790, loss: 6.504e+00\n",
      "Iteration: 2800, loss: 4.967e+00\n",
      "Iteration: 2810, loss: 6.447e+00\n",
      "Iteration: 2820, loss: 7.026e+00\n",
      "Iteration: 2830, loss: 7.483e+00\n",
      "Iteration: 2840, loss: 4.747e+00\n",
      "Iteration: 2850, loss: 4.895e+00\n",
      "Iteration: 2860, loss: 6.317e+00\n",
      "Iteration: 2870, loss: 4.892e+00\n",
      "Iteration: 2880, loss: 4.125e+00\n",
      "Iteration: 2890, loss: 6.596e+00\n",
      "Iteration: 2900, loss: 6.473e+00\n",
      "Iteration: 2910, loss: 1.091e+01\n",
      "Iteration: 2920, loss: 1.030e+01\n",
      "Iteration: 2930, loss: 3.370e+00\n",
      "Iteration: 2940, loss: 4.657e+00\n",
      "Iteration: 2950, loss: 9.310e+00\n",
      "Iteration: 2960, loss: 4.468e+00\n",
      "Iteration: 2970, loss: 7.532e+00\n",
      "Iteration: 2980, loss: 7.847e+00\n",
      "Iteration: 2990, loss: 6.518e+00\n",
      "Iteration: 3000, loss: 4.970e+00\n",
      "Iteration: 3010, loss: 3.703e+00\n",
      "Iteration: 3020, loss: 4.315e+00\n",
      "Iteration: 3030, loss: 5.341e+00\n",
      "Iteration: 3040, loss: 7.071e+00\n",
      "Iteration: 3050, loss: 6.168e+00\n",
      "Iteration: 3060, loss: 6.094e+00\n",
      "Iteration: 3070, loss: 9.459e+00\n",
      "Iteration: 3080, loss: 7.625e+00\n",
      "Iteration: 3090, loss: 5.378e+00\n",
      "Iteration: 3100, loss: 5.366e+00\n",
      "Iteration: 3110, loss: 4.578e+00\n",
      "Iteration: 3120, loss: 5.823e+00\n",
      "Iteration: 3130, loss: 5.675e+00\n",
      "Iteration: 3140, loss: 4.629e+00\n",
      "Iteration: 3150, loss: 5.548e+00\n",
      "Iteration: 3160, loss: 5.590e+00\n",
      "Iteration: 3170, loss: 7.496e+00\n",
      "Iteration: 3180, loss: 4.159e+00\n",
      "Iteration: 3190, loss: 4.367e+00\n",
      "Iteration: 3200, loss: 6.800e+00\n",
      "Iteration: 3210, loss: 5.347e+00\n",
      "Iteration: 3220, loss: 6.157e+00\n",
      "Iteration: 3230, loss: 4.799e+00\n",
      "Iteration: 3240, loss: 4.026e+00\n",
      "Iteration: 3250, loss: 4.095e+00\n",
      "Iteration: 3260, loss: 4.021e+00\n",
      "Iteration: 3270, loss: 4.122e+00\n",
      "Iteration: 3280, loss: 5.913e+00\n",
      "Iteration: 3290, loss: 6.316e+00\n",
      "Iteration: 3300, loss: 2.863e+00\n",
      "Iteration: 3310, loss: 4.699e+00\n",
      "Iteration: 3320, loss: 4.921e+00\n",
      "Iteration: 3330, loss: 3.519e+00\n",
      "Iteration: 3340, loss: 4.726e+00\n",
      "Iteration: 3350, loss: 6.439e+00\n",
      "Iteration: 3360, loss: 2.881e+00\n",
      "Iteration: 3370, loss: 8.563e+00\n",
      "Iteration: 3380, loss: 4.095e+00\n",
      "Iteration: 3390, loss: 5.413e+00\n",
      "Iteration: 3400, loss: 4.352e+00\n",
      "Iteration: 3410, loss: 6.091e+00\n",
      "Iteration: 3420, loss: 3.661e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3430, loss: 6.183e+00\n",
      "Iteration: 3440, loss: 4.105e+00\n",
      "Iteration: 3450, loss: 4.084e+00\n",
      "Iteration: 3460, loss: 3.459e+00\n",
      "Iteration: 3470, loss: 2.216e+00\n",
      "Iteration: 3480, loss: 3.423e+00\n",
      "Iteration: 3490, loss: 2.627e+00\n",
      "Iteration: 3500, loss: 3.807e+00\n",
      "Iteration: 3510, loss: 2.425e+00\n",
      "Iteration: 3520, loss: 4.508e+00\n",
      "Iteration: 3530, loss: 4.556e+00\n",
      "Iteration: 3540, loss: 3.744e+00\n",
      "Iteration: 3550, loss: 3.696e+00\n",
      "Iteration: 3560, loss: 4.761e+00\n",
      "Iteration: 3570, loss: 3.520e+00\n",
      "Iteration: 3580, loss: 3.171e+00\n",
      "Iteration: 3590, loss: 3.237e+00\n",
      "Iteration: 3600, loss: 4.518e+00\n",
      "Iteration: 3610, loss: 2.796e+00\n",
      "Iteration: 3620, loss: 4.832e+00\n",
      "Iteration: 3630, loss: 4.746e+00\n",
      "Iteration: 3640, loss: 4.170e+00\n",
      "Iteration: 3650, loss: 2.878e+00\n",
      "Iteration: 3660, loss: 3.682e+00\n",
      "Iteration: 3670, loss: 2.595e+00\n",
      "Iteration: 3680, loss: 4.157e+00\n",
      "Iteration: 3690, loss: 4.184e+00\n",
      "Iteration: 3700, loss: 2.725e+00\n",
      "Iteration: 3710, loss: 5.414e+00\n",
      "Iteration: 3720, loss: 3.827e+00\n",
      "Iteration: 3730, loss: 4.303e+00\n",
      "Iteration: 3740, loss: 3.645e+00\n",
      "Iteration: 3750, loss: 4.550e+00\n",
      "Iteration: 3760, loss: 3.406e+00\n",
      "Iteration: 3770, loss: 5.606e+00\n",
      "Iteration: 3780, loss: 2.603e+00\n",
      "Iteration: 3790, loss: 2.477e+00\n",
      "Iteration: 3800, loss: 4.122e+00\n",
      "Iteration: 3810, loss: 5.211e+00\n",
      "Iteration: 3820, loss: 5.189e+00\n",
      "Iteration: 3830, loss: 4.115e+00\n",
      "Iteration: 3840, loss: 4.519e+00\n",
      "Iteration: 3850, loss: 4.280e+00\n",
      "Iteration: 3860, loss: 2.624e+00\n",
      "Iteration: 3870, loss: 2.962e+00\n",
      "Iteration: 3880, loss: 3.145e+00\n",
      "Iteration: 3890, loss: 2.809e+00\n",
      "Iteration: 3900, loss: 5.378e+00\n",
      "Iteration: 3910, loss: 2.377e+00\n",
      "Iteration: 3920, loss: 4.907e+00\n",
      "Iteration: 3930, loss: 3.060e+00\n",
      "Iteration: 3940, loss: 3.005e+00\n",
      "Iteration: 3950, loss: 3.657e+00\n",
      "Iteration: 3960, loss: 3.285e+00\n",
      "Iteration: 3970, loss: 2.823e+00\n",
      "Iteration: 3980, loss: 3.175e+00\n",
      "Iteration: 3990, loss: 4.208e+00\n",
      "Iteration: 4000, loss: 2.408e+00\n",
      "Iteration: 4010, loss: 2.656e+00\n",
      "Iteration: 4020, loss: 3.476e+00\n",
      "Iteration: 4030, loss: 3.884e+00\n",
      "Iteration: 4040, loss: 4.202e+00\n",
      "Iteration: 4050, loss: 3.588e+00\n",
      "Iteration: 4060, loss: 3.122e+00\n",
      "Iteration: 4070, loss: 4.460e+00\n",
      "Iteration: 4080, loss: 2.263e+00\n",
      "Iteration: 4090, loss: 2.181e+00\n",
      "Iteration: 4100, loss: 5.430e+00\n",
      "Iteration: 4110, loss: 2.394e+00\n",
      "Iteration: 4120, loss: 2.848e+00\n",
      "Iteration: 4130, loss: 2.165e+00\n",
      "Iteration: 4140, loss: 3.115e+00\n",
      "Iteration: 4150, loss: 2.905e+00\n",
      "Iteration: 4160, loss: 3.125e+00\n",
      "Iteration: 4170, loss: 3.346e+00\n",
      "Iteration: 4180, loss: 2.419e+00\n",
      "Iteration: 4190, loss: 1.942e+00\n",
      "Iteration: 4200, loss: 2.782e+00\n",
      "Iteration: 4210, loss: 2.442e+00\n",
      "Iteration: 4220, loss: 2.384e+00\n",
      "Iteration: 4230, loss: 2.178e+00\n",
      "Iteration: 4240, loss: 3.874e+00\n",
      "Iteration: 4250, loss: 1.651e+00\n",
      "Iteration: 4260, loss: 2.238e+00\n",
      "Iteration: 4270, loss: 4.484e+00\n",
      "Iteration: 4280, loss: 2.464e+00\n",
      "Iteration: 4290, loss: 2.588e+00\n",
      "Iteration: 4300, loss: 2.042e+00\n",
      "Iteration: 4310, loss: 2.326e+00\n",
      "Iteration: 4320, loss: 2.856e+00\n",
      "Iteration: 4330, loss: 2.415e+00\n",
      "Iteration: 4340, loss: 3.373e+00\n",
      "Iteration: 4350, loss: 1.939e+00\n",
      "Iteration: 4360, loss: 2.181e+00\n",
      "Iteration: 4370, loss: 2.720e+00\n",
      "Iteration: 4380, loss: 3.003e+00\n",
      "Iteration: 4390, loss: 4.370e+00\n",
      "Iteration: 4400, loss: 2.215e+00\n",
      "Iteration: 4410, loss: 1.989e+00\n",
      "Iteration: 4420, loss: 2.754e+00\n",
      "Iteration: 4430, loss: 3.117e+00\n",
      "Iteration: 4440, loss: 2.451e+00\n",
      "Iteration: 4450, loss: 2.265e+00\n",
      "Iteration: 4460, loss: 2.969e+00\n",
      "Iteration: 4470, loss: 2.708e+00\n",
      "Iteration: 4480, loss: 2.500e+00\n",
      "Iteration: 4490, loss: 2.353e+00\n",
      "Iteration: 4500, loss: 1.700e+00\n",
      "Iteration: 4510, loss: 2.054e+00\n",
      "Iteration: 4520, loss: 2.652e+00\n",
      "Iteration: 4530, loss: 2.981e+00\n",
      "Iteration: 4540, loss: 2.479e+00\n",
      "Iteration: 4550, loss: 2.391e+00\n",
      "Iteration: 4560, loss: 2.296e+00\n",
      "Iteration: 4570, loss: 2.460e+00\n",
      "Iteration: 4580, loss: 3.057e+00\n",
      "Iteration: 4590, loss: 3.181e+00\n",
      "Iteration: 4600, loss: 2.169e+00\n",
      "Iteration: 4610, loss: 2.533e+00\n",
      "Iteration: 4620, loss: 2.408e+00\n",
      "Iteration: 4630, loss: 2.179e+00\n",
      "Iteration: 4640, loss: 2.479e+00\n",
      "Iteration: 4650, loss: 2.472e+00\n",
      "Iteration: 4660, loss: 3.115e+00\n",
      "Iteration: 4670, loss: 2.702e+00\n",
      "Iteration: 4680, loss: 2.269e+00\n",
      "Iteration: 4690, loss: 3.086e+00\n",
      "Iteration: 4700, loss: 3.252e+00\n",
      "Iteration: 4710, loss: 2.500e+00\n",
      "Iteration: 4720, loss: 2.315e+00\n",
      "Iteration: 4730, loss: 1.441e+00\n",
      "Iteration: 4740, loss: 2.832e+00\n",
      "Iteration: 4750, loss: 2.870e+00\n",
      "Iteration: 4760, loss: 2.909e+00\n",
      "Iteration: 4770, loss: 2.466e+00\n",
      "Iteration: 4780, loss: 2.621e+00\n",
      "Iteration: 4790, loss: 2.480e+00\n",
      "Iteration: 4800, loss: 1.185e+00\n",
      "Iteration: 4810, loss: 2.240e+00\n",
      "Iteration: 4820, loss: 2.046e+00\n",
      "Iteration: 4830, loss: 1.513e+00\n",
      "Iteration: 4840, loss: 1.594e+00\n",
      "Iteration: 4850, loss: 2.031e+00\n",
      "Iteration: 4860, loss: 3.561e+00\n",
      "Iteration: 4870, loss: 3.183e+00\n",
      "Iteration: 4880, loss: 2.262e+00\n",
      "Iteration: 4890, loss: 1.958e+00\n",
      "Iteration: 4900, loss: 1.049e+00\n",
      "Iteration: 4910, loss: 1.634e+00\n",
      "Iteration: 4920, loss: 1.863e+00\n",
      "Iteration: 4930, loss: 1.045e+00\n",
      "Iteration: 4940, loss: 2.986e+00\n",
      "Iteration: 4950, loss: 1.927e+00\n",
      "Iteration: 4960, loss: 2.537e+00\n",
      "Iteration: 4970, loss: 2.334e+00\n",
      "Iteration: 4980, loss: 1.455e+00\n",
      "Iteration: 4990, loss: 1.543e+00\n",
      "Iteration: 5000, loss: 1.766e+00\n",
      "Iteration: 5010, loss: 1.206e+00\n",
      "Iteration: 5020, loss: 1.567e+00\n",
      "Iteration: 5030, loss: 2.144e+00\n",
      "Iteration: 5040, loss: 1.622e+00\n",
      "Iteration: 5050, loss: 2.363e+00\n",
      "Iteration: 5060, loss: 2.551e+00\n",
      "Iteration: 5070, loss: 1.370e+00\n",
      "Iteration: 5080, loss: 1.996e+00\n",
      "Iteration: 5090, loss: 2.323e+00\n",
      "Iteration: 5100, loss: 1.581e+00\n",
      "Iteration: 5110, loss: 1.240e+00\n",
      "Iteration: 5120, loss: 1.848e+00\n",
      "Iteration: 5130, loss: 2.068e+00\n",
      "Iteration: 5140, loss: 1.890e+00\n",
      "Iteration: 5150, loss: 2.291e+00\n",
      "Iteration: 5160, loss: 2.453e+00\n",
      "Iteration: 5170, loss: 2.323e+00\n",
      "Iteration: 5180, loss: 1.313e+00\n",
      "Iteration: 5190, loss: 1.789e+00\n",
      "Iteration: 5200, loss: 1.705e+00\n",
      "Iteration: 5210, loss: 1.424e+00\n",
      "Iteration: 5220, loss: 1.215e+00\n",
      "Iteration: 5230, loss: 1.445e+00\n",
      "Iteration: 5240, loss: 2.780e+00\n",
      "Iteration: 5250, loss: 1.477e+00\n",
      "Iteration: 5260, loss: 2.260e+00\n",
      "Iteration: 5270, loss: 1.631e+00\n",
      "Iteration: 5280, loss: 1.810e+00\n",
      "Iteration: 5290, loss: 1.717e+00\n",
      "Iteration: 5300, loss: 1.699e+00\n",
      "Iteration: 5310, loss: 1.781e+00\n",
      "Iteration: 5320, loss: 1.483e+00\n",
      "Iteration: 5330, loss: 1.900e+00\n",
      "Iteration: 5340, loss: 2.027e+00\n",
      "Iteration: 5350, loss: 2.106e+00\n",
      "Iteration: 5360, loss: 1.758e+00\n",
      "Iteration: 5370, loss: 1.487e+00\n",
      "Iteration: 5380, loss: 2.023e+00\n",
      "Iteration: 5390, loss: 1.577e+00\n",
      "Iteration: 5400, loss: 2.418e+00\n",
      "Iteration: 5410, loss: 2.178e+00\n",
      "Iteration: 5420, loss: 1.634e+00\n",
      "Iteration: 5430, loss: 2.246e+00\n",
      "Iteration: 5440, loss: 1.143e+00\n",
      "Iteration: 5450, loss: 7.753e-01\n",
      "Iteration: 5460, loss: 9.558e-01\n",
      "Iteration: 5470, loss: 1.706e+00\n",
      "Iteration: 5480, loss: 1.880e+00\n",
      "Iteration: 5490, loss: 1.581e+00\n",
      "Iteration: 5500, loss: 1.410e+00\n",
      "Iteration: 5510, loss: 1.718e+00\n",
      "Iteration: 5520, loss: 1.049e+00\n",
      "Iteration: 5530, loss: 1.580e+00\n",
      "Iteration: 5540, loss: 1.609e+00\n",
      "Iteration: 5550, loss: 1.227e+00\n",
      "Iteration: 5560, loss: 1.391e+00\n",
      "Iteration: 5570, loss: 1.957e+00\n",
      "Iteration: 5580, loss: 1.292e+00\n",
      "Iteration: 5590, loss: 1.525e+00\n",
      "Iteration: 5600, loss: 1.444e+00\n",
      "Iteration: 5610, loss: 1.618e+00\n",
      "Iteration: 5620, loss: 1.102e+00\n",
      "Iteration: 5630, loss: 1.505e+00\n",
      "Iteration: 5640, loss: 1.573e+00\n",
      "Iteration: 5650, loss: 1.047e+00\n",
      "Iteration: 5660, loss: 1.390e+00\n",
      "Iteration: 5670, loss: 1.262e+00\n",
      "Iteration: 5680, loss: 1.705e+00\n",
      "Iteration: 5690, loss: 1.980e+00\n",
      "Iteration: 5700, loss: 8.912e-01\n",
      "Iteration: 5710, loss: 1.212e+00\n",
      "Iteration: 5720, loss: 1.407e+00\n",
      "Iteration: 5730, loss: 1.374e+00\n",
      "Iteration: 5740, loss: 1.233e+00\n",
      "Iteration: 5750, loss: 1.658e+00\n",
      "Iteration: 5760, loss: 1.528e+00\n",
      "Iteration: 5770, loss: 1.428e+00\n",
      "Iteration: 5780, loss: 1.184e+00\n",
      "Iteration: 5790, loss: 7.717e-01\n",
      "Iteration: 5800, loss: 8.047e-01\n",
      "Iteration: 5810, loss: 1.012e+00\n",
      "Iteration: 5820, loss: 6.271e-01\n",
      "Iteration: 5830, loss: 1.620e+00\n",
      "Iteration: 5840, loss: 2.274e+00\n",
      "Iteration: 5850, loss: 1.593e+00\n",
      "Iteration: 5860, loss: 1.175e+00\n",
      "Iteration: 5870, loss: 7.680e-01\n",
      "Iteration: 5880, loss: 1.401e+00\n",
      "Iteration: 5890, loss: 1.387e+00\n",
      "Iteration: 5900, loss: 1.018e+00\n",
      "Iteration: 5910, loss: 1.159e+00\n",
      "Iteration: 5920, loss: 1.803e+00\n",
      "Iteration: 5930, loss: 1.465e+00\n",
      "Iteration: 5940, loss: 8.313e-01\n",
      "Iteration: 5950, loss: 1.671e+00\n",
      "Iteration: 5960, loss: 1.041e+00\n",
      "Iteration: 5970, loss: 8.790e-01\n",
      "Iteration: 5980, loss: 1.153e+00\n",
      "Iteration: 5990, loss: 1.017e+00\n",
      "Iteration: 6000, loss: 1.534e+00\n",
      "Iteration: 6010, loss: 7.998e-01\n",
      "Iteration: 6020, loss: 1.814e+00\n",
      "Iteration: 6030, loss: 1.111e+00\n",
      "Iteration: 6040, loss: 9.819e-01\n",
      "Iteration: 6050, loss: 1.301e+00\n",
      "Iteration: 6060, loss: 7.520e-01\n",
      "Iteration: 6070, loss: 6.434e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6080, loss: 9.302e-01\n",
      "Iteration: 6090, loss: 1.408e+00\n",
      "Iteration: 6100, loss: 8.603e-01\n",
      "Iteration: 6110, loss: 7.214e-01\n",
      "Iteration: 6120, loss: 5.334e-01\n",
      "Iteration: 6130, loss: 1.682e+00\n",
      "Iteration: 6140, loss: 1.364e+00\n",
      "Iteration: 6150, loss: 1.669e+00\n",
      "Iteration: 6160, loss: 9.027e-01\n",
      "Iteration: 6170, loss: 8.499e-01\n",
      "Iteration: 6180, loss: 3.408e-01\n",
      "Iteration: 6190, loss: 3.583e-01\n",
      "Iteration: 6200, loss: 1.071e+00\n",
      "Iteration: 6210, loss: 5.761e-01\n",
      "Iteration: 6220, loss: 8.559e-01\n",
      "Iteration: 6230, loss: 1.090e+00\n",
      "Iteration: 6240, loss: 6.756e-01\n",
      "Iteration: 6250, loss: 4.663e-01\n",
      "Iteration: 6260, loss: 1.291e+00\n",
      "Iteration: 6270, loss: 9.980e-01\n",
      "Iteration: 6280, loss: 1.228e+00\n",
      "Iteration: 6290, loss: 8.479e-01\n",
      "Iteration: 6300, loss: 6.606e-01\n",
      "Iteration: 6310, loss: 9.289e-01\n",
      "Iteration: 6320, loss: 9.552e-01\n",
      "Iteration: 6330, loss: 7.721e-01\n",
      "Iteration: 6340, loss: 8.461e-01\n",
      "Iteration: 6350, loss: 8.749e-01\n",
      "Iteration: 6360, loss: 8.980e-01\n",
      "Iteration: 6370, loss: 1.009e+00\n",
      "Iteration: 6380, loss: 1.019e+00\n",
      "Iteration: 6390, loss: 9.254e-01\n",
      "Iteration: 6400, loss: 1.036e+00\n",
      "Iteration: 6410, loss: 5.818e-01\n",
      "Iteration: 6420, loss: 1.127e+00\n",
      "Iteration: 6430, loss: 4.966e-01\n",
      "Iteration: 6440, loss: 9.490e-01\n",
      "Iteration: 6450, loss: 8.658e-01\n",
      "Iteration: 6460, loss: 7.705e-01\n",
      "Iteration: 6470, loss: 7.209e-01\n",
      "Iteration: 6480, loss: 1.104e+00\n",
      "Iteration: 6490, loss: 1.196e+00\n",
      "Iteration: 6500, loss: 9.449e-01\n",
      "Iteration: 6510, loss: 1.163e+00\n",
      "Iteration: 6520, loss: 7.223e-01\n",
      "Iteration: 6530, loss: 9.154e-01\n",
      "Iteration: 6540, loss: 7.504e-01\n",
      "Iteration: 6550, loss: 4.818e-01\n",
      "Iteration: 6560, loss: 6.153e-01\n",
      "Iteration: 6570, loss: 7.303e-01\n",
      "Iteration: 6580, loss: 3.695e-01\n",
      "Iteration: 6590, loss: 8.325e-01\n",
      "Iteration: 6600, loss: 8.140e-01\n",
      "Iteration: 6610, loss: 9.405e-01\n",
      "Iteration: 6620, loss: 6.748e-01\n",
      "Iteration: 6630, loss: 1.120e+00\n",
      "Iteration: 6640, loss: 7.599e-01\n",
      "Iteration: 6650, loss: 8.748e-01\n",
      "Iteration: 6660, loss: 5.531e-01\n",
      "Iteration: 6670, loss: 9.532e-01\n",
      "Iteration: 6680, loss: 6.055e-01\n",
      "Iteration: 6690, loss: 9.583e-01\n",
      "Iteration: 6700, loss: 6.861e-01\n",
      "Iteration: 6710, loss: 8.457e-01\n",
      "Iteration: 6720, loss: 7.878e-01\n",
      "Iteration: 6730, loss: 4.292e-01\n",
      "Iteration: 6740, loss: 1.013e+00\n",
      "Iteration: 6750, loss: 8.752e-01\n",
      "Iteration: 6760, loss: 5.239e-01\n",
      "Iteration: 6770, loss: 7.442e-01\n",
      "Iteration: 6780, loss: 3.309e-01\n",
      "Iteration: 6790, loss: 8.718e-01\n",
      "Iteration: 6800, loss: 1.040e+00\n",
      "Iteration: 6810, loss: 5.895e-01\n",
      "Iteration: 6820, loss: 6.133e-01\n",
      "Iteration: 6830, loss: 6.324e-01\n",
      "Iteration: 6840, loss: 6.031e-01\n",
      "Iteration: 6850, loss: 5.512e-01\n",
      "Iteration: 6860, loss: 9.016e-01\n",
      "Iteration: 6870, loss: 5.876e-01\n",
      "Iteration: 6880, loss: 5.955e-01\n",
      "Iteration: 6890, loss: 7.221e-01\n",
      "Iteration: 6900, loss: 7.510e-01\n",
      "Iteration: 6910, loss: 7.118e-01\n",
      "Iteration: 6920, loss: 4.187e-01\n",
      "Iteration: 6930, loss: 6.386e-01\n",
      "Iteration: 6940, loss: 5.077e-01\n",
      "Iteration: 6950, loss: 1.120e+00\n",
      "Iteration: 6960, loss: 6.669e-01\n",
      "Iteration: 6970, loss: 3.597e-01\n",
      "Iteration: 6980, loss: 6.939e-01\n",
      "Iteration: 6990, loss: 6.914e-01\n",
      "Iteration: 7000, loss: 8.687e-01\n",
      "Iteration: 7010, loss: 8.680e-01\n",
      "Iteration: 7020, loss: 7.472e-01\n",
      "Iteration: 7030, loss: 6.781e-01\n",
      "Iteration: 7040, loss: 7.689e-01\n",
      "Iteration: 7050, loss: 6.777e-01\n",
      "Iteration: 7060, loss: 5.353e-01\n",
      "Iteration: 7070, loss: 5.044e-01\n",
      "Iteration: 7080, loss: 1.179e+00\n",
      "Iteration: 7090, loss: 7.155e-01\n",
      "Iteration: 7100, loss: 7.035e-01\n",
      "Iteration: 7110, loss: 8.412e-01\n",
      "Iteration: 7120, loss: 1.295e+00\n",
      "Iteration: 7130, loss: 5.678e-01\n",
      "Iteration: 7140, loss: 8.496e-01\n",
      "Iteration: 7150, loss: 5.769e-01\n",
      "Iteration: 7160, loss: 5.241e-01\n",
      "Iteration: 7170, loss: 6.335e-01\n",
      "Iteration: 7180, loss: 5.362e-01\n",
      "Iteration: 7190, loss: 5.909e-01\n",
      "Iteration: 7200, loss: 7.694e-01\n",
      "Iteration: 7210, loss: 5.194e-01\n",
      "Iteration: 7220, loss: 7.260e-01\n",
      "Iteration: 7230, loss: 4.223e-01\n",
      "Iteration: 7240, loss: 6.077e-01\n",
      "Iteration: 7250, loss: 6.354e-01\n",
      "Iteration: 7260, loss: 8.494e-01\n",
      "Iteration: 7270, loss: 6.928e-01\n",
      "Iteration: 7280, loss: 9.139e-01\n",
      "Iteration: 7290, loss: 6.023e-01\n",
      "Iteration: 7300, loss: 6.534e-01\n",
      "Iteration: 7310, loss: 4.298e-01\n",
      "Iteration: 7320, loss: 9.514e-01\n",
      "Iteration: 7330, loss: 6.928e-01\n",
      "Iteration: 7340, loss: 4.196e-01\n",
      "Iteration: 7350, loss: 6.776e-01\n",
      "Iteration: 7360, loss: 7.686e-01\n",
      "Iteration: 7370, loss: 8.230e-01\n",
      "Iteration: 7380, loss: 6.193e-01\n",
      "Iteration: 7390, loss: 5.111e-01\n",
      "Iteration: 7400, loss: 4.618e-01\n",
      "Iteration: 7410, loss: 4.282e-01\n",
      "Iteration: 7420, loss: 4.203e-01\n",
      "Iteration: 7430, loss: 4.233e-01\n",
      "Iteration: 7440, loss: 4.959e-01\n",
      "Iteration: 7450, loss: 4.208e-01\n",
      "Iteration: 7460, loss: 4.570e-01\n",
      "Iteration: 7470, loss: 7.136e-01\n",
      "Iteration: 7480, loss: 3.947e-01\n",
      "Iteration: 7490, loss: 6.285e-01\n",
      "Iteration: 7500, loss: 4.536e-01\n",
      "Iteration: 7510, loss: 3.756e-01\n",
      "Iteration: 7520, loss: 6.239e-01\n",
      "Iteration: 7530, loss: 5.733e-01\n",
      "Iteration: 7540, loss: 7.983e-01\n",
      "Iteration: 7550, loss: 5.311e-01\n",
      "Iteration: 7560, loss: 6.684e-01\n",
      "Iteration: 7570, loss: 6.812e-01\n",
      "Iteration: 7580, loss: 6.490e-01\n",
      "Iteration: 7590, loss: 3.489e-01\n",
      "Iteration: 7600, loss: 5.605e-01\n",
      "Iteration: 7610, loss: 7.150e-01\n",
      "Iteration: 7620, loss: 6.704e-01\n",
      "Iteration: 7630, loss: 3.171e-01\n",
      "Iteration: 7640, loss: 7.351e-01\n",
      "Iteration: 7650, loss: 2.584e-01\n",
      "Iteration: 7660, loss: 5.627e-01\n",
      "Iteration: 7670, loss: 6.373e-01\n",
      "Iteration: 7680, loss: 6.495e-01\n",
      "Iteration: 7690, loss: 6.748e-01\n",
      "Iteration: 7700, loss: 3.479e-01\n",
      "Iteration: 7710, loss: 3.973e-01\n",
      "Iteration: 7720, loss: 4.946e-01\n",
      "Iteration: 7730, loss: 5.520e-01\n",
      "Iteration: 7740, loss: 5.769e-01\n",
      "Iteration: 7750, loss: 2.401e-01\n",
      "Iteration: 7760, loss: 4.286e-01\n",
      "Iteration: 7770, loss: 5.014e-01\n",
      "Iteration: 7780, loss: 5.317e-01\n",
      "Iteration: 7790, loss: 6.067e-01\n",
      "Iteration: 7800, loss: 4.356e-01\n",
      "Iteration: 7810, loss: 4.742e-01\n",
      "Iteration: 7820, loss: 5.337e-01\n",
      "Iteration: 7830, loss: 3.818e-01\n",
      "Iteration: 7840, loss: 5.439e-01\n",
      "Iteration: 7850, loss: 4.919e-01\n",
      "Iteration: 7860, loss: 3.765e-01\n",
      "Iteration: 7870, loss: 7.567e-01\n",
      "Iteration: 7880, loss: 8.547e-01\n",
      "Iteration: 7890, loss: 3.882e-01\n",
      "Iteration: 7900, loss: 4.977e-01\n",
      "Iteration: 7910, loss: 7.096e-01\n",
      "Iteration: 7920, loss: 3.595e-01\n",
      "Iteration: 7930, loss: 5.236e-01\n",
      "Iteration: 7940, loss: 4.383e-01\n",
      "Iteration: 7950, loss: 5.061e-01\n",
      "Iteration: 7960, loss: 9.474e-01\n",
      "Iteration: 7970, loss: 6.013e-01\n",
      "Iteration: 7980, loss: 4.681e-01\n",
      "Iteration: 7990, loss: 5.694e-01\n",
      "Iteration: 8000, loss: 4.266e-01\n",
      "Iteration: 8010, loss: 5.471e-01\n",
      "Iteration: 8020, loss: 4.029e-01\n",
      "Iteration: 8030, loss: 6.148e-01\n",
      "Iteration: 8040, loss: 5.499e-01\n",
      "Iteration: 8050, loss: 9.227e-01\n",
      "Iteration: 8060, loss: 4.052e-01\n",
      "Iteration: 8070, loss: 5.871e-01\n",
      "Iteration: 8080, loss: 6.469e-01\n",
      "Iteration: 8090, loss: 3.933e-01\n",
      "Iteration: 8100, loss: 4.931e-01\n",
      "Iteration: 8110, loss: 3.333e-01\n",
      "Iteration: 8120, loss: 4.972e-01\n",
      "Iteration: 8130, loss: 4.066e-01\n",
      "Iteration: 8140, loss: 4.455e-01\n",
      "Iteration: 8150, loss: 3.549e-01\n",
      "Iteration: 8160, loss: 4.765e-01\n",
      "Iteration: 8170, loss: 3.570e-01\n",
      "Iteration: 8180, loss: 2.934e-01\n",
      "Iteration: 8190, loss: 4.966e-01\n",
      "Iteration: 8200, loss: 2.942e-01\n",
      "Iteration: 8210, loss: 3.001e-01\n",
      "Iteration: 8220, loss: 5.778e-01\n",
      "Iteration: 8230, loss: 2.880e-01\n",
      "Iteration: 8240, loss: 5.527e-01\n",
      "Iteration: 8250, loss: 3.543e-01\n",
      "Iteration: 8260, loss: 4.354e-01\n",
      "Iteration: 8270, loss: 5.119e-01\n",
      "Iteration: 8280, loss: 2.465e-01\n",
      "Iteration: 8290, loss: 3.605e-01\n",
      "Iteration: 8300, loss: 5.704e-01\n",
      "Iteration: 8310, loss: 4.704e-01\n",
      "Iteration: 8320, loss: 2.871e-01\n",
      "Iteration: 8330, loss: 2.668e-01\n",
      "Iteration: 8340, loss: 6.704e-01\n",
      "Iteration: 8350, loss: 5.233e-01\n",
      "Iteration: 8360, loss: 3.370e-01\n",
      "Iteration: 8370, loss: 3.400e-01\n",
      "Iteration: 8380, loss: 5.027e-01\n",
      "Iteration: 8390, loss: 3.052e-01\n",
      "Iteration: 8400, loss: 3.095e-01\n",
      "Iteration: 8410, loss: 2.866e-01\n",
      "Iteration: 8420, loss: 2.243e-01\n",
      "Iteration: 8430, loss: 2.990e-01\n",
      "Iteration: 8440, loss: 4.558e-01\n",
      "Iteration: 8450, loss: 3.009e-01\n",
      "Iteration: 8460, loss: 4.392e-01\n",
      "Iteration: 8470, loss: 5.499e-01\n",
      "Iteration: 8480, loss: 2.385e-01\n",
      "Iteration: 8490, loss: 3.930e-01\n",
      "Iteration: 8500, loss: 3.120e-01\n",
      "Iteration: 8510, loss: 4.924e-01\n",
      "Iteration: 8520, loss: 3.545e-01\n",
      "Iteration: 8530, loss: 3.266e-01\n",
      "Iteration: 8540, loss: 4.233e-01\n",
      "Iteration: 8550, loss: 3.225e-01\n",
      "Iteration: 8560, loss: 3.113e-01\n",
      "Iteration: 8570, loss: 3.267e-01\n",
      "Iteration: 8580, loss: 3.091e-01\n",
      "Iteration: 8590, loss: 6.265e-01\n",
      "Iteration: 8600, loss: 3.022e-01\n",
      "Iteration: 8610, loss: 3.897e-01\n",
      "Iteration: 8620, loss: 3.958e-01\n",
      "Iteration: 8630, loss: 3.907e-01\n",
      "Iteration: 8640, loss: 2.725e-01\n",
      "Iteration: 8650, loss: 3.197e-01\n",
      "Iteration: 8660, loss: 3.425e-01\n",
      "Iteration: 8670, loss: 6.563e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8680, loss: 3.397e-01\n",
      "Iteration: 8690, loss: 2.083e-01\n",
      "Iteration: 8700, loss: 2.772e-01\n",
      "Iteration: 8710, loss: 2.115e-01\n",
      "Iteration: 8720, loss: 3.989e-01\n",
      "Iteration: 8730, loss: 2.928e-01\n",
      "Iteration: 8740, loss: 3.837e-01\n",
      "Iteration: 8750, loss: 3.562e-01\n",
      "Iteration: 8760, loss: 1.849e-01\n",
      "Iteration: 8770, loss: 3.378e-01\n",
      "Iteration: 8780, loss: 6.142e-01\n",
      "Iteration: 8790, loss: 2.851e-01\n",
      "Iteration: 8800, loss: 4.861e-01\n",
      "Iteration: 8810, loss: 4.742e-01\n",
      "Iteration: 8820, loss: 4.645e-01\n",
      "Iteration: 8830, loss: 3.168e-01\n",
      "Iteration: 8840, loss: 3.086e-01\n",
      "Iteration: 8850, loss: 2.645e-01\n",
      "Iteration: 8860, loss: 2.102e-01\n",
      "Iteration: 8870, loss: 3.430e-01\n",
      "Iteration: 8880, loss: 4.226e-01\n",
      "Iteration: 8890, loss: 3.222e-01\n",
      "Iteration: 8900, loss: 3.563e-01\n",
      "Iteration: 8910, loss: 3.308e-01\n",
      "Iteration: 8920, loss: 3.053e-01\n",
      "Iteration: 8930, loss: 3.584e-01\n",
      "Iteration: 8940, loss: 2.811e-01\n",
      "Iteration: 8950, loss: 4.008e-01\n",
      "Iteration: 8960, loss: 1.537e-01\n",
      "Iteration: 8970, loss: 4.411e-01\n",
      "Iteration: 8980, loss: 2.856e-01\n",
      "Iteration: 8990, loss: 4.172e-01\n",
      "Iteration: 9000, loss: 3.038e-01\n",
      "Iteration: 9010, loss: 3.961e-01\n",
      "Iteration: 9020, loss: 2.069e-01\n",
      "Iteration: 9030, loss: 1.915e-01\n",
      "Iteration: 9040, loss: 3.263e-01\n",
      "Iteration: 9050, loss: 3.224e-01\n",
      "Iteration: 9060, loss: 3.503e-01\n",
      "Iteration: 9070, loss: 2.733e-01\n",
      "Iteration: 9080, loss: 2.920e-01\n",
      "Iteration: 9090, loss: 1.189e-01\n",
      "Iteration: 9100, loss: 3.133e-01\n",
      "Iteration: 9110, loss: 1.720e-01\n",
      "Iteration: 9120, loss: 2.823e-01\n",
      "Iteration: 9130, loss: 2.710e-01\n",
      "Iteration: 9140, loss: 2.813e-01\n",
      "Iteration: 9150, loss: 3.163e-01\n",
      "Iteration: 9160, loss: 1.774e-01\n",
      "Iteration: 9170, loss: 2.765e-01\n",
      "Iteration: 9180, loss: 2.463e-01\n",
      "Iteration: 9190, loss: 3.025e-01\n",
      "Iteration: 9200, loss: 2.891e-01\n",
      "Iteration: 9210, loss: 1.746e-01\n",
      "Iteration: 9220, loss: 1.464e-01\n",
      "Iteration: 9230, loss: 1.326e-01\n",
      "Iteration: 9240, loss: 1.343e-01\n",
      "Iteration: 9250, loss: 2.795e-01\n",
      "Iteration: 9260, loss: 1.624e-01\n",
      "Iteration: 9270, loss: 2.894e-01\n",
      "Iteration: 9280, loss: 3.319e-01\n",
      "Iteration: 9290, loss: 2.552e-01\n",
      "Iteration: 9300, loss: 2.866e-01\n",
      "Iteration: 9310, loss: 2.891e-01\n",
      "Iteration: 9320, loss: 1.412e-01\n",
      "Iteration: 9330, loss: 2.583e-01\n",
      "Iteration: 9340, loss: 2.390e-01\n",
      "Iteration: 9350, loss: 2.880e-01\n",
      "Iteration: 9360, loss: 3.331e-01\n",
      "Iteration: 9370, loss: 2.796e-01\n",
      "Iteration: 9380, loss: 3.678e-01\n",
      "Iteration: 9390, loss: 2.839e-01\n",
      "Iteration: 9400, loss: 4.370e-01\n",
      "Iteration: 9410, loss: 2.227e-01\n",
      "Iteration: 9420, loss: 3.286e-01\n",
      "Iteration: 9430, loss: 2.809e-01\n",
      "Iteration: 9440, loss: 1.430e-01\n",
      "Iteration: 9450, loss: 3.084e-01\n",
      "Iteration: 9460, loss: 3.778e-01\n",
      "Iteration: 9470, loss: 3.181e-01\n",
      "Iteration: 9480, loss: 3.083e-01\n",
      "Iteration: 9490, loss: 3.653e-01\n",
      "Iteration: 9500, loss: 2.800e-01\n",
      "Iteration: 9510, loss: 4.802e-01\n",
      "Iteration: 9520, loss: 2.574e-01\n",
      "Iteration: 9530, loss: 2.866e-01\n",
      "Iteration: 9540, loss: 3.595e-01\n",
      "Iteration: 9550, loss: 3.518e-01\n",
      "Iteration: 9560, loss: 3.073e-01\n",
      "Iteration: 9570, loss: 2.127e-01\n",
      "Iteration: 9580, loss: 1.980e-01\n",
      "Iteration: 9590, loss: 2.704e-01\n",
      "Iteration: 9600, loss: 3.130e-01\n",
      "Iteration: 9610, loss: 1.860e-01\n",
      "Iteration: 9620, loss: 2.985e-01\n",
      "Iteration: 9630, loss: 2.981e-01\n",
      "Iteration: 9640, loss: 2.350e-01\n",
      "Iteration: 9650, loss: 2.883e-01\n",
      "Iteration: 9660, loss: 2.642e-01\n",
      "Iteration: 9670, loss: 3.496e-01\n",
      "Iteration: 9680, loss: 3.181e-01\n",
      "Iteration: 9690, loss: 3.124e-01\n",
      "Iteration: 9700, loss: 3.498e-01\n",
      "Iteration: 9710, loss: 2.857e-01\n",
      "Iteration: 9720, loss: 1.519e-01\n",
      "Iteration: 9730, loss: 2.910e-01\n",
      "Iteration: 9740, loss: 1.596e-01\n",
      "Iteration: 9750, loss: 1.829e-01\n",
      "Iteration: 9760, loss: 1.409e-01\n",
      "Iteration: 9770, loss: 2.575e-01\n",
      "Iteration: 9780, loss: 2.070e-01\n",
      "Iteration: 9790, loss: 2.826e-01\n",
      "Iteration: 9800, loss: 2.824e-01\n",
      "Iteration: 9810, loss: 1.699e-01\n",
      "Iteration: 9820, loss: 2.080e-01\n",
      "Iteration: 9830, loss: 1.664e-01\n",
      "Iteration: 9840, loss: 2.116e-01\n",
      "Iteration: 9850, loss: 3.041e-01\n",
      "Iteration: 9860, loss: 1.712e-01\n",
      "Iteration: 9870, loss: 2.400e-01\n",
      "Iteration: 9880, loss: 2.722e-01\n",
      "Iteration: 9890, loss: 1.798e-01\n",
      "Iteration: 9900, loss: 2.235e-01\n",
      "Iteration: 9910, loss: 2.130e-01\n",
      "Iteration: 9920, loss: 2.980e-01\n",
      "Iteration: 9930, loss: 1.973e-01\n",
      "Iteration: 9940, loss: 1.992e-01\n",
      "Iteration: 9950, loss: 2.505e-01\n",
      "Iteration: 9960, loss: 1.384e-01\n",
      "Iteration: 9970, loss: 2.098e-01\n",
      "Iteration: 9980, loss: 1.861e-01\n",
      "Iteration: 9990, loss: 2.297e-01\n",
      "Iteration: 10000, loss: 2.751e-01\n",
      "Iteration: 10010, loss: 2.408e-01\n",
      "Iteration: 10020, loss: 1.192e-01\n",
      "Iteration: 10030, loss: 3.227e-01\n",
      "Iteration: 10040, loss: 1.799e-01\n",
      "Iteration: 10050, loss: 1.514e-01\n",
      "Iteration: 10060, loss: 1.958e-01\n",
      "Iteration: 10070, loss: 2.513e-01\n",
      "Iteration: 10080, loss: 1.314e-01\n",
      "Iteration: 10090, loss: 2.600e-01\n",
      "Iteration: 10100, loss: 2.058e-01\n",
      "Iteration: 10110, loss: 2.505e-01\n",
      "Iteration: 10120, loss: 1.949e-01\n",
      "Iteration: 10130, loss: 1.675e-01\n",
      "Iteration: 10140, loss: 2.127e-01\n",
      "Iteration: 10150, loss: 1.647e-01\n",
      "Iteration: 10160, loss: 1.788e-01\n",
      "Iteration: 10170, loss: 2.373e-01\n",
      "Iteration: 10180, loss: 1.601e-01\n",
      "Iteration: 10190, loss: 1.639e-01\n",
      "Iteration: 10200, loss: 2.469e-01\n",
      "Iteration: 10210, loss: 1.597e-01\n",
      "Iteration: 10220, loss: 2.160e-01\n",
      "Iteration: 10230, loss: 1.653e-01\n",
      "Iteration: 10240, loss: 2.030e-01\n",
      "Iteration: 10250, loss: 1.801e-01\n",
      "Iteration: 10260, loss: 1.251e-01\n",
      "Iteration: 10270, loss: 2.466e-01\n",
      "Iteration: 10280, loss: 1.936e-01\n",
      "Iteration: 10290, loss: 2.156e-01\n",
      "Iteration: 10300, loss: 2.067e-01\n",
      "Iteration: 10310, loss: 1.566e-01\n",
      "Iteration: 10320, loss: 2.544e-01\n",
      "Iteration: 10330, loss: 1.402e-01\n",
      "Iteration: 10340, loss: 2.059e-01\n",
      "Iteration: 10350, loss: 2.371e-01\n",
      "Iteration: 10360, loss: 1.481e-01\n",
      "Iteration: 10370, loss: 1.468e-01\n",
      "Iteration: 10380, loss: 1.735e-01\n",
      "Iteration: 10390, loss: 1.624e-01\n",
      "Iteration: 10400, loss: 3.288e-01\n",
      "Iteration: 10410, loss: 2.065e-01\n",
      "Iteration: 10420, loss: 2.411e-01\n",
      "Iteration: 10430, loss: 1.451e-01\n",
      "Iteration: 10440, loss: 7.988e-02\n",
      "Iteration: 10450, loss: 1.781e-01\n",
      "Iteration: 10460, loss: 2.106e-01\n",
      "Iteration: 10470, loss: 1.148e-01\n",
      "Iteration: 10480, loss: 2.048e-01\n",
      "Iteration: 10490, loss: 9.739e-02\n",
      "Iteration: 10500, loss: 1.828e-01\n",
      "Iteration: 10510, loss: 2.680e-01\n",
      "Iteration: 10520, loss: 1.331e-01\n",
      "Iteration: 10530, loss: 2.376e-01\n",
      "Iteration: 10540, loss: 1.660e-01\n",
      "Iteration: 10550, loss: 1.580e-01\n",
      "Iteration: 10560, loss: 1.616e-01\n",
      "Iteration: 10570, loss: 2.206e-01\n",
      "Iteration: 10580, loss: 2.358e-01\n",
      "Iteration: 10590, loss: 1.551e-01\n",
      "Iteration: 10600, loss: 1.976e-01\n",
      "Iteration: 10610, loss: 1.079e-01\n",
      "Iteration: 10620, loss: 1.319e-01\n",
      "Iteration: 10630, loss: 2.004e-01\n",
      "Iteration: 10640, loss: 9.883e-02\n",
      "Iteration: 10650, loss: 1.739e-01\n",
      "Iteration: 10660, loss: 1.158e-01\n",
      "Iteration: 10670, loss: 1.202e-01\n",
      "Iteration: 10680, loss: 1.542e-01\n",
      "Iteration: 10690, loss: 1.159e-01\n",
      "Iteration: 10700, loss: 9.049e-02\n",
      "Iteration: 10710, loss: 1.518e-01\n",
      "Iteration: 10720, loss: 1.441e-01\n",
      "Iteration: 10730, loss: 1.379e-01\n",
      "Iteration: 10740, loss: 2.356e-01\n",
      "Iteration: 10750, loss: 1.497e-01\n",
      "Iteration: 10760, loss: 9.760e-02\n",
      "Iteration: 10770, loss: 1.600e-01\n",
      "Iteration: 10780, loss: 1.655e-01\n",
      "Iteration: 10790, loss: 1.813e-01\n",
      "Iteration: 10800, loss: 2.066e-01\n",
      "Iteration: 10810, loss: 1.792e-01\n",
      "Iteration: 10820, loss: 1.748e-01\n",
      "Iteration: 10830, loss: 2.142e-01\n",
      "Iteration: 10840, loss: 1.514e-01\n",
      "Iteration: 10850, loss: 1.033e-01\n",
      "Iteration: 10860, loss: 1.222e-01\n",
      "Iteration: 10870, loss: 1.034e-01\n",
      "Iteration: 10880, loss: 8.996e-02\n",
      "Iteration: 10890, loss: 1.711e-01\n",
      "Iteration: 10900, loss: 2.186e-01\n",
      "Iteration: 10910, loss: 2.171e-01\n",
      "Iteration: 10920, loss: 1.930e-01\n",
      "Iteration: 10930, loss: 1.470e-01\n",
      "Iteration: 10940, loss: 1.929e-01\n",
      "Iteration: 10950, loss: 1.507e-01\n",
      "Iteration: 10960, loss: 9.849e-02\n",
      "Iteration: 10970, loss: 1.536e-01\n",
      "Iteration: 10980, loss: 1.651e-01\n",
      "Iteration: 10990, loss: 1.222e-01\n",
      "Iteration: 11000, loss: 1.411e-01\n",
      "Iteration: 11010, loss: 9.529e-02\n",
      "Iteration: 11020, loss: 1.645e-01\n",
      "Iteration: 11030, loss: 1.271e-01\n",
      "Iteration: 11040, loss: 1.793e-01\n",
      "Iteration: 11050, loss: 1.041e-01\n",
      "Iteration: 11060, loss: 1.560e-01\n",
      "Iteration: 11070, loss: 1.519e-01\n",
      "Iteration: 11080, loss: 1.439e-01\n",
      "Iteration: 11090, loss: 8.635e-02\n",
      "Iteration: 11100, loss: 1.302e-01\n",
      "Iteration: 11110, loss: 1.267e-01\n",
      "Iteration: 11120, loss: 1.246e-01\n",
      "Iteration: 11130, loss: 7.389e-02\n",
      "Iteration: 11140, loss: 1.504e-01\n",
      "Iteration: 11150, loss: 1.195e-01\n",
      "Iteration: 11160, loss: 1.315e-01\n",
      "Iteration: 11170, loss: 1.866e-01\n",
      "Iteration: 11180, loss: 1.214e-01\n",
      "Iteration: 11190, loss: 1.877e-01\n",
      "Iteration: 11200, loss: 1.442e-01\n",
      "Iteration: 11210, loss: 9.179e-02\n",
      "Iteration: 11220, loss: 1.112e-01\n",
      "Iteration: 11230, loss: 2.186e-01\n",
      "Iteration: 11240, loss: 1.497e-01\n",
      "Iteration: 11250, loss: 1.836e-01\n",
      "Iteration: 11260, loss: 2.249e-01\n",
      "Iteration: 11270, loss: 1.399e-01\n",
      "Iteration: 11280, loss: 1.508e-01\n",
      "Iteration: 11290, loss: 1.909e-01\n",
      "Iteration: 11300, loss: 1.739e-01\n",
      "Iteration: 11310, loss: 1.864e-01\n",
      "Iteration: 11320, loss: 1.167e-01\n",
      "Iteration: 11330, loss: 1.030e-01\n",
      "Iteration: 11340, loss: 9.292e-02\n",
      "Iteration: 11350, loss: 1.224e-01\n",
      "Iteration: 11360, loss: 1.019e-01\n",
      "Iteration: 11370, loss: 6.643e-02\n",
      "Iteration: 11380, loss: 7.789e-02\n",
      "Iteration: 11390, loss: 1.109e-01\n",
      "Iteration: 11400, loss: 9.650e-02\n",
      "Iteration: 11410, loss: 1.408e-01\n",
      "Iteration: 11420, loss: 1.246e-01\n",
      "Iteration: 11430, loss: 1.052e-01\n",
      "Iteration: 11440, loss: 1.619e-01\n",
      "Iteration: 11450, loss: 6.810e-02\n",
      "Iteration: 11460, loss: 1.302e-01\n",
      "Iteration: 11470, loss: 8.892e-02\n",
      "Iteration: 11480, loss: 7.836e-02\n",
      "Iteration: 11490, loss: 1.184e-01\n",
      "Iteration: 11500, loss: 7.123e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11510, loss: 1.895e-01\n",
      "Iteration: 11520, loss: 1.151e-01\n",
      "Iteration: 11530, loss: 1.046e-01\n",
      "Iteration: 11540, loss: 8.622e-02\n",
      "Iteration: 11550, loss: 1.131e-01\n",
      "Iteration: 11560, loss: 1.527e-01\n",
      "Iteration: 11570, loss: 1.362e-01\n",
      "Iteration: 11580, loss: 8.199e-02\n",
      "Iteration: 11590, loss: 8.149e-02\n",
      "Iteration: 11600, loss: 1.741e-01\n",
      "Iteration: 11610, loss: 9.950e-02\n",
      "Iteration: 11620, loss: 1.467e-01\n",
      "Iteration: 11630, loss: 1.611e-01\n",
      "Iteration: 11640, loss: 1.059e-01\n",
      "Iteration: 11650, loss: 9.411e-02\n",
      "Iteration: 11660, loss: 1.621e-01\n",
      "Iteration: 11670, loss: 1.102e-01\n",
      "Iteration: 11680, loss: 7.360e-02\n",
      "Iteration: 11690, loss: 1.449e-01\n",
      "Iteration: 11700, loss: 9.975e-02\n",
      "Iteration: 11710, loss: 9.179e-02\n",
      "Iteration: 11720, loss: 1.425e-01\n",
      "Iteration: 11730, loss: 9.014e-02\n",
      "Iteration: 11740, loss: 1.469e-01\n",
      "Iteration: 11750, loss: 7.956e-02\n",
      "Iteration: 11760, loss: 7.961e-02\n",
      "Iteration: 11770, loss: 1.494e-01\n",
      "Iteration: 11780, loss: 9.751e-02\n",
      "Iteration: 11790, loss: 7.787e-02\n",
      "Iteration: 11800, loss: 1.431e-01\n",
      "Iteration: 11810, loss: 9.460e-02\n",
      "Iteration: 11820, loss: 1.464e-01\n",
      "Iteration: 11830, loss: 9.733e-02\n",
      "Iteration: 11840, loss: 8.244e-02\n",
      "Iteration: 11850, loss: 9.709e-02\n",
      "Iteration: 11860, loss: 6.665e-02\n",
      "Iteration: 11870, loss: 8.989e-02\n",
      "Iteration: 11880, loss: 6.282e-02\n",
      "Iteration: 11890, loss: 1.168e-01\n",
      "Iteration: 11900, loss: 1.053e-01\n",
      "Iteration: 11910, loss: 1.049e-01\n",
      "Iteration: 11920, loss: 1.341e-01\n",
      "Iteration: 11930, loss: 1.097e-01\n",
      "Iteration: 11940, loss: 1.101e-01\n",
      "Iteration: 11950, loss: 1.510e-01\n",
      "Iteration: 11960, loss: 1.723e-01\n",
      "Iteration: 11970, loss: 1.303e-01\n",
      "Iteration: 11980, loss: 6.532e-02\n",
      "Iteration: 11990, loss: 6.834e-02\n",
      "Iteration: 12000, loss: 5.063e-02\n",
      "Iteration: 12010, loss: 6.353e-02\n",
      "Iteration: 12020, loss: 1.089e-01\n",
      "Iteration: 12030, loss: 1.182e-01\n",
      "Iteration: 12040, loss: 1.041e-01\n",
      "Iteration: 12050, loss: 1.225e-01\n",
      "Iteration: 12060, loss: 1.635e-01\n",
      "Iteration: 12070, loss: 5.771e-02\n",
      "Iteration: 12080, loss: 7.420e-02\n",
      "Iteration: 12090, loss: 1.231e-01\n",
      "Iteration: 12100, loss: 6.835e-02\n",
      "Iteration: 12110, loss: 6.903e-02\n",
      "Iteration: 12120, loss: 1.149e-01\n",
      "Iteration: 12130, loss: 5.304e-02\n",
      "Iteration: 12140, loss: 1.139e-01\n",
      "Iteration: 12150, loss: 1.053e-01\n",
      "Iteration: 12160, loss: 7.040e-02\n",
      "Iteration: 12170, loss: 1.321e-01\n",
      "Iteration: 12180, loss: 1.629e-01\n",
      "Iteration: 12190, loss: 1.938e-01\n",
      "Iteration: 12200, loss: 1.525e-01\n",
      "Iteration: 12210, loss: 1.102e-01\n",
      "Iteration: 12220, loss: 8.760e-02\n",
      "Iteration: 12230, loss: 1.407e-01\n",
      "Iteration: 12240, loss: 1.201e-01\n",
      "Iteration: 12250, loss: 1.074e-01\n",
      "Iteration: 12260, loss: 4.720e-02\n",
      "Iteration: 12270, loss: 1.031e-01\n",
      "Iteration: 12280, loss: 6.030e-02\n",
      "Iteration: 12290, loss: 8.634e-02\n",
      "Iteration: 12300, loss: 7.778e-02\n",
      "Iteration: 12310, loss: 1.163e-01\n",
      "Iteration: 12320, loss: 6.919e-02\n",
      "Iteration: 12330, loss: 1.384e-01\n",
      "Iteration: 12340, loss: 7.827e-02\n",
      "Iteration: 12350, loss: 1.134e-01\n",
      "Iteration: 12360, loss: 9.912e-02\n",
      "Iteration: 12370, loss: 1.103e-01\n",
      "Iteration: 12380, loss: 9.079e-02\n",
      "Iteration: 12390, loss: 8.580e-02\n",
      "Iteration: 12400, loss: 5.813e-02\n",
      "Iteration: 12410, loss: 9.505e-02\n",
      "Iteration: 12420, loss: 1.278e-01\n",
      "Iteration: 12430, loss: 9.580e-02\n",
      "Iteration: 12440, loss: 7.099e-02\n",
      "Iteration: 12450, loss: 8.629e-02\n",
      "Iteration: 12460, loss: 5.607e-02\n",
      "Iteration: 12470, loss: 6.159e-02\n",
      "Iteration: 12480, loss: 7.672e-02\n",
      "Iteration: 12490, loss: 8.790e-02\n",
      "Iteration: 12500, loss: 1.034e-01\n",
      "Iteration: 12510, loss: 7.099e-02\n",
      "Iteration: 12520, loss: 8.581e-02\n",
      "Iteration: 12530, loss: 1.056e-01\n",
      "Iteration: 12540, loss: 6.785e-02\n",
      "Iteration: 12550, loss: 4.102e-02\n",
      "Iteration: 12560, loss: 1.031e-01\n",
      "Iteration: 12570, loss: 5.885e-02\n",
      "Iteration: 12580, loss: 1.074e-01\n",
      "Iteration: 12590, loss: 5.954e-02\n",
      "Iteration: 12600, loss: 8.131e-02\n",
      "Iteration: 12610, loss: 1.347e-01\n",
      "Iteration: 12620, loss: 7.807e-02\n",
      "Iteration: 12630, loss: 7.432e-02\n",
      "Iteration: 12640, loss: 1.037e-01\n",
      "Iteration: 12650, loss: 9.516e-02\n",
      "Iteration: 12660, loss: 1.357e-01\n",
      "Iteration: 12670, loss: 5.152e-02\n",
      "Iteration: 12680, loss: 7.936e-02\n",
      "Iteration: 12690, loss: 9.792e-02\n",
      "Iteration: 12700, loss: 9.320e-02\n",
      "Iteration: 12710, loss: 7.322e-02\n",
      "Iteration: 12720, loss: 7.591e-02\n",
      "Iteration: 12730, loss: 1.098e-01\n",
      "Iteration: 12740, loss: 4.673e-02\n",
      "Iteration: 12750, loss: 5.081e-02\n",
      "Iteration: 12760, loss: 7.907e-02\n",
      "Iteration: 12770, loss: 6.973e-02\n",
      "Iteration: 12780, loss: 7.639e-02\n",
      "Iteration: 12790, loss: 1.012e-01\n",
      "Iteration: 12800, loss: 1.147e-01\n",
      "Iteration: 12810, loss: 1.095e-01\n",
      "Iteration: 12820, loss: 6.826e-02\n",
      "Iteration: 12830, loss: 5.749e-02\n",
      "Iteration: 12840, loss: 6.312e-02\n",
      "Iteration: 12850, loss: 4.511e-02\n",
      "Iteration: 12860, loss: 4.112e-02\n",
      "Iteration: 12870, loss: 5.677e-02\n",
      "Iteration: 12880, loss: 5.755e-02\n",
      "Iteration: 12890, loss: 7.977e-02\n",
      "Iteration: 12900, loss: 6.574e-02\n",
      "Iteration: 12910, loss: 7.415e-02\n",
      "Iteration: 12920, loss: 4.913e-02\n",
      "Iteration: 12930, loss: 9.019e-02\n",
      "Iteration: 12940, loss: 7.879e-02\n",
      "Iteration: 12950, loss: 8.777e-02\n",
      "Iteration: 12960, loss: 7.351e-02\n",
      "Iteration: 12970, loss: 7.350e-02\n",
      "Iteration: 12980, loss: 5.480e-02\n",
      "Iteration: 12990, loss: 4.930e-02\n",
      "Iteration: 13000, loss: 7.363e-02\n",
      "Iteration: 13010, loss: 5.075e-02\n",
      "Iteration: 13020, loss: 6.694e-02\n",
      "Iteration: 13030, loss: 3.834e-02\n",
      "Iteration: 13040, loss: 5.572e-02\n",
      "Iteration: 13050, loss: 1.209e-01\n",
      "Iteration: 13060, loss: 7.517e-02\n",
      "Iteration: 13070, loss: 8.324e-02\n",
      "Iteration: 13080, loss: 8.456e-02\n",
      "Iteration: 13090, loss: 7.075e-02\n",
      "Iteration: 13100, loss: 6.360e-02\n",
      "Iteration: 13110, loss: 8.962e-02\n",
      "Iteration: 13120, loss: 7.341e-02\n",
      "Iteration: 13130, loss: 6.298e-02\n",
      "Iteration: 13140, loss: 8.150e-02\n",
      "Iteration: 13150, loss: 5.287e-02\n",
      "Iteration: 13160, loss: 8.108e-02\n",
      "Iteration: 13170, loss: 8.734e-02\n",
      "Iteration: 13180, loss: 6.376e-02\n",
      "Iteration: 13190, loss: 4.717e-02\n",
      "Iteration: 13200, loss: 1.168e-01\n",
      "Iteration: 13210, loss: 8.993e-02\n",
      "Iteration: 13220, loss: 9.392e-02\n",
      "Iteration: 13230, loss: 1.043e-01\n",
      "Iteration: 13240, loss: 7.615e-02\n",
      "Iteration: 13250, loss: 4.802e-02\n",
      "Iteration: 13260, loss: 4.859e-02\n",
      "Iteration: 13270, loss: 6.158e-02\n",
      "Iteration: 13280, loss: 1.158e-01\n",
      "Iteration: 13290, loss: 4.794e-02\n",
      "Iteration: 13300, loss: 2.519e-02\n",
      "Iteration: 13310, loss: 1.018e-01\n",
      "Iteration: 13320, loss: 7.172e-02\n",
      "Iteration: 13330, loss: 1.017e-01\n",
      "Iteration: 13340, loss: 3.202e-02\n",
      "Iteration: 13350, loss: 5.077e-02\n",
      "Iteration: 13360, loss: 6.101e-02\n",
      "Iteration: 13370, loss: 6.214e-02\n",
      "Iteration: 13380, loss: 6.875e-02\n",
      "Iteration: 13390, loss: 4.007e-02\n",
      "Iteration: 13400, loss: 9.653e-02\n",
      "Iteration: 13410, loss: 8.582e-02\n",
      "Iteration: 13420, loss: 6.566e-02\n",
      "Iteration: 13430, loss: 4.981e-02\n",
      "Iteration: 13440, loss: 8.439e-02\n",
      "Iteration: 13450, loss: 5.499e-02\n",
      "Iteration: 13460, loss: 5.587e-02\n",
      "Iteration: 13470, loss: 5.328e-02\n",
      "Iteration: 13480, loss: 4.113e-02\n",
      "Iteration: 13490, loss: 6.118e-02\n",
      "Iteration: 13500, loss: 6.514e-02\n",
      "Iteration: 13510, loss: 5.476e-02\n",
      "Iteration: 13520, loss: 6.861e-02\n",
      "Iteration: 13530, loss: 5.295e-02\n",
      "Iteration: 13540, loss: 8.374e-02\n",
      "Iteration: 13550, loss: 5.007e-02\n",
      "Iteration: 13560, loss: 5.841e-02\n",
      "Iteration: 13570, loss: 6.560e-02\n",
      "Iteration: 13580, loss: 4.947e-02\n",
      "Iteration: 13590, loss: 4.169e-02\n",
      "Iteration: 13600, loss: 6.246e-02\n",
      "Iteration: 13610, loss: 5.978e-02\n",
      "Iteration: 13620, loss: 8.606e-02\n",
      "Iteration: 13630, loss: 5.734e-02\n",
      "Iteration: 13640, loss: 4.629e-02\n",
      "Iteration: 13650, loss: 7.687e-02\n",
      "Iteration: 13660, loss: 5.471e-02\n",
      "Iteration: 13670, loss: 7.437e-02\n",
      "Iteration: 13680, loss: 3.435e-02\n",
      "Iteration: 13690, loss: 5.187e-02\n",
      "Iteration: 13700, loss: 4.880e-02\n",
      "Iteration: 13710, loss: 6.997e-02\n",
      "Iteration: 13720, loss: 4.421e-02\n",
      "Iteration: 13730, loss: 6.230e-02\n",
      "Iteration: 13740, loss: 3.565e-02\n",
      "Iteration: 13750, loss: 7.534e-02\n",
      "Iteration: 13760, loss: 1.029e-01\n",
      "Iteration: 13770, loss: 5.946e-02\n",
      "Iteration: 13780, loss: 2.895e-02\n",
      "Iteration: 13790, loss: 5.793e-02\n",
      "Iteration: 13800, loss: 4.361e-02\n",
      "Iteration: 13810, loss: 3.663e-02\n",
      "Iteration: 13820, loss: 5.486e-02\n",
      "Iteration: 13830, loss: 2.870e-02\n",
      "Iteration: 13840, loss: 4.424e-02\n",
      "Iteration: 13850, loss: 4.904e-02\n",
      "Iteration: 13860, loss: 6.175e-02\n",
      "Iteration: 13870, loss: 4.075e-02\n",
      "Iteration: 13880, loss: 2.181e-02\n",
      "Iteration: 13890, loss: 4.262e-02\n",
      "Iteration: 13900, loss: 1.101e-01\n",
      "Iteration: 13910, loss: 4.561e-02\n",
      "Iteration: 13920, loss: 4.253e-02\n",
      "Iteration: 13930, loss: 5.487e-02\n",
      "Iteration: 13940, loss: 5.766e-02\n",
      "Iteration: 13950, loss: 8.797e-02\n",
      "Iteration: 13960, loss: 5.480e-02\n",
      "Iteration: 13970, loss: 3.803e-02\n",
      "Iteration: 13980, loss: 4.102e-02\n",
      "Iteration: 13990, loss: 5.986e-02\n",
      "Iteration: 14000, loss: 4.511e-02\n",
      "Iteration: 14010, loss: 3.872e-02\n",
      "Iteration: 14020, loss: 4.360e-02\n",
      "Iteration: 14030, loss: 7.098e-02\n",
      "Iteration: 14040, loss: 4.636e-02\n",
      "Iteration: 14050, loss: 4.596e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14060, loss: 6.175e-02\n",
      "Iteration: 14070, loss: 5.988e-02\n",
      "Iteration: 14080, loss: 4.948e-02\n",
      "Iteration: 14090, loss: 3.748e-02\n",
      "Iteration: 14100, loss: 5.408e-02\n",
      "Iteration: 14110, loss: 3.453e-02\n",
      "Iteration: 14120, loss: 5.069e-02\n",
      "Iteration: 14130, loss: 4.340e-02\n",
      "Iteration: 14140, loss: 8.093e-02\n",
      "Iteration: 14150, loss: 3.921e-02\n",
      "Iteration: 14160, loss: 5.232e-02\n",
      "Iteration: 14170, loss: 5.936e-02\n",
      "Iteration: 14180, loss: 4.090e-02\n",
      "Iteration: 14190, loss: 7.100e-02\n",
      "Iteration: 14200, loss: 7.509e-02\n",
      "Iteration: 14210, loss: 2.033e-02\n",
      "Iteration: 14220, loss: 5.435e-02\n",
      "Iteration: 14230, loss: 5.346e-02\n",
      "Iteration: 14240, loss: 3.328e-02\n",
      "Iteration: 14250, loss: 4.387e-02\n",
      "Iteration: 14260, loss: 4.505e-02\n",
      "Iteration: 14270, loss: 8.078e-02\n",
      "Iteration: 14280, loss: 4.405e-02\n",
      "Iteration: 14290, loss: 5.887e-02\n",
      "Iteration: 14300, loss: 7.518e-02\n",
      "Iteration: 14310, loss: 5.965e-02\n",
      "Iteration: 14320, loss: 4.619e-02\n",
      "Iteration: 14330, loss: 5.140e-02\n",
      "Iteration: 14340, loss: 3.124e-02\n",
      "Iteration: 14350, loss: 4.297e-02\n",
      "Iteration: 14360, loss: 4.645e-02\n",
      "Iteration: 14370, loss: 6.521e-02\n",
      "Iteration: 14380, loss: 3.405e-02\n",
      "Iteration: 14390, loss: 5.334e-02\n",
      "Iteration: 14400, loss: 3.283e-02\n",
      "Iteration: 14410, loss: 2.702e-02\n",
      "Iteration: 14420, loss: 3.240e-02\n",
      "Iteration: 14430, loss: 4.300e-02\n",
      "Iteration: 14440, loss: 2.508e-02\n",
      "Iteration: 14450, loss: 3.260e-02\n",
      "Iteration: 14460, loss: 3.408e-02\n",
      "Iteration: 14470, loss: 2.682e-02\n",
      "Iteration: 14480, loss: 4.420e-02\n",
      "Iteration: 14490, loss: 2.759e-02\n",
      "Iteration: 14500, loss: 4.880e-02\n",
      "Iteration: 14510, loss: 2.437e-02\n",
      "Iteration: 14520, loss: 4.083e-02\n",
      "Iteration: 14530, loss: 4.737e-02\n",
      "Iteration: 14540, loss: 3.093e-02\n",
      "Iteration: 14550, loss: 4.080e-02\n",
      "Iteration: 14560, loss: 2.941e-02\n",
      "Iteration: 14570, loss: 2.085e-02\n",
      "Iteration: 14580, loss: 3.839e-02\n",
      "Iteration: 14590, loss: 2.833e-02\n",
      "Iteration: 14600, loss: 6.039e-02\n",
      "Iteration: 14610, loss: 4.335e-02\n",
      "Iteration: 14620, loss: 3.611e-02\n",
      "Iteration: 14630, loss: 5.496e-02\n",
      "Iteration: 14640, loss: 5.200e-02\n",
      "Iteration: 14650, loss: 5.067e-02\n",
      "Iteration: 14660, loss: 3.958e-02\n",
      "Iteration: 14670, loss: 6.480e-02\n",
      "Iteration: 14680, loss: 2.948e-02\n",
      "Iteration: 14690, loss: 4.956e-02\n",
      "Iteration: 14700, loss: 4.615e-02\n",
      "Iteration: 14710, loss: 4.075e-02\n",
      "Iteration: 14720, loss: 2.702e-02\n",
      "Iteration: 14730, loss: 4.753e-02\n",
      "Iteration: 14740, loss: 2.893e-02\n",
      "Iteration: 14750, loss: 5.164e-02\n",
      "Iteration: 14760, loss: 6.757e-02\n",
      "Iteration: 14770, loss: 2.272e-02\n",
      "Iteration: 14780, loss: 3.335e-02\n",
      "Iteration: 14790, loss: 4.205e-02\n",
      "Iteration: 14800, loss: 3.183e-02\n",
      "Iteration: 14810, loss: 4.970e-02\n",
      "Iteration: 14820, loss: 4.798e-02\n",
      "Iteration: 14830, loss: 3.477e-02\n",
      "Iteration: 14840, loss: 3.544e-02\n",
      "Iteration: 14850, loss: 3.125e-02\n",
      "Iteration: 14860, loss: 1.720e-02\n",
      "Iteration: 14870, loss: 4.334e-02\n",
      "Iteration: 14880, loss: 4.514e-02\n",
      "Iteration: 14890, loss: 3.050e-02\n",
      "Iteration: 14900, loss: 3.989e-02\n",
      "Iteration: 14910, loss: 4.171e-02\n",
      "Iteration: 14920, loss: 2.531e-02\n",
      "Iteration: 14930, loss: 3.621e-02\n",
      "Iteration: 14940, loss: 3.754e-02\n",
      "Iteration: 14950, loss: 4.621e-02\n",
      "Iteration: 14960, loss: 5.684e-02\n",
      "Iteration: 14970, loss: 3.464e-02\n",
      "Iteration: 14980, loss: 1.947e-02\n",
      "Iteration: 14990, loss: 4.834e-02\n",
      "Iteration: 15000, loss: 3.503e-02\n",
      "Iteration: 15010, loss: 3.245e-02\n",
      "Iteration: 15020, loss: 3.012e-02\n",
      "Iteration: 15030, loss: 3.277e-02\n",
      "Iteration: 15040, loss: 2.413e-02\n",
      "Iteration: 15050, loss: 3.400e-02\n",
      "Iteration: 15060, loss: 2.003e-02\n",
      "Iteration: 15070, loss: 2.301e-02\n",
      "Iteration: 15080, loss: 3.792e-02\n",
      "Iteration: 15090, loss: 4.770e-02\n",
      "Iteration: 15100, loss: 3.347e-02\n",
      "Iteration: 15110, loss: 4.241e-02\n",
      "Iteration: 15120, loss: 4.405e-02\n",
      "Iteration: 15130, loss: 5.717e-02\n",
      "Iteration: 15140, loss: 5.734e-02\n",
      "Iteration: 15150, loss: 3.758e-02\n",
      "Iteration: 15160, loss: 4.330e-02\n",
      "Iteration: 15170, loss: 4.401e-02\n",
      "Iteration: 15180, loss: 3.366e-02\n",
      "Iteration: 15190, loss: 2.695e-02\n",
      "Iteration: 15200, loss: 3.394e-02\n",
      "Iteration: 15210, loss: 3.928e-02\n",
      "Iteration: 15220, loss: 2.604e-02\n",
      "Iteration: 15230, loss: 2.597e-02\n",
      "Iteration: 15240, loss: 3.788e-02\n",
      "Iteration: 15250, loss: 3.533e-02\n",
      "Iteration: 15260, loss: 2.802e-02\n",
      "Iteration: 15270, loss: 2.902e-02\n",
      "Iteration: 15280, loss: 2.288e-02\n",
      "Iteration: 15290, loss: 4.657e-02\n",
      "Iteration: 15300, loss: 4.456e-02\n",
      "Iteration: 15310, loss: 5.034e-02\n",
      "Iteration: 15320, loss: 3.742e-02\n",
      "Iteration: 15330, loss: 3.901e-02\n",
      "Iteration: 15340, loss: 3.549e-02\n",
      "Iteration: 15350, loss: 4.947e-02\n",
      "Iteration: 15360, loss: 3.879e-02\n",
      "Iteration: 15370, loss: 4.838e-02\n",
      "Iteration: 15380, loss: 5.106e-02\n",
      "Iteration: 15390, loss: 1.962e-02\n",
      "Iteration: 15400, loss: 2.933e-02\n",
      "Iteration: 15410, loss: 2.854e-02\n",
      "Iteration: 15420, loss: 2.286e-02\n",
      "Iteration: 15430, loss: 2.539e-02\n",
      "Iteration: 15440, loss: 2.618e-02\n",
      "Iteration: 15450, loss: 2.872e-02\n",
      "Iteration: 15460, loss: 2.924e-02\n",
      "Iteration: 15470, loss: 2.610e-02\n",
      "Iteration: 15480, loss: 1.679e-02\n",
      "Iteration: 15490, loss: 2.581e-02\n",
      "Iteration: 15500, loss: 1.566e-02\n",
      "Iteration: 15510, loss: 1.927e-02\n",
      "Iteration: 15520, loss: 3.629e-02\n",
      "Iteration: 15530, loss: 2.205e-02\n",
      "Iteration: 15540, loss: 2.586e-02\n",
      "Iteration: 15550, loss: 2.746e-02\n",
      "Iteration: 15560, loss: 2.555e-02\n",
      "Iteration: 15570, loss: 2.566e-02\n",
      "Iteration: 15580, loss: 1.811e-02\n",
      "Iteration: 15590, loss: 2.429e-02\n",
      "Iteration: 15600, loss: 2.666e-02\n",
      "Iteration: 15610, loss: 1.402e-02\n",
      "Iteration: 15620, loss: 1.724e-02\n",
      "Iteration: 15630, loss: 1.800e-02\n",
      "Iteration: 15640, loss: 1.980e-02\n",
      "Iteration: 15650, loss: 4.527e-02\n",
      "Iteration: 15660, loss: 1.904e-02\n",
      "Iteration: 15670, loss: 3.223e-02\n",
      "Iteration: 15680, loss: 2.664e-02\n",
      "Iteration: 15690, loss: 3.922e-02\n",
      "Iteration: 15700, loss: 2.024e-02\n",
      "Iteration: 15710, loss: 3.058e-02\n",
      "Iteration: 15720, loss: 2.851e-02\n",
      "Iteration: 15730, loss: 3.478e-02\n",
      "Iteration: 15740, loss: 4.132e-02\n",
      "Iteration: 15750, loss: 2.956e-02\n",
      "Iteration: 15760, loss: 2.613e-02\n",
      "Iteration: 15770, loss: 2.792e-02\n",
      "Iteration: 15780, loss: 1.638e-02\n",
      "Iteration: 15790, loss: 3.101e-02\n",
      "Iteration: 15800, loss: 5.315e-02\n",
      "Iteration: 15810, loss: 2.946e-02\n",
      "Iteration: 15820, loss: 3.932e-02\n",
      "Iteration: 15830, loss: 2.486e-02\n",
      "Iteration: 15840, loss: 3.346e-02\n",
      "Iteration: 15850, loss: 2.832e-02\n",
      "Iteration: 15860, loss: 4.077e-02\n",
      "Iteration: 15870, loss: 2.802e-02\n",
      "Iteration: 15880, loss: 1.892e-02\n",
      "Iteration: 15890, loss: 2.512e-02\n",
      "Iteration: 15900, loss: 1.195e-02\n",
      "Iteration: 15910, loss: 2.950e-02\n",
      "Iteration: 15920, loss: 1.824e-02\n",
      "Iteration: 15930, loss: 2.216e-02\n",
      "Iteration: 15940, loss: 3.608e-02\n",
      "Iteration: 15950, loss: 4.055e-02\n",
      "Iteration: 15960, loss: 3.013e-02\n",
      "Iteration: 15970, loss: 2.834e-02\n",
      "Iteration: 15980, loss: 3.334e-02\n",
      "Iteration: 15990, loss: 1.944e-02\n",
      "Iteration: 16000, loss: 2.463e-02\n",
      "Iteration: 16010, loss: 1.961e-02\n",
      "Iteration: 16020, loss: 3.500e-02\n",
      "Iteration: 16030, loss: 2.544e-02\n",
      "Iteration: 16040, loss: 2.045e-02\n",
      "Iteration: 16050, loss: 3.080e-02\n",
      "Iteration: 16060, loss: 2.810e-02\n",
      "Iteration: 16070, loss: 2.539e-02\n",
      "Iteration: 16080, loss: 2.932e-02\n",
      "Iteration: 16090, loss: 1.943e-02\n",
      "Iteration: 16100, loss: 3.829e-02\n",
      "Iteration: 16110, loss: 3.688e-02\n",
      "Iteration: 16120, loss: 3.621e-02\n",
      "Iteration: 16130, loss: 1.998e-02\n",
      "Iteration: 16140, loss: 1.930e-02\n",
      "Iteration: 16150, loss: 1.667e-02\n",
      "Iteration: 16160, loss: 2.145e-02\n",
      "Iteration: 16170, loss: 2.968e-02\n",
      "Iteration: 16180, loss: 2.088e-02\n",
      "Iteration: 16190, loss: 1.851e-02\n",
      "Iteration: 16200, loss: 2.159e-02\n",
      "Iteration: 16210, loss: 1.973e-02\n",
      "Iteration: 16220, loss: 1.896e-02\n",
      "Iteration: 16230, loss: 2.234e-02\n",
      "Iteration: 16240, loss: 3.923e-02\n",
      "Iteration: 16250, loss: 2.608e-02\n",
      "Iteration: 16260, loss: 2.139e-02\n",
      "Iteration: 16270, loss: 1.757e-02\n",
      "Iteration: 16280, loss: 4.520e-02\n",
      "Iteration: 16290, loss: 2.468e-02\n",
      "Iteration: 16300, loss: 1.950e-02\n",
      "Iteration: 16310, loss: 1.790e-02\n",
      "Iteration: 16320, loss: 1.837e-02\n",
      "Iteration: 16330, loss: 2.804e-02\n",
      "Iteration: 16340, loss: 2.812e-02\n",
      "Iteration: 16350, loss: 2.240e-02\n",
      "Relative error: 6.245956e-02\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEKCAYAAADuEgmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X90XWWd7/H3t2lSSENre9IRbMkJCIr1gsxMlRHRNXNbGPAHOIwg2Eh/gLHJhYZhqVeNS9S5AZyrc80whTZCS232IHNdLi2XXli0iuhiKYS1ECzIz5uEUpY0QSolhbbJ9/5xzqmn6Umyc87Z5+fntVZWzo/dvb+nafvp8zz7eR5zd0RERMKYUewCRESkfCg0REQkNIWGiIiEptAQEZHQFBoiIhKaQkNEREJTaIiISGgKDRERCU2hISIioc0sdgH51tjY6M3NzcUuQ0SkrDz66KND7r5gquMqLjSam5vp6+srdhkiImXFzAbCHKfuKRERCU2hISIioSk0REQkNIWGiIiEptAQEZHQFBoiImUuCAKam5uZMWMGzc3NBEEQ2bUq7pZbEZFqEgQBra2tjIyMADAwMEBraysAy5cvz/v11NIQESljnZ2dhwMjZWRkhM7Ozkiup9AQESljAwOZ5+QNDg5Gcj2FhohImbrtttsmfK+pqSmSayo0RETKzNjYGLfccgtf+MIXMr5vZnR1dUVybQ2Ei4iUkeHhYW6++WYA/vSnP2U8xt0jGQQHhYaISFkYGxvjoYce4vHHH+e8887jrLPO4o477sg4phGPxyOrQ6EhIlLiHnvsMX7yk58Qi8VoaWlh3rx5AHR1dR1xuy1AfX19ZF1ToNAQESlZhw4d4he/+AW//OUvicfjrFy5EjM7/H6qC6qzs5PBwUGampro6uqKrGsKwNw9spMXw5IlS1z7aYhIuRsYGOD+++9n3rx5nHfeeRx33HGRXs/MHnX3JVMdp5aGiEgJ2bdvH9/5zncAuPTSS3nPe95zROui2BQaIiIl4oUXXuAHP/gBAGvXrmX+/PlFruhoCg0RkSJ77bXXeOCBB9izZw8tLS2ccsopxS5pQprcJyJSRD/+8Y/53ve+x549e7jiiiv4zW9+U7AVa7OhloaISBHs37+fu+++myeffJJly5ZxzjnnFHzF2myopSEiUkDuzte+9jUWLlzIZZddxqZNmw5P0Cv0irXZUEtDRKRAXnzxRa699lruvvtuDh48CCRWo021JiZamTaqFWuzoZaGiEiEgiAgHo8zY8YMTj/9dO69997DgZGSak1MtDJtVCvWZqOooWFmG83sFTP73QTvm5n9m5k9Z2aPm9lfFbpGEZFsBUHAVVddxeDgIO7O3r17j+p+ShkcHKSrq4v6+vojXo96WZDpKnZL4w7g/EnevwA4NfnVCtxagJpERHI2OjrK1VdfzZtvvhnq+KamJpYvX05PTw/xeBwzIx6P09PTUzKD4FDkMQ13f9DMmic55CLgB55Y6+TXZvY2MzvB3V8uSIEiIll45ZVX2Lp1K6+99lqo49NbE8uXLy+pkBiv1AfCFwIvpj3flXztiNAws1YSLZGS6vsTkeqyf/9+brvtNt58800+9rGP0dTUlHEQOxaL0dDQULBFBvOp1EMj04IrR62w6O49QA8kFiyMuigRkfGeeuop7rrrLgCuu+465syZww033JBx6fLu7u6yCYnxij2mMZVdwIlpzxcBu4tUi4gIkBjgTs3ajsfjfPWrX2Xbtm2MjIxwxx138La3vY3m5maAkh+jmK5Sb2lsBa42sx8CZwF7NZ4hIsU0ftb24OAgN954I5DYmzu13URqNndPTw/9/f3FKjfvirqfhpndCfwt0Aj8AbgeqAVw9/WWWA/430ncYTUCrHL3STfL0H4aIhKlxsZGhoeHQx8fj8fLIjTKYj8Nd798ivcd+G8FKkdEZFLt7e3TCgwordnc+VDqYxoiIgUVBAGNjY2YGWZGY2MjQRCwbt06br11+lPFKu2OzlIf0xARKZggCFi1atURy3wMDw+zatWqrHbPK7XZ3PmgloaIVLX0O6FWrFhx1LpQAAcPHuTAgQOhzpcKl0q4UyoTtTREpGqNvxNqdHQ0q/PMmDGDsbEx4vF4WU3Uy4ZCQ0SqVqb9K6arra2NW265JU8VlT51T4lI1cr1zqZYLFZVgQEKDRGpYvPnz8/p13d3d+epkvKh0BCRqpO6rXa6cy7SxWKxih67mIjGNESkqowf/M5GatHBaqSWhohUla985SvTCoy6ujra2toqatHBXKilISJVIQgCvvSlL7F799QLZdfU1DA6OloVt9BOl0JDRCrehg0bWLt2bagJerFYjKGhoQJUVZ4UGiJSsXp7e7nmmmtCb7sK1XlH1HRoTENEKkL6ciDNzc185jOfYcWKFaEDw8xoa2tTV9QU1NIQkbIVBAGdnZ0MDAwctQHSwMBA6PNo7CI8hYaIlKXxt85mu6FcuWySVCrUPSUiZSPVBWVmtLS05LxuVCUuXR41tTREpCwEQcDq1atDL1E+lVgsRnd3t7qkpkktDREpaanWRUtLS14CIzXgPTQ0pMDIgloaIlKy2tvbWb9+fdbjFeNpwDt3Cg0RKUlBEOQ9MDTgnTt1T4lISers7MwqMGpra6mrqzviNQ14549CQ0RKSnt7OzNnzpzWPIuUmpoaNm3axMaNG7XAYETUPSUiJaG9vZ1bb701619vZmzevPlwOCgkoqHQEJGiW7ZsGTt27Mj615sZa9asUVAUgLqnRKSo2tvbcwqMeDzOli1bqm6v7mJRS0NEiiZ1h1S2ent71booMLU0RKTg0ifsZXOHlFakLR6FhogURBAENDY2Hl43Kpu7o0DdUcVW1O4pMzsf6AZqgNvc/aZx768E/ifwUvKlf3f32wpapIjkLF/rRmmCXvEVraVhZjXAOuACYDFwuZktznDoXe5+ZvJLgSFS4tJXop05cyZmxooVK3IODE3QKw3F7J76APCcu7/g7geAHwIXFbEeEclRao+LVNfT6OjoEd+zpQl6paOYobEQeDHt+a7ka+P9o5k9bmY/MrMTC1OaiGSjo6Mj5z0u0tXX19Pb20t/f78Co0QUMzQsw2vjb6O4G2h29zOA7cDmjCcyazWzPjPr27NnT57LFJEwgiBgeHg4b+dT66I0Wb5WkJz2hc0+CHzD3f8++fwrAO5+4wTH1wCvuvvcyc67ZMkS7+vry3e5IjKFhoYG3njjjbycy8wYGxvLy7kkHDN71N2XTHVcMVsajwCnmtlJZlYHXAZsTT/AzE5Ie3oh8FQB6xORSbS3tzNjxgzMDDPLW2AANDU15e1ckl9Fu+XW3Q+Z2dXAfSRuud3o7jvN7FtAn7tvBdaa2YXAIeBVYGWx6hWRP8t1ccHJ6C6p0lbUyX3uvs3d3+Xu73T3ruRrX08GBu7+FXd/r7u/z93/zt1/X8x6RapV6jbaGTNm0NzcnNfAaGtr0zLmZURrT4nIpIIgYNWqVRw8eBAg65ncmbS1tWlmd5nRMiIiMqnVq1cfDox8icVi9Pb2KjDKkEJDRI6Q3hU1a9asnGdyjxeLxRgaGlIXVJlS95SIHJaa0Z2aoJfvwADo7u7O+zmlcNTSEJEjlirP54zu8WKxmFoYZU4tDZEqN36gOyr19fVqZVQAtTREqlgQBFxxxRWRBoZupa0sammIVKlly5bltDf3VOrr6xUUFUgtDZEqkb5znplFGhixWEyBUaEUGiIVLhUWLS0teV2FNqWhoeHw49T8C91SW7nUPSVSwaIe5F68eDE7d+6M5NxSmtTSEKlA6bfQRhUYS5cuVWBUIYWGSIVJtS7yuUZUutraWnp7e9m+fXsk55fSFrp7yswWAvH0X+PuD0ZRlIhkL4q1olJisRjd3d0ar6hioULDzL4NfBp4EkjtEO+AQkOkRARBwOc///lIlv4A6O3tVVhI6JbGJ4F3u/tbURYjItmJclMkSCxhrsAQCD+m8QJQG2UhIhLe+DkXUQVGPB7XEuZyhLAtjRHgMTPbARxubbj72kiqEpEJRd2qAM3mlomFDY2tyS8RKaIgCCIPjHg8TldXlwJDMgoVGu6+2czqgHclX3ra3aNdElNEjhB1C8PM2LJli8JCJhVqTMPM/hZ4FlgH3AI8Y2YfibAuEeHPk/SiHLeARGCsWbNGgSFTCts99V3gPHd/GsDM3gXcCfx1VIWJVKsgCOjs7Ixsct546o6S6QgbGrWpwABw92fMTHdTieRZe3s769evx90Lcr14PE5/f39BriWVIWxo9JnZ7cCW5PPlwKPRlCRSnYIgKGhg1NfX09XVVZBrSeUIO0+jDdgJrAU6SMwMXxNVUSLVJrWDXtSB0dDQoJ30JCdh7556C/jX5JeI5FEQBLS0tER+nba2Nk3Sk5xNGhpm9p/ufqmZPUFirakjuPsZkVUmUuEKMeCduitKYSH5MlVLoyP5/eNRFyJSTYIgYMWKFYyOjk59cJbUspAoTDqm4e4vJx+2u/tA+hfQHn15IpUjfc5FS0tLZIGR2nJVgSFRCHv31LnAfx/32gUZXhORDBYuXMju3bsjvYaWLpdCmLSlYWZtyfGM08zs8bSv/wc8kevFzex8M3vazJ4zsy9neH+Wmd2VfP83Ztac6zVFCikIAmprayMNjIaGBgWGFMxULY3/AP4vcCOQ/o/66+7+ai4XNrMaEsuSnAvsAh4xs63u/mTaYVcCf3T3U8zsMiC1GZRIyQqCgI6ODoaHhyO9Tm1tLZs2bVJYSEFNNaax1937gW7g1bTxjINmdlaO1/4A8Jy7v+DuB4AfAheNO+YiYHPy8Y+ApWZmOV5XJDKp+RZRB8bs2bMVGFIUYSf33QrsS3v+RvK1XCwEXkx7viv5WsZj3P0QsBeIjT+RmbWaWZ+Z9e3ZsyfHskSy19HRwdjYWKTX6O3tZd++fQoMKYqwoWGeNlXV3ccIP4g+4TkzvDZ+LkiYY3D3Hndf4u5LFixYkGNZItkJgiDyFkYsFlNYSFGF3u7VzNaaWW3yq4PEFrC52AWcmPZ8ETB+tPDwMWY2E5gL5DSWIpJvQRBw7LHHRj6ru7a2lu7u7kivITKVsKGxBjgbeInEP+RnAa05XvsR4FQzOym5wdNlHL074FZgRfLxp4CfeaFWcxMJITWG8eabb0Zy/lgsdnitKI1hSCkIu/bUKyT+Uc8bdz9kZlcD9wE1wEZ332lm3wL63H0rcDuwxcyeI9HCyGsNItko1N1RS5cuZfv27ZFeQ2S6plp76kvu/i9mdjOZxxLW5nJxd98GbBv32tfTHr8JXJLLNUTyqRCLC9bU1NDa2qoZ3VKSpmppPJX83hd1ISKlLuo9ujVBT8rBpKHh7ncnv2+e7DiRShb14oL19fXa20LKxlTdU3eToVsqxd0vzHtFIiWgUPt0x2Ixuru7FRhSNqbqnvpO8vvFwPFAb/L55UB/RDWJFFUQBKxcuZJDhw5Fep1YLMbQ0FCk1xDJt6m6p34BYGb/7O4fSXvrbjN7MNLKRIpkzZo1kQdGfX295lxIWQo7T2OBmZ2cemJmJwGaei0Vad++fVMflINYLKYxDClbYUPjn4AHzOwBM3sA+DlwbWRViRRYEAQ0NjYS5XqY8Xic3t5ehoaGFBhStsJO7rvXzE4FTku+9Ht3fyu6skQKJ6pbac2MLVu2KCCkooQKDTOrB64D4u7+OTM71cze7e7/J9ryRPKvEHdGaTa3VKqw3VObgAPAB5PPdwH/I5KKRCIUBAGtra2RBkZvb68CQypW2NB4p7v/C3AQwN33k3nZcpGS1tnZycjISCTnrqmp0axuqXhhQ+OAmR1LcqKfmb0T0JiGlIX29nZmzpyJmUXWwojH42zevFmBIRUv7EZK1wP3AieaWQB8CFgZVVEi+RL1elFtbW1aWFCqypShkdyT+/ckZoX/DYluqQ5311RWKUmFWgJEgSHVaMrQcHc3s5+4+18D9xSgJpGsBUHA6tWrOXDgQKTX0diFVKuwYxq/NrP3R1qJSI5Su+hFHRhtbW0KDKlaYUPj70gEx/Nm9riZPWFmj0dZmMh0pFoYY2NjkV0jNaNbXVJSzcIOhF8QaRUiWYp6/EJ7XYgcaar9NI4B1gCnAE8At7t7tMt/ioSUmqiX73kXxxxzDG+99RZNTU10dXUpMETSTNXS2ExiQt8vSbQ2FgMdURclMpWodtObPXt25KvcipSzqUJjsbufDmBmtwMPR1+SyJ+ldz/V1NQwOjrK7NmzeeONN/J+rdraWjZs2JD384pUkqlC42DqgbsfinLZaJHxxnc/pVoVUQTG7Nmz2bBhg7qiRKYwVWi8z8z+lHxswLHJ50ZiCsecSKuTqtbR0RHZOlEp8Xhc4xYi0zDVdq81hSpEJF0QBAwPD0d6Dc3oFpm+sLfcihRUR0d091vU1NTQ2tqqwBDJgkJDSlK+WxmabyGSH2FnhIsUTBAEeT1fPB5XYIjkiUJDiiYIApqbm5kxYwaNjY0cd9xxmBktLS15OX9qU6T+/n4FhkieqHtKimL87bT57o6KxWJ0d3crLETyrCihYWbzgbuAZqAfuNTd/5jhuFESy5cADLr7hYWqUaIV5barsViMoSFt9yIShWJ1T30Z2OHupwI7ks8z2e/uZya/FBgVZHBwMJLz1tXV0d3dHcm5RaR4oXERiXWtSH7/ZJHqkCJpamrKy3lmz559+HEsFmPjxo3qkhKJULFC4+3u/jJA8vtfTHDcMWbWZ2a/NjMFSwXp6urK+RwNDQ3s27cPd8fdGRoaUmCIRCyyMQ0z2w4cn+Gtzmmcpsndd5vZycDPzOwJd38+w7VagVbI3/9gJX9Siw4ODg5SX1/P/v37c94sqa6ujvXr1+epQhEJK7LQcPdlE71nZn8wsxPc/WUzOwF4ZYJz7E5+f8HMHgD+EjgqNNy9B+gBWLJkieehfMmTZcuWsWPHjsPPc1lssKGhgTfeeEP7XIgUUbG6p7YCK5KPVwA/HX+Amc0zs1nJx43Ah4AnC1ah5Ky9vf2IwMhWTU0NbW1tvP7664yNjWnehUgRFSs0bgLONbNngXOTzzGzJWZ2W/KY9wB9ZvZb4OfATe6u0Cgh6ZPzmpubaW9vP+J5rt1HbW1tuDuHDh3SOlEiJcLcK6s3Z8mSJd7X11fsMipeVFutpqu0P5sipczMHnX3JVMdp2VEJCtRTs6DxHpRIlJ6FBqSlagm50Fi29V83JIrIvmn0JCsRHVrcywWY9OmTRroFilRCg2ZtiAI2LdvX17OFY/H6e3t1QQ9kTKhVW5lWtrb21m/fn3Og9TxeJz+/v78FCUiBaOWhoQWBEFeAqO+vl5jFiJlSqEhoXV2dmYdGPF4HDPTLnoiZU7dUzKp1LpRAwMDWZ9DXVEilUOhIRMKgoCVK1dy6NChrM+hriiRyqLuKTls/LIgV155ZajAmD17Nm1tbdTU1BzxurqiRCqPlhERILdlQcws56XORaS4tIyITEsuy4JoDxOR6qHQECD7ZUHq6uo0ZiFSRRQaVWb8uEUQBEB2rQXtyS1SfRQaVSQ1bjEwMIC7MzAwQGtrK0EQ0NXVRX19/RHHz5o1i6VLl2Jmh19raGg4vOyHlvwQqT665bYKTDbXYmRkhM7OTvr7+3nppZe44YYb2Lt3L4sWLeKmm25SKIjIERQaFS7MXVGDg4Ns376dkZERvv3tb/O5z32OGTPUCBWRoyk0KkyqVTE4OEhTUxNDQ0NT3hXV2NjI/v37Wbt2LfPnzy9QpSJSjhQaFWR8qyLM0h81NTVcd911fPzjHz9i7EJEJBNN7qsgzc3N014jat68ebz66qsRVSQi5UKT+6pQNnMtXnvttQgqEZFKpdCoAKm5F9m0GjWbW0SmQ2MaZS6XNaNqa2s1m1tEpkUtjTKX7ZpRsViMTZs2aR6GiEyLQqNMpbqkpjPwXV9fr9ncIpITdU+VoSAIWL16NQcOHAj9a+LxOF1dXQoKEcmJQqMMdXR0TDswtN2qiOSDuqfKRPrqtMPDwxMeN36CnrZbFZF8UmiUgfGr005my5YtxONxzEzbrYpI3mlGeBkIO+Adi8UYGhoqQEUiUmlKeka4mV1iZjvNbMzMJizSzM43s6fN7Dkz+3IhaywlYWZ619bW0t3dXYBqRKSaFat76nfAxcCDEx1gZjXAOuACYDFwuZktLkx5pWWiWds1NTWHu6E050JECqEooeHuT7n701Mc9gHgOXd/wd0PAD8ELoq+utIQBAGNjY2YWcauqfr6ejZv3szY2Bj9/f0KDBEpiFIeCF8IvJj2fFfytaOYWauZ9ZlZ3549ewpSXJSCIGDVqlUT3iWlAW4RKZbI5mmY2Xbg+Axvdbr7T8OcIsNrGUft3b0H6IHEQHjoIktUZ2cnBw8ezPie5lyISDFF1tJw92Xu/l8yfIUJDEi0LE5Me74I2J3/Sgsnfa5Fc3MzQRAcdcwrr7wy6Z1S2Sx/LiKSL6U8I/wR4FQzOwl4CbgM+ExxS8pOEAR0dHQc0d00MDBAa2srAMuXL8fdeeihh7j//vuZO3cue/fuzXguLWUuIsVUrFtu/8HMdgEfBO4xs/uSr7/DzLYBuPsh4GrgPuAp4D/dfWcx6s1WajC7paUl4/jEyMgInZ2d7N27l4cffpidO3fyqU99inXr1lFbW3vU8XV1dZrdLSJFpcl9EZnOPhfXX3895557LmefffbhZUDGt05isRjd3d0a/BaRSISd3KfQiEjYWdxz587loYceYvHiqpyCIiIloqRnhFeDMAPWs2bN4uabb1ZgiEjZUGjkQaa7oqYasJ4/fz633347n/3sZwtUpYhI7hQaORq/Am3qrqiPfvSj1NfXH3X87Nmz6e3tZXh4WOMTIlJ2FBo5yrRH98jICNu2baOnp4eFCxdiZixYsIDvf//77Nu3T2EhImWrlOdplIWJxi4GBwd59tlnueqqq7jkkktYvHjxURskiYiUG7U0cjTR2MWcOXMAuOaaa3jve9+rwBCRiqDQyFFXV9dRYxezZs3i61//Ot/4xjeIxWJFqkxEJP8UGjlavnw5PT09LFiwAIB58+axYcMGrrvuuiJXJiKSfxrTyNHo6Ci1tbW0t7ezdOlSPvzhDxe7JBGRyCg0suTu7Nixg+eff56zzz6biy++mJkz9dspIpVN3VNJYZYtTxkcHOSb3/wmv/rVr/jEJz7B6aefrsAQkaqgf+k4enHB8cuWp7g7zzzzDHfeeSd1dXV88YtfzLgarYhIpdKChUy8uGD6LnlPP/0099xzD2eccQZnnnkmjY2N+ShXRKQkhF2wUC0NJp+gNzo6yne/+11GRkY4+eSTWbp0qeZciEjVUmiQmKCXqaWxaNEigiBgZGSElStX0tzcXPjiRERKiEKDxAS98Rsm1dbWcs4553Duuedy/PHHq3UhIoJCA/jzYHdnZyeDg4PMmTOHpUuXsm7dOubNm1fk6kRESodCI2n58uV85CMfIQgCTjvtND75yU8WuyQRkZKj0EizaNEirr32Wo455philyIiUpI0uS+NmSkwREQmodAQEZHQFBoiIhKaQkNEREJTaIiISGgKDRERCU2hISIioSk0REQkNIWGiIiEVnH7aZjZHuDoJWvLWyMwVOwiCqiaPq8+a+Uqt88bd/cFUx1UcaFRicysL8zmKJWimj6vPmvlqtTPq+4pEREJTaEhIiKhKTTKQ0+xCyiwavq8+qyVqyI/r8Y0REQkNLU0REQkNIVGCTKzS8xsp5mNmdmEd1+Y2flm9rSZPWdmXy5kjflkZvPN7H4zezb5PeMeu2Y2amaPJb+2FrrOXEz1szKzWWZ2V/L935hZc+GrzI8Qn3Wlme1J+1leVYw688HMNprZK2b2uwneNzP7t+TvxeNm9leFrjHfFBql6XfAxcCDEx1gZjXAOuACYDFwuZktLkx5efdlYIe7nwrsSD7PZL+7n5n8urBw5eUm5M/qSuCP7n4K8L+Abxe2yvyYxp/Lu9J+lrcVtMj8ugM4f5L3LwBOTX61ArcWoKZIKTRKkLs/5e5PT3HYB4Dn3P0Fdz8A/BC4KPrqInERsDn5eDNQaRu0h/lZpf8e/AhYamZWwBrzpZL+XE7J3R8EXp3kkIuAH3jCr4G3mdkJhakuGgqN8rUQeDHt+a7ka+Xo7e7+MkDy+19McNwxZtZnZr82s3IKljA/q8PHuPshYC8QK0h1+RX2z+U/JrtrfmRmJxamtKKopL+nAMwsdgHVysy2A8dneKvT3X8a5hQZXivZW+Em+7zTOE2Tu+82s5OBn5nZE+7+fH4qjFSYn1VZ/TwnEeZz3A3c6e5vmdkaEi2s/xp5ZcVRKT/XwxQaReLuy3I8xS4g/X9oi4DdOZ4zMpN9XjP7g5md4O4vJ5vur0xwjt3J7y+Y2QPAXwLlEBphflapY3aZ2UxgLpN3e5SqKT+ruw+nPf0+ZTp+E1JZ/T0NQ91T5esR4FQzO8nM6oDLgLK6oyjNVmBF8vEK4KiWlpnNM7NZyceNwIeAJwtWYW7C/KzSfw8+BfzMy3MS1ZSfdVyf/oXAUwWsr9C2Alck76L6G2Bvqiu2bLm7vkrsC/gHEv9DeQv4A3Bf8vV3ANvSjvso8AyJ/213FrvuHD5vjMRdU88mv89Pvr4EuC35+GzgCeC3ye9XFrvuaX7Go35WwLeAC5OPjwH+N/Ac8DBwcrFrjvCz3gjsTP4sfw6cVuyac/isdwIvAweTf2evBNYAa5LvG4m7yZ5P/rldUuyac/3SjHAREQlN3VMiIhKaQkNEREJTaIiISGgKDRERCU2hISIioSk0RLI0btXdx/K50rCZnWlmH83X+UTyRTPCRbK3393PjOjcZ5KYp7ItovOLZEUtDZE8MrO5yb0k3p18fqeZfS75+Nbkgos7zeybab/m/Wb2kJn91sweNrO5JCbDfTrZgvl0cT6NyNE0uU8kS2Y2SmKWb8qN7n6XmZ1L4h/9bmClu5+fPH6+u7+a3HNiB7AW+H3y69Pu/oiZzQFGgBYSs4evLuBHEpmSuqdEspexe8rd7zezS0gsH/G+tLcuNbNWEn/vTiCxSZEDL7v7I8lf+yeA8txKQ6qBuqdE8szMZgDvAfYD85OvnQR8AVjq7mcA95BYb8rM9BwXAAAAsUlEQVQo86WypbooNETy759IrNx6ObDRzGqBOcAbwF4zezuJbUAh0TX1DjN7P4CZHZdcGv114LiCVy4yBY1piGQpw5jGvcBGEku7f8DdXzezfwVed/frzewO4CzgBRIrGG919zuSgXEzcCyJ1skyoA64D6glOVZSoI8lMimFhoiIhKbuKRERCU2hISIioSk0REQkNIWGiIiEptAQEZHQFBoiIhKaQkNEREJTaIiISGj/H49Kdzj/gKb1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(1234)\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    def gen_matrix(N, CondNum = 10.0):\n",
    "        A = np.random.randn(N,N)\n",
    "        U,S,V = np.linalg.svd(A)\n",
    "        S[S!=0] = np.linspace(CondNum,1,N)\n",
    "        A = np.matmul(U, np.matmul(np.diag(S), V.T))\n",
    "        return A\n",
    "    cnd = [5, 15, 25, 40, 50]\n",
    "    fig444 = plt.figure(444)\n",
    "    fig444.clf()\n",
    "    ax444 = fig444.add_subplot(111)\n",
    "\n",
    "    fig555 = plt.figure(555)\n",
    "    fig555.clf()\n",
    "    ax555 = fig555.add_subplot(111)\n",
    "\n",
    "\n",
    "    for cn in cnd:\n",
    "    # Generate data\n",
    "        N = 512\n",
    "        CondNum = cn\n",
    "        A = gen_matrix(N, CondNum)\n",
    "        b = np.random.randn(N,1)\n",
    "        x_star = np.linalg.solve(A,b)\n",
    "        \n",
    "        # Define optimizer\n",
    "        optimizer = SGD(N, lr = 1e-3)\n",
    "        \n",
    "        # Define model\n",
    "        x0 = np.random.randn(N,1)\n",
    "        model = LinearSystem(A, b, x0)\n",
    "        \n",
    "        # Solve\n",
    "        x_pred = model.solve(optimizer, batch_size = 32, tol = 1e-2)\n",
    "        \n",
    "        # Print error\n",
    "        error = np.linalg.norm(x_star - x_pred, 2)/np.linalg.norm(x_star, 2)\n",
    "        print('Relative error: %e' % (error))\n",
    "        \n",
    "        # Plot\n",
    "        ax444.plot(x_star, x_star, 'k--', linewidth = 1, alpha = 0.5)\n",
    "        ax444.plot(x_star, x_pred, 'ko')\n",
    "        ax444.set_xlabel('Exact')\n",
    "        ax444.set_ylabel('Prediction')\n",
    "        loss = np.asarray(model.loss_log)\n",
    "        l = loss.mean(1)\n",
    "        ax555.plot(l)\n",
    "        ax555.set_xlabel('Iteration')\n",
    "        ax555.set_ylabel('Loss')\n",
    "        ax555.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method does not converge for large values of the condition number because the matrix is coming closer to being singular. Also, the larger the condition number becomes the larger the iteration number needed for the problem to converge becomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import dia_matrix\n",
    "from numpy.linalg import inv       \n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "class IRLS:\n",
    "    # Initialize model class\n",
    "    def __init__(self, X_t, y_t, X_v, y_v):\n",
    "      \n",
    "        self.X = X_t\n",
    "        self.y = y_t\n",
    "        \n",
    "        self.X_v = X_v\n",
    "        self.y_v = y_v\n",
    "        \n",
    "        self.n = X_t.shape[0]\n",
    "\n",
    "        #store the loss and # of iterations\n",
    "        self.loss_log_t = []\n",
    "        self.loss_log_v = []\n",
    "        self.iterations = []\n",
    "        \n",
    "    # Fetches a mini-batch of data\n",
    "    def fetch_minibatch(self, X, Y, N_batch):\n",
    "        idx = np.random.choice(self.n, N_batch, replace=False)\n",
    "        X_batch = X[idx,:]\n",
    "        Y_batch = Y[idx,:]\n",
    "        return X_batch, Y_batch\n",
    "      \n",
    "    def invlogit(self,eta):\n",
    "        return(1.0/(1.0+np.exp(-eta)))\n",
    "        \n",
    "    def compute_loss(self, y_batch, a):\n",
    "        loss = y_batch*np.log(a+ 1e-7) + (1. - y_batch)*np.log(1. - a + 1e-7)\n",
    "        return -(1./y_batch.shape[0])*np.sum(loss)\n",
    "    \n",
    "    def IRLS_train(self, num_steps = 500, batch_size = 64):\n",
    "    \n",
    "        # Initialization\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "\n",
    "        n,p = X.shape\n",
    "        W    = dia_matrix((batch_size,batch_size))        \n",
    "        X_batch, y_batch =  self.fetch_minibatch(X, y, batch_size)\n",
    "        \n",
    "        d1 = inv(np.matmul(X_batch.T, X_batch))\n",
    "        d2 = np.matmul(X_batch.T,y_batch)\n",
    "        \n",
    "        beta = np.matmul(d1,d2)  \n",
    "        \n",
    "        for it in range(0, num_steps):\n",
    "            X_batch, y_batch =  self.fetch_minibatch(X, y, batch_size)\n",
    "\n",
    "            ai = self.invlogit(np.matmul(X_batch, beta))\n",
    "            \n",
    "            l = np.multiply(ai,(1. - ai))\n",
    "            W.setdiag(l.ravel())\n",
    "            d1 = np.matmul(X_batch.T, W.toarray())\n",
    "            d2 = np.matmul(d1,X_batch) + 1e-7*np.eye(p)\n",
    "            d2 = inv(d2) \n",
    "            d3 = np.matmul(X_batch.T,(ai-y_batch))\n",
    "            d4 = np.matmul(d2,d3)\n",
    "            \n",
    "            beta_star = beta - d4\n",
    "                        \n",
    "            loss_t = self.compute_loss(y_batch,ai)\n",
    "            \n",
    "            ai_v   = self.predict(self.X_v,beta)\n",
    "            loss_v = self.compute_loss(self.y_v,ai_v)\n",
    "            \n",
    "            beta = beta_star\n",
    "            \n",
    "            if it % 1 == 0:\n",
    "                self.loss_log_t.append(loss_t)\n",
    "                self.loss_log_v.append(loss_v)\n",
    "                self.iterations.append(it)\n",
    "                print(\"Iteration: %d, Training Loss: %.3e , Validation Loss: %.3e\" \\\n",
    "                      % (it, loss_t, loss_v))\n",
    "\n",
    "            # Stochastic gradient descent update\n",
    "        self.beta = beta\n",
    "        #Store the loss function\n",
    "        self.iterations = np.asarray(self.iterations)\n",
    "        self.loss_log_t = np.asarray(self.loss_log_t)\n",
    "        self.loss_log_t = np.reshape(self.loss_log_t, (self.iterations.shape[0], 1))\n",
    "        self.loss_log_v = np.asarray(self.loss_log_v)\n",
    "        self.loss_log_v = np.reshape(self.loss_log_v, (self.iterations.shape[0], 1))\n",
    "\n",
    "        return beta\n",
    "\n",
    "          \n",
    "    def predict(self, X_star, beta):\n",
    "        w = np.reshape(beta,(self.X.shape[1], 1))\n",
    "        y_pred = self.sigmoid(np.matmul(X_star, w))\n",
    "        return np.around(y_pred, decimals = 0) \n",
    "  \n",
    "    # Logistic sigmoid function\n",
    "    def sigmoid(self, x):\n",
    "        return 0.5*(np.tanh(x) + 1.0)\n",
    "    \n",
    "    def accuracy(self, y_test, test_labels):\n",
    "        # Compute confusion matrix\n",
    "        self.M = confusion_matrix(y_test, test_labels)\n",
    "        \n",
    "        # False positives, False negatives, True positives, True negatives\n",
    "        FP = self.M.sum(axis=0) - np.diag(self.M)  \n",
    "        FN = self.M.sum(axis=1) - np.diag(self.M)\n",
    "        TP = np.diag(self.M)\n",
    "        TN = self.M.sum() - (FP + FN + TP)\n",
    "                \n",
    "        # Overall accuracy\n",
    "        ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        return ACC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Training Loss: 6.359e-01 , Validation Loss: 4.373e+00\n",
      "Iteration: 1, Training Loss: 1.473e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2, Training Loss: 1.219e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 3, Training Loss: 3.595e-02 , Validation Loss: 3.447e-01\n",
      "Iteration: 4, Training Loss: 2.157e-02 , Validation Loss: 3.447e-01\n",
      "Iteration: 5, Training Loss: 1.370e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6, Training Loss: 1.125e+00 , Validation Loss: 5.879e+00\n",
      "Iteration: 7, Training Loss: 7.356e-01 , Validation Loss: 3.103e+00\n",
      "Iteration: 8, Training Loss: 8.171e+00 , Validation Loss: 1.038e+01\n",
      "Iteration: 9, Training Loss: 5.541e+00 , Validation Loss: 5.262e+00\n",
      "Iteration: 10, Training Loss: 2.770e+00 , Validation Loss: 5.879e+00\n",
      "Iteration: 11, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 12, Training Loss: 3.274e+00 , Validation Loss: 5.879e+00\n",
      "Iteration: 13, Training Loss: 5.037e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 14, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 15, Training Loss: 3.022e+00 , Validation Loss: 5.855e+00\n",
      "Iteration: 16, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 17, Training Loss: 1.763e+00 , Validation Loss: 1.125e+00\n",
      "Iteration: 18, Training Loss: 1.007e+00 , Validation Loss: 4.597e-01\n",
      "Iteration: 19, Training Loss: 2.267e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 20, Training Loss: 2.770e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 21, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 22, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 24, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 25, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 26, Training Loss: 1.259e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 27, Training Loss: 2.770e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 28, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 29, Training Loss: 3.778e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 30, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 31, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 32, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 33, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 34, Training Loss: 2.015e+00 , Validation Loss: 1.597e+00\n",
      "Iteration: 35, Training Loss: 2.267e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 36, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 37, Training Loss: 5.037e-01 , Validation Loss: 1.198e+00\n",
      "Iteration: 38, Training Loss: 2.015e+00 , Validation Loss: 1.706e+00\n",
      "Iteration: 39, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 40, Training Loss: 2.518e+00 , Validation Loss: 2.504e+00\n",
      "Iteration: 41, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 42, Training Loss: 2.518e+00 , Validation Loss: 3.677e+00\n",
      "Iteration: 43, Training Loss: 2.267e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 44, Training Loss: 1.511e+00 , Validation Loss: 2.286e+00\n",
      "Iteration: 45, Training Loss: 1.410e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 46, Training Loss: 4.030e+00 , Validation Loss: 5.879e+00\n",
      "Iteration: 47, Training Loss: 1.259e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 48, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 49, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 50, Training Loss: 1.007e+00 , Validation Loss: 1.294e+00\n",
      "Iteration: 51, Training Loss: 1.763e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 52, Training Loss: 1.763e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 53, Training Loss: 1.385e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 54, Training Loss: 2.518e+00 , Validation Loss: 5.873e+00\n",
      "Iteration: 55, Training Loss: 2.770e+00 , Validation Loss: 5.661e+00\n",
      "Iteration: 56, Training Loss: 2.770e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 57, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 58, Training Loss: 3.022e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 59, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 60, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 61, Training Loss: 2.015e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 62, Training Loss: 2.015e+00 , Validation Loss: 2.504e+00\n",
      "Iteration: 63, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 64, Training Loss: 1.511e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 65, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 66, Training Loss: 2.015e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 67, Training Loss: 1.763e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 68, Training Loss: 2.015e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 69, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 70, Training Loss: 2.267e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 71, Training Loss: 1.511e+00 , Validation Loss: 2.117e+00\n",
      "Iteration: 72, Training Loss: 2.770e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 73, Training Loss: 2.770e+00 , Validation Loss: 3.901e+00\n",
      "Iteration: 74, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 75, Training Loss: 2.770e+00 , Validation Loss: 5.879e+00\n",
      "Iteration: 76, Training Loss: 3.526e+00 , Validation Loss: 5.764e+00\n",
      "Iteration: 77, Training Loss: 2.770e+00 , Validation Loss: 1.433e+00\n",
      "Iteration: 78, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 79, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 80, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 81, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 82, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 83, Training Loss: 1.511e+00 , Validation Loss: 1.597e+00\n",
      "Iteration: 84, Training Loss: 2.518e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 85, Training Loss: 5.037e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 86, Training Loss: 2.015e+00 , Validation Loss: 1.827e+00\n",
      "Iteration: 87, Training Loss: 2.518e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 88, Training Loss: 2.518e-01 , Validation Loss: 1.282e+00\n",
      "Iteration: 89, Training Loss: 1.511e+00 , Validation Loss: 1.542e+00\n",
      "Iteration: 90, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 91, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 92, Training Loss: 1.511e+00 , Validation Loss: 1.669e+00\n",
      "Iteration: 93, Training Loss: 2.518e+00 , Validation Loss: 2.395e+00\n",
      "Iteration: 94, Training Loss: 2.267e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 95, Training Loss: 2.770e+00 , Validation Loss: 2.341e+00\n",
      "Iteration: 96, Training Loss: 3.022e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 97, Training Loss: 3.274e+00 , Validation Loss: 3.998e+00\n",
      "Iteration: 98, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 99, Training Loss: 4.281e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 100, Training Loss: 1.763e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 101, Training Loss: 2.015e+00 , Validation Loss: 1.421e+00\n",
      "Iteration: 102, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 103, Training Loss: 3.526e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 104, Training Loss: 1.763e+00 , Validation Loss: 1.397e+00\n",
      "Iteration: 105, Training Loss: 2.015e+00 , Validation Loss: 2.135e+00\n",
      "Iteration: 106, Training Loss: 2.267e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 107, Training Loss: 2.015e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 108, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 109, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 110, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 111, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 112, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 113, Training Loss: 2.267e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 114, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 115, Training Loss: 2.015e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 116, Training Loss: 1.511e+00 , Validation Loss: 1.778e+00\n",
      "Iteration: 117, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 118, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 119, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 120, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 121, Training Loss: 2.267e+00 , Validation Loss: 3.042e+00\n",
      "Iteration: 122, Training Loss: 1.158e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 123, Training Loss: 2.770e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 124, Training Loss: 1.007e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 125, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 126, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 127, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 128, Training Loss: 1.259e+00 , Validation Loss: 1.288e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gkissas/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:66: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 129, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 130, Training Loss: 2.015e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 131, Training Loss: 2.015e+00 , Validation Loss: 1.282e+00\n",
      "Iteration: 132, Training Loss: 1.511e+00 , Validation Loss: 1.294e+00\n",
      "Iteration: 133, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 134, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 135, Training Loss: 2.015e+00 , Validation Loss: 1.433e+00\n",
      "Iteration: 136, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 137, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 138, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 139, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 140, Training Loss: 1.763e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 141, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 142, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 143, Training Loss: 1.511e+00 , Validation Loss: 1.240e+00\n",
      "Iteration: 144, Training Loss: 1.511e+00 , Validation Loss: 1.996e+00\n",
      "Iteration: 145, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 146, Training Loss: 3.022e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 147, Training Loss: 1.007e+00 , Validation Loss: 1.355e+00\n",
      "Iteration: 148, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 149, Training Loss: 1.259e+00 , Validation Loss: 1.603e+00\n",
      "Iteration: 150, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 151, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 152, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 153, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 154, Training Loss: 1.511e+00 , Validation Loss: 1.996e+00\n",
      "Iteration: 155, Training Loss: 1.259e+00 , Validation Loss: 1.754e+00\n",
      "Iteration: 156, Training Loss: 2.518e+00 , Validation Loss: 3.151e+00\n",
      "Iteration: 157, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 158, Training Loss: 2.518e+00 , Validation Loss: 3.103e+00\n",
      "Iteration: 159, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 160, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 161, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 162, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 163, Training Loss: 3.778e+00 , Validation Loss: 5.873e+00\n",
      "Iteration: 164, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 165, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 166, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 167, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 168, Training Loss: 2.015e+00 , Validation Loss: 1.415e+00\n",
      "Iteration: 169, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 170, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 171, Training Loss: 1.511e+00 , Validation Loss: 1.083e+00\n",
      "Iteration: 172, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 173, Training Loss: 1.007e+00 , Validation Loss: 1.367e+00\n",
      "Iteration: 174, Training Loss: 3.274e+00 , Validation Loss: 3.756e+00\n",
      "Iteration: 175, Training Loss: 1.410e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 176, Training Loss: 4.785e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 177, Training Loss: 1.259e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 178, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 179, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 180, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 181, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 182, Training Loss: 2.015e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 183, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 184, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 185, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 186, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 187, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 188, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 189, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 190, Training Loss: 1.259e+00 , Validation Loss: 4.476e-01\n",
      "Iteration: 191, Training Loss: 7.555e-01 , Validation Loss: 1.058e+00\n",
      "Iteration: 192, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 193, Training Loss: 1.763e+00 , Validation Loss: 1.325e+00\n",
      "Iteration: 194, Training Loss: 2.518e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 195, Training Loss: 1.007e+00 , Validation Loss: 1.887e+00\n",
      "Iteration: 196, Training Loss: 3.526e+00 , Validation Loss: 3.103e+00\n",
      "Iteration: 197, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 198, Training Loss: 2.015e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 199, Training Loss: 2.015e+00 , Validation Loss: 3.593e+00\n",
      "Iteration: 200, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 201, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 202, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 203, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 204, Training Loss: 1.511e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 205, Training Loss: 1.259e+00 , Validation Loss: 1.282e+00\n",
      "Iteration: 206, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 207, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 208, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 209, Training Loss: 3.274e+00 , Validation Loss: 3.163e+00\n",
      "Iteration: 210, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 211, Training Loss: 4.533e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 212, Training Loss: 2.518e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 213, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 214, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 215, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 216, Training Loss: 1.763e+00 , Validation Loss: 1.058e+00\n",
      "Iteration: 217, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 218, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 219, Training Loss: 2.518e+00 , Validation Loss: 1.470e+00\n",
      "Iteration: 220, Training Loss: 9.066e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 221, Training Loss: 2.770e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 222, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 223, Training Loss: 7.555e-01 , Validation Loss: 1.355e+00\n",
      "Iteration: 224, Training Loss: 2.518e+00 , Validation Loss: 2.510e+00\n",
      "Iteration: 225, Training Loss: 2.518e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 226, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 227, Training Loss: 2.015e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 228, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 229, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 230, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 231, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 232, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 233, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 234, Training Loss: 2.015e+00 , Validation Loss: 1.742e+00\n",
      "Iteration: 235, Training Loss: 2.518e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 236, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 237, Training Loss: 2.267e+00 , Validation Loss: 2.425e+00\n",
      "Iteration: 238, Training Loss: 2.267e+00 , Validation Loss: 1.657e+00\n",
      "Iteration: 239, Training Loss: 1.511e+00 , Validation Loss: 1.004e+00\n",
      "Iteration: 240, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 241, Training Loss: 4.785e+00 , Validation Loss: 5.873e+00\n",
      "Iteration: 242, Training Loss: 1.763e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 243, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 244, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 245, Training Loss: 1.511e+00 , Validation Loss: 1.506e+00\n",
      "Iteration: 246, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 247, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 248, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 249, Training Loss: 1.511e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 250, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 251, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 252, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 253, Training Loss: 2.518e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 254, Training Loss: 3.274e+00 , Validation Loss: 3.762e+00\n",
      "Iteration: 255, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 256, Training Loss: 2.518e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 257, Training Loss: 2.267e+00 , Validation Loss: 3.030e+00\n",
      "Iteration: 258, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 259, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 260, Training Loss: 1.763e+00 , Validation Loss: 1.433e+00\n",
      "Iteration: 261, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 262, Training Loss: 2.518e+00 , Validation Loss: 2.050e+00\n",
      "Iteration: 263, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 264, Training Loss: 4.281e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 265, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 266, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 267, Training Loss: 1.259e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 268, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 269, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 270, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 271, Training Loss: 1.763e+00 , Validation Loss: 1.161e+00\n",
      "Iteration: 272, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 273, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 274, Training Loss: 2.518e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 275, Training Loss: 2.267e+00 , Validation Loss: 3.103e+00\n",
      "Iteration: 276, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 277, Training Loss: 1.259e+00 , Validation Loss: 1.700e+00\n",
      "Iteration: 278, Training Loss: 2.770e+00 , Validation Loss: 2.945e+00\n",
      "Iteration: 279, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 280, Training Loss: 4.030e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 281, Training Loss: 2.267e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 282, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 283, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 284, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 285, Training Loss: 2.015e+00 , Validation Loss: 1.028e+00\n",
      "Iteration: 286, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 287, Training Loss: 1.763e+00 , Validation Loss: 1.530e+00\n",
      "Iteration: 288, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 289, Training Loss: 1.511e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 290, Training Loss: 1.007e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 291, Training Loss: 1.511e+00 , Validation Loss: 5.080e-01\n",
      "Iteration: 292, Training Loss: 2.267e+00 , Validation Loss: 2.830e+00\n",
      "Iteration: 293, Training Loss: 5.541e+00 , Validation Loss: 2.329e+00\n",
      "Iteration: 294, Training Loss: 2.770e+00 , Validation Loss: 3.877e+00\n",
      "Iteration: 295, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 296, Training Loss: 1.259e+00 , Validation Loss: 1.337e+00\n",
      "Iteration: 297, Training Loss: 2.518e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 298, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 299, Training Loss: 2.267e+00 , Validation Loss: 2.074e+00\n",
      "Iteration: 300, Training Loss: 1.259e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 301, Training Loss: 1.763e+00 , Validation Loss: 2.933e+00\n",
      "Iteration: 302, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 303, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 304, Training Loss: 1.763e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 305, Training Loss: 2.015e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 306, Training Loss: 1.511e+00 , Validation Loss: 1.500e+00\n",
      "Iteration: 307, Training Loss: 5.037e-01 , Validation Loss: 1.409e+00\n",
      "Iteration: 308, Training Loss: 2.015e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 309, Training Loss: 2.267e+00 , Validation Loss: 9.737e-01\n",
      "Iteration: 310, Training Loss: 2.015e+00 , Validation Loss: 3.115e+00\n",
      "Iteration: 311, Training Loss: 7.555e+00 , Validation Loss: 1.023e+01\n",
      "Iteration: 312, Training Loss: 3.526e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 313, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 314, Training Loss: 2.267e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 315, Training Loss: 4.533e+00 , Validation Loss: 5.697e+00\n",
      "Iteration: 316, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 317, Training Loss: 3.526e+00 , Validation Loss: 5.371e+00\n",
      "Iteration: 318, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 319, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 320, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 321, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 322, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 323, Training Loss: 1.511e+00 , Validation Loss: 1.125e+00\n",
      "Iteration: 324, Training Loss: 2.518e+00 , Validation Loss: 3.097e+00\n",
      "Iteration: 325, Training Loss: 2.267e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 326, Training Loss: 2.267e+00 , Validation Loss: 1.482e+00\n",
      "Iteration: 327, Training Loss: 2.518e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 328, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 329, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 330, Training Loss: 1.511e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 331, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 332, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 333, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 334, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 335, Training Loss: 2.015e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 336, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 337, Training Loss: 2.015e+00 , Validation Loss: 2.087e+00\n",
      "Iteration: 338, Training Loss: 2.015e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 339, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 340, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 341, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 342, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 343, Training Loss: 1.511e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 344, Training Loss: 2.267e+00 , Validation Loss: 4.524e+00\n",
      "Iteration: 345, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 346, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 347, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 348, Training Loss: 1.259e+00 , Validation Loss: 1.633e+00\n",
      "Iteration: 349, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 350, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 351, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 352, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 353, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 354, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 355, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 356, Training Loss: 2.267e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 357, Training Loss: 1.763e+00 , Validation Loss: 3.369e+00\n",
      "Iteration: 358, Training Loss: 2.015e+00 , Validation Loss: 1.155e+00\n",
      "Iteration: 359, Training Loss: 1.511e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 360, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 361, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 362, Training Loss: 2.518e+00 , Validation Loss: 3.224e+00\n",
      "Iteration: 363, Training Loss: 3.022e+00 , Validation Loss: 2.558e+00\n",
      "Iteration: 364, Training Loss: 2.770e+00 , Validation Loss: 4.131e+00\n",
      "Iteration: 365, Training Loss: 1.385e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 366, Training Loss: 4.533e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 367, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 368, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 369, Training Loss: 7.555e-01 , Validation Loss: 1.657e+00\n",
      "Iteration: 370, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 371, Training Loss: 1.511e+00 , Validation Loss: 1.252e+00\n",
      "Iteration: 372, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 373, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 374, Training Loss: 1.511e+00 , Validation Loss: 1.494e+00\n",
      "Iteration: 375, Training Loss: 2.518e+00 , Validation Loss: 2.843e+00\n",
      "Iteration: 376, Training Loss: 2.015e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 377, Training Loss: 7.555e-01 , Validation Loss: 1.966e+00\n",
      "Iteration: 378, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 379, Training Loss: 1.763e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 380, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 381, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 382, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 383, Training Loss: 3.274e+00 , Validation Loss: 4.693e+00\n",
      "Iteration: 384, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 385, Training Loss: 3.274e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 386, Training Loss: 1.763e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 387, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 388, Training Loss: 1.511e+00 , Validation Loss: 1.663e+00\n",
      "Iteration: 389, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 390, Training Loss: 1.259e+00 , Validation Loss: 4.415e-01\n",
      "Iteration: 391, Training Loss: 1.511e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 392, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 393, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 394, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 395, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 396, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 397, Training Loss: 1.763e+00 , Validation Loss: 2.359e+00\n",
      "Iteration: 398, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 399, Training Loss: 1.259e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 400, Training Loss: 1.511e+00 , Validation Loss: 1.234e+00\n",
      "Iteration: 401, Training Loss: 2.518e+00 , Validation Loss: 3.508e+00\n",
      "Iteration: 402, Training Loss: 1.259e+00 , Validation Loss: 1.040e+00\n",
      "Iteration: 403, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 404, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 405, Training Loss: 2.015e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 406, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 407, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 408, Training Loss: 1.511e+00 , Validation Loss: 1.778e+00\n",
      "Iteration: 409, Training Loss: 1.058e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 410, Training Loss: 2.267e+00 , Validation Loss: 5.879e+00\n",
      "Iteration: 411, Training Loss: 4.533e+00 , Validation Loss: 5.842e+00\n",
      "Iteration: 412, Training Loss: 1.184e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 413, Training Loss: 4.030e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 414, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 415, Training Loss: 3.274e+00 , Validation Loss: 5.855e+00\n",
      "Iteration: 416, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 417, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 418, Training Loss: 2.770e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 419, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 420, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 421, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 422, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 423, Training Loss: 7.555e-01 , Validation Loss: 1.143e+00\n",
      "Iteration: 424, Training Loss: 1.007e+00 , Validation Loss: 1.077e+00\n",
      "Iteration: 425, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 426, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 427, Training Loss: 2.267e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 428, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 429, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 430, Training Loss: 7.555e-01 , Validation Loss: 1.748e+00\n",
      "Iteration: 431, Training Loss: 1.511e+00 , Validation Loss: 1.282e+00\n",
      "Iteration: 432, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 433, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 434, Training Loss: 1.259e+00 , Validation Loss: 1.772e+00\n",
      "Iteration: 435, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 436, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 437, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 438, Training Loss: 7.555e-01 , Validation Loss: 1.361e+00\n",
      "Iteration: 439, Training Loss: 1.763e+00 , Validation Loss: 3.012e+00\n",
      "Iteration: 440, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 441, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 442, Training Loss: 1.511e+00 , Validation Loss: 2.673e+00\n",
      "Iteration: 443, Training Loss: 2.015e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 444, Training Loss: 4.281e+00 , Validation Loss: 4.778e+00\n",
      "Iteration: 445, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 446, Training Loss: 3.274e+00 , Validation Loss: 5.758e+00\n",
      "Iteration: 447, Training Loss: 3.274e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 448, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 449, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 450, Training Loss: 1.763e+00 , Validation Loss: 2.697e+00\n",
      "Iteration: 451, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 452, Training Loss: 1.763e+00 , Validation Loss: 1.542e+00\n",
      "Iteration: 453, Training Loss: 1.259e+00 , Validation Loss: 2.758e+00\n",
      "Iteration: 454, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 455, Training Loss: 2.015e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 456, Training Loss: 1.763e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 457, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 458, Training Loss: 2.518e+00 , Validation Loss: 1.875e+00\n",
      "Iteration: 459, Training Loss: 1.133e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 460, Training Loss: 3.526e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 461, Training Loss: 1.259e+00 , Validation Loss: 1.427e+00\n",
      "Iteration: 462, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 463, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 464, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 465, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 466, Training Loss: 3.022e+00 , Validation Loss: 3.115e+00\n",
      "Iteration: 467, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 468, Training Loss: 5.037e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 469, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 470, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 471, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 472, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 473, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 474, Training Loss: 1.511e+00 , Validation Loss: 1.349e+00\n",
      "Iteration: 475, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 476, Training Loss: 1.007e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 477, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 478, Training Loss: 2.267e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 479, Training Loss: 2.770e+00 , Validation Loss: 3.623e+00\n",
      "Iteration: 480, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 481, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 482, Training Loss: 2.518e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 483, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 484, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 485, Training Loss: 2.015e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 486, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 487, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 488, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 489, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 490, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 491, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 492, Training Loss: 2.518e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 493, Training Loss: 2.518e+00 , Validation Loss: 1.198e+00\n",
      "Iteration: 494, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 495, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 496, Training Loss: 3.778e+00 , Validation Loss: 4.476e+00\n",
      "Iteration: 497, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 498, Training Loss: 3.022e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 499, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 500, Training Loss: 5.037e-01 , Validation Loss: 1.282e+00\n",
      "Iteration: 501, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 502, Training Loss: 1.511e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 503, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 504, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 505, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 506, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 507, Training Loss: 5.037e-01 , Validation Loss: 1.119e+00\n",
      "Iteration: 508, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 509, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 510, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 511, Training Loss: 1.259e+00 , Validation Loss: 2.256e+00\n",
      "Iteration: 512, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 513, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 514, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 515, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 516, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 517, Training Loss: 1.007e+00 , Validation Loss: 2.897e+00\n",
      "Iteration: 518, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 519, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 520, Training Loss: 2.267e+00 , Validation Loss: 2.087e+00\n",
      "Iteration: 521, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 522, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 523, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 524, Training Loss: 1.259e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 525, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 526, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 527, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 528, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 529, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 530, Training Loss: 1.763e+00 , Validation Loss: 1.530e+00\n",
      "Iteration: 531, Training Loss: 2.518e+00 , Validation Loss: 1.833e+00\n",
      "Iteration: 532, Training Loss: 3.778e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 533, Training Loss: 1.385e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 534, Training Loss: 4.281e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 535, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 536, Training Loss: 1.511e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 537, Training Loss: 1.007e+00 , Validation Loss: 1.264e+00\n",
      "Iteration: 538, Training Loss: 1.007e+00 , Validation Loss: 1.663e+00\n",
      "Iteration: 539, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 540, Training Loss: 1.763e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 541, Training Loss: 1.007e+00 , Validation Loss: 1.300e+00\n",
      "Iteration: 542, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 543, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 544, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 545, Training Loss: 2.015e+00 , Validation Loss: 2.129e+00\n",
      "Iteration: 546, Training Loss: 3.526e+00 , Validation Loss: 3.992e+00\n",
      "Iteration: 547, Training Loss: 2.015e+00 , Validation Loss: 2.147e+00\n",
      "Iteration: 548, Training Loss: 2.015e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 549, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 550, Training Loss: 1.511e+00 , Validation Loss: 2.068e+00\n",
      "Iteration: 551, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 552, Training Loss: 2.267e+00 , Validation Loss: 2.165e+00\n",
      "Iteration: 553, Training Loss: 2.015e+00 , Validation Loss: 2.897e+00\n",
      "Iteration: 554, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 555, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 556, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 557, Training Loss: 5.037e-01 , Validation Loss: 1.331e+00\n",
      "Iteration: 558, Training Loss: 2.267e+00 , Validation Loss: 3.762e+00\n",
      "Iteration: 559, Training Loss: 1.158e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 560, Training Loss: 4.030e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 561, Training Loss: 3.022e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 562, Training Loss: 2.518e+00 , Validation Loss: 3.205e+00\n",
      "Iteration: 563, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 564, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 565, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 566, Training Loss: 1.511e+00 , Validation Loss: 1.343e+00\n",
      "Iteration: 567, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 568, Training Loss: 2.267e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 569, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 570, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 571, Training Loss: 1.511e+00 , Validation Loss: 8.044e-01\n",
      "Iteration: 572, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 573, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 574, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 575, Training Loss: 7.555e-01 , Validation Loss: 1.228e+00\n",
      "Iteration: 576, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 577, Training Loss: 1.259e+00 , Validation Loss: 1.294e+00\n",
      "Iteration: 578, Training Loss: 3.778e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 579, Training Loss: 3.022e+00 , Validation Loss: 5.842e+00\n",
      "Iteration: 580, Training Loss: 1.410e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 581, Training Loss: 3.526e+00 , Validation Loss: 5.873e+00\n",
      "Iteration: 582, Training Loss: 3.022e+00 , Validation Loss: 2.093e+00\n",
      "Iteration: 583, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 584, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 585, Training Loss: 2.015e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 586, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 587, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 588, Training Loss: 1.511e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 589, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 590, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 591, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 592, Training Loss: 1.763e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 593, Training Loss: 2.015e+00 , Validation Loss: 1.827e+00\n",
      "Iteration: 594, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 595, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 596, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 597, Training Loss: 2.267e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 598, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 599, Training Loss: 1.007e+00 , Validation Loss: 1.645e+00\n",
      "Iteration: 600, Training Loss: 4.533e+00 , Validation Loss: 5.050e+00\n",
      "Iteration: 601, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 602, Training Loss: 2.518e+00 , Validation Loss: 4.548e+00\n",
      "Iteration: 603, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 604, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 605, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 606, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 607, Training Loss: 2.015e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 608, Training Loss: 1.360e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 609, Training Loss: 2.770e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 610, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 611, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 612, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 613, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 614, Training Loss: 1.007e+00 , Validation Loss: 1.693e+00\n",
      "Iteration: 615, Training Loss: 1.007e+00 , Validation Loss: 1.760e+00\n",
      "Iteration: 616, Training Loss: 1.511e+00 , Validation Loss: 2.691e+00\n",
      "Iteration: 617, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 618, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 619, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 620, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 621, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 622, Training Loss: 1.007e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 623, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 624, Training Loss: 5.037e-01 , Validation Loss: 1.294e+00\n",
      "Iteration: 625, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 626, Training Loss: 7.555e-01 , Validation Loss: 1.077e+00\n",
      "Iteration: 627, Training Loss: 3.022e+00 , Validation Loss: 3.587e+00\n",
      "Iteration: 628, Training Loss: 2.015e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 629, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 630, Training Loss: 2.518e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 631, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 632, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 633, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 634, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 635, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 636, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 637, Training Loss: 2.015e+00 , Validation Loss: 3.387e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 638, Training Loss: 3.274e+00 , Validation Loss: 3.810e+00\n",
      "Iteration: 639, Training Loss: 3.526e+00 , Validation Loss: 3.720e+00\n",
      "Iteration: 640, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 641, Training Loss: 2.770e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 642, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 643, Training Loss: 1.763e+00 , Validation Loss: 3.859e+00\n",
      "Iteration: 644, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 645, Training Loss: 1.511e+00 , Validation Loss: 1.222e+00\n",
      "Iteration: 646, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 647, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 648, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 649, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 650, Training Loss: 1.511e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 651, Training Loss: 1.007e+00 , Validation Loss: 1.264e+00\n",
      "Iteration: 652, Training Loss: 2.518e-01 , Validation Loss: 3.266e-01\n",
      "Iteration: 653, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 654, Training Loss: 1.007e+00 , Validation Loss: 1.325e+00\n",
      "Iteration: 655, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 656, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 657, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 658, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 659, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 660, Training Loss: 2.267e+00 , Validation Loss: 2.855e+00\n",
      "Iteration: 661, Training Loss: 1.360e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 662, Training Loss: 2.518e+00 , Validation Loss: 5.873e+00\n",
      "Iteration: 663, Training Loss: 3.274e+00 , Validation Loss: 5.449e+00\n",
      "Iteration: 664, Training Loss: 1.763e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 665, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 666, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 667, Training Loss: 2.015e+00 , Validation Loss: 1.367e+00\n",
      "Iteration: 668, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 669, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 670, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 671, Training Loss: 1.259e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 672, Training Loss: 1.259e+00 , Validation Loss: 1.246e+00\n",
      "Iteration: 673, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 674, Training Loss: 7.555e-01 , Validation Loss: 1.294e+00\n",
      "Iteration: 675, Training Loss: 1.511e+00 , Validation Loss: 1.228e+00\n",
      "Iteration: 676, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 677, Training Loss: 2.267e+00 , Validation Loss: 1.990e+00\n",
      "Iteration: 678, Training Loss: 4.030e+00 , Validation Loss: 3.284e+00\n",
      "Iteration: 679, Training Loss: 4.030e+00 , Validation Loss: 5.855e+00\n",
      "Iteration: 680, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 681, Training Loss: 3.022e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 682, Training Loss: 1.511e+00 , Validation Loss: 1.760e+00\n",
      "Iteration: 683, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 684, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 685, Training Loss: 1.511e+00 , Validation Loss: 1.445e+00\n",
      "Iteration: 686, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 687, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 688, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 689, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 690, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 691, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 692, Training Loss: 3.274e+00 , Validation Loss: 2.256e+00\n",
      "Iteration: 693, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 694, Training Loss: 2.770e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 695, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 696, Training Loss: 7.555e-01 , Validation Loss: 1.204e+00\n",
      "Iteration: 697, Training Loss: 1.511e+00 , Validation Loss: 2.189e+00\n",
      "Iteration: 698, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 699, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 700, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 701, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 702, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 703, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 704, Training Loss: 1.259e+00 , Validation Loss: 1.663e+00\n",
      "Iteration: 705, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 706, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 707, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 708, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 709, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 710, Training Loss: 2.518e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 711, Training Loss: 2.015e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 712, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 713, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 714, Training Loss: 1.763e+00 , Validation Loss: 1.379e+00\n",
      "Iteration: 715, Training Loss: 1.259e+00 , Validation Loss: 2.395e+00\n",
      "Iteration: 716, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 717, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 718, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 719, Training Loss: 1.259e+00 , Validation Loss: 3.332e+00\n",
      "Iteration: 720, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 721, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 722, Training Loss: 2.518e+00 , Validation Loss: 4.845e+00\n",
      "Iteration: 723, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 724, Training Loss: 2.518e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 725, Training Loss: 1.511e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 726, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 727, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 728, Training Loss: 1.007e+00 , Validation Loss: 1.101e+00\n",
      "Iteration: 729, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 730, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 731, Training Loss: 1.763e+00 , Validation Loss: 5.655e+00\n",
      "Iteration: 732, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 733, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 734, Training Loss: 2.267e+00 , Validation Loss: 2.068e+00\n",
      "Iteration: 735, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 736, Training Loss: 3.778e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 737, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 738, Training Loss: 3.274e+00 , Validation Loss: 1.760e+00\n",
      "Iteration: 739, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 740, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 741, Training Loss: 2.267e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 742, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 743, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 744, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 745, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 746, Training Loss: 2.015e+00 , Validation Loss: 2.056e+00\n",
      "Iteration: 747, Training Loss: 1.259e+00 , Validation Loss: 9.556e-01\n",
      "Iteration: 748, Training Loss: 1.259e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 749, Training Loss: 1.007e+00 , Validation Loss: 1.990e+00\n",
      "Iteration: 750, Training Loss: 1.763e+00 , Validation Loss: 4.234e-01\n",
      "Iteration: 751, Training Loss: 1.007e+00 , Validation Loss: 2.165e+00\n",
      "Iteration: 752, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 753, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 754, Training Loss: 1.259e+00 , Validation Loss: 1.379e+00\n",
      "Iteration: 755, Training Loss: 9.066e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 756, Training Loss: 4.533e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 757, Training Loss: 3.778e+00 , Validation Loss: 4.131e+00\n",
      "Iteration: 758, Training Loss: 1.259e+00 , Validation Loss: 9.435e-01\n",
      "Iteration: 759, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 760, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 761, Training Loss: 1.259e+00 , Validation Loss: 1.228e+00\n",
      "Iteration: 762, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 763, Training Loss: 2.015e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 764, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 765, Training Loss: 2.518e+00 , Validation Loss: 2.165e+00\n",
      "Iteration: 766, Training Loss: 2.518e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 767, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 768, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 769, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 770, Training Loss: 1.007e+00 , Validation Loss: 1.083e+00\n",
      "Iteration: 771, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 772, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 773, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 774, Training Loss: 1.007e+00 , Validation Loss: 2.970e+00\n",
      "Iteration: 775, Training Loss: 1.511e+00 , Validation Loss: 1.046e+00\n",
      "Iteration: 776, Training Loss: 8.563e+00 , Validation Loss: 9.362e+00\n",
      "Iteration: 777, Training Loss: 1.763e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 778, Training Loss: 2.015e+00 , Validation Loss: 2.637e+00\n",
      "Iteration: 779, Training Loss: 2.267e+00 , Validation Loss: 1.028e+00\n",
      "Iteration: 780, Training Loss: 2.267e+00 , Validation Loss: 2.111e+00\n",
      "Iteration: 781, Training Loss: 1.007e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 782, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 783, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 784, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 785, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 786, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 787, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 788, Training Loss: 1.763e+00 , Validation Loss: 2.214e+00\n",
      "Iteration: 789, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 790, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 791, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 792, Training Loss: 7.555e-01 , Validation Loss: 9.012e-01\n",
      "Iteration: 793, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 794, Training Loss: 3.022e+00 , Validation Loss: 3.750e+00\n",
      "Iteration: 795, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 796, Training Loss: 3.526e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 797, Training Loss: 1.763e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 798, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 799, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 800, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 801, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 802, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 803, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 804, Training Loss: 1.259e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 805, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 806, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 807, Training Loss: 3.274e+00 , Validation Loss: 4.457e+00\n",
      "Iteration: 808, Training Loss: 1.158e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 809, Training Loss: 2.267e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 810, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 811, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 812, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 813, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 814, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 815, Training Loss: 1.259e+00 , Validation Loss: 2.794e+00\n",
      "Iteration: 816, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 817, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 818, Training Loss: 1.511e+00 , Validation Loss: 1.034e+00\n",
      "Iteration: 819, Training Loss: 2.518e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 820, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 821, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 822, Training Loss: 1.763e+00 , Validation Loss: 2.165e+00\n",
      "Iteration: 823, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 824, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 825, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 826, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 827, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 828, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 829, Training Loss: 7.555e-01 , Validation Loss: 1.204e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 830, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 831, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 832, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 833, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 834, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 835, Training Loss: 3.022e+00 , Validation Loss: 4.772e+00\n",
      "Iteration: 836, Training Loss: 1.385e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 837, Training Loss: 2.267e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 838, Training Loss: 3.274e+00 , Validation Loss: 3.193e+00\n",
      "Iteration: 839, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 840, Training Loss: 3.778e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 841, Training Loss: 2.518e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 842, Training Loss: 1.763e+00 , Validation Loss: 1.325e+00\n",
      "Iteration: 843, Training Loss: 1.511e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 844, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 845, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 846, Training Loss: 1.007e+00 , Validation Loss: 4.355e-01\n",
      "Iteration: 847, Training Loss: 2.267e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 848, Training Loss: 1.259e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 849, Training Loss: 1.259e+00 , Validation Loss: 1.282e+00\n",
      "Iteration: 850, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 851, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 852, Training Loss: 1.511e+00 , Validation Loss: 2.873e+00\n",
      "Iteration: 853, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 854, Training Loss: 2.518e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 855, Training Loss: 1.511e+00 , Validation Loss: 2.939e+00\n",
      "Iteration: 856, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 857, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 858, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 859, Training Loss: 7.555e-01 , Validation Loss: 1.119e+00\n",
      "Iteration: 860, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 861, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 862, Training Loss: 4.281e+00 , Validation Loss: 5.758e+00\n",
      "Iteration: 863, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 864, Training Loss: 4.281e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 865, Training Loss: 8.563e+00 , Validation Loss: 7.856e+00\n",
      "Iteration: 866, Training Loss: 3.526e+00 , Validation Loss: 5.782e+00\n",
      "Iteration: 867, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 868, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 869, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 870, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 871, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 872, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 873, Training Loss: 2.518e+00 , Validation Loss: 1.833e+00\n",
      "Iteration: 874, Training Loss: 2.518e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 875, Training Loss: 1.763e+00 , Validation Loss: 1.784e+00\n",
      "Iteration: 876, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 877, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 878, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 879, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 880, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 881, Training Loss: 1.763e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 882, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 883, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 884, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 885, Training Loss: 1.007e+00 , Validation Loss: 2.074e+00\n",
      "Iteration: 886, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 887, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 888, Training Loss: 7.555e-01 , Validation Loss: 1.724e+00\n",
      "Iteration: 889, Training Loss: 1.511e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 890, Training Loss: 3.022e+00 , Validation Loss: 2.123e+00\n",
      "Iteration: 891, Training Loss: 2.015e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 892, Training Loss: 2.015e+00 , Validation Loss: 1.191e+00\n",
      "Iteration: 893, Training Loss: 1.763e+00 , Validation Loss: 6.048e-01\n",
      "Iteration: 894, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 895, Training Loss: 2.015e+00 , Validation Loss: 4.609e+00\n",
      "Iteration: 896, Training Loss: 7.304e+00 , Validation Loss: 5.830e+00\n",
      "Iteration: 897, Training Loss: 3.274e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 898, Training Loss: 1.259e+00 , Validation Loss: 6.471e-01\n",
      "Iteration: 899, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 900, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 901, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 902, Training Loss: 1.511e+00 , Validation Loss: 2.044e+00\n",
      "Iteration: 903, Training Loss: 2.518e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 904, Training Loss: 1.511e+00 , Validation Loss: 2.449e+00\n",
      "Iteration: 905, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 906, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 907, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 908, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 909, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 910, Training Loss: 1.007e+00 , Validation Loss: 1.978e+00\n",
      "Iteration: 911, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 912, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 913, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 914, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 915, Training Loss: 1.511e+00 , Validation Loss: 1.542e+00\n",
      "Iteration: 916, Training Loss: 1.763e+00 , Validation Loss: 2.528e+00\n",
      "Iteration: 917, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 918, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 919, Training Loss: 1.259e+00 , Validation Loss: 3.459e+00\n",
      "Iteration: 920, Training Loss: 7.555e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 921, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 922, Training Loss: 2.770e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 923, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 924, Training Loss: 3.274e+00 , Validation Loss: 5.873e+00\n",
      "Iteration: 925, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 926, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 927, Training Loss: 7.555e-01 , Validation Loss: 1.343e+00\n",
      "Iteration: 928, Training Loss: 7.555e-01 , Validation Loss: 1.149e+00\n",
      "Iteration: 929, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 930, Training Loss: 1.511e+00 , Validation Loss: 2.649e+00\n",
      "Iteration: 931, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 932, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 933, Training Loss: 1.007e+00 , Validation Loss: 3.133e+00\n",
      "Iteration: 934, Training Loss: 1.007e+00 , Validation Loss: 1.294e+00\n",
      "Iteration: 935, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 936, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 937, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 938, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 939, Training Loss: 2.267e+00 , Validation Loss: 3.726e+00\n",
      "Iteration: 940, Training Loss: 1.360e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 941, Training Loss: 3.778e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 942, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 943, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 944, Training Loss: 1.511e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 945, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 946, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 947, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 948, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 949, Training Loss: 4.281e+00 , Validation Loss: 5.062e+00\n",
      "Iteration: 950, Training Loss: 1.158e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 951, Training Loss: 4.281e+00 , Validation Loss: 5.480e+00\n",
      "Iteration: 952, Training Loss: 8.815e+00 , Validation Loss: 1.023e+01\n",
      "Iteration: 953, Training Loss: 2.518e+00 , Validation Loss: 5.177e+00\n",
      "Iteration: 954, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 955, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 956, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 957, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 958, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 959, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 960, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 961, Training Loss: 1.259e+00 , Validation Loss: 1.464e+00\n",
      "Iteration: 962, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 963, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 964, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 965, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 966, Training Loss: 2.518e-01 , Validation Loss: 1.173e+00\n",
      "Iteration: 967, Training Loss: 1.259e+00 , Validation Loss: 1.415e+00\n",
      "Iteration: 968, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 969, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 970, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 971, Training Loss: 1.511e+00 , Validation Loss: 1.651e+00\n",
      "Iteration: 972, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 973, Training Loss: 2.518e+00 , Validation Loss: 2.250e+00\n",
      "Iteration: 974, Training Loss: 1.007e+00 , Validation Loss: 1.427e+00\n",
      "Iteration: 975, Training Loss: 2.015e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 976, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 977, Training Loss: 1.259e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 978, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 979, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 980, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 981, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 982, Training Loss: 1.763e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 983, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 984, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 985, Training Loss: 7.555e-01 , Validation Loss: 1.439e+00\n",
      "Iteration: 986, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 987, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 988, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 989, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 990, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 991, Training Loss: 2.518e-01 , Validation Loss: 1.343e+00\n",
      "Iteration: 992, Training Loss: 1.763e+00 , Validation Loss: 3.375e+00\n",
      "Iteration: 993, Training Loss: 2.267e+00 , Validation Loss: 1.010e+00\n",
      "Iteration: 994, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 995, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 996, Training Loss: 2.015e+00 , Validation Loss: 5.643e+00\n",
      "Iteration: 997, Training Loss: 1.511e+00 , Validation Loss: 1.972e+00\n",
      "Iteration: 998, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 999, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1000, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1001, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 1002, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1003, Training Loss: 1.007e+00 , Validation Loss: 1.119e+00\n",
      "Iteration: 1004, Training Loss: 1.007e+00 , Validation Loss: 1.445e+00\n",
      "Iteration: 1005, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1006, Training Loss: 1.259e+00 , Validation Loss: 1.204e+00\n",
      "Iteration: 1007, Training Loss: 2.518e+00 , Validation Loss: 1.294e+00\n",
      "Iteration: 1008, Training Loss: 3.526e+00 , Validation Loss: 5.855e+00\n",
      "Iteration: 1009, Training Loss: 1.083e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1010, Training Loss: 2.267e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1011, Training Loss: 2.518e+00 , Validation Loss: 3.544e+00\n",
      "Iteration: 1012, Training Loss: 1.511e+00 , Validation Loss: 1.155e+00\n",
      "Iteration: 1013, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1014, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 1015, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 1016, Training Loss: 2.015e+00 , Validation Loss: 1.349e+00\n",
      "Iteration: 1017, Training Loss: 2.015e+00 , Validation Loss: 1.833e+00\n",
      "Iteration: 1018, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1019, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1020, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1021, Training Loss: 1.259e+00 , Validation Loss: 4.597e-01\n",
      "Iteration: 1022, Training Loss: 2.267e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1023, Training Loss: 2.770e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1024, Training Loss: 1.763e+00 , Validation Loss: 2.177e+00\n",
      "Iteration: 1025, Training Loss: 1.511e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 1026, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1027, Training Loss: 3.778e+00 , Validation Loss: 5.056e+00\n",
      "Iteration: 1028, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1029, Training Loss: 4.281e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1030, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1031, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 1032, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1033, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 1034, Training Loss: 2.518e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1035, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1036, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1037, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1038, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1039, Training Loss: 1.511e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 1040, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1041, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 1042, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 1043, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1044, Training Loss: 2.267e+00 , Validation Loss: 1.869e+00\n",
      "Iteration: 1045, Training Loss: 2.015e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 1046, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1047, Training Loss: 1.511e+00 , Validation Loss: 2.141e+00\n",
      "Iteration: 1048, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1049, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1050, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1051, Training Loss: 1.259e+00 , Validation Loss: 1.730e+00\n",
      "Iteration: 1052, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 1053, Training Loss: 2.770e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 1054, Training Loss: 4.030e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1055, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1056, Training Loss: 3.526e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1057, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1058, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1059, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1060, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 1061, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1062, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1063, Training Loss: 1.511e+00 , Validation Loss: 1.554e+00\n",
      "Iteration: 1064, Training Loss: 2.518e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 1065, Training Loss: 2.518e+00 , Validation Loss: 1.730e+00\n",
      "Iteration: 1066, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1067, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1068, Training Loss: 1.007e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 1069, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1070, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1071, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1072, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1073, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1074, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1075, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1076, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 1077, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 1078, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1079, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1080, Training Loss: 2.770e+00 , Validation Loss: 2.673e+00\n",
      "Iteration: 1081, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1082, Training Loss: 1.763e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 1083, Training Loss: 3.022e+00 , Validation Loss: 5.473e+00\n",
      "Iteration: 1084, Training Loss: 3.274e+00 , Validation Loss: 1.518e+00\n",
      "Iteration: 1085, Training Loss: 1.511e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 1086, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1087, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 1088, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1089, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 1090, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1091, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1092, Training Loss: 7.555e-01 , Validation Loss: 1.228e+00\n",
      "Iteration: 1093, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1094, Training Loss: 1.259e+00 , Validation Loss: 1.294e+00\n",
      "Iteration: 1095, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1096, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1097, Training Loss: 7.555e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 1098, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1099, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1100, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1101, Training Loss: 5.037e-01 , Validation Loss: 2.111e+00\n",
      "Iteration: 1102, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1103, Training Loss: 2.267e+00 , Validation Loss: 1.990e+00\n",
      "Iteration: 1104, Training Loss: 1.360e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1105, Training Loss: 2.015e+00 , Validation Loss: 5.873e+00\n",
      "Iteration: 1106, Training Loss: 3.778e+00 , Validation Loss: 5.836e+00\n",
      "Iteration: 1107, Training Loss: 1.184e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1108, Training Loss: 2.518e+00 , Validation Loss: 5.830e+00\n",
      "Iteration: 1109, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1110, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1111, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 1112, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1113, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1114, Training Loss: 2.015e+00 , Validation Loss: 1.331e+00\n",
      "Iteration: 1115, Training Loss: 1.007e+00 , Validation Loss: 1.022e+00\n",
      "Iteration: 1116, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1117, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1118, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1119, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1120, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1121, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1122, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1123, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1124, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1125, Training Loss: 3.526e+00 , Validation Loss: 4.929e+00\n",
      "Iteration: 1126, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1127, Training Loss: 3.778e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1128, Training Loss: 2.518e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1129, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 1130, Training Loss: 2.015e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 1131, Training Loss: 1.511e+00 , Validation Loss: 1.911e+00\n",
      "Iteration: 1132, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1133, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1134, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1135, Training Loss: 2.267e+00 , Validation Loss: 1.778e+00\n",
      "Iteration: 1136, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1137, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1138, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1139, Training Loss: 1.511e+00 , Validation Loss: 2.480e+00\n",
      "Iteration: 1140, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1141, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1142, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1143, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1144, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1145, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1146, Training Loss: 2.518e+00 , Validation Loss: 4.022e+00\n",
      "Iteration: 1147, Training Loss: 1.360e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1148, Training Loss: 2.770e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 1149, Training Loss: 1.007e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 1150, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 1151, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1152, Training Loss: 1.007e+00 , Validation Loss: 1.294e+00\n",
      "Iteration: 1153, Training Loss: 3.022e+00 , Validation Loss: 3.732e+00\n",
      "Iteration: 1154, Training Loss: 2.770e+00 , Validation Loss: 1.101e+00\n",
      "Iteration: 1155, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1156, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1157, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 1158, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1159, Training Loss: 1.007e+00 , Validation Loss: 1.034e+00\n",
      "Iteration: 1160, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1161, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1162, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1163, Training Loss: 2.267e+00 , Validation Loss: 1.814e+00\n",
      "Iteration: 1164, Training Loss: 2.015e+00 , Validation Loss: 1.064e+00\n",
      "Iteration: 1165, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1166, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1167, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1168, Training Loss: 1.007e+00 , Validation Loss: 1.669e+00\n",
      "Iteration: 1169, Training Loss: 7.555e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 1170, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1171, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 1172, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1173, Training Loss: 3.022e+00 , Validation Loss: 2.208e+00\n",
      "Iteration: 1174, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1175, Training Loss: 2.518e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1176, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1177, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 1178, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1179, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1180, Training Loss: 2.015e+00 , Validation Loss: 3.381e+00\n",
      "Iteration: 1181, Training Loss: 7.555e-01 , Validation Loss: 1.439e+00\n",
      "Iteration: 1182, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1183, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1184, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1185, Training Loss: 3.274e+00 , Validation Loss: 3.883e+00\n",
      "Iteration: 1186, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1187, Training Loss: 3.274e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1188, Training Loss: 1.511e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 1189, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1190, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 1191, Training Loss: 1.763e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 1192, Training Loss: 3.274e+00 , Validation Loss: 3.822e+00\n",
      "Iteration: 1193, Training Loss: 4.281e+00 , Validation Loss: 4.397e+00\n",
      "Iteration: 1194, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1195, Training Loss: 7.555e-01 , Validation Loss: 1.603e+00\n",
      "Iteration: 1196, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1197, Training Loss: 1.259e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 1198, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 1199, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1200, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1201, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1202, Training Loss: 1.259e+00 , Validation Loss: 1.984e+00\n",
      "Iteration: 1203, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1204, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1205, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1206, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1207, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1208, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1209, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1210, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1211, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1212, Training Loss: 7.555e-01 , Validation Loss: 1.814e+00\n",
      "Iteration: 1213, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1214, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1215, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1216, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1217, Training Loss: 5.289e+00 , Validation Loss: 4.838e+00\n",
      "Iteration: 1218, Training Loss: 1.436e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1219, Training Loss: 2.518e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1220, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1221, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1222, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 1223, Training Loss: 7.555e-01 , Validation Loss: 1.445e+00\n",
      "Iteration: 1224, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1225, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 1226, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 1227, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1228, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1229, Training Loss: 1.511e+00 , Validation Loss: 1.712e+00\n",
      "Iteration: 1230, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1231, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1232, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1233, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1234, Training Loss: 3.526e+00 , Validation Loss: 2.220e+00\n",
      "Iteration: 1235, Training Loss: 1.763e+00 , Validation Loss: 9.677e-01\n",
      "Iteration: 1236, Training Loss: 1.511e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 1237, Training Loss: 2.015e+00 , Validation Loss: 2.141e+00\n",
      "Iteration: 1238, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1239, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 1240, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1241, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1242, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1243, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1244, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 1245, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1246, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1247, Training Loss: 3.526e+00 , Validation Loss: 4.766e+00\n",
      "Iteration: 1248, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1249, Training Loss: 4.030e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1250, Training Loss: 2.267e+00 , Validation Loss: 1.506e+00\n",
      "Iteration: 1251, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1252, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1253, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 1254, Training Loss: 2.267e+00 , Validation Loss: 1.633e+00\n",
      "Iteration: 1255, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1256, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1257, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1258, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1259, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1260, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1261, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 1262, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1263, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1264, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1265, Training Loss: 3.274e+00 , Validation Loss: 2.008e+00\n",
      "Iteration: 1266, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1267, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1268, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1269, Training Loss: 1.511e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 1270, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1271, Training Loss: 3.778e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1272, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1273, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1274, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 1275, Training Loss: 1.511e+00 , Validation Loss: 1.693e+00\n",
      "Iteration: 1276, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1277, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1278, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1279, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1280, Training Loss: 2.015e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 1281, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1282, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1283, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1284, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1285, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1286, Training Loss: 1.259e+00 , Validation Loss: 1.524e+00\n",
      "Iteration: 1287, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1288, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1289, Training Loss: 3.022e+00 , Validation Loss: 3.405e+00\n",
      "Iteration: 1290, Training Loss: 1.007e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1291, Training Loss: 3.022e+00 , Validation Loss: 5.848e+00\n",
      "Iteration: 1292, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1293, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 1294, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1295, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1296, Training Loss: -1.000e-07 , Validation Loss: 3.326e-01\n",
      "Iteration: 1297, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 1298, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1299, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1300, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1301, Training Loss: 2.770e+00 , Validation Loss: 3.205e+00\n",
      "Iteration: 1302, Training Loss: 6.296e+00 , Validation Loss: 5.044e+00\n",
      "Iteration: 1303, Training Loss: 1.007e+00 , Validation Loss: 1.294e+00\n",
      "Iteration: 1304, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1305, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1306, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1307, Training Loss: 1.007e+00 , Validation Loss: 3.326e-01\n",
      "Iteration: 1308, Training Loss: 5.037e-01 , Validation Loss: 1.294e+00\n",
      "Iteration: 1309, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1310, Training Loss: 1.007e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 1311, Training Loss: 1.763e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 1312, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1313, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1314, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1315, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1316, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1317, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1318, Training Loss: 1.763e+00 , Validation Loss: 2.074e+00\n",
      "Iteration: 1319, Training Loss: 1.259e+00 , Validation Loss: 5.020e-01\n",
      "Iteration: 1320, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1321, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1322, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 1323, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1324, Training Loss: 1.259e+00 , Validation Loss: 1.149e+00\n",
      "Iteration: 1325, Training Loss: 1.007e+00 , Validation Loss: 1.167e+00\n",
      "Iteration: 1326, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1327, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1328, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1329, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1330, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1331, Training Loss: 2.015e+00 , Validation Loss: 1.427e+00\n",
      "Iteration: 1332, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1333, Training Loss: 2.518e+00 , Validation Loss: 5.873e+00\n",
      "Iteration: 1334, Training Loss: 3.526e+00 , Validation Loss: 2.359e+00\n",
      "Iteration: 1335, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1336, Training Loss: 4.030e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1337, Training Loss: 2.267e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 1338, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1339, Training Loss: 7.555e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 1340, Training Loss: 1.763e+00 , Validation Loss: 1.252e+00\n",
      "Iteration: 1341, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1342, Training Loss: 2.518e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 1343, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1344, Training Loss: 5.037e-01 , Validation Loss: 3.326e-01\n",
      "Iteration: 1345, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1346, Training Loss: 7.555e-01 , Validation Loss: 2.038e+00\n",
      "Iteration: 1347, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1348, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1349, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1350, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1351, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1352, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1353, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1354, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1355, Training Loss: 7.555e-01 , Validation Loss: 8.588e-01\n",
      "Iteration: 1356, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 1357, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1358, Training Loss: 5.037e-01 , Validation Loss: 1.095e+00\n",
      "Iteration: 1359, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1360, Training Loss: 1.259e+00 , Validation Loss: 1.137e+00\n",
      "Iteration: 1361, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1362, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1363, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1364, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1365, Training Loss: 3.526e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1366, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1367, Training Loss: 2.267e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1368, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1369, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1370, Training Loss: 2.518e+00 , Validation Loss: 3.169e+00\n",
      "Iteration: 1371, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1372, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1373, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1374, Training Loss: 2.770e+00 , Validation Loss: 5.008e+00\n",
      "Iteration: 1375, Training Loss: 2.518e+00 , Validation Loss: 3.078e+00\n",
      "Iteration: 1376, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 1377, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1378, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1379, Training Loss: 5.037e-01 , Validation Loss: 2.111e+00\n",
      "Iteration: 1380, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1381, Training Loss: 1.259e+00 , Validation Loss: 4.355e-01\n",
      "Iteration: 1382, Training Loss: 1.763e+00 , Validation Loss: 1.222e+00\n",
      "Iteration: 1383, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1384, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1385, Training Loss: 1.763e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 1386, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1387, Training Loss: 3.778e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1388, Training Loss: 1.511e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 1389, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1390, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1391, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1392, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1393, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1394, Training Loss: 1.007e+00 , Validation Loss: 1.119e+00\n",
      "Iteration: 1395, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1396, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1397, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1398, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1399, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1400, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1401, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1402, Training Loss: 2.518e-01 , Validation Loss: 1.645e+00\n",
      "Iteration: 1403, Training Loss: 2.770e+00 , Validation Loss: 2.443e+00\n",
      "Iteration: 1404, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1405, Training Loss: 3.778e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 1406, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1407, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1408, Training Loss: 7.555e-01 , Validation Loss: 1.119e+00\n",
      "Iteration: 1409, Training Loss: 2.518e+00 , Validation Loss: 3.605e+00\n",
      "Iteration: 1410, Training Loss: 2.015e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 1411, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1412, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1413, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1414, Training Loss: 1.259e+00 , Validation Loss: 1.331e+00\n",
      "Iteration: 1415, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1416, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 1417, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1418, Training Loss: 1.763e+00 , Validation Loss: 1.597e+00\n",
      "Iteration: 1419, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1420, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1421, Training Loss: 2.015e+00 , Validation Loss: 3.526e+00\n",
      "Iteration: 1422, Training Loss: 3.778e+00 , Validation Loss: 4.270e+00\n",
      "Iteration: 1423, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1424, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1425, Training Loss: 2.770e+00 , Validation Loss: 5.734e+00\n",
      "Iteration: 1426, Training Loss: 5.541e+00 , Validation Loss: 8.237e+00\n",
      "Iteration: 1427, Training Loss: 2.267e+00 , Validation Loss: 3.895e+00\n",
      "Iteration: 1428, Training Loss: 3.778e+00 , Validation Loss: 3.556e+00\n",
      "Iteration: 1429, Training Loss: 1.763e+00 , Validation Loss: 2.183e+00\n",
      "Iteration: 1430, Training Loss: 2.267e+00 , Validation Loss: 2.093e+00\n",
      "Iteration: 1431, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1432, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1433, Training Loss: 3.274e+00 , Validation Loss: 3.030e+00\n",
      "Iteration: 1434, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1435, Training Loss: 2.518e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1436, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1437, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1438, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1439, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1440, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1441, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 1442, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1443, Training Loss: 2.770e+00 , Validation Loss: 2.752e+00\n",
      "Iteration: 1444, Training Loss: 1.007e+00 , Validation Loss: 1.790e+00\n",
      "Iteration: 1445, Training Loss: 2.267e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 1446, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1447, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1448, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1449, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1450, Training Loss: 1.007e+00 , Validation Loss: 1.325e+00\n",
      "Iteration: 1451, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1452, Training Loss: 1.763e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 1453, Training Loss: 2.518e+00 , Validation Loss: 3.097e+00\n",
      "Iteration: 1454, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1455, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1456, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1457, Training Loss: 1.259e+00 , Validation Loss: 2.691e+00\n",
      "Iteration: 1458, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 1459, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 1460, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1461, Training Loss: 2.267e+00 , Validation Loss: 2.776e+00\n",
      "Iteration: 1462, Training Loss: 5.289e+00 , Validation Loss: 4.784e+00\n",
      "Iteration: 1463, Training Loss: 1.763e+00 , Validation Loss: 3.544e+00\n",
      "Iteration: 1464, Training Loss: 3.526e+00 , Validation Loss: 1.524e+00\n",
      "Iteration: 1465, Training Loss: 3.274e+00 , Validation Loss: 3.822e+00\n",
      "Iteration: 1466, Training Loss: 8.311e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 1467, Training Loss: 7.555e-01 , Validation Loss: 2.958e+00\n",
      "Iteration: 1468, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1469, Training Loss: 2.267e+00 , Validation Loss: 2.292e+00\n",
      "Iteration: 1470, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1471, Training Loss: 1.259e+00 , Validation Loss: 8.649e-01\n",
      "Iteration: 1472, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1473, Training Loss: 1.007e+00 , Validation Loss: 1.851e+00\n",
      "Iteration: 1474, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1475, Training Loss: 4.533e+00 , Validation Loss: 3.218e+00\n",
      "Iteration: 1476, Training Loss: 1.461e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1477, Training Loss: 3.022e+00 , Validation Loss: 4.972e+00\n",
      "Iteration: 1478, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1479, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1480, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1481, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1482, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1483, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1484, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1485, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1486, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1487, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1488, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1489, Training Loss: 2.015e+00 , Validation Loss: 2.474e+00\n",
      "Iteration: 1490, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 1491, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1492, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1493, Training Loss: 1.511e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 1494, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1495, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1496, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1497, Training Loss: 2.015e+00 , Validation Loss: 1.300e+00\n",
      "Iteration: 1498, Training Loss: 1.511e+00 , Validation Loss: 1.143e+00\n",
      "Iteration: 1499, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1500, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1501, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1502, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1503, Training Loss: 1.007e+00 , Validation Loss: 3.828e+00\n",
      "Iteration: 1504, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1505, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1506, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1507, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 1508, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1509, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1510, Training Loss: 1.259e+00 , Validation Loss: 1.627e+00\n",
      "Iteration: 1511, Training Loss: 3.022e+00 , Validation Loss: 2.274e+00\n",
      "Iteration: 1512, Training Loss: 2.267e+00 , Validation Loss: 4.312e+00\n",
      "Iteration: 1513, Training Loss: 4.785e+00 , Validation Loss: 5.262e+00\n",
      "Iteration: 1514, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1515, Training Loss: 3.022e+00 , Validation Loss: 1.875e+00\n",
      "Iteration: 1516, Training Loss: 6.296e+00 , Validation Loss: 8.449e+00\n",
      "Iteration: 1517, Training Loss: 2.267e+00 , Validation Loss: 4.790e+00\n",
      "Iteration: 1518, Training Loss: 1.511e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 1519, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1520, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1521, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1522, Training Loss: 1.511e+00 , Validation Loss: 2.208e+00\n",
      "Iteration: 1523, Training Loss: 1.511e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 1524, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1525, Training Loss: 1.259e+00 , Validation Loss: 2.111e+00\n",
      "Iteration: 1526, Training Loss: 7.555e-01 , Validation Loss: 9.556e-01\n",
      "Iteration: 1527, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1528, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1529, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 1530, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1531, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1532, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1533, Training Loss: 1.007e+00 , Validation Loss: 1.119e+00\n",
      "Iteration: 1534, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1535, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1536, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1537, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1538, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1539, Training Loss: 1.007e+00 , Validation Loss: 1.452e+00\n",
      "Iteration: 1540, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1541, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 1542, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 1543, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1544, Training Loss: 2.267e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 1545, Training Loss: 4.785e+00 , Validation Loss: 7.469e+00\n",
      "Iteration: 1546, Training Loss: 2.267e+00 , Validation Loss: 4.790e+00\n",
      "Iteration: 1547, Training Loss: 7.555e-01 , Validation Loss: 1.427e+00\n",
      "Iteration: 1548, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1549, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 1550, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 1551, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1552, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1553, Training Loss: 3.526e+00 , Validation Loss: 5.189e+00\n",
      "Iteration: 1554, Training Loss: 1.108e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1555, Training Loss: 2.518e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1556, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 1557, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1558, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1559, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1560, Training Loss: 7.555e-01 , Validation Loss: 1.579e+00\n",
      "Iteration: 1561, Training Loss: 2.267e+00 , Validation Loss: 1.452e+00\n",
      "Iteration: 1562, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1563, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 1564, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1565, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1566, Training Loss: 2.015e+00 , Validation Loss: 4.597e+00\n",
      "Iteration: 1567, Training Loss: 1.259e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 1568, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 1569, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1570, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1571, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1572, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1573, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1574, Training Loss: 2.267e+00 , Validation Loss: 2.982e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1575, Training Loss: 1.511e+00 , Validation Loss: 1.191e+00\n",
      "Iteration: 1576, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1577, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1578, Training Loss: 1.511e+00 , Validation Loss: 3.411e+00\n",
      "Iteration: 1579, Training Loss: 1.763e+00 , Validation Loss: 1.681e+00\n",
      "Iteration: 1580, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1581, Training Loss: 1.511e+00 , Validation Loss: 1.621e+00\n",
      "Iteration: 1582, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1583, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 1584, Training Loss: 3.022e+00 , Validation Loss: 5.292e+00\n",
      "Iteration: 1585, Training Loss: 7.555e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 1586, Training Loss: 2.770e+00 , Validation Loss: 5.026e+00\n",
      "Iteration: 1587, Training Loss: 3.022e+00 , Validation Loss: 1.772e+00\n",
      "Iteration: 1588, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1589, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1590, Training Loss: 1.511e+00 , Validation Loss: 2.679e+00\n",
      "Iteration: 1591, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1592, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1593, Training Loss: 1.259e+00 , Validation Loss: 1.760e+00\n",
      "Iteration: 1594, Training Loss: 3.022e+00 , Validation Loss: 4.935e+00\n",
      "Iteration: 1595, Training Loss: 8.815e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 1596, Training Loss: 3.274e+00 , Validation Loss: 5.080e+00\n",
      "Iteration: 1597, Training Loss: 1.007e+00 , Validation Loss: 1.264e+00\n",
      "Iteration: 1598, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1599, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1600, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1601, Training Loss: 1.259e+00 , Validation Loss: 1.639e+00\n",
      "Iteration: 1602, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1603, Training Loss: 1.007e+00 , Validation Loss: 2.044e+00\n",
      "Iteration: 1604, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1605, Training Loss: 2.015e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1606, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1607, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1608, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1609, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1610, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1611, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1612, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1613, Training Loss: 2.267e+00 , Validation Loss: 4.530e+00\n",
      "Iteration: 1614, Training Loss: 1.511e+00 , Validation Loss: 8.528e-01\n",
      "Iteration: 1615, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1616, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1617, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1618, Training Loss: 2.770e+00 , Validation Loss: 5.080e+00\n",
      "Iteration: 1619, Training Loss: 2.770e+00 , Validation Loss: 3.901e+00\n",
      "Iteration: 1620, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1621, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1622, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1623, Training Loss: 2.015e+00 , Validation Loss: 3.236e+00\n",
      "Iteration: 1624, Training Loss: 2.267e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 1625, Training Loss: 2.267e+00 , Validation Loss: 2.939e+00\n",
      "Iteration: 1626, Training Loss: 1.763e+00 , Validation Loss: 2.050e+00\n",
      "Iteration: 1627, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1628, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1629, Training Loss: 1.511e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 1630, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1631, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1632, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1633, Training Loss: 1.763e+00 , Validation Loss: 2.885e+00\n",
      "Iteration: 1634, Training Loss: 1.763e+00 , Validation Loss: 2.177e+00\n",
      "Iteration: 1635, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1636, Training Loss: 2.015e+00 , Validation Loss: 4.191e+00\n",
      "Iteration: 1637, Training Loss: 6.800e+00 , Validation Loss: 7.953e+00\n",
      "Iteration: 1638, Training Loss: 4.281e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1639, Training Loss: 6.800e+00 , Validation Loss: 6.671e+00\n",
      "Iteration: 1640, Training Loss: 2.518e+00 , Validation Loss: 3.611e+00\n",
      "Iteration: 1641, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 1642, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1643, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1644, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1645, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 1646, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 1647, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 1648, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1649, Training Loss: 3.022e+00 , Validation Loss: 2.564e+00\n",
      "Iteration: 1650, Training Loss: 9.570e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 1651, Training Loss: 3.526e+00 , Validation Loss: 5.570e+00\n",
      "Iteration: 1652, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1653, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1654, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1655, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 1656, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1657, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1658, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1659, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1660, Training Loss: 2.015e+00 , Validation Loss: 1.464e+00\n",
      "Iteration: 1661, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1662, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1663, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1664, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1665, Training Loss: 1.007e+00 , Validation Loss: 2.177e+00\n",
      "Iteration: 1666, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1667, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1668, Training Loss: 1.007e+00 , Validation Loss: 2.220e+00\n",
      "Iteration: 1669, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1670, Training Loss: 1.259e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 1671, Training Loss: 1.511e+00 , Validation Loss: 9.254e-01\n",
      "Iteration: 1672, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1673, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1674, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1675, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1676, Training Loss: 2.267e+00 , Validation Loss: 4.034e+00\n",
      "Iteration: 1677, Training Loss: 3.526e+00 , Validation Loss: 3.877e+00\n",
      "Iteration: 1678, Training Loss: 2.518e+00 , Validation Loss: 2.939e+00\n",
      "Iteration: 1679, Training Loss: 1.511e+00 , Validation Loss: 1.149e+00\n",
      "Iteration: 1680, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1681, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1682, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1683, Training Loss: 2.770e+00 , Validation Loss: 4.518e+00\n",
      "Iteration: 1684, Training Loss: 3.526e+00 , Validation Loss: 5.207e+00\n",
      "Iteration: 1685, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1686, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1687, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1688, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1689, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 1690, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1691, Training Loss: 2.015e+00 , Validation Loss: 2.625e+00\n",
      "Iteration: 1692, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1693, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 1694, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1695, Training Loss: 1.763e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 1696, Training Loss: 1.763e+00 , Validation Loss: 4.457e+00\n",
      "Iteration: 1697, Training Loss: 2.015e+00 , Validation Loss: 1.760e+00\n",
      "Iteration: 1698, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1699, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 1700, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1701, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1702, Training Loss: 1.763e+00 , Validation Loss: 4.240e+00\n",
      "Iteration: 1703, Training Loss: 2.015e+00 , Validation Loss: 1.712e+00\n",
      "Iteration: 1704, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1705, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1706, Training Loss: 3.022e+00 , Validation Loss: 5.032e+00\n",
      "Iteration: 1707, Training Loss: 7.052e+00 , Validation Loss: 8.371e+00\n",
      "Iteration: 1708, Training Loss: 3.274e+00 , Validation Loss: 5.105e+00\n",
      "Iteration: 1709, Training Loss: 3.274e+00 , Validation Loss: 3.689e+00\n",
      "Iteration: 1710, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1711, Training Loss: 1.511e+00 , Validation Loss: 1.929e+00\n",
      "Iteration: 1712, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 1713, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1714, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1715, Training Loss: 1.007e+00 , Validation Loss: 2.395e+00\n",
      "Iteration: 1716, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1717, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 1718, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1719, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1720, Training Loss: 1.763e+00 , Validation Loss: 1.645e+00\n",
      "Iteration: 1721, Training Loss: 2.267e+00 , Validation Loss: 2.068e+00\n",
      "Iteration: 1722, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1723, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1724, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1725, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1726, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1727, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 1728, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 1729, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1730, Training Loss: 3.526e+00 , Validation Loss: 3.453e+00\n",
      "Iteration: 1731, Training Loss: 1.360e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1732, Training Loss: 3.274e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1733, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1734, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1735, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1736, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1737, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1738, Training Loss: 2.015e+00 , Validation Loss: 2.637e+00\n",
      "Iteration: 1739, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1740, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 1741, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1742, Training Loss: 1.511e+00 , Validation Loss: 1.790e+00\n",
      "Iteration: 1743, Training Loss: 2.518e+00 , Validation Loss: 1.433e+00\n",
      "Iteration: 1744, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1745, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1746, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1747, Training Loss: 1.511e+00 , Validation Loss: 1.500e+00\n",
      "Iteration: 1748, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1749, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1750, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1751, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1752, Training Loss: 1.763e+00 , Validation Loss: 4.161e+00\n",
      "Iteration: 1753, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1754, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1755, Training Loss: 5.037e-01 , Validation Loss: 1.911e+00\n",
      "Iteration: 1756, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1757, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1758, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 1759, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1760, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1761, Training Loss: 2.015e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 1762, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 1763, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1764, Training Loss: 2.267e+00 , Validation Loss: 3.472e+00\n",
      "Iteration: 1765, Training Loss: 6.296e+00 , Validation Loss: 8.062e+00\n",
      "Iteration: 1766, Training Loss: 3.274e+00 , Validation Loss: 5.032e+00\n",
      "Iteration: 1767, Training Loss: 7.304e+00 , Validation Loss: 6.490e+00\n",
      "Iteration: 1768, Training Loss: 3.022e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1769, Training Loss: 1.259e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 1770, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1771, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1772, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1773, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1774, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 1775, Training Loss: 2.518e+00 , Validation Loss: 2.262e+00\n",
      "Iteration: 1776, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1777, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1778, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1779, Training Loss: 1.259e+00 , Validation Loss: 1.627e+00\n",
      "Iteration: 1780, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1781, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1782, Training Loss: 1.259e+00 , Validation Loss: 1.706e+00\n",
      "Iteration: 1783, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1784, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 1785, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1786, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1787, Training Loss: 2.267e+00 , Validation Loss: 4.760e+00\n",
      "Iteration: 1788, Training Loss: 6.800e+00 , Validation Loss: 4.584e+00\n",
      "Iteration: 1789, Training Loss: 3.526e+00 , Validation Loss: 5.728e+00\n",
      "Iteration: 1790, Training Loss: 1.007e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1791, Training Loss: 4.030e+00 , Validation Loss: 5.855e+00\n",
      "Iteration: 1792, Training Loss: 2.015e+00 , Validation Loss: 1.845e+00\n",
      "Iteration: 1793, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1794, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 1795, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1796, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1797, Training Loss: 5.037e-01 , Validation Loss: 1.458e+00\n",
      "Iteration: 1798, Training Loss: 2.770e+00 , Validation Loss: 2.625e+00\n",
      "Iteration: 1799, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1800, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 1801, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1802, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1803, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1804, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1805, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1806, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 1807, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 1808, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 1809, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1810, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1811, Training Loss: 2.015e+00 , Validation Loss: 2.129e+00\n",
      "Iteration: 1812, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1813, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1814, Training Loss: 1.007e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 1815, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1816, Training Loss: 1.763e+00 , Validation Loss: 4.004e+00\n",
      "Iteration: 1817, Training Loss: 7.555e-01 , Validation Loss: 1.016e+00\n",
      "Iteration: 1818, Training Loss: 1.007e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 1819, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1820, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1821, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1822, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1823, Training Loss: 1.007e+00 , Validation Loss: 4.234e-01\n",
      "Iteration: 1824, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 1825, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 1826, Training Loss: 2.015e+00 , Validation Loss: 4.403e+00\n",
      "Iteration: 1827, Training Loss: 9.318e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 1828, Training Loss: 2.267e+00 , Validation Loss: 5.855e+00\n",
      "Iteration: 1829, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1830, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 1831, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1832, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1833, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1834, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1835, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1836, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1837, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1838, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1839, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 1840, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 1841, Training Loss: 2.015e+00 , Validation Loss: 2.788e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1842, Training Loss: 2.518e+00 , Validation Loss: 2.824e+00\n",
      "Iteration: 1843, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1844, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1845, Training Loss: 2.770e+00 , Validation Loss: 2.026e+00\n",
      "Iteration: 1846, Training Loss: 6.548e+00 , Validation Loss: 5.607e+00\n",
      "Iteration: 1847, Training Loss: 2.015e+00 , Validation Loss: 4.724e+00\n",
      "Iteration: 1848, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1849, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1850, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1851, Training Loss: 3.022e+00 , Validation Loss: 3.732e+00\n",
      "Iteration: 1852, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1853, Training Loss: 3.022e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1854, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1855, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1856, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1857, Training Loss: 1.259e+00 , Validation Loss: 1.585e+00\n",
      "Iteration: 1858, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1859, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1860, Training Loss: 1.007e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 1861, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1862, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 1863, Training Loss: 2.015e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1864, Training Loss: 5.037e-01 , Validation Loss: 1.633e+00\n",
      "Iteration: 1865, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1866, Training Loss: 1.763e+00 , Validation Loss: 3.617e+00\n",
      "Iteration: 1867, Training Loss: 1.259e+00 , Validation Loss: 5.020e-01\n",
      "Iteration: 1868, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1869, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1870, Training Loss: 2.015e+00 , Validation Loss: 2.371e+00\n",
      "Iteration: 1871, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1872, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1873, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1874, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1875, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1876, Training Loss: 1.007e+00 , Validation Loss: 1.530e+00\n",
      "Iteration: 1877, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1878, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1879, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 1880, Training Loss: 5.037e-01 , Validation Loss: 1.155e+00\n",
      "Iteration: 1881, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1882, Training Loss: 2.518e+00 , Validation Loss: 3.611e+00\n",
      "Iteration: 1883, Training Loss: 1.108e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1884, Training Loss: 2.770e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1885, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1886, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1887, Training Loss: 1.511e+00 , Validation Loss: 1.718e+00\n",
      "Iteration: 1888, Training Loss: 1.763e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 1889, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1890, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1891, Training Loss: 1.763e+00 , Validation Loss: 1.337e+00\n",
      "Iteration: 1892, Training Loss: 1.007e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 1893, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1894, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1895, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1896, Training Loss: 2.518e+00 , Validation Loss: 2.044e+00\n",
      "Iteration: 1897, Training Loss: 2.770e+00 , Validation Loss: 3.272e+00\n",
      "Iteration: 1898, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1899, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1900, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1901, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1902, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1903, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1904, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1905, Training Loss: 1.511e+00 , Validation Loss: 2.576e+00\n",
      "Iteration: 1906, Training Loss: 2.267e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 1907, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 1908, Training Loss: 1.259e+00 , Validation Loss: 1.282e+00\n",
      "Iteration: 1909, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1910, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 1911, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1912, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1913, Training Loss: 2.518e-01 , Validation Loss: 9.133e-01\n",
      "Iteration: 1914, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1915, Training Loss: 2.015e+00 , Validation Loss: 4.790e+00\n",
      "Iteration: 1916, Training Loss: 2.015e+00 , Validation Loss: 2.038e+00\n",
      "Iteration: 1917, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1918, Training Loss: 1.511e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 1919, Training Loss: 3.778e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 1920, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1921, Training Loss: 4.030e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1922, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1923, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1924, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1925, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 1926, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1927, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1928, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1929, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1930, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1931, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 1932, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1933, Training Loss: 1.007e+00 , Validation Loss: 1.591e+00\n",
      "Iteration: 1934, Training Loss: 1.511e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 1935, Training Loss: 1.259e+00 , Validation Loss: 4.476e-01\n",
      "Iteration: 1936, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1937, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1938, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1939, Training Loss: 4.281e+00 , Validation Loss: 4.663e+00\n",
      "Iteration: 1940, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1941, Training Loss: 2.770e+00 , Validation Loss: 5.534e+00\n",
      "Iteration: 1942, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1943, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1944, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1945, Training Loss: 1.007e+00 , Validation Loss: 8.891e-01\n",
      "Iteration: 1946, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 1947, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1948, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1949, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1950, Training Loss: 1.259e+00 , Validation Loss: 2.056e+00\n",
      "Iteration: 1951, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1952, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1953, Training Loss: 2.518e+00 , Validation Loss: 2.486e+00\n",
      "Iteration: 1954, Training Loss: 4.785e+00 , Validation Loss: 2.843e+00\n",
      "Iteration: 1955, Training Loss: 2.267e+00 , Validation Loss: 3.260e+00\n",
      "Iteration: 1956, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 1957, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 1958, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1959, Training Loss: 1.259e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 1960, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1961, Training Loss: 2.267e+00 , Validation Loss: 4.542e+00\n",
      "Iteration: 1962, Training Loss: 3.274e+00 , Validation Loss: 2.649e+00\n",
      "Iteration: 1963, Training Loss: 1.511e+00 , Validation Loss: 2.135e+00\n",
      "Iteration: 1964, Training Loss: 2.518e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 1965, Training Loss: 1.259e+00 , Validation Loss: 2.673e+00\n",
      "Iteration: 1966, Training Loss: 1.259e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 1967, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1968, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1969, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1970, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1971, Training Loss: 1.259e+00 , Validation Loss: 1.687e+00\n",
      "Iteration: 1972, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1973, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1974, Training Loss: 2.267e+00 , Validation Loss: 4.252e+00\n",
      "Iteration: 1975, Training Loss: 5.289e+00 , Validation Loss: 5.365e+00\n",
      "Iteration: 1976, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1977, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1978, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1979, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1980, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1981, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 1982, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 1983, Training Loss: 2.770e+00 , Validation Loss: 4.179e+00\n",
      "Iteration: 1984, Training Loss: 1.058e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 1985, Training Loss: 4.533e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 1986, Training Loss: 4.030e+00 , Validation Loss: 3.296e+00\n",
      "Iteration: 1987, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1988, Training Loss: 2.015e+00 , Validation Loss: 1.512e+00\n",
      "Iteration: 1989, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 1990, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 1991, Training Loss: 1.511e+00 , Validation Loss: 2.129e+00\n",
      "Iteration: 1992, Training Loss: 1.259e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 1993, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 1994, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 1995, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 1996, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 1997, Training Loss: 2.770e+00 , Validation Loss: 2.214e+00\n",
      "Iteration: 1998, Training Loss: 3.274e+00 , Validation Loss: 2.462e+00\n",
      "Iteration: 1999, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2000, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 2001, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2002, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2003, Training Loss: 2.015e+00 , Validation Loss: 2.141e+00\n",
      "Iteration: 2004, Training Loss: 2.015e+00 , Validation Loss: 1.010e+00\n",
      "Iteration: 2005, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2006, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2007, Training Loss: 7.555e-01 , Validation Loss: 1.470e+00\n",
      "Iteration: 2008, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2009, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2010, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2011, Training Loss: 1.259e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 2012, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2013, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2014, Training Loss: 1.259e+00 , Validation Loss: 2.697e+00\n",
      "Iteration: 2015, Training Loss: 1.007e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 2016, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2017, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2018, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2019, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2020, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2021, Training Loss: 2.267e+00 , Validation Loss: 4.711e+00\n",
      "Iteration: 2022, Training Loss: 2.015e+00 , Validation Loss: 1.518e+00\n",
      "Iteration: 2023, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 2024, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2025, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2026, Training Loss: 2.518e-01 , Validation Loss: 9.616e-01\n",
      "Iteration: 2027, Training Loss: 3.022e+00 , Validation Loss: 4.264e+00\n",
      "Iteration: 2028, Training Loss: 3.022e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 2029, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2030, Training Loss: 1.763e+00 , Validation Loss: 3.502e+00\n",
      "Iteration: 2031, Training Loss: 3.022e+00 , Validation Loss: 1.264e+00\n",
      "Iteration: 2032, Training Loss: 2.770e+00 , Validation Loss: 4.990e+00\n",
      "Iteration: 2033, Training Loss: 6.296e+00 , Validation Loss: 8.062e+00\n",
      "Iteration: 2034, Training Loss: 3.022e+00 , Validation Loss: 4.911e+00\n",
      "Iteration: 2035, Training Loss: 2.770e+00 , Validation Loss: 3.459e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2036, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2037, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2038, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2039, Training Loss: 2.518e-01 , Validation Loss: 1.488e+00\n",
      "Iteration: 2040, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2041, Training Loss: 2.518e+00 , Validation Loss: 3.998e+00\n",
      "Iteration: 2042, Training Loss: 2.267e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 2043, Training Loss: 1.511e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 2044, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2045, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2046, Training Loss: 1.763e+00 , Validation Loss: 2.873e+00\n",
      "Iteration: 2047, Training Loss: 1.511e+00 , Validation Loss: 9.919e-01\n",
      "Iteration: 2048, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 2049, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2050, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2051, Training Loss: 7.555e-01 , Validation Loss: 1.796e+00\n",
      "Iteration: 2052, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2053, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2054, Training Loss: 1.259e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 2055, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2056, Training Loss: 1.259e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 2057, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2058, Training Loss: 2.015e+00 , Validation Loss: 4.040e+00\n",
      "Iteration: 2059, Training Loss: 1.763e+00 , Validation Loss: 2.939e+00\n",
      "Iteration: 2060, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2061, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2062, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2063, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2064, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2065, Training Loss: 2.770e+00 , Validation Loss: 5.213e+00\n",
      "Iteration: 2066, Training Loss: 1.108e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2067, Training Loss: 4.281e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 2068, Training Loss: 3.022e+00 , Validation Loss: 2.383e+00\n",
      "Iteration: 2069, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2070, Training Loss: 1.259e+00 , Validation Loss: 2.141e+00\n",
      "Iteration: 2071, Training Loss: 7.555e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 2072, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2073, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2074, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2075, Training Loss: 1.763e+00 , Validation Loss: 1.458e+00\n",
      "Iteration: 2076, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2077, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2078, Training Loss: 2.015e+00 , Validation Loss: 1.687e+00\n",
      "Iteration: 2079, Training Loss: 5.037e-01 , Validation Loss: 2.595e+00\n",
      "Iteration: 2080, Training Loss: 2.770e+00 , Validation Loss: 1.827e+00\n",
      "Iteration: 2081, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2082, Training Loss: 1.511e+00 , Validation Loss: 2.208e+00\n",
      "Iteration: 2083, Training Loss: 1.259e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 2084, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 2085, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2086, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2087, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2088, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2089, Training Loss: 3.526e+00 , Validation Loss: 4.778e+00\n",
      "Iteration: 2090, Training Loss: 1.184e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2091, Training Loss: 3.274e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 2092, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2093, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 2094, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2095, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2096, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2097, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2098, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2099, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2100, Training Loss: 5.037e-01 , Validation Loss: 1.482e+00\n",
      "Iteration: 2101, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2102, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2103, Training Loss: 7.555e-01 , Validation Loss: 1.191e+00\n",
      "Iteration: 2104, Training Loss: 1.007e+00 , Validation Loss: 1.252e+00\n",
      "Iteration: 2105, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2106, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2107, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2108, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2109, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2110, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2111, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2112, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2113, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2114, Training Loss: 1.763e+00 , Validation Loss: 3.756e+00\n",
      "Iteration: 2115, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 2116, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2117, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 2118, Training Loss: 1.259e+00 , Validation Loss: 2.141e+00\n",
      "Iteration: 2119, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2120, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2121, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2122, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2123, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2124, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2125, Training Loss: 2.518e+00 , Validation Loss: 4.361e+00\n",
      "Iteration: 2126, Training Loss: 1.259e+00 , Validation Loss: 2.365e+00\n",
      "Iteration: 2127, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 2128, Training Loss: 2.267e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2129, Training Loss: 2.770e+00 , Validation Loss: 5.105e+00\n",
      "Iteration: 2130, Training Loss: 1.763e+00 , Validation Loss: 1.693e+00\n",
      "Iteration: 2131, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2132, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2133, Training Loss: 1.007e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 2134, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2135, Training Loss: 1.511e+00 , Validation Loss: 2.564e+00\n",
      "Iteration: 2136, Training Loss: 2.015e+00 , Validation Loss: 1.681e+00\n",
      "Iteration: 2137, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2138, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2139, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2140, Training Loss: 3.022e+00 , Validation Loss: 5.183e+00\n",
      "Iteration: 2141, Training Loss: 7.052e+00 , Validation Loss: 7.784e+00\n",
      "Iteration: 2142, Training Loss: 2.267e+00 , Validation Loss: 4.639e+00\n",
      "Iteration: 2143, Training Loss: 1.259e+00 , Validation Loss: 1.530e+00\n",
      "Iteration: 2144, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2145, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2146, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2147, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2148, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2149, Training Loss: 5.037e-01 , Validation Loss: 1.893e+00\n",
      "Iteration: 2150, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 2151, Training Loss: 1.763e+00 , Validation Loss: 2.703e+00\n",
      "Iteration: 2152, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2153, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2154, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2155, Training Loss: 2.518e+00 , Validation Loss: 4.137e+00\n",
      "Iteration: 2156, Training Loss: 1.007e+00 , Validation Loss: 1.337e+00\n",
      "Iteration: 2157, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2158, Training Loss: 4.030e+00 , Validation Loss: 4.814e+00\n",
      "Iteration: 2159, Training Loss: 1.284e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2160, Training Loss: 1.763e+00 , Validation Loss: 5.099e+00\n",
      "Iteration: 2161, Training Loss: 1.511e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2162, Training Loss: 1.259e+00 , Validation Loss: 1.706e+00\n",
      "Iteration: 2163, Training Loss: 7.555e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 2164, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2165, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2166, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2167, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2168, Training Loss: 1.259e+00 , Validation Loss: 1.566e+00\n",
      "Iteration: 2169, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2170, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 2171, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2172, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2173, Training Loss: 2.518e+00 , Validation Loss: 2.177e+00\n",
      "Iteration: 2174, Training Loss: 3.022e+00 , Validation Loss: 2.522e+00\n",
      "Iteration: 2175, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2176, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2177, Training Loss: 1.259e+00 , Validation Loss: 1.270e+00\n",
      "Iteration: 2178, Training Loss: 1.511e+00 , Validation Loss: 5.443e-01\n",
      "Iteration: 2179, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2180, Training Loss: 1.763e+00 , Validation Loss: 4.572e+00\n",
      "Iteration: 2181, Training Loss: 2.015e+00 , Validation Loss: 1.554e+00\n",
      "Iteration: 2182, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2183, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2184, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2185, Training Loss: 1.511e+00 , Validation Loss: 4.996e+00\n",
      "Iteration: 2186, Training Loss: 1.763e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 2187, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2188, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 2189, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 2190, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2191, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2192, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2193, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2194, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2195, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 2196, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2197, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2198, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2199, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2200, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2201, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2202, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2203, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2204, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2205, Training Loss: 1.511e+00 , Validation Loss: 1.827e+00\n",
      "Iteration: 2206, Training Loss: 1.259e+00 , Validation Loss: 1.040e+00\n",
      "Iteration: 2207, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2208, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2209, Training Loss: 2.518e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 2210, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2211, Training Loss: 3.274e+00 , Validation Loss: 4.572e+00\n",
      "Iteration: 2212, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2213, Training Loss: 4.785e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 2214, Training Loss: 1.763e+00 , Validation Loss: 1.355e+00\n",
      "Iteration: 2215, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2216, Training Loss: 1.259e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 2217, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 2218, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2219, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2220, Training Loss: 1.511e+00 , Validation Loss: 1.591e+00\n",
      "Iteration: 2221, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2222, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2223, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2224, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2225, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2226, Training Loss: 1.763e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2227, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2228, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2229, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2230, Training Loss: 2.015e+00 , Validation Loss: 4.796e+00\n",
      "Iteration: 2231, Training Loss: 1.763e+00 , Validation Loss: 1.397e+00\n",
      "Iteration: 2232, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 2233, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2234, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2235, Training Loss: 3.022e+00 , Validation Loss: 4.899e+00\n",
      "Iteration: 2236, Training Loss: 7.555e+00 , Validation Loss: 1.023e+01\n",
      "Iteration: 2237, Training Loss: 1.007e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 2238, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2239, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2240, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2241, Training Loss: 1.259e+00 , Validation Loss: 1.210e+00\n",
      "Iteration: 2242, Training Loss: 1.007e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 2243, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 2244, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2245, Training Loss: 3.526e+00 , Validation Loss: 4.439e+00\n",
      "Iteration: 2246, Training Loss: 1.108e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2247, Training Loss: 3.274e+00 , Validation Loss: 5.522e+00\n",
      "Iteration: 2248, Training Loss: 1.511e+00 , Validation Loss: 8.588e-01\n",
      "Iteration: 2249, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 2250, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2251, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2252, Training Loss: 1.763e+00 , Validation Loss: 1.911e+00\n",
      "Iteration: 2253, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2254, Training Loss: 1.511e+00 , Validation Loss: 1.560e+00\n",
      "Iteration: 2255, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2256, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2257, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 2258, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 2259, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2260, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2261, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2262, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2263, Training Loss: 2.770e+00 , Validation Loss: 4.584e+00\n",
      "Iteration: 2264, Training Loss: 2.518e+00 , Validation Loss: 2.250e+00\n",
      "Iteration: 2265, Training Loss: 1.259e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 2266, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2267, Training Loss: 7.555e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 2268, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2269, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2270, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2271, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2272, Training Loss: 1.511e+00 , Validation Loss: 1.131e+00\n",
      "Iteration: 2273, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 2274, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2275, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2276, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2277, Training Loss: 3.274e+00 , Validation Loss: 2.734e+00\n",
      "Iteration: 2278, Training Loss: 8.815e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 2279, Training Loss: 3.274e+00 , Validation Loss: 5.467e+00\n",
      "Iteration: 2280, Training Loss: 1.259e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 2281, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2282, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2283, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2284, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2285, Training Loss: 1.007e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 2286, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2287, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2288, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 2289, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2290, Training Loss: 2.518e+00 , Validation Loss: 4.790e+00\n",
      "Iteration: 2291, Training Loss: 2.518e+00 , Validation Loss: 3.000e+00\n",
      "Iteration: 2292, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 2293, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2294, Training Loss: 1.007e+00 , Validation Loss: 1.960e+00\n",
      "Iteration: 2295, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2296, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2297, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2298, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2299, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2300, Training Loss: 1.763e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2301, Training Loss: 4.030e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 2302, Training Loss: 7.807e+00 , Validation Loss: 1.021e+01\n",
      "Iteration: 2303, Training Loss: 2.015e+00 , Validation Loss: 4.778e+00\n",
      "Iteration: 2304, Training Loss: 1.763e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 2305, Training Loss: 1.763e+00 , Validation Loss: 3.332e+00\n",
      "Iteration: 2306, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2307, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2308, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2309, Training Loss: 2.267e+00 , Validation Loss: 2.195e+00\n",
      "Iteration: 2310, Training Loss: 1.259e+00 , Validation Loss: 9.193e-01\n",
      "Iteration: 2311, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2312, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 2313, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2314, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2315, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 2316, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2317, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2318, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2319, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2320, Training Loss: 1.259e+00 , Validation Loss: 2.619e+00\n",
      "Iteration: 2321, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2322, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2323, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2324, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2325, Training Loss: 1.259e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 2326, Training Loss: 3.022e+00 , Validation Loss: 4.488e+00\n",
      "Iteration: 2327, Training Loss: 8.311e+00 , Validation Loss: 8.098e+00\n",
      "Iteration: 2328, Training Loss: 5.037e+00 , Validation Loss: 5.848e+00\n",
      "Iteration: 2329, Training Loss: 1.360e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2330, Training Loss: 3.022e+00 , Validation Loss: 5.855e+00\n",
      "Iteration: 2331, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 2332, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2333, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2334, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2335, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2336, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2337, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2338, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 2339, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2340, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2341, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2342, Training Loss: 3.022e+00 , Validation Loss: 3.877e+00\n",
      "Iteration: 2343, Training Loss: 1.007e+00 , Validation Loss: 1.252e+00\n",
      "Iteration: 2344, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2345, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 2346, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2347, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2348, Training Loss: 1.007e+00 , Validation Loss: 1.827e+00\n",
      "Iteration: 2349, Training Loss: 2.518e+00 , Validation Loss: 2.038e+00\n",
      "Iteration: 2350, Training Loss: 3.526e+00 , Validation Loss: 1.536e+00\n",
      "Iteration: 2351, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2352, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2353, Training Loss: 2.518e+00 , Validation Loss: 3.199e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2354, Training Loss: 2.015e+00 , Validation Loss: 9.133e-01\n",
      "Iteration: 2355, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2356, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2357, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2358, Training Loss: 1.007e+00 , Validation Loss: 1.820e+00\n",
      "Iteration: 2359, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2360, Training Loss: 1.511e+00 , Validation Loss: 1.512e+00\n",
      "Iteration: 2361, Training Loss: 1.007e+00 , Validation Loss: 1.204e+00\n",
      "Iteration: 2362, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2363, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2364, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2365, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2366, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2367, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 2368, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2369, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2370, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2371, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 2372, Training Loss: 1.511e+00 , Validation Loss: 1.572e+00\n",
      "Iteration: 2373, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2374, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 2375, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2376, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2377, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2378, Training Loss: 2.518e+00 , Validation Loss: 3.689e+00\n",
      "Iteration: 2379, Training Loss: 2.518e+00 , Validation Loss: 4.107e+00\n",
      "Iteration: 2380, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2381, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2382, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2383, Training Loss: 1.007e+00 , Validation Loss: 1.270e+00\n",
      "Iteration: 2384, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 2385, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2386, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2387, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2388, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2389, Training Loss: 1.259e+00 , Validation Loss: 2.226e+00\n",
      "Iteration: 2390, Training Loss: 5.037e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 2391, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 2392, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2393, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 2394, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 2395, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2396, Training Loss: 1.763e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 2397, Training Loss: 1.007e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 2398, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 2399, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2400, Training Loss: -1.000e-07 , Validation Loss: 9.919e-01\n",
      "Iteration: 2401, Training Loss: 7.555e-01 , Validation Loss: 9.919e-01\n",
      "Iteration: 2402, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2403, Training Loss: 3.274e+00 , Validation Loss: 3.889e+00\n",
      "Iteration: 2404, Training Loss: 1.360e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2405, Training Loss: 2.770e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 2406, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2407, Training Loss: 1.007e+00 , Validation Loss: 1.639e+00\n",
      "Iteration: 2408, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 2409, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2410, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2411, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2412, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2413, Training Loss: 2.267e+00 , Validation Loss: 1.439e+00\n",
      "Iteration: 2414, Training Loss: 1.259e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 2415, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2416, Training Loss: 7.555e-01 , Validation Loss: 2.703e+00\n",
      "Iteration: 2417, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2418, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2419, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2420, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2421, Training Loss: 1.511e+00 , Validation Loss: 2.885e+00\n",
      "Iteration: 2422, Training Loss: 1.511e+00 , Validation Loss: 6.048e-01\n",
      "Iteration: 2423, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2424, Training Loss: 1.763e+00 , Validation Loss: 2.915e+00\n",
      "Iteration: 2425, Training Loss: 1.763e+00 , Validation Loss: 1.542e+00\n",
      "Iteration: 2426, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2427, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2428, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2429, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2430, Training Loss: 1.763e+00 , Validation Loss: 2.044e+00\n",
      "Iteration: 2431, Training Loss: 1.259e+00 , Validation Loss: 1.046e+00\n",
      "Iteration: 2432, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2433, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2434, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2435, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2436, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2437, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2438, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2439, Training Loss: 3.526e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 2440, Training Loss: 4.533e+00 , Validation Loss: 7.917e+00\n",
      "Iteration: 2441, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 2442, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2443, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2444, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2445, Training Loss: 1.763e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 2446, Training Loss: 1.259e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 2447, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2448, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2449, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2450, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2451, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2452, Training Loss: 2.015e+00 , Validation Loss: 3.423e+00\n",
      "Iteration: 2453, Training Loss: 2.015e+00 , Validation Loss: 1.947e+00\n",
      "Iteration: 2454, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 2455, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2456, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2457, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2458, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2459, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2460, Training Loss: 2.770e+00 , Validation Loss: 4.482e+00\n",
      "Iteration: 2461, Training Loss: 8.059e+00 , Validation Loss: 9.000e+00\n",
      "Iteration: 2462, Training Loss: 3.274e+00 , Validation Loss: 5.582e+00\n",
      "Iteration: 2463, Training Loss: 7.555e-01 , Validation Loss: 1.385e+00\n",
      "Iteration: 2464, Training Loss: 2.015e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 2465, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2466, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2467, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2468, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 2469, Training Loss: 7.555e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 2470, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2471, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2472, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2473, Training Loss: 7.555e-01 , Validation Loss: 1.978e+00\n",
      "Iteration: 2474, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2475, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2476, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2477, Training Loss: 2.015e+00 , Validation Loss: 2.522e+00\n",
      "Iteration: 2478, Training Loss: 4.785e+00 , Validation Loss: 4.463e+00\n",
      "Iteration: 2479, Training Loss: 1.259e+00 , Validation Loss: 9.314e-01\n",
      "Iteration: 2480, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2481, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2482, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2483, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2484, Training Loss: 1.259e+00 , Validation Loss: 1.216e+00\n",
      "Iteration: 2485, Training Loss: 2.518e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 2486, Training Loss: 1.511e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 2487, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2488, Training Loss: 1.007e+00 , Validation Loss: 2.740e+00\n",
      "Iteration: 2489, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2490, Training Loss: 2.770e+00 , Validation Loss: 4.536e+00\n",
      "Iteration: 2491, Training Loss: 5.289e+00 , Validation Loss: 4.917e+00\n",
      "Iteration: 2492, Training Loss: 1.763e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 2493, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2494, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2495, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2496, Training Loss: 1.007e+00 , Validation Loss: 1.591e+00\n",
      "Iteration: 2497, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2498, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2499, Training Loss: 5.037e-01 , Validation Loss: 1.191e+00\n",
      "Iteration: 2500, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 2501, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2502, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 2503, Training Loss: 4.281e+00 , Validation Loss: 5.080e+00\n",
      "Iteration: 2504, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2505, Training Loss: 3.022e+00 , Validation Loss: 5.855e+00\n",
      "Iteration: 2506, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2507, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2508, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 2509, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2510, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2511, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2512, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2513, Training Loss: 5.037e-01 , Validation Loss: 1.125e+00\n",
      "Iteration: 2514, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 2515, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2516, Training Loss: 1.763e+00 , Validation Loss: 4.645e+00\n",
      "Iteration: 2517, Training Loss: 1.763e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 2518, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2519, Training Loss: 1.259e+00 , Validation Loss: 6.048e-01\n",
      "Iteration: 2520, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2521, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 2522, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2523, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2524, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2525, Training Loss: 2.518e+00 , Validation Loss: 4.893e+00\n",
      "Iteration: 2526, Training Loss: 2.518e+00 , Validation Loss: 3.109e+00\n",
      "Iteration: 2527, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2528, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2529, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2530, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2531, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2532, Training Loss: 7.555e-01 , Validation Loss: 1.125e+00\n",
      "Iteration: 2533, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2534, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2535, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2536, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2537, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2538, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2539, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 2540, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2541, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2542, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2543, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2544, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2545, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2546, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2547, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2548, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2549, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2550, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2551, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 2552, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 2553, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 2554, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 2555, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2556, Training Loss: 2.267e+00 , Validation Loss: 3.466e+00\n",
      "Iteration: 2557, Training Loss: 7.555e-01 , Validation Loss: 1.790e+00\n",
      "Iteration: 2558, Training Loss: 1.511e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 2559, Training Loss: 2.267e+00 , Validation Loss: 4.101e+00\n",
      "Iteration: 2560, Training Loss: 1.259e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 2561, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2562, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2563, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2564, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2565, Training Loss: 2.267e+00 , Validation Loss: 2.528e+00\n",
      "Iteration: 2566, Training Loss: 5.792e+00 , Validation Loss: 6.078e+00\n",
      "Iteration: 2567, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2568, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2569, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2570, Training Loss: 4.785e+00 , Validation Loss: 4.917e+00\n",
      "Iteration: 2571, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2572, Training Loss: 3.526e+00 , Validation Loss: 5.824e+00\n",
      "Iteration: 2573, Training Loss: 1.511e+00 , Validation Loss: 9.556e-01\n",
      "Iteration: 2574, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2575, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2576, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 2577, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2578, Training Loss: 5.037e-01 , Validation Loss: 1.119e+00\n",
      "Iteration: 2579, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2580, Training Loss: 5.037e-01 , Validation Loss: 1.264e+00\n",
      "Iteration: 2581, Training Loss: 1.007e+00 , Validation Loss: 7.681e-01\n",
      "Iteration: 2582, Training Loss: 1.007e+00 , Validation Loss: 3.320e+00\n",
      "Iteration: 2583, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 2584, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2585, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2586, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2587, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2588, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 2589, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2590, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2591, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 2592, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2593, Training Loss: 7.555e-01 , Validation Loss: 1.433e+00\n",
      "Iteration: 2594, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2595, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2596, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2597, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2598, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2599, Training Loss: 1.007e+00 , Validation Loss: 3.919e+00\n",
      "Iteration: 2600, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2601, Training Loss: 1.259e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 2602, Training Loss: 1.007e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 2603, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2604, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2605, Training Loss: 5.037e-01 , Validation Loss: 2.945e+00\n",
      "Iteration: 2606, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2607, Training Loss: 1.007e+00 , Validation Loss: 1.028e+00\n",
      "Iteration: 2608, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2609, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2610, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 2611, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2612, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2613, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2614, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2615, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2616, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2617, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2618, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2619, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2620, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2621, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2622, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2623, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2624, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2625, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2626, Training Loss: 1.007e+00 , Validation Loss: 3.478e+00\n",
      "Iteration: 2627, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 2628, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2629, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2630, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2631, Training Loss: 3.274e+00 , Validation Loss: 4.566e+00\n",
      "Iteration: 2632, Training Loss: 1.083e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2633, Training Loss: 1.511e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 2634, Training Loss: 7.555e-01 , Validation Loss: 1.300e+00\n",
      "Iteration: 2635, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2636, Training Loss: 7.555e-01 , Validation Loss: 1.131e+00\n",
      "Iteration: 2637, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 2638, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2639, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2640, Training Loss: 1.763e+00 , Validation Loss: 2.389e+00\n",
      "Iteration: 2641, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2642, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 2643, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2644, Training Loss: 1.763e+00 , Validation Loss: 2.595e+00\n",
      "Iteration: 2645, Training Loss: 1.007e+00 , Validation Loss: 8.891e-01\n",
      "Iteration: 2646, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2647, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2648, Training Loss: 7.555e-01 , Validation Loss: 1.506e+00\n",
      "Iteration: 2649, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 2650, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2651, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 2652, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2653, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2654, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2655, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 2656, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 2657, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2658, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2659, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2660, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2661, Training Loss: 2.518e-01 , Validation Loss: 1.802e+00\n",
      "Iteration: 2662, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2663, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2664, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 2665, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 2666, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 2667, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2668, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2669, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2670, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2671, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2672, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2673, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2674, Training Loss: 2.267e+00 , Validation Loss: 3.514e+00\n",
      "Iteration: 2675, Training Loss: 1.259e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 2676, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2677, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2678, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2679, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2680, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2681, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2682, Training Loss: 1.007e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 2683, Training Loss: 7.555e-01 , Validation Loss: 3.484e+00\n",
      "Iteration: 2684, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2685, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2686, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 2687, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2688, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2689, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 2690, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2691, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2692, Training Loss: 1.511e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 2693, Training Loss: 3.778e+00 , Validation Loss: 5.594e+00\n",
      "Iteration: 2694, Training Loss: 8.563e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 2695, Training Loss: 3.778e+00 , Validation Loss: 5.830e+00\n",
      "Iteration: 2696, Training Loss: 7.807e+00 , Validation Loss: 6.768e+00\n",
      "Iteration: 2697, Training Loss: 2.770e+00 , Validation Loss: 3.351e+00\n",
      "Iteration: 2698, Training Loss: 1.763e+00 , Validation Loss: 2.413e+00\n",
      "Iteration: 2699, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 2700, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2701, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2702, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2703, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2704, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2705, Training Loss: 2.267e+00 , Validation Loss: 3.266e+00\n",
      "Iteration: 2706, Training Loss: 1.007e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 2707, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2708, Training Loss: 1.763e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2709, Training Loss: 2.267e+00 , Validation Loss: 4.814e+00\n",
      "Iteration: 2710, Training Loss: 1.259e+00 , Validation Loss: 1.712e+00\n",
      "Iteration: 2711, Training Loss: 7.555e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 2712, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 2713, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2714, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2715, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2716, Training Loss: 3.526e+00 , Validation Loss: 4.415e+00\n",
      "Iteration: 2717, Training Loss: 1.511e+00 , Validation Loss: 4.016e+00\n",
      "Iteration: 2718, Training Loss: 1.511e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 2719, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2720, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2721, Training Loss: 1.511e+00 , Validation Loss: 2.238e+00\n",
      "Iteration: 2722, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2723, Training Loss: 5.037e-01 , Validation Loss: 3.599e+00\n",
      "Iteration: 2724, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2725, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2726, Training Loss: 7.555e-01 , Validation Loss: 1.591e+00\n",
      "Iteration: 2727, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2728, Training Loss: 1.259e+00 , Validation Loss: 1.802e+00\n",
      "Iteration: 2729, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2730, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2731, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2732, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2733, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2734, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2735, Training Loss: 4.533e+00 , Validation Loss: 4.645e+00\n",
      "Iteration: 2736, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2737, Training Loss: 3.022e+00 , Validation Loss: 5.068e+00\n",
      "Iteration: 2738, Training Loss: 1.259e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 2739, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2740, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2741, Training Loss: 7.555e-01 , Validation Loss: 1.125e+00\n",
      "Iteration: 2742, Training Loss: 3.274e+00 , Validation Loss: 4.627e+00\n",
      "Iteration: 2743, Training Loss: 1.007e+00 , Validation Loss: 1.318e+00\n",
      "Iteration: 2744, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2745, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2746, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2747, Training Loss: 2.518e-01 , Validation Loss: 1.270e+00\n",
      "Iteration: 2748, Training Loss: 1.259e+00 , Validation Loss: 2.062e+00\n",
      "Iteration: 2749, Training Loss: 1.007e+00 , Validation Loss: 4.778e-01\n",
      "Iteration: 2750, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 2751, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2752, Training Loss: 1.259e+00 , Validation Loss: 1.566e+00\n",
      "Iteration: 2753, Training Loss: 1.511e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 2754, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 2755, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2756, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2757, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2758, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 2759, Training Loss: 3.778e+00 , Validation Loss: 5.008e+00\n",
      "Iteration: 2760, Training Loss: 5.289e+00 , Validation Loss: 4.609e+00\n",
      "Iteration: 2761, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2762, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2763, Training Loss: 5.037e-01 , Validation Loss: 1.131e+00\n",
      "Iteration: 2764, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2765, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2766, Training Loss: 2.267e+00 , Validation Loss: 2.081e+00\n",
      "Iteration: 2767, Training Loss: 2.015e+00 , Validation Loss: 5.141e-01\n",
      "Iteration: 2768, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2769, Training Loss: 2.518e+00 , Validation Loss: 2.964e+00\n",
      "Iteration: 2770, Training Loss: 1.763e+00 , Validation Loss: 2.861e+00\n",
      "Iteration: 2771, Training Loss: 2.267e+00 , Validation Loss: 6.653e-01\n",
      "Iteration: 2772, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2773, Training Loss: 2.518e+00 , Validation Loss: 3.949e+00\n",
      "Iteration: 2774, Training Loss: 2.770e+00 , Validation Loss: 1.839e+00\n",
      "Iteration: 2775, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2776, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2777, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 2778, Training Loss: 2.518e+00 , Validation Loss: 2.812e+00\n",
      "Iteration: 2779, Training Loss: 2.015e+00 , Validation Loss: 3.078e+00\n",
      "Iteration: 2780, Training Loss: 1.007e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 2781, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2782, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2783, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2784, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2785, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2786, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 2787, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 2788, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2789, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2790, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2791, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2792, Training Loss: 1.511e+00 , Validation Loss: 2.093e+00\n",
      "Iteration: 2793, Training Loss: 2.770e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 2794, Training Loss: 4.030e+00 , Validation Loss: 5.588e+00\n",
      "Iteration: 2795, Training Loss: 6.044e+00 , Validation Loss: 1.017e+01\n",
      "Iteration: 2796, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2797, Training Loss: 1.007e+00 , Validation Loss: 1.603e+00\n",
      "Iteration: 2798, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2799, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2800, Training Loss: 2.015e+00 , Validation Loss: 3.883e+00\n",
      "Iteration: 2801, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2802, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 2803, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2804, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2805, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2806, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2807, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2808, Training Loss: 5.037e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 2809, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2810, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2811, Training Loss: 5.037e-01 , Validation Loss: 1.222e+00\n",
      "Iteration: 2812, Training Loss: 2.015e+00 , Validation Loss: 1.772e+00\n",
      "Iteration: 2813, Training Loss: 2.518e+00 , Validation Loss: 2.292e+00\n",
      "Iteration: 2814, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2815, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2816, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2817, Training Loss: 1.007e+00 , Validation Loss: 1.845e+00\n",
      "Iteration: 2818, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 2819, Training Loss: 2.015e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 2820, Training Loss: 1.007e+00 , Validation Loss: 3.218e+00\n",
      "Iteration: 2821, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2822, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2823, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2824, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 2825, Training Loss: 1.763e+00 , Validation Loss: 4.191e+00\n",
      "Iteration: 2826, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 2827, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2828, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2829, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2830, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2831, Training Loss: 1.511e+00 , Validation Loss: 3.441e+00\n",
      "Iteration: 2832, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2833, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2834, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 2835, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 2836, Training Loss: 7.555e-01 , Validation Loss: 2.746e+00\n",
      "Iteration: 2837, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 2838, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 2839, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 2840, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 2841, Training Loss: 2.518e-01 , Validation Loss: 1.615e+00\n",
      "Iteration: 2842, Training Loss: 2.770e+00 , Validation Loss: 3.907e+00\n",
      "Iteration: 2843, Training Loss: 3.274e+00 , Validation Loss: 4.040e+00\n",
      "Iteration: 2844, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2845, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 2846, Training Loss: 2.770e+00 , Validation Loss: 4.790e+00\n",
      "Iteration: 2847, Training Loss: 2.770e+00 , Validation Loss: 3.363e+00\n",
      "Iteration: 2848, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 2849, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2850, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2851, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 2852, Training Loss: 1.259e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 2853, Training Loss: 1.259e+00 , Validation Loss: 2.552e+00\n",
      "Iteration: 2854, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2855, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 2856, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2857, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2858, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2859, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2860, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2861, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2862, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 2863, Training Loss: 3.778e+00 , Validation Loss: 4.300e+00\n",
      "Iteration: 2864, Training Loss: 1.184e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2865, Training Loss: 3.526e+00 , Validation Loss: 5.498e+00\n",
      "Iteration: 2866, Training Loss: 1.259e+00 , Validation Loss: 1.572e+00\n",
      "Iteration: 2867, Training Loss: 1.511e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 2868, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 2869, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2870, Training Loss: 2.518e+00 , Validation Loss: 2.171e+00\n",
      "Iteration: 2871, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2872, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2873, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 2874, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2875, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2876, Training Loss: 3.274e+00 , Validation Loss: 2.667e+00\n",
      "Iteration: 2877, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2878, Training Loss: 2.770e+00 , Validation Loss: 5.334e+00\n",
      "Iteration: 2879, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 2880, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2881, Training Loss: 7.555e-01 , Validation Loss: 1.119e+00\n",
      "Iteration: 2882, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2883, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2884, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2885, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2886, Training Loss: 7.555e-01 , Validation Loss: 1.833e+00\n",
      "Iteration: 2887, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2888, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2889, Training Loss: 5.037e-01 , Validation Loss: 1.923e+00\n",
      "Iteration: 2890, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2891, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2892, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2893, Training Loss: 7.555e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 2894, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2895, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2896, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 2897, Training Loss: 1.259e+00 , Validation Loss: 2.099e+00\n",
      "Iteration: 2898, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2899, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2900, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2901, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2902, Training Loss: 2.518e+00 , Validation Loss: 3.865e+00\n",
      "Iteration: 2903, Training Loss: 2.015e+00 , Validation Loss: 1.778e+00\n",
      "Iteration: 2904, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2905, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2906, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2907, Training Loss: 3.022e+00 , Validation Loss: 2.915e+00\n",
      "Iteration: 2908, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2909, Training Loss: 3.526e+00 , Validation Loss: 5.855e+00\n",
      "Iteration: 2910, Training Loss: 1.763e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 2911, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2912, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2913, Training Loss: 1.007e+00 , Validation Loss: 1.512e+00\n",
      "Iteration: 2914, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2915, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2916, Training Loss: 5.037e-01 , Validation Loss: 2.008e+00\n",
      "Iteration: 2917, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2918, Training Loss: 1.511e+00 , Validation Loss: 1.772e+00\n",
      "Iteration: 2919, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2920, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2921, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2922, Training Loss: 1.007e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 2923, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 2924, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2925, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2926, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2927, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2928, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2929, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 2930, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2931, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2932, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2933, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2934, Training Loss: 4.281e+00 , Validation Loss: 4.814e+00\n",
      "Iteration: 2935, Training Loss: 1.360e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 2936, Training Loss: 3.274e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 2937, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2938, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2939, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2940, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2941, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2942, Training Loss: 2.518e-01 , Validation Loss: 1.385e+00\n",
      "Iteration: 2943, Training Loss: 2.267e+00 , Validation Loss: 2.552e+00\n",
      "Iteration: 2944, Training Loss: 1.763e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 2945, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2946, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 2947, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2948, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2949, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2950, Training Loss: 2.015e+00 , Validation Loss: 2.558e+00\n",
      "Iteration: 2951, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 2952, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2953, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2954, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2955, Training Loss: 1.511e+00 , Validation Loss: 1.917e+00\n",
      "Iteration: 2956, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2957, Training Loss: 1.007e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 2958, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2959, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2960, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2961, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 2962, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2963, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2964, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2965, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 2966, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2967, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2968, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2969, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2970, Training Loss: 3.274e+00 , Validation Loss: 4.040e+00\n",
      "Iteration: 2971, Training Loss: 8.311e+00 , Validation Loss: 8.612e+00\n",
      "Iteration: 2972, Training Loss: 4.533e+00 , Validation Loss: 5.788e+00\n",
      "Iteration: 2973, Training Loss: 8.815e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 2974, Training Loss: 1.511e+00 , Validation Loss: 1.808e+00\n",
      "Iteration: 2975, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 2976, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 2977, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2978, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2979, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2980, Training Loss: 2.518e+00 , Validation Loss: 2.195e+00\n",
      "Iteration: 2981, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 2982, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2983, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 2984, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2985, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2986, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2987, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 2988, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2989, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2990, Training Loss: 3.778e+00 , Validation Loss: 4.778e+00\n",
      "Iteration: 2991, Training Loss: 4.030e+00 , Validation Loss: 4.330e+00\n",
      "Iteration: 2992, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 2993, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2994, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2995, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 2996, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 2997, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 2998, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 2999, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3000, Training Loss: 5.037e-01 , Validation Loss: 2.020e+00\n",
      "Iteration: 3001, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3002, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 3003, Training Loss: 3.526e+00 , Validation Loss: 4.917e+00\n",
      "Iteration: 3004, Training Loss: 3.526e+00 , Validation Loss: 4.306e+00\n",
      "Iteration: 3005, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3006, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3007, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3008, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3009, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3010, Training Loss: 2.267e+00 , Validation Loss: 4.657e+00\n",
      "Iteration: 3011, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 3012, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 3013, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3014, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 3015, Training Loss: 7.555e-01 , Validation Loss: 2.093e+00\n",
      "Iteration: 3016, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3017, Training Loss: 1.259e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 3018, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 3019, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 3020, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3021, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3022, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3023, Training Loss: 1.007e+00 , Validation Loss: 3.236e+00\n",
      "Iteration: 3024, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 3025, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3026, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3027, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3028, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3029, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3030, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3031, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3032, Training Loss: 5.037e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 3033, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3034, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 3035, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 3036, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3037, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3038, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3039, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3040, Training Loss: 2.267e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3041, Training Loss: 3.526e+00 , Validation Loss: 4.082e+00\n",
      "Iteration: 3042, Training Loss: 1.108e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 3043, Training Loss: 3.022e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 3044, Training Loss: 1.511e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 3045, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3046, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3047, Training Loss: 5.037e-01 , Validation Loss: 1.355e+00\n",
      "Iteration: 3048, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3049, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3050, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3051, Training Loss: 7.555e-01 , Validation Loss: 2.407e+00\n",
      "Iteration: 3052, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3053, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 3054, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3055, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3056, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3057, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3058, Training Loss: 3.274e+00 , Validation Loss: 4.693e+00\n",
      "Iteration: 3059, Training Loss: 2.015e+00 , Validation Loss: 3.296e+00\n",
      "Iteration: 3060, Training Loss: 1.259e+00 , Validation Loss: 8.225e-01\n",
      "Iteration: 3061, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3062, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 3063, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3064, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3065, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3066, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3067, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3068, Training Loss: 3.022e+00 , Validation Loss: 4.881e+00\n",
      "Iteration: 3069, Training Loss: 1.360e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 3070, Training Loss: 4.030e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 3071, Training Loss: 1.259e+00 , Validation Loss: 1.433e+00\n",
      "Iteration: 3072, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 3073, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 3074, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 3075, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3076, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 3077, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3078, Training Loss: 7.555e-01 , Validation Loss: 1.343e+00\n",
      "Iteration: 3079, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3080, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3081, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3082, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3083, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3084, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3085, Training Loss: 1.259e+00 , Validation Loss: 2.123e+00\n",
      "Iteration: 3086, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3087, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3088, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3089, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3090, Training Loss: 2.015e+00 , Validation Loss: 3.435e+00\n",
      "Iteration: 3091, Training Loss: 1.259e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 3092, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3093, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 3094, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 3095, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3096, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 3097, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3098, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3099, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3100, Training Loss: 2.015e+00 , Validation Loss: 3.326e+00\n",
      "Iteration: 3101, Training Loss: 7.555e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 3102, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 3103, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3104, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3105, Training Loss: 3.778e+00 , Validation Loss: 3.937e+00\n",
      "Iteration: 3106, Training Loss: 2.770e+00 , Validation Loss: 2.226e+00\n",
      "Iteration: 3107, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3108, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3109, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3110, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3111, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3112, Training Loss: 1.763e+00 , Validation Loss: 2.443e+00\n",
      "Iteration: 3113, Training Loss: 1.007e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 3114, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 3115, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3116, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3117, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3118, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3119, Training Loss: 2.267e+00 , Validation Loss: 3.272e+00\n",
      "Iteration: 3120, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 3121, Training Loss: 1.259e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 3122, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3123, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3124, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 3125, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3126, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3127, Training Loss: -1.000e-07 , Validation Loss: 7.742e-01\n",
      "Iteration: 3128, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 3129, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 3130, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3131, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3132, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3133, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3134, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3135, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3136, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3137, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 3138, Training Loss: 2.518e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 3139, Training Loss: 3.274e+00 , Validation Loss: 1.784e+00\n",
      "Iteration: 3140, Training Loss: 2.518e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 3141, Training Loss: 5.792e+00 , Validation Loss: 4.953e+00\n",
      "Iteration: 3142, Training Loss: 1.007e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 3143, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3144, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3145, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3146, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3147, Training Loss: 1.511e+00 , Validation Loss: 1.718e+00\n",
      "Iteration: 3148, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 3149, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 3150, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 3151, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3152, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3153, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3154, Training Loss: 7.555e-01 , Validation Loss: 1.107e+00\n",
      "Iteration: 3155, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3156, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3157, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3158, Training Loss: 2.267e+00 , Validation Loss: 4.439e+00\n",
      "Iteration: 3159, Training Loss: 2.518e+00 , Validation Loss: 2.099e+00\n",
      "Iteration: 3160, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3161, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3162, Training Loss: 1.763e+00 , Validation Loss: 3.085e+00\n",
      "Iteration: 3163, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 3164, Training Loss: 1.259e+00 , Validation Loss: 4.355e-01\n",
      "Iteration: 3165, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3166, Training Loss: 1.259e+00 , Validation Loss: 2.909e+00\n",
      "Iteration: 3167, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3168, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 3169, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3170, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3171, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3172, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 3173, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3174, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3175, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3176, Training Loss: 2.015e+00 , Validation Loss: 3.653e+00\n",
      "Iteration: 3177, Training Loss: 1.007e+00 , Validation Loss: 6.653e-01\n",
      "Iteration: 3178, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 3179, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3180, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3181, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3182, Training Loss: 2.267e+00 , Validation Loss: 3.744e+00\n",
      "Iteration: 3183, Training Loss: 3.022e+00 , Validation Loss: 3.538e+00\n",
      "Iteration: 3184, Training Loss: -1.000e-07 , Validation Loss: 6.411e-01\n",
      "Iteration: 3185, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 3186, Training Loss: 3.022e+00 , Validation Loss: 4.681e+00\n",
      "Iteration: 3187, Training Loss: 1.763e+00 , Validation Loss: 3.326e+00\n",
      "Iteration: 3188, Training Loss: 1.007e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 3189, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 3190, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3191, Training Loss: 2.518e+00 , Validation Loss: 4.119e+00\n",
      "Iteration: 3192, Training Loss: 7.304e+00 , Validation Loss: 1.020e+01\n",
      "Iteration: 3193, Training Loss: 2.015e+00 , Validation Loss: 3.308e+00\n",
      "Iteration: 3194, Training Loss: 1.007e+00 , Validation Loss: 1.137e+00\n",
      "Iteration: 3195, Training Loss: 1.007e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 3196, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3197, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3198, Training Loss: 2.267e+00 , Validation Loss: 3.828e+00\n",
      "Iteration: 3199, Training Loss: 1.007e+00 , Validation Loss: 9.677e-01\n",
      "Iteration: 3200, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 3201, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 3202, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3203, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3204, Training Loss: 2.267e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 3205, Training Loss: 2.518e+00 , Validation Loss: 2.407e+00\n",
      "Iteration: 3206, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3207, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3208, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3209, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 3210, Training Loss: 1.259e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 3211, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3212, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3213, Training Loss: 1.007e+00 , Validation Loss: 1.427e+00\n",
      "Iteration: 3214, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3215, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3216, Training Loss: 2.518e+00 , Validation Loss: 3.599e+00\n",
      "Iteration: 3217, Training Loss: 1.511e+00 , Validation Loss: 2.788e+00\n",
      "Iteration: 3218, Training Loss: 1.511e+00 , Validation Loss: 9.012e-01\n",
      "Iteration: 3219, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3220, Training Loss: -1.000e-07 , Validation Loss: 6.895e-01\n",
      "Iteration: 3221, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 3222, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 3223, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 3224, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3225, Training Loss: -1.000e-07 , Validation Loss: 8.286e-01\n",
      "Iteration: 3226, Training Loss: -1.000e-07 , Validation Loss: 8.286e-01\n",
      "Iteration: 3227, Training Loss: -1.000e-07 , Validation Loss: 8.286e-01\n",
      "Iteration: 3228, Training Loss: 7.555e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 3229, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 3230, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 3231, Training Loss: 1.007e+00 , Validation Loss: 7.742e-01\n",
      "Iteration: 3232, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3233, Training Loss: -1.000e-07 , Validation Loss: 7.983e-01\n",
      "Iteration: 3234, Training Loss: 2.518e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 3235, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 3236, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3237, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 3238, Training Loss: 3.022e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 3239, Training Loss: 3.778e+00 , Validation Loss: 3.901e+00\n",
      "Iteration: 3240, Training Loss: 2.518e-01 , Validation Loss: 9.979e-01\n",
      "Iteration: 3241, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 3242, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3243, Training Loss: 1.259e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 3244, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3245, Training Loss: 7.555e-01 , Validation Loss: 1.748e+00\n",
      "Iteration: 3246, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3247, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3248, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3249, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3250, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3251, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3252, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3253, Training Loss: 1.259e+00 , Validation Loss: 1.706e+00\n",
      "Iteration: 3254, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3255, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3256, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3257, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 3258, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 3259, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3260, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3261, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3262, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3263, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3264, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3265, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3266, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3267, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3268, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 3269, Training Loss: 1.007e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 3270, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3271, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3272, Training Loss: -1.000e-07 , Validation Loss: 7.923e-01\n",
      "Iteration: 3273, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 3274, Training Loss: 7.555e-01 , Validation Loss: 2.855e+00\n",
      "Iteration: 3275, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3276, Training Loss: 7.555e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 3277, Training Loss: 1.511e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 3278, Training Loss: 1.007e+00 , Validation Loss: 1.149e+00\n",
      "Iteration: 3279, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3280, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3281, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 3282, Training Loss: 2.267e+00 , Validation Loss: 3.574e+00\n",
      "Iteration: 3283, Training Loss: 2.015e+00 , Validation Loss: 1.216e+00\n",
      "Iteration: 3284, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3285, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 3286, Training Loss: 5.037e-01 , Validation Loss: 1.524e+00\n",
      "Iteration: 3287, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3288, Training Loss: 2.267e+00 , Validation Loss: 3.877e+00\n",
      "Iteration: 3289, Training Loss: 2.015e+00 , Validation Loss: 2.044e+00\n",
      "Iteration: 3290, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3291, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 3292, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 3293, Training Loss: 1.007e+00 , Validation Loss: 1.318e+00\n",
      "Iteration: 3294, Training Loss: 5.037e-01 , Validation Loss: 3.459e+00\n",
      "Iteration: 3295, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3296, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 3297, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 3298, Training Loss: 7.555e-01 , Validation Loss: 9.193e-01\n",
      "Iteration: 3299, Training Loss: 3.274e+00 , Validation Loss: 4.349e+00\n",
      "Iteration: 3300, Training Loss: 1.083e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 3301, Training Loss: 2.518e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 3302, Training Loss: -1.000e-07 , Validation Loss: 4.597e-01\n",
      "Iteration: 3303, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 3304, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3305, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3306, Training Loss: 5.037e-01 , Validation Loss: 9.556e-01\n",
      "Iteration: 3307, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3308, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3309, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3310, Training Loss: 1.259e+00 , Validation Loss: 4.899e-01\n",
      "Iteration: 3311, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3312, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 3313, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 3314, Training Loss: 2.518e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 3315, Training Loss: 6.044e+00 , Validation Loss: 5.588e+00\n",
      "Iteration: 3316, Training Loss: 5.037e-01 , Validation Loss: 3.998e+00\n",
      "Iteration: 3317, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3318, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3319, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3320, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3321, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3322, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3323, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3324, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3325, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3326, Training Loss: 7.555e-01 , Validation Loss: 2.867e+00\n",
      "Iteration: 3327, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3328, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3329, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3330, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3331, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3332, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3333, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 3334, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3335, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3336, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3337, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3338, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3339, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3340, Training Loss: 3.778e+00 , Validation Loss: 5.086e+00\n",
      "Iteration: 3341, Training Loss: 4.030e+00 , Validation Loss: 4.421e+00\n",
      "Iteration: 3342, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 3343, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3344, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3345, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3346, Training Loss: 1.763e+00 , Validation Loss: 1.355e+00\n",
      "Iteration: 3347, Training Loss: 1.259e+00 , Validation Loss: 5.020e-01\n",
      "Iteration: 3348, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3349, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3350, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3351, Training Loss: 1.763e+00 , Validation Loss: 2.855e+00\n",
      "Iteration: 3352, Training Loss: 7.555e-01 , Validation Loss: 8.830e-01\n",
      "Iteration: 3353, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3354, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3355, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3356, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3357, Training Loss: 3.274e+00 , Validation Loss: 4.972e+00\n",
      "Iteration: 3358, Training Loss: 3.022e+00 , Validation Loss: 2.921e+00\n",
      "Iteration: 3359, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3360, Training Loss: 3.022e+00 , Validation Loss: 4.409e+00\n",
      "Iteration: 3361, Training Loss: 7.555e-01 , Validation Loss: 9.435e-01\n",
      "Iteration: 3362, Training Loss: 1.259e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 3363, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3364, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3365, Training Loss: 2.015e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3366, Training Loss: 3.022e+00 , Validation Loss: 4.155e+00\n",
      "Iteration: 3367, Training Loss: 5.289e+00 , Validation Loss: 7.989e+00\n",
      "Iteration: 3368, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 3369, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3370, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3371, Training Loss: 1.763e+00 , Validation Loss: 1.282e+00\n",
      "Iteration: 3372, Training Loss: 1.511e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 3373, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3374, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3375, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3376, Training Loss: -1.000e-07 , Validation Loss: 9.737e-01\n",
      "Iteration: 3377, Training Loss: 2.518e-01 , Validation Loss: 9.737e-01\n",
      "Iteration: 3378, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3379, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 3380, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3381, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3382, Training Loss: 2.267e+00 , Validation Loss: 4.004e+00\n",
      "Iteration: 3383, Training Loss: 1.007e+00 , Validation Loss: 1.089e+00\n",
      "Iteration: 3384, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3385, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 3386, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3387, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3388, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3389, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 3390, Training Loss: 1.007e+00 , Validation Loss: 2.256e+00\n",
      "Iteration: 3391, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3392, Training Loss: 2.267e+00 , Validation Loss: 3.532e+00\n",
      "Iteration: 3393, Training Loss: 2.267e+00 , Validation Loss: 1.137e+00\n",
      "Iteration: 3394, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3395, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3396, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 3397, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 3398, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 3399, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 3400, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 3401, Training Loss: 2.015e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 3402, Training Loss: 1.259e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 3403, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3404, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3405, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 3406, Training Loss: 3.274e+00 , Validation Loss: 4.312e+00\n",
      "Iteration: 3407, Training Loss: 6.044e+00 , Validation Loss: 8.618e+00\n",
      "Iteration: 3408, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3409, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3410, Training Loss: 2.015e+00 , Validation Loss: 4.324e+00\n",
      "Iteration: 3411, Training Loss: 1.259e+00 , Validation Loss: 8.346e-01\n",
      "Iteration: 3412, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 3413, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3414, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3415, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 3416, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3417, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 3418, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3419, Training Loss: 2.015e+00 , Validation Loss: 4.590e+00\n",
      "Iteration: 3420, Training Loss: 1.763e+00 , Validation Loss: 1.675e+00\n",
      "Iteration: 3421, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3422, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 3423, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3424, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3425, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3426, Training Loss: 2.015e+00 , Validation Loss: 3.635e+00\n",
      "Iteration: 3427, Training Loss: 1.511e+00 , Validation Loss: 9.919e-01\n",
      "Iteration: 3428, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 3429, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3430, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3431, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3432, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3433, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3434, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3435, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3436, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3437, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3438, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 3439, Training Loss: 2.267e+00 , Validation Loss: 4.978e+00\n",
      "Iteration: 3440, Training Loss: 1.511e+00 , Validation Loss: 1.760e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3441, Training Loss: -1.000e-07 , Validation Loss: 4.838e-01\n",
      "Iteration: 3442, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 3443, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3444, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 3445, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3446, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3447, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3448, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3449, Training Loss: 2.267e+00 , Validation Loss: 3.254e+00\n",
      "Iteration: 3450, Training Loss: 1.763e+00 , Validation Loss: 3.574e+00\n",
      "Iteration: 3451, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 3452, Training Loss: 7.555e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 3453, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 3454, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3455, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 3456, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3457, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3458, Training Loss: 1.763e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3459, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3460, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3461, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3462, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3463, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 3464, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3465, Training Loss: 1.007e+00 , Validation Loss: 2.002e+00\n",
      "Iteration: 3466, Training Loss: 2.518e+00 , Validation Loss: 3.943e+00\n",
      "Iteration: 3467, Training Loss: 2.518e+00 , Validation Loss: 3.363e+00\n",
      "Iteration: 3468, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 3469, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 3470, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3471, Training Loss: 2.015e+00 , Validation Loss: 3.605e+00\n",
      "Iteration: 3472, Training Loss: 2.267e+00 , Validation Loss: 1.675e+00\n",
      "Iteration: 3473, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3474, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3475, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3476, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3477, Training Loss: 2.267e+00 , Validation Loss: 3.677e+00\n",
      "Iteration: 3478, Training Loss: 4.030e+00 , Validation Loss: 4.107e+00\n",
      "Iteration: 3479, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3480, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 3481, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3482, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3483, Training Loss: 2.267e+00 , Validation Loss: 3.641e+00\n",
      "Iteration: 3484, Training Loss: 2.015e+00 , Validation Loss: 4.010e+00\n",
      "Iteration: 3485, Training Loss: 7.555e-01 , Validation Loss: 9.677e-01\n",
      "Iteration: 3486, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3487, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3488, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3489, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3490, Training Loss: 3.022e+00 , Validation Loss: 5.201e+00\n",
      "Iteration: 3491, Training Loss: 1.259e+00 , Validation Loss: 2.304e+00\n",
      "Iteration: 3492, Training Loss: 1.007e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 3493, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3494, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3495, Training Loss: 7.555e-01 , Validation Loss: 2.673e+00\n",
      "Iteration: 3496, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3497, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3498, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3499, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3500, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3501, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3502, Training Loss: 7.555e-01 , Validation Loss: 3.828e+00\n",
      "Iteration: 3503, Training Loss: 1.259e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 3504, Training Loss: 2.267e+00 , Validation Loss: 4.046e+00\n",
      "Iteration: 3505, Training Loss: 2.518e+00 , Validation Loss: 2.734e+00\n",
      "Iteration: 3506, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 3507, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 3508, Training Loss: 1.259e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 3509, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 3510, Training Loss: 2.267e+00 , Validation Loss: 3.804e+00\n",
      "Iteration: 3511, Training Loss: 1.007e+00 , Validation Loss: 9.254e-01\n",
      "Iteration: 3512, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3513, Training Loss: 1.511e+00 , Validation Loss: 1.228e+00\n",
      "Iteration: 3514, Training Loss: 1.763e+00 , Validation Loss: 3.889e+00\n",
      "Iteration: 3515, Training Loss: 7.555e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 3516, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 3517, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3518, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3519, Training Loss: 1.511e+00 , Validation Loss: 1.597e+00\n",
      "Iteration: 3520, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 3521, Training Loss: -1.000e-07 , Validation Loss: 7.318e-01\n",
      "Iteration: 3522, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 3523, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3524, Training Loss: 2.770e+00 , Validation Loss: 4.179e+00\n",
      "Iteration: 3525, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 3526, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3527, Training Loss: 2.015e+00 , Validation Loss: 4.010e+00\n",
      "Iteration: 3528, Training Loss: 2.015e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 3529, Training Loss: 1.259e+00 , Validation Loss: 3.883e+00\n",
      "Iteration: 3530, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 3531, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 3532, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3533, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3534, Training Loss: 5.037e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 3535, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3536, Training Loss: 1.511e+00 , Validation Loss: 5.189e+00\n",
      "Iteration: 3537, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3538, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 3539, Training Loss: 5.037e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 3540, Training Loss: 1.007e+00 , Validation Loss: 2.050e+00\n",
      "Iteration: 3541, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 3542, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 3543, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 3544, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 3545, Training Loss: -1.000e-07 , Validation Loss: 7.137e-01\n",
      "Iteration: 3546, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 3547, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 3548, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3549, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3550, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 3551, Training Loss: 2.518e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 3552, Training Loss: 5.037e-01 , Validation Loss: 1.724e+00\n",
      "Iteration: 3553, Training Loss: 1.511e+00 , Validation Loss: 3.411e+00\n",
      "Iteration: 3554, Training Loss: 1.511e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 3555, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 3556, Training Loss: 3.022e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 3557, Training Loss: 6.548e+00 , Validation Loss: 6.562e+00\n",
      "Iteration: 3558, Training Loss: 2.518e+00 , Validation Loss: 4.790e+00\n",
      "Iteration: 3559, Training Loss: 1.763e+00 , Validation Loss: 1.651e+00\n",
      "Iteration: 3560, Training Loss: 7.555e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 3561, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 3562, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3563, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3564, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 3565, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3566, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3567, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3568, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3569, Training Loss: 1.007e+00 , Validation Loss: 2.395e+00\n",
      "Iteration: 3570, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3571, Training Loss: 2.015e+00 , Validation Loss: 3.665e+00\n",
      "Iteration: 3572, Training Loss: 2.267e+00 , Validation Loss: 2.679e+00\n",
      "Iteration: 3573, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3574, Training Loss: 1.007e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 3575, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3576, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3577, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3578, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3579, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 3580, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3581, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3582, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3583, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3584, Training Loss: 3.274e+00 , Validation Loss: 4.119e+00\n",
      "Iteration: 3585, Training Loss: 5.541e+00 , Validation Loss: 5.740e+00\n",
      "Iteration: 3586, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3587, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3588, Training Loss: 2.518e+00 , Validation Loss: 4.264e+00\n",
      "Iteration: 3589, Training Loss: 2.267e+00 , Validation Loss: 1.185e+00\n",
      "Iteration: 3590, Training Loss: 1.511e+00 , Validation Loss: 3.085e+00\n",
      "Iteration: 3591, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 3592, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3593, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3594, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3595, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3596, Training Loss: 1.763e+00 , Validation Loss: 4.209e+00\n",
      "Iteration: 3597, Training Loss: 1.007e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 3598, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3599, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3600, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3601, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3602, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 3603, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3604, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3605, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3606, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3607, Training Loss: 3.022e+00 , Validation Loss: 4.905e+00\n",
      "Iteration: 3608, Training Loss: 3.022e+00 , Validation Loss: 3.502e+00\n",
      "Iteration: 3609, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 3610, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3611, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 3612, Training Loss: 5.037e-01 , Validation Loss: 1.585e+00\n",
      "Iteration: 3613, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3614, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3615, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3616, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3617, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3618, Training Loss: 3.022e+00 , Validation Loss: 4.482e+00\n",
      "Iteration: 3619, Training Loss: 9.570e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 3620, Training Loss: 2.015e+00 , Validation Loss: 4.996e+00\n",
      "Iteration: 3621, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 3622, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3623, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3624, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3625, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3626, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 3627, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3628, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3629, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3630, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3631, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3632, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3633, Training Loss: 1.763e+00 , Validation Loss: 2.945e+00\n",
      "Iteration: 3634, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 3635, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3636, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 3637, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3638, Training Loss: 3.022e+00 , Validation Loss: 4.427e+00\n",
      "Iteration: 3639, Training Loss: 3.778e+00 , Validation Loss: 4.167e+00\n",
      "Iteration: 3640, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 3641, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3642, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3643, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3644, Training Loss: 2.267e+00 , Validation Loss: 3.587e+00\n",
      "Iteration: 3645, Training Loss: 5.037e-01 , Validation Loss: 1.403e+00\n",
      "Iteration: 3646, Training Loss: 1.007e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 3647, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 3648, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3649, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3650, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3651, Training Loss: 7.555e-01 , Validation Loss: 9.858e-01\n",
      "Iteration: 3652, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3653, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3654, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3655, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 3656, Training Loss: 1.007e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 3657, Training Loss: 3.274e+00 , Validation Loss: 4.790e+00\n",
      "Iteration: 3658, Training Loss: 2.015e+00 , Validation Loss: 3.320e+00\n",
      "Iteration: 3659, Training Loss: 1.259e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 3660, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3661, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3662, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3663, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3664, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3665, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3666, Training Loss: -1.000e-07 , Validation Loss: 7.197e-01\n",
      "Iteration: 3667, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 3668, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3669, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3670, Training Loss: 7.555e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 3671, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3672, Training Loss: 7.555e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 3673, Training Loss: 2.015e+00 , Validation Loss: 3.496e+00\n",
      "Iteration: 3674, Training Loss: 1.259e+00 , Validation Loss: 1.560e+00\n",
      "Iteration: 3675, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3676, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 3677, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3678, Training Loss: 1.511e+00 , Validation Loss: 3.544e+00\n",
      "Iteration: 3679, Training Loss: 1.259e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 3680, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3681, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 3682, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3683, Training Loss: 1.511e+00 , Validation Loss: 2.111e+00\n",
      "Iteration: 3684, Training Loss: 2.267e+00 , Validation Loss: 1.107e+00\n",
      "Iteration: 3685, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3686, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3687, Training Loss: 1.259e+00 , Validation Loss: 2.322e+00\n",
      "Iteration: 3688, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3689, Training Loss: 1.259e+00 , Validation Loss: 1.198e+00\n",
      "Iteration: 3690, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 3691, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3692, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3693, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3694, Training Loss: 1.511e+00 , Validation Loss: 3.181e+00\n",
      "Iteration: 3695, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 3696, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3697, Training Loss: 2.518e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 3698, Training Loss: 1.763e+00 , Validation Loss: 2.068e+00\n",
      "Iteration: 3699, Training Loss: 1.763e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 3700, Training Loss: 2.770e+00 , Validation Loss: 5.038e+00\n",
      "Iteration: 3701, Training Loss: 1.511e+00 , Validation Loss: 9.254e-01\n",
      "Iteration: 3702, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 3703, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3704, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3705, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3706, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3707, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3708, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3709, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 3710, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3711, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3712, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3713, Training Loss: 1.007e+00 , Validation Loss: 3.472e+00\n",
      "Iteration: 3714, Training Loss: 2.267e+00 , Validation Loss: 8.346e-01\n",
      "Iteration: 3715, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3716, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3717, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 3718, Training Loss: 3.274e+00 , Validation Loss: 4.633e+00\n",
      "Iteration: 3719, Training Loss: 5.037e+00 , Validation Loss: 4.107e+00\n",
      "Iteration: 3720, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3721, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3722, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3723, Training Loss: 1.763e+00 , Validation Loss: 4.119e+00\n",
      "Iteration: 3724, Training Loss: 1.763e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 3725, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3726, Training Loss: 1.259e+00 , Validation Loss: 4.355e+00\n",
      "Iteration: 3727, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3728, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3729, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3730, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3731, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3732, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3733, Training Loss: 3.022e+00 , Validation Loss: 4.990e+00\n",
      "Iteration: 3734, Training Loss: 1.511e+00 , Validation Loss: 3.072e+00\n",
      "Iteration: 3735, Training Loss: 1.259e+00 , Validation Loss: 9.677e-01\n",
      "Iteration: 3736, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3737, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3738, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 3739, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3740, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3741, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3742, Training Loss: 1.511e+00 , Validation Loss: 3.562e+00\n",
      "Iteration: 3743, Training Loss: 1.511e+00 , Validation Loss: 9.556e-01\n",
      "Iteration: 3744, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3745, Training Loss: 1.259e+00 , Validation Loss: 9.435e-01\n",
      "Iteration: 3746, Training Loss: 5.037e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 3747, Training Loss: 7.555e-01 , Validation Loss: 1.198e+00\n",
      "Iteration: 3748, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 3749, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3750, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3751, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 3752, Training Loss: 1.259e+00 , Validation Loss: 1.288e+00\n",
      "Iteration: 3753, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3754, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 3755, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 3756, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 3757, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 3758, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3759, Training Loss: 2.518e-01 , Validation Loss: 8.407e-01\n",
      "Iteration: 3760, Training Loss: 2.518e-01 , Validation Loss: 2.220e+00\n",
      "Iteration: 3761, Training Loss: 3.274e+00 , Validation Loss: 5.026e+00\n",
      "Iteration: 3762, Training Loss: 3.274e+00 , Validation Loss: 4.778e+00\n",
      "Iteration: 3763, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3764, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3765, Training Loss: 1.259e+00 , Validation Loss: 1.046e+00\n",
      "Iteration: 3766, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3767, Training Loss: 7.555e-01 , Validation Loss: 1.385e+00\n",
      "Iteration: 3768, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3769, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3770, Training Loss: 1.007e+00 , Validation Loss: 3.212e+00\n",
      "Iteration: 3771, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 3772, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3773, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 3774, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 3775, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3776, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 3777, Training Loss: 7.555e-01 , Validation Loss: 1.028e+00\n",
      "Iteration: 3778, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3779, Training Loss: 7.555e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 3780, Training Loss: 1.007e+00 , Validation Loss: 2.897e+00\n",
      "Iteration: 3781, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3782, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 3783, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3784, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3785, Training Loss: 2.518e-01 , Validation Loss: 1.796e+00\n",
      "Iteration: 3786, Training Loss: 2.518e+00 , Validation Loss: 4.167e+00\n",
      "Iteration: 3787, Training Loss: 2.267e+00 , Validation Loss: 2.734e+00\n",
      "Iteration: 3788, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 3789, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3790, Training Loss: 1.007e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 3791, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3792, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3793, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3794, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3795, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3796, Training Loss: 5.037e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 3797, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3798, Training Loss: 2.518e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 3799, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 3800, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 3801, Training Loss: 3.274e+00 , Validation Loss: 4.917e+00\n",
      "Iteration: 3802, Training Loss: 9.066e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 3803, Training Loss: 1.259e+00 , Validation Loss: 5.219e+00\n",
      "Iteration: 3804, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3805, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3806, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3807, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3808, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3809, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3810, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 3811, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 3812, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3813, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3814, Training Loss: 3.526e+00 , Validation Loss: 5.062e+00\n",
      "Iteration: 3815, Training Loss: 4.030e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 3816, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 3817, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3818, Training Loss: 5.037e-01 , Validation Loss: 8.830e-01\n",
      "Iteration: 3819, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 3820, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3821, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3822, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3823, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3824, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3825, Training Loss: 1.259e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 3826, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 3827, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3828, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3829, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3830, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 3831, Training Loss: 2.267e+00 , Validation Loss: 3.901e+00\n",
      "Iteration: 3832, Training Loss: 2.518e+00 , Validation Loss: 2.335e+00\n",
      "Iteration: 3833, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3834, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3835, Training Loss: 1.007e+00 , Validation Loss: 5.020e-01\n",
      "Iteration: 3836, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3837, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3838, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3839, Training Loss: 2.518e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 3840, Training Loss: 1.259e+00 , Validation Loss: 1.530e+00\n",
      "Iteration: 3841, Training Loss: 1.007e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 3842, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 3843, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3844, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3845, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 3846, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3847, Training Loss: 1.259e+00 , Validation Loss: 2.389e+00\n",
      "Iteration: 3848, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3849, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 3850, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3851, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3852, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3853, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3854, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3855, Training Loss: 1.007e+00 , Validation Loss: 3.169e+00\n",
      "Iteration: 3856, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3857, Training Loss: 5.037e-01 , Validation Loss: 1.615e+00\n",
      "Iteration: 3858, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3859, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 3860, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3861, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3862, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3863, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3864, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 3865, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3866, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3867, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3868, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3869, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3870, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3871, Training Loss: 1.259e+00 , Validation Loss: 7.681e-01\n",
      "Iteration: 3872, Training Loss: 2.015e+00 , Validation Loss: 4.216e+00\n",
      "Iteration: 3873, Training Loss: 7.555e-01 , Validation Loss: 9.012e-01\n",
      "Iteration: 3874, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3875, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3876, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3877, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3878, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 3879, Training Loss: 2.518e-01 , Validation Loss: 1.875e+00\n",
      "Iteration: 3880, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 3881, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3882, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 3883, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 3884, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3885, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3886, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3887, Training Loss: 1.007e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 3888, Training Loss: 1.511e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 3889, Training Loss: 5.037e-01 , Validation Loss: 9.375e-01\n",
      "Iteration: 3890, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3891, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 3892, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3893, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3894, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3895, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3896, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 3897, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3898, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3899, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3900, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3901, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3902, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 3903, Training Loss: 1.259e+00 , Validation Loss: 3.278e+00\n",
      "Iteration: 3904, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3905, Training Loss: 1.259e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 3906, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3907, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 3908, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3909, Training Loss: 1.007e+00 , Validation Loss: 1.246e+00\n",
      "Iteration: 3910, Training Loss: 4.785e+00 , Validation Loss: 5.564e+00\n",
      "Iteration: 3911, Training Loss: 1.108e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 3912, Training Loss: 3.022e+00 , Validation Loss: 5.625e+00\n",
      "Iteration: 3913, Training Loss: 1.511e+00 , Validation Loss: 1.337e+00\n",
      "Iteration: 3914, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 3915, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 3916, Training Loss: 1.763e+00 , Validation Loss: 2.897e+00\n",
      "Iteration: 3917, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 3918, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3919, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3920, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3921, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3922, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3923, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3924, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3925, Training Loss: 1.511e+00 , Validation Loss: 2.619e+00\n",
      "Iteration: 3926, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3927, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3928, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3929, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3930, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3931, Training Loss: 1.511e+00 , Validation Loss: 4.457e+00\n",
      "Iteration: 3932, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3933, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3934, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3935, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3936, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3937, Training Loss: 2.518e+00 , Validation Loss: 4.010e+00\n",
      "Iteration: 3938, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 3939, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3940, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3941, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 3942, Training Loss: 5.037e-01 , Validation Loss: 1.506e+00\n",
      "Iteration: 3943, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3944, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3945, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3946, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3947, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3948, Training Loss: 1.259e+00 , Validation Loss: 1.216e+00\n",
      "Iteration: 3949, Training Loss: 1.511e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 3950, Training Loss: 1.511e+00 , Validation Loss: 2.576e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3951, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3952, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 3953, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3954, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 3955, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 3956, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 3957, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3958, Training Loss: 2.015e+00 , Validation Loss: 4.209e+00\n",
      "Iteration: 3959, Training Loss: 1.007e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 3960, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3961, Training Loss: 2.267e+00 , Validation Loss: 5.171e+00\n",
      "Iteration: 3962, Training Loss: 1.007e+00 , Validation Loss: 1.766e+00\n",
      "Iteration: 3963, Training Loss: 1.259e+00 , Validation Loss: 8.528e-01\n",
      "Iteration: 3964, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3965, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 3966, Training Loss: 2.518e-01 , Validation Loss: 2.697e+00\n",
      "Iteration: 3967, Training Loss: 3.274e+00 , Validation Loss: 4.070e+00\n",
      "Iteration: 3968, Training Loss: 5.792e+00 , Validation Loss: 8.008e+00\n",
      "Iteration: 3969, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3970, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3971, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3972, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3973, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3974, Training Loss: 1.511e+00 , Validation Loss: 1.712e+00\n",
      "Iteration: 3975, Training Loss: 7.555e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 3976, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3977, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 3978, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 3979, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3980, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 3981, Training Loss: 7.555e-01 , Validation Loss: 2.347e+00\n",
      "Iteration: 3982, Training Loss: 1.007e+00 , Validation Loss: 3.732e+00\n",
      "Iteration: 3983, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 3984, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3985, Training Loss: 1.763e+00 , Validation Loss: 4.161e+00\n",
      "Iteration: 3986, Training Loss: 1.763e+00 , Validation Loss: 1.185e+00\n",
      "Iteration: 3987, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 3988, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 3989, Training Loss: -1.000e-07 , Validation Loss: 6.230e-01\n",
      "Iteration: 3990, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 3991, Training Loss: 4.030e+00 , Validation Loss: 4.911e+00\n",
      "Iteration: 3992, Training Loss: 1.083e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 3993, Training Loss: 4.281e+00 , Validation Loss: 5.582e+00\n",
      "Iteration: 3994, Training Loss: 1.007e+00 , Validation Loss: 1.845e+00\n",
      "Iteration: 3995, Training Loss: 1.007e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 3996, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 3997, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 3998, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 3999, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4000, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4001, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4002, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4003, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4004, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4005, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4006, Training Loss: 2.770e+00 , Validation Loss: 2.250e+00\n",
      "Iteration: 4007, Training Loss: 1.511e+00 , Validation Loss: 2.310e+00\n",
      "Iteration: 4008, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 4009, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 4010, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4011, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4012, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 4013, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4014, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4015, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4016, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4017, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4018, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4019, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4020, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4021, Training Loss: 5.037e-01 , Validation Loss: 1.125e+00\n",
      "Iteration: 4022, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4023, Training Loss: 3.778e+00 , Validation Loss: 2.921e+00\n",
      "Iteration: 4024, Training Loss: 1.184e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 4025, Training Loss: 4.533e+00 , Validation Loss: 4.766e+00\n",
      "Iteration: 4026, Training Loss: 1.007e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 4027, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 4028, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4029, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4030, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4031, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4032, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4033, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4034, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4035, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 4036, Training Loss: 2.770e+00 , Validation Loss: 1.645e+00\n",
      "Iteration: 4037, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 4038, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4039, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4040, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4041, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4042, Training Loss: 7.555e-01 , Validation Loss: 1.167e+00\n",
      "Iteration: 4043, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4044, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4045, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 4046, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4047, Training Loss: 1.259e+00 , Validation Loss: 9.737e-01\n",
      "Iteration: 4048, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 4049, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 4050, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4051, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4052, Training Loss: 2.267e+00 , Validation Loss: 4.252e+00\n",
      "Iteration: 4053, Training Loss: 1.007e+00 , Validation Loss: 1.107e+00\n",
      "Iteration: 4054, Training Loss: 1.259e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 4055, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4056, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4057, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4058, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4059, Training Loss: 2.518e+00 , Validation Loss: 3.387e+00\n",
      "Iteration: 4060, Training Loss: 6.296e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 4061, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4062, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4063, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4064, Training Loss: 2.015e+00 , Validation Loss: 2.794e+00\n",
      "Iteration: 4065, Training Loss: 1.763e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 4066, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4067, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4068, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4069, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4070, Training Loss: 3.022e+00 , Validation Loss: 4.693e+00\n",
      "Iteration: 4071, Training Loss: 1.259e+00 , Validation Loss: 2.341e+00\n",
      "Iteration: 4072, Training Loss: 1.511e+00 , Validation Loss: 1.004e+00\n",
      "Iteration: 4073, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4074, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4075, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4076, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4077, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4078, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4079, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4080, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4081, Training Loss: 1.511e+00 , Validation Loss: 2.964e+00\n",
      "Iteration: 4082, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 4083, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4084, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4085, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4086, Training Loss: 1.511e+00 , Validation Loss: 1.452e+00\n",
      "Iteration: 4087, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4088, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4089, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4090, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4091, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4092, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4093, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4094, Training Loss: 4.281e+00 , Validation Loss: 4.857e+00\n",
      "Iteration: 4095, Training Loss: 9.822e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 4096, Training Loss: 3.274e+00 , Validation Loss: 4.724e+00\n",
      "Iteration: 4097, Training Loss: 2.267e+00 , Validation Loss: 2.074e+00\n",
      "Iteration: 4098, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 4099, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 4100, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 4101, Training Loss: 7.555e-01 , Validation Loss: 1.119e+00\n",
      "Iteration: 4102, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4103, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4104, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4105, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4106, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4107, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4108, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4109, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4110, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4111, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4112, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 4113, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4114, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4115, Training Loss: 2.518e+00 , Validation Loss: 3.865e+00\n",
      "Iteration: 4116, Training Loss: 2.518e-01 , Validation Loss: 1.760e+00\n",
      "Iteration: 4117, Training Loss: 1.511e+00 , Validation Loss: 1.530e+00\n",
      "Iteration: 4118, Training Loss: 1.259e+00 , Validation Loss: 4.778e-01\n",
      "Iteration: 4119, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4120, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4121, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4122, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 4123, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 4124, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4125, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4126, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4127, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4128, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4129, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4130, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4131, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 4132, Training Loss: 3.274e+00 , Validation Loss: 3.986e+00\n",
      "Iteration: 4133, Training Loss: 8.563e+00 , Validation Loss: 8.764e+00\n",
      "Iteration: 4134, Training Loss: 4.785e+00 , Validation Loss: 4.421e+00\n",
      "Iteration: 4135, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 4136, Training Loss: 2.267e+00 , Validation Loss: 1.149e+00\n",
      "Iteration: 4137, Training Loss: 1.763e+00 , Validation Loss: 1.433e+00\n",
      "Iteration: 4138, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 4139, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4140, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4141, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4142, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4143, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4144, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4145, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4146, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4147, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4148, Training Loss: 1.259e+00 , Validation Loss: 2.117e+00\n",
      "Iteration: 4149, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 4150, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4151, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4152, Training Loss: 5.037e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 4153, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4154, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 4155, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4156, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4157, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4158, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4159, Training Loss: 1.511e+00 , Validation Loss: 1.004e+00\n",
      "Iteration: 4160, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4161, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 4162, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4163, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4164, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4165, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4166, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4167, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4168, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4169, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4170, Training Loss: 7.555e-01 , Validation Loss: 1.597e+00\n",
      "Iteration: 4171, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4172, Training Loss: 2.267e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 4173, Training Loss: 1.007e+00 , Validation Loss: 2.818e+00\n",
      "Iteration: 4174, Training Loss: 1.763e+00 , Validation Loss: 1.433e+00\n",
      "Iteration: 4175, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 4176, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4177, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4178, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 4179, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4180, Training Loss: 2.518e+00 , Validation Loss: 3.695e+00\n",
      "Iteration: 4181, Training Loss: 2.015e+00 , Validation Loss: 3.877e+00\n",
      "Iteration: 4182, Training Loss: 1.259e+00 , Validation Loss: 9.798e-01\n",
      "Iteration: 4183, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4184, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 4185, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4186, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 4187, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 4188, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 4189, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 4190, Training Loss: 2.015e+00 , Validation Loss: 3.665e+00\n",
      "Iteration: 4191, Training Loss: 1.259e+00 , Validation Loss: 1.572e+00\n",
      "Iteration: 4192, Training Loss: 1.259e+00 , Validation Loss: 7.802e-01\n",
      "Iteration: 4193, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4194, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4195, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4196, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 4197, Training Loss: 2.267e+00 , Validation Loss: 3.937e+00\n",
      "Iteration: 4198, Training Loss: 1.763e+00 , Validation Loss: 1.095e+00\n",
      "Iteration: 4199, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4200, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 4201, Training Loss: 2.770e+00 , Validation Loss: 3.544e+00\n",
      "Iteration: 4202, Training Loss: 2.518e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 4203, Training Loss: 1.511e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 4204, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4205, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4206, Training Loss: 7.555e-01 , Validation Loss: 2.546e+00\n",
      "Iteration: 4207, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4208, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 4209, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4210, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4211, Training Loss: 4.030e+00 , Validation Loss: 4.391e+00\n",
      "Iteration: 4212, Training Loss: 8.059e+00 , Validation Loss: 1.013e+01\n",
      "Iteration: 4213, Training Loss: 2.770e+00 , Validation Loss: 4.343e+00\n",
      "Iteration: 4214, Training Loss: 2.015e+00 , Validation Loss: 1.409e+00\n",
      "Iteration: 4215, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4216, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4217, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4218, Training Loss: 2.518e+00 , Validation Loss: 4.179e+00\n",
      "Iteration: 4219, Training Loss: 1.763e+00 , Validation Loss: 2.183e+00\n",
      "Iteration: 4220, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 4221, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4222, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4223, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4224, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4225, Training Loss: 1.763e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 4226, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 4227, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 4228, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4229, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4230, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 4231, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4232, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4233, Training Loss: -1.000e-07 , Validation Loss: 6.411e-01\n",
      "Iteration: 4234, Training Loss: 2.518e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 4235, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4236, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4237, Training Loss: 1.763e+00 , Validation Loss: 4.167e+00\n",
      "Iteration: 4238, Training Loss: 1.511e+00 , Validation Loss: 6.048e-01\n",
      "Iteration: 4239, Training Loss: 1.511e+00 , Validation Loss: 3.514e+00\n",
      "Iteration: 4240, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4241, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4242, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4243, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4244, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4245, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4246, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4247, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4248, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4249, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4250, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4251, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4252, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4253, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4254, Training Loss: 5.037e-01 , Validation Loss: 1.161e+00\n",
      "Iteration: 4255, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4256, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4257, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4258, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4259, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4260, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 4261, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4262, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4263, Training Loss: 7.555e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 4264, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4265, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4266, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4267, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4268, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 4269, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4270, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4271, Training Loss: 1.763e+00 , Validation Loss: 3.490e+00\n",
      "Iteration: 4272, Training Loss: 2.015e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 4273, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4274, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4275, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4276, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4277, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4278, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 4279, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4280, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4281, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4282, Training Loss: 1.007e+00 , Validation Loss: 1.724e+00\n",
      "Iteration: 4283, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4284, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 4285, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4286, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4287, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4288, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4289, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4290, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4291, Training Loss: 7.555e-01 , Validation Loss: 2.220e+00\n",
      "Iteration: 4292, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4293, Training Loss: 5.037e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 4294, Training Loss: 7.555e-01 , Validation Loss: 8.951e-01\n",
      "Iteration: 4295, Training Loss: 3.022e+00 , Validation Loss: 4.288e+00\n",
      "Iteration: 4296, Training Loss: 7.304e+00 , Validation Loss: 8.304e+00\n",
      "Iteration: 4297, Training Loss: 4.533e+00 , Validation Loss: 5.141e+00\n",
      "Iteration: 4298, Training Loss: 5.289e+00 , Validation Loss: 4.560e+00\n",
      "Iteration: 4299, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4300, Training Loss: 2.518e+00 , Validation Loss: 4.760e+00\n",
      "Iteration: 4301, Training Loss: 2.267e+00 , Validation Loss: 8.528e-01\n",
      "Iteration: 4302, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4303, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4304, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4305, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4306, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4307, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4308, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4309, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4310, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4311, Training Loss: 1.763e+00 , Validation Loss: 1.833e+00\n",
      "Iteration: 4312, Training Loss: 2.015e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 4313, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4314, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4315, Training Loss: 1.259e+00 , Validation Loss: 3.109e+00\n",
      "Iteration: 4316, Training Loss: 7.555e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 4317, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4318, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4319, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4320, Training Loss: 1.259e+00 , Validation Loss: 3.175e+00\n",
      "Iteration: 4321, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4322, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4323, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4324, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4325, Training Loss: 1.511e+00 , Validation Loss: 2.891e+00\n",
      "Iteration: 4326, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 4327, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 4328, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4329, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4330, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4331, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 4332, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 4333, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4334, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4335, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4336, Training Loss: 5.037e-01 , Validation Loss: 2.492e+00\n",
      "Iteration: 4337, Training Loss: 5.037e-01 , Validation Loss: 1.675e+00\n",
      "Iteration: 4338, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4339, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4340, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4341, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 4342, Training Loss: 1.007e+00 , Validation Loss: 2.226e+00\n",
      "Iteration: 4343, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4344, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4345, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 4346, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4347, Training Loss: 5.037e-01 , Validation Loss: 8.407e-01\n",
      "Iteration: 4348, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4349, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 4350, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 4351, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4352, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4353, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 4354, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 4355, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4356, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4357, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4358, Training Loss: 2.518e-01 , Validation Loss: 1.258e+00\n",
      "Iteration: 4359, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 4360, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4361, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 4362, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4363, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4364, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 4365, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4366, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 4367, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4368, Training Loss: 1.511e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 4369, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 4370, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4371, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4372, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4373, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4374, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4375, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4376, Training Loss: 7.555e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 4377, Training Loss: 1.259e+00 , Validation Loss: 4.252e+00\n",
      "Iteration: 4378, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4379, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4380, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4381, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4382, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4383, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4384, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4385, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4386, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4387, Training Loss: 2.518e-01 , Validation Loss: 1.403e+00\n",
      "Iteration: 4388, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 4389, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4390, Training Loss: 7.555e-01 , Validation Loss: 1.433e+00\n",
      "Iteration: 4391, Training Loss: 1.763e+00 , Validation Loss: 4.076e+00\n",
      "Iteration: 4392, Training Loss: 1.511e+00 , Validation Loss: 1.095e+00\n",
      "Iteration: 4393, Training Loss: -1.000e-07 , Validation Loss: 4.597e-01\n",
      "Iteration: 4394, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 4395, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4396, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4397, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4398, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4399, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4400, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4401, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4402, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4403, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4404, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 4405, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 4406, Training Loss: 2.518e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 4407, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4408, Training Loss: 1.511e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 4409, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 4410, Training Loss: 7.555e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 4411, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 4412, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 4413, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4414, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4415, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 4416, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4417, Training Loss: 5.037e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 4418, Training Loss: 7.555e-01 , Validation Loss: 1.210e+00\n",
      "Iteration: 4419, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4420, Training Loss: 2.015e+00 , Validation Loss: 4.324e+00\n",
      "Iteration: 4421, Training Loss: 2.015e+00 , Validation Loss: 2.099e+00\n",
      "Iteration: 4422, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4423, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4424, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 4425, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 4426, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4427, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 4428, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 4429, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4430, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4431, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4432, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 4433, Training Loss: 5.037e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 4434, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4435, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 4436, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4437, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4438, Training Loss: 7.555e-01 , Validation Loss: 1.978e+00\n",
      "Iteration: 4439, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4440, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4441, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4442, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 4443, Training Loss: -1.000e-07 , Validation Loss: 9.616e-01\n",
      "Iteration: 4444, Training Loss: 2.518e-01 , Validation Loss: 9.616e-01\n",
      "Iteration: 4445, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 4446, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4447, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4448, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4449, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4450, Training Loss: 3.274e+00 , Validation Loss: 4.415e+00\n",
      "Iteration: 4451, Training Loss: 7.052e+00 , Validation Loss: 9.725e+00\n",
      "Iteration: 4452, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4453, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4454, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4455, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4456, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4457, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4458, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4459, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4460, Training Loss: 5.037e-01 , Validation Loss: 2.147e+00\n",
      "Iteration: 4461, Training Loss: 2.518e-01 , Validation Loss: 1.034e+00\n",
      "Iteration: 4462, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4463, Training Loss: 5.037e-01 , Validation Loss: 2.788e+00\n",
      "Iteration: 4464, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4465, Training Loss: 1.763e+00 , Validation Loss: 3.042e+00\n",
      "Iteration: 4466, Training Loss: 2.770e+00 , Validation Loss: 2.153e+00\n",
      "Iteration: 4467, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 4468, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 4469, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4470, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4471, Training Loss: 2.518e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 4472, Training Loss: 1.511e+00 , Validation Loss: 3.580e+00\n",
      "Iteration: 4473, Training Loss: 1.007e+00 , Validation Loss: 6.471e-01\n",
      "Iteration: 4474, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4475, Training Loss: 5.037e-01 , Validation Loss: 8.830e-01\n",
      "Iteration: 4476, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4477, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4478, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 4479, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4480, Training Loss: 2.518e-01 , Validation Loss: 1.198e+00\n",
      "Iteration: 4481, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 4482, Training Loss: 1.007e+00 , Validation Loss: 1.179e+00\n",
      "Iteration: 4483, Training Loss: 3.526e+00 , Validation Loss: 5.201e+00\n",
      "Iteration: 4484, Training Loss: 6.548e+00 , Validation Loss: 8.873e+00\n",
      "Iteration: 4485, Training Loss: 2.267e+00 , Validation Loss: 2.504e+00\n",
      "Iteration: 4486, Training Loss: 2.267e+00 , Validation Loss: 3.157e+00\n",
      "Iteration: 4487, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 4488, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4489, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 4490, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4491, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4492, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4493, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4494, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4495, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4496, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4497, Training Loss: 2.518e+00 , Validation Loss: 3.012e+00\n",
      "Iteration: 4498, Training Loss: 2.770e+00 , Validation Loss: 2.449e+00\n",
      "Iteration: 4499, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 4500, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 4501, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4502, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4503, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4504, Training Loss: 1.007e+00 , Validation Loss: 3.254e+00\n",
      "Iteration: 4505, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4506, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4507, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4508, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 4509, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4510, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4511, Training Loss: 1.259e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 4512, Training Loss: 1.007e+00 , Validation Loss: 3.738e+00\n",
      "Iteration: 4513, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4514, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4515, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 4516, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4517, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4518, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4519, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4520, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4521, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4522, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4523, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 4524, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4525, Training Loss: 1.259e+00 , Validation Loss: 2.383e+00\n",
      "Iteration: 4526, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 4527, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4528, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4529, Training Loss: 2.518e-01 , Validation Loss: 1.028e+00\n",
      "Iteration: 4530, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 4531, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 4532, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4533, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 4534, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 4535, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 4536, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 4537, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 4538, Training Loss: 3.274e+00 , Validation Loss: 4.911e+00\n",
      "Iteration: 4539, Training Loss: 6.548e+00 , Validation Loss: 9.768e+00\n",
      "Iteration: 4540, Training Loss: 3.526e+00 , Validation Loss: 3.974e+00\n",
      "Iteration: 4541, Training Loss: 2.770e+00 , Validation Loss: 1.954e+00\n",
      "Iteration: 4542, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 4543, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4544, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4545, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4546, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4547, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4548, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4549, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4550, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4551, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4552, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4553, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4554, Training Loss: 2.518e+00 , Validation Loss: 2.522e+00\n",
      "Iteration: 4555, Training Loss: 2.015e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 4556, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4557, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4558, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4559, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4560, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4561, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4562, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4563, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4564, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4565, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4566, Training Loss: 1.259e+00 , Validation Loss: 1.736e+00\n",
      "Iteration: 4567, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 4568, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4569, Training Loss: 2.015e+00 , Validation Loss: 3.750e+00\n",
      "Iteration: 4570, Training Loss: 1.763e+00 , Validation Loss: 8.346e-01\n",
      "Iteration: 4571, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4572, Training Loss: 1.511e+00 , Validation Loss: 2.377e+00\n",
      "Iteration: 4573, Training Loss: 1.763e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 4574, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4575, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4576, Training Loss: 1.007e+00 , Validation Loss: 2.516e+00\n",
      "Iteration: 4577, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4578, Training Loss: 1.007e+00 , Validation Loss: 6.048e-01\n",
      "Iteration: 4579, Training Loss: 3.022e+00 , Validation Loss: 5.008e+00\n",
      "Iteration: 4580, Training Loss: 2.770e+00 , Validation Loss: 1.899e+00\n",
      "Iteration: 4581, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4582, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4583, Training Loss: 7.555e-01 , Validation Loss: 3.012e+00\n",
      "Iteration: 4584, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4585, Training Loss: 5.037e-01 , Validation Loss: 1.077e+00\n",
      "Iteration: 4586, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 4587, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4588, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4589, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4590, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4591, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4592, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 4593, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4594, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4595, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4596, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 4597, Training Loss: 7.555e-01 , Validation Loss: 3.538e+00\n",
      "Iteration: 4598, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 4599, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4600, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4601, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4602, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4603, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4604, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4605, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4606, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4607, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4608, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4609, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 4610, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 4611, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4612, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4613, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4614, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4615, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4616, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4617, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 4618, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 4619, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4620, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4621, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 4622, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4623, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 4624, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 4625, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4626, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4627, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4628, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4629, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4630, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4631, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4632, Training Loss: 5.037e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 4633, Training Loss: 7.555e-01 , Validation Loss: 1.240e+00\n",
      "Iteration: 4634, Training Loss: 1.763e+00 , Validation Loss: 3.447e+00\n",
      "Iteration: 4635, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 4636, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 4637, Training Loss: 2.770e+00 , Validation Loss: 4.107e+00\n",
      "Iteration: 4638, Training Loss: 5.792e+00 , Validation Loss: 4.530e+00\n",
      "Iteration: 4639, Training Loss: 4.030e+00 , Validation Loss: 5.183e+00\n",
      "Iteration: 4640, Training Loss: 2.770e+00 , Validation Loss: 2.462e+00\n",
      "Iteration: 4641, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4642, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4643, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4644, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4645, Training Loss: 1.511e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 4646, Training Loss: 7.555e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 4647, Training Loss: -1.000e-07 , Validation Loss: 5.322e-01\n",
      "Iteration: 4648, Training Loss: 1.259e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 4649, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4650, Training Loss: 5.037e-01 , Validation Loss: 1.554e+00\n",
      "Iteration: 4651, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4652, Training Loss: 1.763e+00 , Validation Loss: 2.740e+00\n",
      "Iteration: 4653, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 4654, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4655, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4656, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4657, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4658, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4659, Training Loss: 7.555e-01 , Validation Loss: 9.556e-01\n",
      "Iteration: 4660, Training Loss: 1.763e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 4661, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4662, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4663, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4664, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4665, Training Loss: 1.007e+00 , Validation Loss: 7.802e-01\n",
      "Iteration: 4666, Training Loss: 1.511e+00 , Validation Loss: 3.012e+00\n",
      "Iteration: 4667, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 4668, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 4669, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4670, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4671, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 4672, Training Loss: 1.007e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 4673, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 4674, Training Loss: 7.555e-01 , Validation Loss: 1.941e+00\n",
      "Iteration: 4675, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4676, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4677, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4678, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4679, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4680, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 4681, Training Loss: 5.037e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 4682, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4683, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 4684, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4685, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4686, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 4687, Training Loss: 1.007e+00 , Validation Loss: 2.068e+00\n",
      "Iteration: 4688, Training Loss: 1.259e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 4689, Training Loss: 1.259e+00 , Validation Loss: 2.504e+00\n",
      "Iteration: 4690, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4691, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4692, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4693, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 4694, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 4695, Training Loss: 2.518e-01 , Validation Loss: 1.621e+00\n",
      "Iteration: 4696, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 4697, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4698, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4699, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 4700, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4701, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 4702, Training Loss: 7.555e-01 , Validation Loss: 3.230e+00\n",
      "Iteration: 4703, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4704, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4705, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 4706, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4707, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4708, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4709, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4710, Training Loss: 1.763e+00 , Validation Loss: 3.792e+00\n",
      "Iteration: 4711, Training Loss: 1.511e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 4712, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4713, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4714, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4715, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4716, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4717, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4718, Training Loss: 4.785e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 4719, Training Loss: 1.108e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 4720, Training Loss: 3.274e+00 , Validation Loss: 2.329e+00\n",
      "Iteration: 4721, Training Loss: 3.778e+00 , Validation Loss: 3.816e+00\n",
      "Iteration: 4722, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 4723, Training Loss: 1.007e+00 , Validation Loss: 4.476e-01\n",
      "Iteration: 4724, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4725, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4726, Training Loss: 5.037e-01 , Validation Loss: 1.058e+00\n",
      "Iteration: 4727, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 4728, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4729, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4730, Training Loss: 3.526e+00 , Validation Loss: 5.092e+00\n",
      "Iteration: 4731, Training Loss: 2.267e+00 , Validation Loss: 2.395e+00\n",
      "Iteration: 4732, Training Loss: 1.007e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 4733, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4734, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 4735, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4736, Training Loss: 1.259e+00 , Validation Loss: 4.034e+00\n",
      "Iteration: 4737, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4738, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4739, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4740, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4741, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4742, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 4743, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4744, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4745, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4746, Training Loss: 2.015e+00 , Validation Loss: 5.643e+00\n",
      "Iteration: 4747, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4748, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4749, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4750, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 4751, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4752, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4753, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4754, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4755, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4756, Training Loss: 1.511e+00 , Validation Loss: 3.242e+00\n",
      "Iteration: 4757, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 4758, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4759, Training Loss: 3.022e+00 , Validation Loss: 4.512e+00\n",
      "Iteration: 4760, Training Loss: 3.526e+00 , Validation Loss: 4.312e+00\n",
      "Iteration: 4761, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4762, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4763, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 4764, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 4765, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 4766, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4767, Training Loss: 5.037e-01 , Validation Loss: 1.010e+00\n",
      "Iteration: 4768, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4769, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4770, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4771, Training Loss: 1.259e+00 , Validation Loss: 2.171e+00\n",
      "Iteration: 4772, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4773, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4774, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4775, Training Loss: -1.000e-07 , Validation Loss: 7.379e-01\n",
      "Iteration: 4776, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 4777, Training Loss: 1.259e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 4778, Training Loss: 1.259e+00 , Validation Loss: 4.288e+00\n",
      "Iteration: 4779, Training Loss: 1.259e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 4780, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 4781, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 4782, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4783, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4784, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4785, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4786, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 4787, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 4788, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4789, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4790, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4791, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4792, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4793, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 4794, Training Loss: 2.770e+00 , Validation Loss: 5.177e+00\n",
      "Iteration: 4795, Training Loss: 2.267e+00 , Validation Loss: 1.972e+00\n",
      "Iteration: 4796, Training Loss: 1.007e+00 , Validation Loss: 5.080e-01\n",
      "Iteration: 4797, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4798, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4799, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 4800, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4801, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4802, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4803, Training Loss: -1.000e-07 , Validation Loss: 7.621e-01\n",
      "Iteration: 4804, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 4805, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 4806, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4807, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4808, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4809, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4810, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4811, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4812, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4813, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 4814, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 4815, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 4816, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 4817, Training Loss: 2.015e+00 , Validation Loss: 3.919e+00\n",
      "Iteration: 4818, Training Loss: 1.259e+00 , Validation Loss: 9.677e-01\n",
      "Iteration: 4819, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 4820, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 4821, Training Loss: 1.763e+00 , Validation Loss: 4.234e-01\n",
      "Iteration: 4822, Training Loss: 1.763e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 4823, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4824, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 4825, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4826, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 4827, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 4828, Training Loss: 1.007e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 4829, Training Loss: -1.000e-07 , Validation Loss: 9.254e-01\n",
      "Iteration: 4830, Training Loss: 2.518e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 4831, Training Loss: 1.007e+00 , Validation Loss: 5.988e-01\n",
      "Iteration: 4832, Training Loss: 3.022e+00 , Validation Loss: 4.397e+00\n",
      "Iteration: 4833, Training Loss: 5.289e+00 , Validation Loss: 5.576e+00\n",
      "Iteration: 4834, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4835, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4836, Training Loss: 7.555e-01 , Validation Loss: 2.062e+00\n",
      "Iteration: 4837, Training Loss: 5.037e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 4838, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4839, Training Loss: 1.763e+00 , Validation Loss: 2.903e+00\n",
      "Iteration: 4840, Training Loss: 2.015e+00 , Validation Loss: 1.899e+00\n",
      "Iteration: 4841, Training Loss: 1.763e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 4842, Training Loss: 7.555e-01 , Validation Loss: 2.322e+00\n",
      "Iteration: 4843, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4844, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 4845, Training Loss: 1.007e+00 , Validation Loss: 1.095e+00\n",
      "Iteration: 4846, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4847, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4848, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4849, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4850, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 4851, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4852, Training Loss: 1.007e+00 , Validation Loss: 2.377e+00\n",
      "Iteration: 4853, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 4854, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 4855, Training Loss: 1.259e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 4856, Training Loss: 7.555e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 4857, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 4858, Training Loss: 3.778e+00 , Validation Loss: 4.784e+00\n",
      "Iteration: 4859, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 4860, Training Loss: 3.526e+00 , Validation Loss: 5.842e+00\n",
      "Iteration: 4861, Training Loss: 3.526e+00 , Validation Loss: 1.778e+00\n",
      "Iteration: 4862, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4863, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4864, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 4865, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4866, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4867, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4868, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4869, Training Loss: 2.267e+00 , Validation Loss: 4.095e+00\n",
      "Iteration: 4870, Training Loss: 1.511e+00 , Validation Loss: 1.264e+00\n",
      "Iteration: 4871, Training Loss: 7.555e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 4872, Training Loss: 1.763e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 4873, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4874, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4875, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4876, Training Loss: 7.555e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 4877, Training Loss: 3.274e+00 , Validation Loss: 5.334e+00\n",
      "Iteration: 4878, Training Loss: 2.518e+00 , Validation Loss: 3.387e+00\n",
      "Iteration: 4879, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 4880, Training Loss: 2.015e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 4881, Training Loss: 1.763e+00 , Validation Loss: 3.236e+00\n",
      "Iteration: 4882, Training Loss: 2.518e+00 , Validation Loss: 2.419e+00\n",
      "Iteration: 4883, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 4884, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 4885, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 4886, Training Loss: 1.511e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4887, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4888, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4889, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4890, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4891, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4892, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4893, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4894, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4895, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4896, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4897, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4898, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4899, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4900, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4901, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4902, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4903, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4904, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4905, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 4906, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 4907, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 4908, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4909, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4910, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4911, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4912, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4913, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4914, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4915, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4916, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4917, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4918, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4919, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4920, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4921, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4922, Training Loss: 2.770e+00 , Validation Loss: 5.649e+00\n",
      "Iteration: 4923, Training Loss: 2.015e+00 , Validation Loss: 1.143e+00\n",
      "Iteration: 4924, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4925, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 4926, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4927, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4928, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4929, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4930, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4931, Training Loss: 1.511e+00 , Validation Loss: 3.036e+00\n",
      "Iteration: 4932, Training Loss: 2.015e+00 , Validation Loss: 8.467e-01\n",
      "Iteration: 4933, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4934, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 4935, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4936, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4937, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4938, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4939, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4940, Training Loss: -1.000e-07 , Validation Loss: 4.597e-01\n",
      "Iteration: 4941, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 4942, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4943, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4944, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4945, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 4946, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4947, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4948, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 4949, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4950, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4951, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4952, Training Loss: 1.259e+00 , Validation Loss: 2.982e+00\n",
      "Iteration: 4953, Training Loss: 1.763e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 4954, Training Loss: 2.770e+00 , Validation Loss: 3.466e+00\n",
      "Iteration: 4955, Training Loss: 1.007e+00 , Validation Loss: 2.226e+00\n",
      "Iteration: 4956, Training Loss: 1.259e+00 , Validation Loss: 1.143e+00\n",
      "Iteration: 4957, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4958, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4959, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4960, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4961, Training Loss: 1.511e+00 , Validation Loss: 4.149e+00\n",
      "Iteration: 4962, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4963, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 4964, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4965, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4966, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4967, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4968, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4969, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 4970, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4971, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4972, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 4973, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 4974, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4975, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 4976, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4977, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 4978, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4979, Training Loss: 1.007e+00 , Validation Loss: 2.843e+00\n",
      "Iteration: 4980, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4981, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 4982, Training Loss: 1.259e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 4983, Training Loss: 2.518e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 4984, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 4985, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4986, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4987, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 4988, Training Loss: 3.274e+00 , Validation Loss: 4.258e+00\n",
      "Iteration: 4989, Training Loss: 2.267e+00 , Validation Loss: 3.599e+00\n",
      "Iteration: 4990, Training Loss: 5.037e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 4991, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4992, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4993, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 4994, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 4995, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 4996, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4997, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 4998, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 4999, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5000, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5001, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5002, Training Loss: 2.518e+00 , Validation Loss: 5.092e+00\n",
      "Iteration: 5003, Training Loss: 1.763e+00 , Validation Loss: 2.208e+00\n",
      "Iteration: 5004, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 5005, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 5006, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 5007, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 5008, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5009, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5010, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5011, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5012, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5013, Training Loss: 1.007e+00 , Validation Loss: 2.044e+00\n",
      "Iteration: 5014, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5015, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 5016, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5017, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5018, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5019, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5020, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5021, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5022, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5023, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5024, Training Loss: 1.763e+00 , Validation Loss: 2.274e+00\n",
      "Iteration: 5025, Training Loss: -1.000e-07 , Validation Loss: 8.286e-01\n",
      "Iteration: 5026, Training Loss: 1.259e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 5027, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5028, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5029, Training Loss: 2.518e-01 , Validation Loss: 8.407e-01\n",
      "Iteration: 5030, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 5031, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 5032, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5033, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5034, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 5035, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5036, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 5037, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5038, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5039, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5040, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5041, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5042, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 5043, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5044, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5045, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5046, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5047, Training Loss: 7.555e-01 , Validation Loss: 1.240e+00\n",
      "Iteration: 5048, Training Loss: 2.518e-01 , Validation Loss: 1.064e+00\n",
      "Iteration: 5049, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 5050, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 5051, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5052, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5053, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5054, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5055, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5056, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5057, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5058, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 5059, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5060, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5061, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5062, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5063, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5064, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 5065, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5066, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5067, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5068, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 5069, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5070, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5071, Training Loss: 2.267e+00 , Validation Loss: 3.490e+00\n",
      "Iteration: 5072, Training Loss: 1.259e+00 , Validation Loss: 6.653e-01\n",
      "Iteration: 5073, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5074, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5075, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5076, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 5077, Training Loss: 2.518e-01 , Validation Loss: 2.262e+00\n",
      "Iteration: 5078, Training Loss: -1.000e-07 , Validation Loss: 6.653e-01\n",
      "Iteration: 5079, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 5080, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 5081, Training Loss: 1.763e+00 , Validation Loss: 3.732e+00\n",
      "Iteration: 5082, Training Loss: 1.259e+00 , Validation Loss: 8.951e-01\n",
      "Iteration: 5083, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 5084, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5085, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5086, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5087, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5088, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5089, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 5090, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5091, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5092, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 5093, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 5094, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5095, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5096, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5097, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5098, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5099, Training Loss: 7.555e-01 , Validation Loss: 2.576e+00\n",
      "Iteration: 5100, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 5101, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 5102, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5103, Training Loss: 2.015e+00 , Validation Loss: 4.167e+00\n",
      "Iteration: 5104, Training Loss: 1.007e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 5105, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 5106, Training Loss: 1.007e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 5107, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 5108, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5109, Training Loss: 5.037e-01 , Validation Loss: 1.881e+00\n",
      "Iteration: 5110, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5111, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 5112, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5113, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5114, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 5115, Training Loss: 2.015e+00 , Validation Loss: 4.433e+00\n",
      "Iteration: 5116, Training Loss: 1.007e+00 , Validation Loss: 1.331e+00\n",
      "Iteration: 5117, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 5118, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5119, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5120, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5121, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 5122, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 5123, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 5124, Training Loss: 1.259e+00 , Validation Loss: 4.959e-01\n",
      "Iteration: 5125, Training Loss: 3.526e+00 , Validation Loss: 5.189e+00\n",
      "Iteration: 5126, Training Loss: 1.184e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 5127, Training Loss: 3.526e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 5128, Training Loss: 1.763e+00 , Validation Loss: 1.409e+00\n",
      "Iteration: 5129, Training Loss: 2.267e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 5130, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5131, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5132, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5133, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5134, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5135, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5136, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5137, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5138, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5139, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 5140, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5141, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5142, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5143, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5144, Training Loss: 2.015e+00 , Validation Loss: 3.127e+00\n",
      "Iteration: 5145, Training Loss: 7.555e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 5146, Training Loss: 2.267e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 5147, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5148, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5149, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5150, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5151, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5152, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5153, Training Loss: 1.511e+00 , Validation Loss: 2.322e+00\n",
      "Iteration: 5154, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 5155, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 5156, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5157, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5158, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5159, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5160, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5161, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5162, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5163, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5164, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5165, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5166, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 5167, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5168, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 5169, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 5170, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 5171, Training Loss: 1.007e+00 , Validation Loss: 7.742e-01\n",
      "Iteration: 5172, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5173, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5174, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5175, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5176, Training Loss: 3.022e+00 , Validation Loss: 5.207e+00\n",
      "Iteration: 5177, Training Loss: 2.770e+00 , Validation Loss: 1.990e+00\n",
      "Iteration: 5178, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5179, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5180, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5181, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5182, Training Loss: 3.274e+00 , Validation Loss: 4.300e+00\n",
      "Iteration: 5183, Training Loss: 8.059e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 5184, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5185, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5186, Training Loss: 2.267e+00 , Validation Loss: 3.925e+00\n",
      "Iteration: 5187, Training Loss: 2.015e+00 , Validation Loss: 1.839e+00\n",
      "Iteration: 5188, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 5189, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5190, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5191, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5192, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5193, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 5194, Training Loss: 2.518e-01 , Validation Loss: 1.966e+00\n",
      "Iteration: 5195, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5196, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5197, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5198, Training Loss: 1.259e+00 , Validation Loss: 1.125e+00\n",
      "Iteration: 5199, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5200, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5201, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5202, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5203, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5204, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5205, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 5206, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5207, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5208, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5209, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5210, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5211, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5212, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 5213, Training Loss: 5.037e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 5214, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5215, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5216, Training Loss: 1.763e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5217, Training Loss: 3.526e+00 , Validation Loss: 5.401e+00\n",
      "Iteration: 5218, Training Loss: 1.763e+00 , Validation Loss: 1.706e+00\n",
      "Iteration: 5219, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5220, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 5221, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5222, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5223, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5224, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5225, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5226, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5227, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5228, Training Loss: 3.526e+00 , Validation Loss: 4.463e+00\n",
      "Iteration: 5229, Training Loss: 5.541e+00 , Validation Loss: 4.639e+00\n",
      "Iteration: 5230, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 5231, Training Loss: 2.015e+00 , Validation Loss: 2.788e+00\n",
      "Iteration: 5232, Training Loss: 1.007e+00 , Validation Loss: 1.252e+00\n",
      "Iteration: 5233, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 5234, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 5235, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5236, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5237, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5238, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5239, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5240, Training Loss: 1.259e+00 , Validation Loss: 3.472e+00\n",
      "Iteration: 5241, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5242, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5243, Training Loss: 1.259e+00 , Validation Loss: 1.149e+00\n",
      "Iteration: 5244, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5245, Training Loss: 1.511e+00 , Validation Loss: 2.558e+00\n",
      "Iteration: 5246, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5247, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 5248, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5249, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5250, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5251, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 5252, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5253, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5254, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 5255, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5256, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5257, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5258, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5259, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 5260, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5261, Training Loss: 5.037e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 5262, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5263, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5264, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 5265, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5266, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5267, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5268, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5269, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 5270, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5271, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5272, Training Loss: 3.778e+00 , Validation Loss: 5.286e+00\n",
      "Iteration: 5273, Training Loss: 3.022e+00 , Validation Loss: 2.105e+00\n",
      "Iteration: 5274, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5275, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5276, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5277, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5278, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5279, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5280, Training Loss: 5.037e-01 , Validation Loss: 1.391e+00\n",
      "Iteration: 5281, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5282, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5283, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 5284, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5285, Training Loss: 1.763e+00 , Validation Loss: 3.496e+00\n",
      "Iteration: 5286, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 5287, Training Loss: -1.000e-07 , Validation Loss: 1.185e+00\n",
      "Iteration: 5288, Training Loss: 7.555e-01 , Validation Loss: 1.185e+00\n",
      "Iteration: 5289, Training Loss: 1.763e+00 , Validation Loss: 4.161e+00\n",
      "Iteration: 5290, Training Loss: 1.763e+00 , Validation Loss: 8.467e-01\n",
      "Iteration: 5291, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5292, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5293, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5294, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 5295, Training Loss: 7.555e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 5296, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 5297, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5298, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5299, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 5300, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5301, Training Loss: 4.030e+00 , Validation Loss: 5.147e+00\n",
      "Iteration: 5302, Training Loss: 1.158e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 5303, Training Loss: 4.030e+00 , Validation Loss: 4.470e+00\n",
      "Iteration: 5304, Training Loss: 3.022e+00 , Validation Loss: 3.459e+00\n",
      "Iteration: 5305, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 5306, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5307, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5308, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 5309, Training Loss: 1.259e+00 , Validation Loss: 1.718e+00\n",
      "Iteration: 5310, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 5311, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5312, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5313, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5314, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5315, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5316, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 5317, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 5318, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5319, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5320, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5321, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5322, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5323, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5324, Training Loss: 2.015e+00 , Validation Loss: 3.429e+00\n",
      "Iteration: 5325, Training Loss: 5.037e-01 , Validation Loss: 1.222e+00\n",
      "Iteration: 5326, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 5327, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5328, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5329, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5330, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5331, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5332, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5333, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5334, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5335, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5336, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5337, Training Loss: 1.007e+00 , Validation Loss: 1.306e+00\n",
      "Iteration: 5338, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5339, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5340, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5341, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5342, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5343, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5344, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5345, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5346, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5347, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 5348, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5349, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5350, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5351, Training Loss: 1.007e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 5352, Training Loss: 2.770e+00 , Validation Loss: 4.143e+00\n",
      "Iteration: 5353, Training Loss: 2.015e+00 , Validation Loss: 2.135e+00\n",
      "Iteration: 5354, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 5355, Training Loss: 1.259e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 5356, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5357, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 5358, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5359, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5360, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 5361, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5362, Training Loss: 7.555e-01 , Validation Loss: 3.611e+00\n",
      "Iteration: 5363, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 5364, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5365, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5366, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 5367, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5368, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5369, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5370, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5371, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5372, Training Loss: -1.000e-07 , Validation Loss: 7.983e-01\n",
      "Iteration: 5373, Training Loss: 7.555e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 5374, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 5375, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5376, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5377, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5378, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5379, Training Loss: 5.037e-01 , Validation Loss: 1.621e+00\n",
      "Iteration: 5380, Training Loss: 1.763e+00 , Validation Loss: 3.732e+00\n",
      "Iteration: 5381, Training Loss: 5.037e-01 , Validation Loss: 1.179e+00\n",
      "Iteration: 5382, Training Loss: 1.763e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 5383, Training Loss: 1.007e+00 , Validation Loss: 1.748e+00\n",
      "Iteration: 5384, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 5385, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5386, Training Loss: 1.259e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 5387, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 5388, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5389, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 5390, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5391, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5392, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5393, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5394, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5395, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 5396, Training Loss: 1.259e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 5397, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 5398, Training Loss: 5.037e-01 , Validation Loss: 3.641e+00\n",
      "Iteration: 5399, Training Loss: 1.259e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 5400, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5401, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5402, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5403, Training Loss: 5.037e-01 , Validation Loss: 9.979e-01\n",
      "Iteration: 5404, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5405, Training Loss: 1.511e+00 , Validation Loss: 1.179e+00\n",
      "Iteration: 5406, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5407, Training Loss: 3.526e+00 , Validation Loss: 4.052e+00\n",
      "Iteration: 5408, Training Loss: 9.318e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 5409, Training Loss: 1.763e+00 , Validation Loss: 3.901e+00\n",
      "Iteration: 5410, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 5411, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5412, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 5413, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5414, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5415, Training Loss: 7.555e-01 , Validation Loss: 1.355e+00\n",
      "Iteration: 5416, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 5417, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5418, Training Loss: 1.007e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 5419, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5420, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5421, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5422, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 5423, Training Loss: 1.007e+00 , Validation Loss: 1.198e+00\n",
      "Iteration: 5424, Training Loss: 2.518e+00 , Validation Loss: 5.564e+00\n",
      "Iteration: 5425, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 5426, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5427, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5428, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5429, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5430, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5431, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5432, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5433, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5434, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5435, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5436, Training Loss: 2.015e+00 , Validation Loss: 3.683e+00\n",
      "Iteration: 5437, Training Loss: 2.015e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 5438, Training Loss: 7.555e-01 , Validation Loss: 2.032e+00\n",
      "Iteration: 5439, Training Loss: 1.007e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 5440, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5441, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5442, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 5443, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5444, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 5445, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 5446, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5447, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5448, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5449, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 5450, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 5451, Training Loss: 2.267e+00 , Validation Loss: 5.207e+00\n",
      "Iteration: 5452, Training Loss: 5.037e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 5453, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 5454, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5455, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5456, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5457, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5458, Training Loss: 2.518e-01 , Validation Loss: 1.651e+00\n",
      "Iteration: 5459, Training Loss: 2.518e+00 , Validation Loss: 4.361e+00\n",
      "Iteration: 5460, Training Loss: 7.555e-01 , Validation Loss: 1.445e+00\n",
      "Iteration: 5461, Training Loss: 7.555e-01 , Validation Loss: 9.193e-01\n",
      "Iteration: 5462, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 5463, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5464, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 5465, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5466, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5467, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5468, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5469, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 5470, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5471, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5472, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5473, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5474, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5475, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5476, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5477, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 5478, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 5479, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5480, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5481, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 5482, Training Loss: 1.007e+00 , Validation Loss: 8.346e-01\n",
      "Iteration: 5483, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 5484, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5485, Training Loss: 5.037e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 5486, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5487, Training Loss: -1.000e-07 , Validation Loss: 8.467e-01\n",
      "Iteration: 5488, Training Loss: 5.037e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 5489, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5490, Training Loss: 2.518e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 5491, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 5492, Training Loss: 1.007e+00 , Validation Loss: 2.570e+00\n",
      "Iteration: 5493, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5494, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5495, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 5496, Training Loss: 5.037e-01 , Validation Loss: 1.893e+00\n",
      "Iteration: 5497, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 5498, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 5499, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 5500, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 5501, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5502, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5503, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5504, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5505, Training Loss: -1.000e-07 , Validation Loss: 7.983e-01\n",
      "Iteration: 5506, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 5507, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5508, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 5509, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 5510, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5511, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5512, Training Loss: 7.555e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 5513, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5514, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5515, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5516, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5517, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5518, Training Loss: 3.778e+00 , Validation Loss: 5.171e+00\n",
      "Iteration: 5519, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 5520, Training Loss: 3.022e+00 , Validation Loss: 5.855e+00\n",
      "Iteration: 5521, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 5522, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 5523, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 5524, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5525, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 5526, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5527, Training Loss: 7.555e-01 , Validation Loss: 1.294e+00\n",
      "Iteration: 5528, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 5529, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5530, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5531, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5532, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5533, Training Loss: 1.763e+00 , Validation Loss: 4.191e+00\n",
      "Iteration: 5534, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 5535, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 5536, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5537, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5538, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5539, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5540, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5541, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5542, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5543, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5544, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5545, Training Loss: 2.518e-01 , Validation Loss: 2.304e+00\n",
      "Iteration: 5546, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5547, Training Loss: 2.518e-01 , Validation Loss: 2.365e+00\n",
      "Iteration: 5548, Training Loss: -1.000e-07 , Validation Loss: 7.318e-01\n",
      "Iteration: 5549, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 5550, Training Loss: -1.000e-07 , Validation Loss: 7.379e-01\n",
      "Iteration: 5551, Training Loss: 1.259e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 5552, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5553, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5554, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 5555, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 5556, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5557, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5558, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 5559, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5560, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5561, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5562, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5563, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5564, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5565, Training Loss: 5.037e-01 , Validation Loss: 1.306e+00\n",
      "Iteration: 5566, Training Loss: 1.259e+00 , Validation Loss: 1.693e+00\n",
      "Iteration: 5567, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 5568, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5569, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 5570, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5571, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5572, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5573, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 5574, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 5575, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5576, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5577, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5578, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5579, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 5580, Training Loss: 3.274e+00 , Validation Loss: 4.173e+00\n",
      "Iteration: 5581, Training Loss: 5.541e+00 , Validation Loss: 6.127e+00\n",
      "Iteration: 5582, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5583, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5584, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5585, Training Loss: 5.037e-01 , Validation Loss: 3.181e+00\n",
      "Iteration: 5586, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 5587, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5588, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5589, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5590, Training Loss: 2.015e+00 , Validation Loss: 3.302e+00\n",
      "Iteration: 5591, Training Loss: 1.511e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 5592, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5593, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5594, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5595, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5596, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5597, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5598, Training Loss: 1.259e+00 , Validation Loss: 2.873e+00\n",
      "Iteration: 5599, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 5600, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5601, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5602, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5603, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5604, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5605, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5606, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 5607, Training Loss: 2.770e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 5608, Training Loss: 5.037e-01 , Validation Loss: 2.068e+00\n",
      "Iteration: 5609, Training Loss: 1.259e+00 , Validation Loss: 1.579e+00\n",
      "Iteration: 5610, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5611, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5612, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5613, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5614, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5615, Training Loss: 7.555e-01 , Validation Loss: 2.214e+00\n",
      "Iteration: 5616, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5617, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5618, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5619, Training Loss: 5.037e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 5620, Training Loss: 1.511e+00 , Validation Loss: 3.187e+00\n",
      "Iteration: 5621, Training Loss: 7.555e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 5622, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 5623, Training Loss: -1.000e-07 , Validation Loss: 7.379e-01\n",
      "Iteration: 5624, Training Loss: -1.000e-07 , Validation Loss: 7.379e-01\n",
      "Iteration: 5625, Training Loss: -1.000e-07 , Validation Loss: 7.379e-01\n",
      "Iteration: 5626, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 5627, Training Loss: 1.007e+00 , Validation Loss: 2.008e+00\n",
      "Iteration: 5628, Training Loss: 1.007e+00 , Validation Loss: 3.707e+00\n",
      "Iteration: 5629, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5630, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 5631, Training Loss: 1.511e+00 , Validation Loss: 3.943e+00\n",
      "Iteration: 5632, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 5633, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 5634, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 5635, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 5636, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 5637, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 5638, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5639, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5640, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 5641, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 5642, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 5643, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5644, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 5645, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 5646, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 5647, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 5648, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5649, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5650, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5651, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5652, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 5653, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 5654, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 5655, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 5656, Training Loss: -1.000e-07 , Validation Loss: 8.770e-01\n",
      "Iteration: 5657, Training Loss: 1.259e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 5658, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5659, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 5660, Training Loss: 1.007e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 5661, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 5662, Training Loss: 2.267e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 5663, Training Loss: 7.555e-01 , Validation Loss: 1.010e+00\n",
      "Iteration: 5664, Training Loss: 1.007e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 5665, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5666, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5667, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 5668, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 5669, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5670, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5671, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5672, Training Loss: 1.259e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 5673, Training Loss: 1.007e+00 , Validation Loss: 1.179e+00\n",
      "Iteration: 5674, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 5675, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 5676, Training Loss: 1.763e+00 , Validation Loss: 3.907e+00\n",
      "Iteration: 5677, Training Loss: 1.007e+00 , Validation Loss: 1.095e+00\n",
      "Iteration: 5678, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5679, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5680, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5681, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5682, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5683, Training Loss: 1.007e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 5684, Training Loss: 3.778e+00 , Validation Loss: 5.564e+00\n",
      "Iteration: 5685, Training Loss: 9.822e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 5686, Training Loss: 4.281e+00 , Validation Loss: 5.195e+00\n",
      "Iteration: 5687, Training Loss: 6.800e+00 , Validation Loss: 8.612e+00\n",
      "Iteration: 5688, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5689, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 5690, Training Loss: 1.511e+00 , Validation Loss: 1.893e+00\n",
      "Iteration: 5691, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5692, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5693, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5694, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5695, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5696, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5697, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5698, Training Loss: 5.037e-01 , Validation Loss: 1.240e+00\n",
      "Iteration: 5699, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5700, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 5701, Training Loss: 1.007e+00 , Validation Loss: 2.147e+00\n",
      "Iteration: 5702, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5703, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 5704, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5705, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5706, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5707, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5708, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5709, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5710, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5711, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5712, Training Loss: 1.007e+00 , Validation Loss: 2.407e+00\n",
      "Iteration: 5713, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5714, Training Loss: 1.763e+00 , Validation Loss: 4.657e+00\n",
      "Iteration: 5715, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5716, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5717, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5718, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5719, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5720, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5721, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5722, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5723, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5724, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5725, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5726, Training Loss: 3.778e+00 , Validation Loss: 4.137e+00\n",
      "Iteration: 5727, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 5728, Training Loss: 5.037e+00 , Validation Loss: 5.800e+00\n",
      "Iteration: 5729, Training Loss: 2.518e+00 , Validation Loss: 2.576e+00\n",
      "Iteration: 5730, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 5731, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 5732, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5733, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5734, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5735, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5736, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5737, Training Loss: 3.526e+00 , Validation Loss: 2.643e+00\n",
      "Iteration: 5738, Training Loss: 2.267e+00 , Validation Loss: 1.766e+00\n",
      "Iteration: 5739, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 5740, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5741, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 5742, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5743, Training Loss: 1.259e+00 , Validation Loss: 1.331e+00\n",
      "Iteration: 5744, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 5745, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5746, Training Loss: 5.037e-01 , Validation Loss: 1.119e+00\n",
      "Iteration: 5747, Training Loss: 2.770e+00 , Validation Loss: 3.357e+00\n",
      "Iteration: 5748, Training Loss: 2.015e+00 , Validation Loss: 2.280e+00\n",
      "Iteration: 5749, Training Loss: 2.015e+00 , Validation Loss: 5.141e-01\n",
      "Iteration: 5750, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 5751, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5752, Training Loss: 2.015e+00 , Validation Loss: 3.435e+00\n",
      "Iteration: 5753, Training Loss: 1.007e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 5754, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 5755, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5756, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5757, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5758, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 5759, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5760, Training Loss: 1.007e+00 , Validation Loss: 1.488e+00\n",
      "Iteration: 5761, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5762, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 5763, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5764, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5765, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5766, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5767, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5768, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5769, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 5770, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5771, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5772, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 5773, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5774, Training Loss: -1.000e-07 , Validation Loss: 1.077e+00\n",
      "Iteration: 5775, Training Loss: 7.555e-01 , Validation Loss: 1.077e+00\n",
      "Iteration: 5776, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5777, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5778, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5779, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5780, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5781, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5782, Training Loss: 1.007e+00 , Validation Loss: 9.677e-01\n",
      "Iteration: 5783, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5784, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5785, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5786, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5787, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5788, Training Loss: 1.259e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 5789, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5790, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5791, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5792, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5793, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 5794, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5795, Training Loss: 2.518e+00 , Validation Loss: 3.520e+00\n",
      "Iteration: 5796, Training Loss: 7.555e-01 , Validation Loss: 1.046e+00\n",
      "Iteration: 5797, Training Loss: 1.259e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 5798, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5799, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 5800, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5801, Training Loss: 2.015e+00 , Validation Loss: 3.695e+00\n",
      "Iteration: 5802, Training Loss: 1.511e+00 , Validation Loss: 1.167e+00\n",
      "Iteration: 5803, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5804, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5805, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 5806, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 5807, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 5808, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 5809, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 5810, Training Loss: 4.030e+00 , Validation Loss: 4.838e+00\n",
      "Iteration: 5811, Training Loss: 1.158e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 5812, Training Loss: 2.518e+00 , Validation Loss: 5.570e+00\n",
      "Iteration: 5813, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 5814, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5815, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5816, Training Loss: 5.037e-01 , Validation Loss: 1.827e+00\n",
      "Iteration: 5817, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5818, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5819, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5820, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5821, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5822, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5823, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5824, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5825, Training Loss: 7.555e-01 , Validation Loss: 1.566e+00\n",
      "Iteration: 5826, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5827, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5828, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5829, Training Loss: 1.259e+00 , Validation Loss: 1.554e+00\n",
      "Iteration: 5830, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5831, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5832, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5833, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5834, Training Loss: 1.007e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 5835, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5836, Training Loss: 5.037e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 5837, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5838, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5839, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5840, Training Loss: 2.015e+00 , Validation Loss: 3.459e+00\n",
      "Iteration: 5841, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 5842, Training Loss: 1.511e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 5843, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5844, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5845, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 5846, Training Loss: 2.015e+00 , Validation Loss: 4.167e+00\n",
      "Iteration: 5847, Training Loss: 5.037e-01 , Validation Loss: 1.077e+00\n",
      "Iteration: 5848, Training Loss: 1.007e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 5849, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 5850, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5851, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 5852, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 5853, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5854, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5855, Training Loss: 5.037e-01 , Validation Loss: 1.591e+00\n",
      "Iteration: 5856, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5857, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5858, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5859, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5860, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5861, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 5862, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5863, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5864, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 5865, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5866, Training Loss: 1.259e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 5867, Training Loss: 1.007e+00 , Validation Loss: 6.532e-01\n",
      "Iteration: 5868, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 5869, Training Loss: 1.259e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 5870, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5871, Training Loss: 1.511e+00 , Validation Loss: 3.556e+00\n",
      "Iteration: 5872, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 5873, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 5874, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5875, Training Loss: 4.030e+00 , Validation Loss: 4.929e+00\n",
      "Iteration: 5876, Training Loss: 7.052e+00 , Validation Loss: 7.354e+00\n",
      "Iteration: 5877, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 5878, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5879, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5880, Training Loss: 1.763e+00 , Validation Loss: 3.218e+00\n",
      "Iteration: 5881, Training Loss: 2.267e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 5882, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5883, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5884, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5885, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5886, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5887, Training Loss: 1.259e+00 , Validation Loss: 2.486e+00\n",
      "Iteration: 5888, Training Loss: 1.007e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 5889, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5890, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5891, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5892, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5893, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5894, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5895, Training Loss: 1.259e+00 , Validation Loss: 5.504e-01\n",
      "Iteration: 5896, Training Loss: 1.511e+00 , Validation Loss: 3.822e+00\n",
      "Iteration: 5897, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 5898, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5899, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 5900, Training Loss: 5.037e-01 , Validation Loss: 1.234e+00\n",
      "Iteration: 5901, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 5902, Training Loss: 5.037e-01 , Validation Loss: 1.179e+00\n",
      "Iteration: 5903, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5904, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 5905, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 5906, Training Loss: 1.511e+00 , Validation Loss: 3.955e+00\n",
      "Iteration: 5907, Training Loss: 2.518e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 5908, Training Loss: 1.763e+00 , Validation Loss: 4.457e+00\n",
      "Iteration: 5909, Training Loss: 7.555e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 5910, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 5911, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5912, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 5913, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5914, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5915, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5916, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5917, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5918, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5919, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5920, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5921, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5922, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5923, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5924, Training Loss: 2.770e+00 , Validation Loss: 4.173e+00\n",
      "Iteration: 5925, Training Loss: 2.015e+00 , Validation Loss: 1.258e+00\n",
      "Iteration: 5926, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 5927, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 5928, Training Loss: 7.555e-01 , Validation Loss: 1.004e+00\n",
      "Iteration: 5929, Training Loss: 3.022e+00 , Validation Loss: 4.439e+00\n",
      "Iteration: 5930, Training Loss: 2.770e+00 , Validation Loss: 3.798e+00\n",
      "Iteration: 5931, Training Loss: 1.259e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 5932, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5933, Training Loss: 7.555e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 5934, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 5935, Training Loss: 2.518e-01 , Validation Loss: 9.858e-01\n",
      "Iteration: 5936, Training Loss: 2.518e+00 , Validation Loss: 4.167e+00\n",
      "Iteration: 5937, Training Loss: 1.763e+00 , Validation Loss: 2.740e+00\n",
      "Iteration: 5938, Training Loss: 2.015e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 5939, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5940, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5941, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5942, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5943, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5944, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5945, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 5946, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 5947, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5948, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 5949, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 5950, Training Loss: 2.518e+00 , Validation Loss: 5.080e+00\n",
      "Iteration: 5951, Training Loss: 1.259e+00 , Validation Loss: 1.645e+00\n",
      "Iteration: 5952, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 5953, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5954, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5955, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 5956, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 5957, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 5958, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 5959, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5960, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 5961, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 5962, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 5963, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5964, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 5965, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5966, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5967, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5968, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5969, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5970, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 5971, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5972, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5973, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5974, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5975, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5976, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5977, Training Loss: 1.763e+00 , Validation Loss: 3.641e+00\n",
      "Iteration: 5978, Training Loss: 2.015e+00 , Validation Loss: 1.736e+00\n",
      "Iteration: 5979, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 5980, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 5981, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5982, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5983, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 5984, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 5985, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 5986, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 5987, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 5988, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 5989, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 5990, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5991, Training Loss: 5.037e-01 , Validation Loss: 1.228e+00\n",
      "Iteration: 5992, Training Loss: 1.763e+00 , Validation Loss: 2.147e+00\n",
      "Iteration: 5993, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5994, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5995, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5996, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 5997, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5998, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 5999, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 6000, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 6001, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6002, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 6003, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6004, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6005, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6006, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6007, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6008, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 6009, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 6010, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 6011, Training Loss: 1.007e+00 , Validation Loss: 3.205e+00\n",
      "Iteration: 6012, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 6013, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6014, Training Loss: -1.000e-07 , Validation Loss: 1.131e+00\n",
      "Iteration: 6015, Training Loss: 5.037e-01 , Validation Loss: 1.131e+00\n",
      "Iteration: 6016, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6017, Training Loss: 7.555e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 6018, Training Loss: 3.274e+00 , Validation Loss: 4.082e+00\n",
      "Iteration: 6019, Training Loss: 1.763e+00 , Validation Loss: 1.947e+00\n",
      "Iteration: 6020, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 6021, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6022, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6023, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6024, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6025, Training Loss: 5.037e-01 , Validation Loss: 8.830e-01\n",
      "Iteration: 6026, Training Loss: 7.555e-01 , Validation Loss: 1.379e+00\n",
      "Iteration: 6027, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 6028, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6029, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 6030, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 6031, Training Loss: 1.259e+00 , Validation Loss: 2.867e+00\n",
      "Iteration: 6032, Training Loss: 1.511e+00 , Validation Loss: 4.838e-01\n",
      "Iteration: 6033, Training Loss: 3.778e+00 , Validation Loss: 3.792e+00\n",
      "Iteration: 6034, Training Loss: 7.807e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 6035, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6036, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 6037, Training Loss: 1.511e+00 , Validation Loss: 2.480e+00\n",
      "Iteration: 6038, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 6039, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6040, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 6041, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6042, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6043, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6044, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6045, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6046, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6047, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 6048, Training Loss: 1.763e+00 , Validation Loss: 3.647e+00\n",
      "Iteration: 6049, Training Loss: 1.763e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 6050, Training Loss: 2.770e+00 , Validation Loss: 4.318e+00\n",
      "Iteration: 6051, Training Loss: 1.259e+00 , Validation Loss: 1.294e+00\n",
      "Iteration: 6052, Training Loss: 1.007e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 6053, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6054, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6055, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6056, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6057, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6058, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6059, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6060, Training Loss: 2.015e+00 , Validation Loss: 3.562e+00\n",
      "Iteration: 6061, Training Loss: 1.007e+00 , Validation Loss: 1.095e+00\n",
      "Iteration: 6062, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 6063, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6064, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6065, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 6066, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 6067, Training Loss: 3.274e+00 , Validation Loss: 4.494e+00\n",
      "Iteration: 6068, Training Loss: 3.526e+00 , Validation Loss: 4.191e+00\n",
      "Iteration: 6069, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 6070, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6071, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6072, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6073, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6074, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6075, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6076, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6077, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6078, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6079, Training Loss: 2.518e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 6080, Training Loss: 2.770e+00 , Validation Loss: 3.871e+00\n",
      "Iteration: 6081, Training Loss: 1.007e+00 , Validation Loss: 1.204e+00\n",
      "Iteration: 6082, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 6083, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 6084, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 6085, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 6086, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6087, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6088, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6089, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6090, Training Loss: 7.555e-01 , Validation Loss: 8.407e-01\n",
      "Iteration: 6091, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6092, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6093, Training Loss: 1.511e+00 , Validation Loss: 3.453e+00\n",
      "Iteration: 6094, Training Loss: 1.763e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 6095, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 6096, Training Loss: 2.267e+00 , Validation Loss: 3.913e+00\n",
      "Iteration: 6097, Training Loss: 2.518e+00 , Validation Loss: 2.528e+00\n",
      "Iteration: 6098, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6099, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 6100, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 6101, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6102, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6103, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6104, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6105, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6106, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 6107, Training Loss: 1.007e+00 , Validation Loss: 2.238e+00\n",
      "Iteration: 6108, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 6109, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6110, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6111, Training Loss: 5.037e-01 , Validation Loss: 1.391e+00\n",
      "Iteration: 6112, Training Loss: 2.015e+00 , Validation Loss: 3.030e+00\n",
      "Iteration: 6113, Training Loss: 1.511e+00 , Validation Loss: 1.077e+00\n",
      "Iteration: 6114, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6115, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6116, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6117, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6118, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6119, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6120, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6121, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6122, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6123, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6124, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6125, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6126, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6127, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 6128, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 6129, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 6130, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6131, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6132, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6133, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6134, Training Loss: 7.555e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 6135, Training Loss: 2.518e+00 , Validation Loss: 3.738e+00\n",
      "Iteration: 6136, Training Loss: 1.511e+00 , Validation Loss: 2.195e+00\n",
      "Iteration: 6137, Training Loss: 1.763e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 6138, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 6139, Training Loss: 1.007e+00 , Validation Loss: 4.234e-01\n",
      "Iteration: 6140, Training Loss: 5.037e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 6141, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6142, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6143, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 6144, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6145, Training Loss: 5.037e-01 , Validation Loss: 1.373e+00\n",
      "Iteration: 6146, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6147, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 6148, Training Loss: 1.007e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 6149, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6150, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6151, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 6152, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6153, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6154, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 6155, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6156, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 6157, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6158, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 6159, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 6160, Training Loss: 7.555e-01 , Validation Loss: 1.210e+00\n",
      "Iteration: 6161, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6162, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 6163, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6164, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6165, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6166, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6167, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6168, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6169, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 6170, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6171, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 6172, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6173, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 6174, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 6175, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6176, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6177, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6178, Training Loss: 1.007e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 6179, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6180, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 6181, Training Loss: 2.015e+00 , Validation Loss: 5.117e+00\n",
      "Iteration: 6182, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6183, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6184, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6185, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 6186, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 6187, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6188, Training Loss: 5.037e-01 , Validation Loss: 1.161e+00\n",
      "Iteration: 6189, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6190, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6191, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6192, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6193, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6194, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6195, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 6196, Training Loss: 1.763e+00 , Validation Loss: 4.016e+00\n",
      "Iteration: 6197, Training Loss: 2.518e-01 , Validation Loss: 1.022e+00\n",
      "Iteration: 6198, Training Loss: 2.518e-01 , Validation Loss: 9.133e-01\n",
      "Iteration: 6199, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 6200, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 6201, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 6202, Training Loss: -1.000e-07 , Validation Loss: 4.838e-01\n",
      "Iteration: 6203, Training Loss: 1.007e+00 , Validation Loss: 4.838e-01\n",
      "Iteration: 6204, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6205, Training Loss: 5.037e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 6206, Training Loss: 5.037e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 6207, Training Loss: 2.518e+00 , Validation Loss: 4.578e+00\n",
      "Iteration: 6208, Training Loss: 2.015e+00 , Validation Loss: 2.256e+00\n",
      "Iteration: 6209, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 6210, Training Loss: 7.555e-01 , Validation Loss: 1.923e+00\n",
      "Iteration: 6211, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 6212, Training Loss: 1.007e+00 , Validation Loss: 1.064e+00\n",
      "Iteration: 6213, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 6214, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6215, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6216, Training Loss: 1.259e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 6217, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6218, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6219, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6220, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6221, Training Loss: 2.267e+00 , Validation Loss: 3.526e+00\n",
      "Iteration: 6222, Training Loss: 1.763e+00 , Validation Loss: 1.264e+00\n",
      "Iteration: 6223, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 6224, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6225, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6226, Training Loss: 1.259e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 6227, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6228, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6229, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6230, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6231, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6232, Training Loss: 2.518e-01 , Validation Loss: 1.016e+00\n",
      "Iteration: 6233, Training Loss: 2.518e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 6234, Training Loss: 1.259e+00 , Validation Loss: 2.595e+00\n",
      "Iteration: 6235, Training Loss: 1.259e+00 , Validation Loss: 1.010e+00\n",
      "Iteration: 6236, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6237, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 6238, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6239, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6240, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 6241, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 6242, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 6243, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 6244, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 6245, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6246, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6247, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6248, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 6249, Training Loss: 1.007e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 6250, Training Loss: 1.259e+00 , Validation Loss: 2.716e+00\n",
      "Iteration: 6251, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 6252, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6253, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6254, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6255, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6256, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6257, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6258, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 6259, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 6260, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 6261, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6262, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6263, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6264, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6265, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6266, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6267, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 6268, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 6269, Training Loss: 1.259e+00 , Validation Loss: 3.695e+00\n",
      "Iteration: 6270, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 6271, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 6272, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 6273, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6274, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6275, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 6276, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 6277, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 6278, Training Loss: 1.259e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 6279, Training Loss: 7.555e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 6280, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6281, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 6282, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6283, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 6284, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6285, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 6286, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 6287, Training Loss: 2.015e+00 , Validation Loss: 4.494e+00\n",
      "Iteration: 6288, Training Loss: 5.037e-01 , Validation Loss: 1.331e+00\n",
      "Iteration: 6289, Training Loss: 1.763e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 6290, Training Loss: 1.259e+00 , Validation Loss: 1.373e+00\n",
      "Iteration: 6291, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6292, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6293, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 6294, Training Loss: 2.518e-01 , Validation Loss: 1.409e+00\n",
      "Iteration: 6295, Training Loss: 3.274e+00 , Validation Loss: 4.070e+00\n",
      "Iteration: 6296, Training Loss: 1.763e+00 , Validation Loss: 1.875e+00\n",
      "Iteration: 6297, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6298, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6299, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6300, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6301, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 6302, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6303, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6304, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6305, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6306, Training Loss: 2.267e+00 , Validation Loss: 4.209e+00\n",
      "Iteration: 6307, Training Loss: 2.015e+00 , Validation Loss: 1.669e+00\n",
      "Iteration: 6308, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 6309, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6310, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6311, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 6312, Training Loss: -1.000e-07 , Validation Loss: 1.911e+00\n",
      "Iteration: 6313, Training Loss: 2.518e-01 , Validation Loss: 1.911e+00\n",
      "Iteration: 6314, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 6315, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6316, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6317, Training Loss: 3.526e+00 , Validation Loss: 4.778e+00\n",
      "Iteration: 6318, Training Loss: 6.548e+00 , Validation Loss: 5.074e+00\n",
      "Iteration: 6319, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6320, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6321, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6322, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6323, Training Loss: 2.015e+00 , Validation Loss: 3.296e+00\n",
      "Iteration: 6324, Training Loss: 2.267e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 6325, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6326, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6327, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6328, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6329, Training Loss: 3.274e+00 , Validation Loss: 5.226e+00\n",
      "Iteration: 6330, Training Loss: 2.015e+00 , Validation Loss: 1.712e+00\n",
      "Iteration: 6331, Training Loss: 1.259e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 6332, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6333, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6334, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6335, Training Loss: 2.015e+00 , Validation Loss: 4.004e+00\n",
      "Iteration: 6336, Training Loss: 5.037e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 6337, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6338, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6339, Training Loss: 1.007e+00 , Validation Loss: 4.234e-01\n",
      "Iteration: 6340, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6341, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6342, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6343, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6344, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6345, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6346, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 6347, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 6348, Training Loss: 2.518e+00 , Validation Loss: 2.625e+00\n",
      "Iteration: 6349, Training Loss: 5.037e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 6350, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6351, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6352, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 6353, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6354, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6355, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6356, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 6357, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 6358, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 6359, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6360, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6361, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 6362, Training Loss: 1.763e+00 , Validation Loss: 2.776e+00\n",
      "Iteration: 6363, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6364, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 6365, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6366, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 6367, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 6368, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6369, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6370, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 6371, Training Loss: 2.518e+00 , Validation Loss: 4.403e+00\n",
      "Iteration: 6372, Training Loss: 1.763e+00 , Validation Loss: 1.439e+00\n",
      "Iteration: 6373, Training Loss: 1.007e+00 , Validation Loss: 4.838e-01\n",
      "Iteration: 6374, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6375, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6376, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6377, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 6378, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6379, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 6380, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 6381, Training Loss: 1.259e+00 , Validation Loss: 3.048e+00\n",
      "Iteration: 6382, Training Loss: 1.259e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 6383, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6384, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6385, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6386, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6387, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 6388, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 6389, Training Loss: 5.037e-01 , Validation Loss: 1.312e+00\n",
      "Iteration: 6390, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 6391, Training Loss: 1.763e+00 , Validation Loss: 3.756e+00\n",
      "Iteration: 6392, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 6393, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 6394, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6395, Training Loss: 2.267e+00 , Validation Loss: 1.990e+00\n",
      "Iteration: 6396, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 6397, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6398, Training Loss: 3.526e+00 , Validation Loss: 4.784e+00\n",
      "Iteration: 6399, Training Loss: 1.511e+00 , Validation Loss: 3.459e+00\n",
      "Iteration: 6400, Training Loss: 1.259e+00 , Validation Loss: 1.167e+00\n",
      "Iteration: 6401, Training Loss: 7.555e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 6402, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6403, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6404, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6405, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 6406, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 6407, Training Loss: 2.518e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 6408, Training Loss: 1.259e+00 , Validation Loss: 2.062e+00\n",
      "Iteration: 6409, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6410, Training Loss: 5.037e-01 , Validation Loss: 2.280e+00\n",
      "Iteration: 6411, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6412, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6413, Training Loss: -1.000e-07 , Validation Loss: 6.774e-01\n",
      "Iteration: 6414, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 6415, Training Loss: 3.274e+00 , Validation Loss: 4.391e+00\n",
      "Iteration: 6416, Training Loss: 1.007e+00 , Validation Loss: 2.347e+00\n",
      "Iteration: 6417, Training Loss: 1.007e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 6418, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6419, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 6420, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6421, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6422, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 6423, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6424, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 6425, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6426, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6427, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6428, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6429, Training Loss: 1.259e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 6430, Training Loss: 7.555e-01 , Validation Loss: 1.367e+00\n",
      "Iteration: 6431, Training Loss: -1.000e-07 , Validation Loss: 6.834e-01\n",
      "Iteration: 6432, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 6433, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 6434, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 6435, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 6436, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 6437, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6438, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6439, Training Loss: -1.000e-07 , Validation Loss: 7.983e-01\n",
      "Iteration: 6440, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 6441, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6442, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 6443, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6444, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6445, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6446, Training Loss: 1.511e+00 , Validation Loss: 1.579e+00\n",
      "Iteration: 6447, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6448, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6449, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6450, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6451, Training Loss: 2.518e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 6452, Training Loss: 1.259e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 6453, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 6454, Training Loss: 7.555e-01 , Validation Loss: 1.083e+00\n",
      "Iteration: 6455, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6456, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6457, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6458, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6459, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6460, Training Loss: 2.015e+00 , Validation Loss: 4.246e+00\n",
      "Iteration: 6461, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 6462, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 6463, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6464, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 6465, Training Loss: 2.015e+00 , Validation Loss: 1.518e+00\n",
      "Iteration: 6466, Training Loss: 2.015e+00 , Validation Loss: 8.044e-01\n",
      "Iteration: 6467, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 6468, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 6469, Training Loss: 1.511e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 6470, Training Loss: 1.007e+00 , Validation Loss: 3.133e+00\n",
      "Iteration: 6471, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6472, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 6473, Training Loss: 1.259e+00 , Validation Loss: 1.161e+00\n",
      "Iteration: 6474, Training Loss: 1.259e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 6475, Training Loss: 7.555e-01 , Validation Loss: 3.284e+00\n",
      "Iteration: 6476, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6477, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6478, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6479, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 6480, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6481, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6482, Training Loss: 2.015e+00 , Validation Loss: 4.131e+00\n",
      "Iteration: 6483, Training Loss: 1.259e+00 , Validation Loss: 1.246e+00\n",
      "Iteration: 6484, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6485, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 6486, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 6487, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6488, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 6489, Training Loss: 1.007e+00 , Validation Loss: 2.443e+00\n",
      "Iteration: 6490, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 6491, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6492, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 6493, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6494, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 6495, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 6496, Training Loss: 1.007e+00 , Validation Loss: 1.935e+00\n",
      "Iteration: 6497, Training Loss: -1.000e-07 , Validation Loss: 6.169e-01\n",
      "Iteration: 6498, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 6499, Training Loss: 7.555e-01 , Validation Loss: 2.353e+00\n",
      "Iteration: 6500, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6501, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6502, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6503, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6504, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 6505, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 6506, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 6507, Training Loss: 2.267e+00 , Validation Loss: 3.732e+00\n",
      "Iteration: 6508, Training Loss: 2.518e+00 , Validation Loss: 1.258e+00\n",
      "Iteration: 6509, Training Loss: 3.022e+00 , Validation Loss: 4.022e+00\n",
      "Iteration: 6510, Training Loss: 8.815e+00 , Validation Loss: 7.125e+00\n",
      "Iteration: 6511, Training Loss: 4.281e+00 , Validation Loss: 5.232e+00\n",
      "Iteration: 6512, Training Loss: 4.533e+00 , Validation Loss: 4.421e+00\n",
      "Iteration: 6513, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6514, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6515, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 6516, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6517, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6518, Training Loss: 1.511e+00 , Validation Loss: 4.076e+00\n",
      "Iteration: 6519, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6520, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6521, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6522, Training Loss: 1.763e+00 , Validation Loss: 3.187e+00\n",
      "Iteration: 6523, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6524, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 6525, Training Loss: 2.015e+00 , Validation Loss: 3.707e+00\n",
      "Iteration: 6526, Training Loss: 1.007e+00 , Validation Loss: 6.350e-01\n",
      "Iteration: 6527, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 6528, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6529, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 6530, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6531, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 6532, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6533, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6534, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6535, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6536, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 6537, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 6538, Training Loss: 2.015e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6539, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6540, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 6541, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6542, Training Loss: 2.518e+00 , Validation Loss: 2.921e+00\n",
      "Iteration: 6543, Training Loss: 1.511e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 6544, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6545, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6546, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 6547, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 6548, Training Loss: 1.763e+00 , Validation Loss: 3.883e+00\n",
      "Iteration: 6549, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 6550, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 6551, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6552, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6553, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 6554, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 6555, Training Loss: 2.518e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 6556, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 6557, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 6558, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6559, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6560, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6561, Training Loss: 3.022e+00 , Validation Loss: 5.111e+00\n",
      "Iteration: 6562, Training Loss: 2.015e+00 , Validation Loss: 1.276e+00\n",
      "Iteration: 6563, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6564, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6565, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6566, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6567, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 6568, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6569, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6570, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 6571, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6572, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 6573, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 6574, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6575, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6576, Training Loss: 1.007e+00 , Validation Loss: 1.990e+00\n",
      "Iteration: 6577, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6578, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6579, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6580, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 6581, Training Loss: 1.007e+00 , Validation Loss: 1.101e+00\n",
      "Iteration: 6582, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 6583, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6584, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6585, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6586, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 6587, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6588, Training Loss: 3.022e+00 , Validation Loss: 4.790e+00\n",
      "Iteration: 6589, Training Loss: 7.555e-01 , Validation Loss: 1.845e+00\n",
      "Iteration: 6590, Training Loss: 2.015e+00 , Validation Loss: 1.234e+00\n",
      "Iteration: 6591, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6592, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6593, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6594, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6595, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6596, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6597, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6598, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6599, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 6600, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 6601, Training Loss: 2.518e-01 , Validation Loss: 9.737e-01\n",
      "Iteration: 6602, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 6603, Training Loss: 3.022e+00 , Validation Loss: 5.159e+00\n",
      "Iteration: 6604, Training Loss: 2.518e+00 , Validation Loss: 1.899e+00\n",
      "Iteration: 6605, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6606, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6607, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 6608, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 6609, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 6610, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 6611, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6612, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6613, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 6614, Training Loss: 7.555e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 6615, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 6616, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6617, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 6618, Training Loss: 7.555e-01 , Validation Loss: 1.119e+00\n",
      "Iteration: 6619, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6620, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 6621, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 6622, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 6623, Training Loss: 1.007e+00 , Validation Loss: 1.379e+00\n",
      "Iteration: 6624, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 6625, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6626, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 6627, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 6628, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6629, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6630, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 6631, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 6632, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6633, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 6634, Training Loss: 7.555e-01 , Validation Loss: 1.494e+00\n",
      "Iteration: 6635, Training Loss: 1.007e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 6636, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6637, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6638, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 6639, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6640, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 6641, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6642, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 6643, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 6644, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 6645, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 6646, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 6647, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 6648, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6649, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6650, Training Loss: 7.555e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 6651, Training Loss: 2.015e+00 , Validation Loss: 3.278e+00\n",
      "Iteration: 6652, Training Loss: 5.037e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 6653, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 6654, Training Loss: 1.259e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 6655, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 6656, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 6657, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6658, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6659, Training Loss: 7.555e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 6660, Training Loss: 2.518e-01 , Validation Loss: 1.216e+00\n",
      "Iteration: 6661, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 6662, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6663, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6664, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 6665, Training Loss: 1.259e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 6666, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 6667, Training Loss: 4.030e+00 , Validation Loss: 5.195e+00\n",
      "Iteration: 6668, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 6669, Training Loss: 3.778e+00 , Validation Loss: 5.861e+00\n",
      "Iteration: 6670, Training Loss: 1.511e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 6671, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6672, Training Loss: 1.763e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 6673, Training Loss: 1.511e+00 , Validation Loss: 4.748e+00\n",
      "Iteration: 6674, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6675, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6676, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 6677, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6678, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6679, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6680, Training Loss: 1.007e+00 , Validation Loss: 4.167e+00\n",
      "Iteration: 6681, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6682, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6683, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6684, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6685, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6686, Training Loss: 1.511e+00 , Validation Loss: 3.429e+00\n",
      "Iteration: 6687, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 6688, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6689, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6690, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6691, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6692, Training Loss: 2.267e+00 , Validation Loss: 3.447e+00\n",
      "Iteration: 6693, Training Loss: 7.555e-01 , Validation Loss: 1.760e+00\n",
      "Iteration: 6694, Training Loss: 1.511e+00 , Validation Loss: 1.119e+00\n",
      "Iteration: 6695, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6696, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 6697, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6698, Training Loss: 5.037e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 6699, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6700, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6701, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 6702, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 6703, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 6704, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 6705, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 6706, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6707, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6708, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6709, Training Loss: 7.555e-01 , Validation Loss: 2.492e+00\n",
      "Iteration: 6710, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6711, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 6712, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 6713, Training Loss: 7.555e-01 , Validation Loss: 2.377e+00\n",
      "Iteration: 6714, Training Loss: 2.015e+00 , Validation Loss: 4.222e+00\n",
      "Iteration: 6715, Training Loss: 1.763e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 6716, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 6717, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 6718, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 6719, Training Loss: 7.555e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 6720, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6721, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6722, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6723, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6724, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 6725, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 6726, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 6727, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6728, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6729, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6730, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6731, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 6732, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6733, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 6734, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 6735, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 6736, Training Loss: 7.555e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 6737, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 6738, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6739, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6740, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6741, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6742, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 6743, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 6744, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 6745, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 6746, Training Loss: -1.000e-07 , Validation Loss: 1.881e+00\n",
      "Iteration: 6747, Training Loss: 2.518e-01 , Validation Loss: 1.881e+00\n",
      "Iteration: 6748, Training Loss: 5.037e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 6749, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6750, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6751, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6752, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 6753, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6754, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6755, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 6756, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 6757, Training Loss: 7.555e-01 , Validation Loss: 1.161e+00\n",
      "Iteration: 6758, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6759, Training Loss: 7.555e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 6760, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 6761, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 6762, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 6763, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6764, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 6765, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6766, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6767, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6768, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 6769, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6770, Training Loss: 2.518e-01 , Validation Loss: 1.077e+00\n",
      "Iteration: 6771, Training Loss: 1.259e+00 , Validation Loss: 5.443e-01\n",
      "Iteration: 6772, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6773, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6774, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6775, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6776, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 6777, Training Loss: 1.007e+00 , Validation Loss: 5.504e-01\n",
      "Iteration: 6778, Training Loss: 1.763e+00 , Validation Loss: 4.167e+00\n",
      "Iteration: 6779, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 6780, Training Loss: 2.518e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 6781, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 6782, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6783, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6784, Training Loss: 2.518e-01 , Validation Loss: 8.407e-01\n",
      "Iteration: 6785, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 6786, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 6787, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 6788, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 6789, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 6790, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6791, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 6792, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6793, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6794, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6795, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6796, Training Loss: 1.259e+00 , Validation Loss: 1.077e+00\n",
      "Iteration: 6797, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 6798, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6799, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 6800, Training Loss: 1.511e+00 , Validation Loss: 4.270e+00\n",
      "Iteration: 6801, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 6802, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 6803, Training Loss: -1.000e-07 , Validation Loss: 7.621e-01\n",
      "Iteration: 6804, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 6805, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6806, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6807, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6808, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 6809, Training Loss: 1.259e+00 , Validation Loss: 2.020e+00\n",
      "Iteration: 6810, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6811, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6812, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6813, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6814, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 6815, Training Loss: 5.037e-01 , Validation Loss: 1.742e+00\n",
      "Iteration: 6816, Training Loss: 1.007e+00 , Validation Loss: 8.225e-01\n",
      "Iteration: 6817, Training Loss: 4.030e+00 , Validation Loss: 5.111e+00\n",
      "Iteration: 6818, Training Loss: 9.066e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 6819, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6820, Training Loss: 1.007e+00 , Validation Loss: 2.353e+00\n",
      "Iteration: 6821, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6822, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6823, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6824, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6825, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 6826, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6827, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6828, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 6829, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 6830, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6831, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6832, Training Loss: 1.511e+00 , Validation Loss: 2.970e+00\n",
      "Iteration: 6833, Training Loss: 1.763e+00 , Validation Loss: 5.988e-01\n",
      "Iteration: 6834, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6835, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6836, Training Loss: 1.259e+00 , Validation Loss: 2.637e+00\n",
      "Iteration: 6837, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6838, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6839, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6840, Training Loss: 3.022e+00 , Validation Loss: 5.044e+00\n",
      "Iteration: 6841, Training Loss: 2.015e+00 , Validation Loss: 1.778e+00\n",
      "Iteration: 6842, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 6843, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6844, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6845, Training Loss: 2.770e+00 , Validation Loss: 3.853e+00\n",
      "Iteration: 6846, Training Loss: 2.770e+00 , Validation Loss: 3.459e+00\n",
      "Iteration: 6847, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6848, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 6849, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6850, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6851, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6852, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6853, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6854, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 6855, Training Loss: 2.015e+00 , Validation Loss: 3.798e+00\n",
      "Iteration: 6856, Training Loss: 1.763e+00 , Validation Loss: 1.107e+00\n",
      "Iteration: 6857, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 6858, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6859, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6860, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6861, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6862, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 6863, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 6864, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 6865, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6866, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6867, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 6868, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6869, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6870, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6871, Training Loss: 1.007e+00 , Validation Loss: 4.355e-01\n",
      "Iteration: 6872, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6873, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6874, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 6875, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6876, Training Loss: 2.015e+00 , Validation Loss: 3.478e+00\n",
      "Iteration: 6877, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 6878, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6879, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 6880, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 6881, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6882, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6883, Training Loss: 7.555e-01 , Validation Loss: 2.201e+00\n",
      "Iteration: 6884, Training Loss: 2.518e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 6885, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 6886, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 6887, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6888, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 6889, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6890, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6891, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6892, Training Loss: 3.022e+00 , Validation Loss: 4.457e+00\n",
      "Iteration: 6893, Training Loss: 3.526e+00 , Validation Loss: 4.397e+00\n",
      "Iteration: 6894, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 6895, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 6896, Training Loss: 3.022e+00 , Validation Loss: 4.058e+00\n",
      "Iteration: 6897, Training Loss: 2.518e+00 , Validation Loss: 1.954e+00\n",
      "Iteration: 6898, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 6899, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6900, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6901, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6902, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 6903, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 6904, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6905, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6906, Training Loss: 2.015e+00 , Validation Loss: 3.054e+00\n",
      "Iteration: 6907, Training Loss: 1.763e+00 , Validation Loss: 7.802e-01\n",
      "Iteration: 6908, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6909, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6910, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 6911, Training Loss: 4.030e+00 , Validation Loss: 5.728e+00\n",
      "Iteration: 6912, Training Loss: 4.785e+00 , Validation Loss: 4.433e+00\n",
      "Iteration: 6913, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 6914, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6915, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 6916, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6917, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6918, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6919, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6920, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6921, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6922, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6923, Training Loss: 5.037e-01 , Validation Loss: 2.734e+00\n",
      "Iteration: 6924, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6925, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6926, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6927, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 6928, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6929, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6930, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6931, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6932, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6933, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6934, Training Loss: 7.555e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 6935, Training Loss: 2.015e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 6936, Training Loss: 1.511e+00 , Validation Loss: 1.004e+00\n",
      "Iteration: 6937, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 6938, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6939, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6940, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6941, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6942, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6943, Training Loss: 2.770e+00 , Validation Loss: 4.228e+00\n",
      "Iteration: 6944, Training Loss: 7.555e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 6945, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 6946, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6947, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 6948, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 6949, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6950, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6951, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 6952, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6953, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 6954, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6955, Training Loss: 1.259e+00 , Validation Loss: 3.357e+00\n",
      "Iteration: 6956, Training Loss: 1.259e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 6957, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6958, Training Loss: 3.022e+00 , Validation Loss: 4.409e+00\n",
      "Iteration: 6959, Training Loss: 1.763e+00 , Validation Loss: 1.954e+00\n",
      "Iteration: 6960, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 6961, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6962, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6963, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6964, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6965, Training Loss: 1.007e+00 , Validation Loss: 2.359e+00\n",
      "Iteration: 6966, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6967, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 6968, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6969, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6970, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 6971, Training Loss: -1.000e-07 , Validation Loss: 1.276e+00\n",
      "Iteration: 6972, Training Loss: -1.000e-07 , Validation Loss: 1.276e+00\n",
      "Iteration: 6973, Training Loss: 1.763e+00 , Validation Loss: 1.276e+00\n",
      "Iteration: 6974, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 6975, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6976, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6977, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 6978, Training Loss: -1.000e-07 , Validation Loss: 6.411e-01\n",
      "Iteration: 6979, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 6980, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6981, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6982, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6983, Training Loss: 1.007e+00 , Validation Loss: 2.401e+00\n",
      "Iteration: 6984, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6985, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6986, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 6987, Training Loss: 1.259e+00 , Validation Loss: 3.054e+00\n",
      "Iteration: 6988, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 6989, Training Loss: 1.007e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 6990, Training Loss: 3.778e+00 , Validation Loss: 5.044e+00\n",
      "Iteration: 6991, Training Loss: 9.318e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 6992, Training Loss: 5.037e-01 , Validation Loss: 1.488e+00\n",
      "Iteration: 6993, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 6994, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6995, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6996, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 6997, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 6998, Training Loss: 7.555e-01 , Validation Loss: 1.476e+00\n",
      "Iteration: 6999, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7000, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7001, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7002, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7003, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7004, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7005, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7006, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7007, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7008, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7009, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7010, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7011, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7012, Training Loss: 3.778e+00 , Validation Loss: 3.998e+00\n",
      "Iteration: 7013, Training Loss: 4.533e+00 , Validation Loss: 5.467e+00\n",
      "Iteration: 7014, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7015, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7016, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 7017, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7018, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 7019, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7020, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7021, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7022, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7023, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7024, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7025, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7026, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7027, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7028, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7029, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7030, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7031, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7032, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7033, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7034, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7035, Training Loss: 3.022e+00 , Validation Loss: 2.794e+00\n",
      "Iteration: 7036, Training Loss: 2.518e+00 , Validation Loss: 3.683e+00\n",
      "Iteration: 7037, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 7038, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 7039, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7040, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 7041, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7042, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7043, Training Loss: 1.511e+00 , Validation Loss: 3.544e+00\n",
      "Iteration: 7044, Training Loss: 1.259e+00 , Validation Loss: 1.077e+00\n",
      "Iteration: 7045, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7046, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7047, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7048, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7049, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 7050, Training Loss: 1.511e+00 , Validation Loss: 4.022e+00\n",
      "Iteration: 7051, Training Loss: 1.511e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 7052, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7053, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 7054, Training Loss: 1.007e+00 , Validation Loss: 1.839e+00\n",
      "Iteration: 7055, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 7056, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 7057, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7058, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7059, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 7060, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7061, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 7062, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 7063, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 7064, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 7065, Training Loss: 1.007e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 7066, Training Loss: 5.037e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 7067, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 7068, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7069, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7070, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7071, Training Loss: 7.555e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 7072, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7073, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 7074, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 7075, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7076, Training Loss: 7.555e-01 , Validation Loss: 9.556e-01\n",
      "Iteration: 7077, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 7078, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7079, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7080, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7081, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7082, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7083, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7084, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 7085, Training Loss: -1.000e-07 , Validation Loss: 7.802e-01\n",
      "Iteration: 7086, Training Loss: 5.037e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 7087, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7088, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7089, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7090, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7091, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7092, Training Loss: 2.518e-01 , Validation Loss: 1.445e+00\n",
      "Iteration: 7093, Training Loss: 2.518e+00 , Validation Loss: 4.264e+00\n",
      "Iteration: 7094, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7095, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7096, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 7097, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7098, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 7099, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7100, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 7101, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7102, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 7103, Training Loss: -1.000e-07 , Validation Loss: 1.748e+00\n",
      "Iteration: 7104, Training Loss: 1.007e+00 , Validation Loss: 1.748e+00\n",
      "Iteration: 7105, Training Loss: 1.511e+00 , Validation Loss: 3.599e+00\n",
      "Iteration: 7106, Training Loss: 1.259e+00 , Validation Loss: 6.653e-01\n",
      "Iteration: 7107, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7108, Training Loss: 7.555e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 7109, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7110, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7111, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 7112, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 7113, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 7114, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7115, Training Loss: 1.007e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 7116, Training Loss: 1.007e+00 , Validation Loss: 2.292e+00\n",
      "Iteration: 7117, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 7118, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 7119, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 7120, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 7121, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7122, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 7123, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 7124, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7125, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7126, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7127, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7128, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7129, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7130, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 7131, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7132, Training Loss: 2.518e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 7133, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 7134, Training Loss: 5.037e-01 , Validation Loss: 1.113e+00\n",
      "Iteration: 7135, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7136, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7137, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7138, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 7139, Training Loss: 5.037e-01 , Validation Loss: 1.143e+00\n",
      "Iteration: 7140, Training Loss: 2.518e-01 , Validation Loss: 1.802e+00\n",
      "Iteration: 7141, Training Loss: 1.007e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 7142, Training Loss: 7.555e-01 , Validation Loss: 1.294e+00\n",
      "Iteration: 7143, Training Loss: 2.518e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 7144, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 7145, Training Loss: 3.778e+00 , Validation Loss: 5.117e+00\n",
      "Iteration: 7146, Training Loss: 4.785e+00 , Validation Loss: 4.699e+00\n",
      "Iteration: 7147, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7148, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 7149, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 7150, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7151, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7152, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 7153, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 7154, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7155, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7156, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7157, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7158, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 7159, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 7160, Training Loss: 1.007e+00 , Validation Loss: 2.697e+00\n",
      "Iteration: 7161, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 7162, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 7163, Training Loss: 1.007e+00 , Validation Loss: 4.234e-01\n",
      "Iteration: 7164, Training Loss: 2.518e-01 , Validation Loss: 1.343e+00\n",
      "Iteration: 7165, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 7166, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7167, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 7168, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 7169, Training Loss: 2.518e+00 , Validation Loss: 4.016e+00\n",
      "Iteration: 7170, Training Loss: 1.259e+00 , Validation Loss: 1.518e+00\n",
      "Iteration: 7171, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 7172, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 7173, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 7174, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7175, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7176, Training Loss: 1.763e+00 , Validation Loss: 3.133e+00\n",
      "Iteration: 7177, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 7178, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7179, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 7180, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 7181, Training Loss: 1.007e+00 , Validation Loss: 8.467e-01\n",
      "Iteration: 7182, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 7183, Training Loss: 5.037e-01 , Validation Loss: 1.778e+00\n",
      "Iteration: 7184, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 7185, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 7186, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 7187, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 7188, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7189, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 7190, Training Loss: 5.037e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 7191, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7192, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7193, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 7194, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 7195, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7196, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7197, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7198, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 7199, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 7200, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7201, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7202, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 7203, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7204, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7205, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 7206, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7207, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 7208, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 7209, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 7210, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 7211, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7212, Training Loss: 1.007e+00 , Validation Loss: 1.071e+00\n",
      "Iteration: 7213, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7214, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7215, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7216, Training Loss: 1.511e+00 , Validation Loss: 5.988e-01\n",
      "Iteration: 7217, Training Loss: 5.037e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 7218, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7219, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7220, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 7221, Training Loss: 1.259e+00 , Validation Loss: 5.141e-01\n",
      "Iteration: 7222, Training Loss: 4.030e+00 , Validation Loss: 4.572e+00\n",
      "Iteration: 7223, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 7224, Training Loss: 3.022e+00 , Validation Loss: 5.836e+00\n",
      "Iteration: 7225, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 7226, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7227, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7228, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7229, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7230, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7231, Training Loss: 5.037e-01 , Validation Loss: 1.113e+00\n",
      "Iteration: 7232, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7233, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7234, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7235, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7236, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7237, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7238, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7239, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7240, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7241, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7242, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7243, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7244, Training Loss: 2.267e+00 , Validation Loss: 3.683e+00\n",
      "Iteration: 7245, Training Loss: 7.555e-01 , Validation Loss: 1.796e+00\n",
      "Iteration: 7246, Training Loss: 1.007e+00 , Validation Loss: 1.306e+00\n",
      "Iteration: 7247, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7248, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 7249, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7250, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7251, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 7252, Training Loss: 5.037e-01 , Validation Loss: 9.072e-01\n",
      "Iteration: 7253, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7254, Training Loss: 1.007e+00 , Validation Loss: 4.597e-01\n",
      "Iteration: 7255, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7256, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7257, Training Loss: 7.555e-01 , Validation Loss: 4.119e+00\n",
      "Iteration: 7258, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7259, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7260, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7261, Training Loss: 4.030e+00 , Validation Loss: 3.992e+00\n",
      "Iteration: 7262, Training Loss: 7.052e+00 , Validation Loss: 8.782e+00\n",
      "Iteration: 7263, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7264, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7265, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7266, Training Loss: 7.555e-01 , Validation Loss: 2.214e+00\n",
      "Iteration: 7267, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7268, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7269, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7270, Training Loss: 2.518e+00 , Validation Loss: 4.306e+00\n",
      "Iteration: 7271, Training Loss: 1.259e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 7272, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7273, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 7274, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7275, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7276, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7277, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7278, Training Loss: 1.007e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 7279, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 7280, Training Loss: 1.763e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 7281, Training Loss: 1.763e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 7282, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7283, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7284, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 7285, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 7286, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7287, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7288, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 7289, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7290, Training Loss: 2.518e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 7291, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7292, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7293, Training Loss: 1.511e+00 , Validation Loss: 2.855e+00\n",
      "Iteration: 7294, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 7295, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7296, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7297, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7298, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7299, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 7300, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7301, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7302, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7303, Training Loss: 1.007e+00 , Validation Loss: 2.546e+00\n",
      "Iteration: 7304, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7305, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7306, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7307, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7308, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7309, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7310, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 7311, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7312, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7313, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 7314, Training Loss: 1.511e+00 , Validation Loss: 2.643e+00\n",
      "Iteration: 7315, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7316, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7317, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7318, Training Loss: 1.763e+00 , Validation Loss: 3.847e+00\n",
      "Iteration: 7319, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7320, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7321, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7322, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7323, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 7324, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7325, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7326, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7327, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7328, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7329, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7330, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 7331, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 7332, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7333, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 7334, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7335, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7336, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7337, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7338, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7339, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 7340, Training Loss: 7.555e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 7341, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 7342, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7343, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 7344, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 7345, Training Loss: 2.770e+00 , Validation Loss: 5.044e+00\n",
      "Iteration: 7346, Training Loss: 2.267e+00 , Validation Loss: 1.954e+00\n",
      "Iteration: 7347, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7348, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 7349, Training Loss: 1.259e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 7350, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7351, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 7352, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7353, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7354, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7355, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7356, Training Loss: 1.007e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 7357, Training Loss: 3.526e+00 , Validation Loss: 5.032e+00\n",
      "Iteration: 7358, Training Loss: 6.296e+00 , Validation Loss: 8.207e+00\n",
      "Iteration: 7359, Training Loss: 1.007e+00 , Validation Loss: 4.476e-01\n",
      "Iteration: 7360, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7361, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7362, Training Loss: 1.259e+00 , Validation Loss: 3.103e+00\n",
      "Iteration: 7363, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7364, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7365, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7366, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7367, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 7368, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 7369, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 7370, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 7371, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7372, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7373, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7374, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7375, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7376, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7377, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7378, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7379, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 7380, Training Loss: 3.778e+00 , Validation Loss: 4.699e+00\n",
      "Iteration: 7381, Training Loss: 3.778e+00 , Validation Loss: 4.615e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7382, Training Loss: 1.007e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 7383, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 7384, Training Loss: 1.007e+00 , Validation Loss: 4.415e-01\n",
      "Iteration: 7385, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7386, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7387, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7388, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7389, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7390, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7391, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7392, Training Loss: 4.030e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 7393, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 7394, Training Loss: 2.267e+00 , Validation Loss: 5.649e+00\n",
      "Iteration: 7395, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7396, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 7397, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7398, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 7399, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7400, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7401, Training Loss: 1.511e+00 , Validation Loss: 3.593e+00\n",
      "Iteration: 7402, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 7403, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7404, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7405, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7406, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7407, Training Loss: 3.778e+00 , Validation Loss: 5.219e+00\n",
      "Iteration: 7408, Training Loss: 1.511e+00 , Validation Loss: 1.572e+00\n",
      "Iteration: 7409, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 7410, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 7411, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7412, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7413, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7414, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7415, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7416, Training Loss: 1.511e+00 , Validation Loss: 3.345e+00\n",
      "Iteration: 7417, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 7418, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 7419, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7420, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7421, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7422, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 7423, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7424, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7425, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7426, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7427, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7428, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7429, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 7430, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 7431, Training Loss: 2.518e-01 , Validation Loss: 2.117e+00\n",
      "Iteration: 7432, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7433, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 7434, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7435, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7436, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7437, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7438, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7439, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7440, Training Loss: 3.274e+00 , Validation Loss: 4.397e+00\n",
      "Iteration: 7441, Training Loss: 1.259e+00 , Validation Loss: 3.532e+00\n",
      "Iteration: 7442, Training Loss: 1.007e+00 , Validation Loss: 1.439e+00\n",
      "Iteration: 7443, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 7444, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7445, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 7446, Training Loss: 2.770e+00 , Validation Loss: 4.318e+00\n",
      "Iteration: 7447, Training Loss: 1.763e+00 , Validation Loss: 1.415e+00\n",
      "Iteration: 7448, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7449, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 7450, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7451, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7452, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7453, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7454, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7455, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7456, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7457, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7458, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7459, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 7460, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 7461, Training Loss: 1.511e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 7462, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7463, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7464, Training Loss: 7.555e-01 , Validation Loss: 2.474e+00\n",
      "Iteration: 7465, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7466, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7467, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 7468, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7469, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 7470, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7471, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7472, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7473, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7474, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 7475, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 7476, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 7477, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7478, Training Loss: 1.007e+00 , Validation Loss: 3.568e+00\n",
      "Iteration: 7479, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7480, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7481, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 7482, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7483, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7484, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 7485, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7486, Training Loss: 1.259e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 7487, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 7488, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7489, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7490, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7491, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7492, Training Loss: 1.007e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 7493, Training Loss: 4.281e+00 , Validation Loss: 5.201e+00\n",
      "Iteration: 7494, Training Loss: 4.030e+00 , Validation Loss: 4.621e+00\n",
      "Iteration: 7495, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 7496, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7497, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7498, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7499, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 7500, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7501, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7502, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7503, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7504, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7505, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 7506, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7507, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 7508, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7509, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 7510, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 7511, Training Loss: 7.555e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 7512, Training Loss: 1.007e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 7513, Training Loss: 1.007e+00 , Validation Loss: 2.177e+00\n",
      "Iteration: 7514, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7515, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 7516, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 7517, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 7518, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7519, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7520, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 7521, Training Loss: 2.518e-01 , Validation Loss: 8.830e-01\n",
      "Iteration: 7522, Training Loss: 2.267e+00 , Validation Loss: 3.375e+00\n",
      "Iteration: 7523, Training Loss: 1.007e+00 , Validation Loss: 1.185e+00\n",
      "Iteration: 7524, Training Loss: 1.763e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 7525, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 7526, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7527, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 7528, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7529, Training Loss: 2.770e+00 , Validation Loss: 3.526e+00\n",
      "Iteration: 7530, Training Loss: 1.007e+00 , Validation Loss: 1.107e+00\n",
      "Iteration: 7531, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 7532, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 7533, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7534, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 7535, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7536, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7537, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 7538, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7539, Training Loss: -1.000e-07 , Validation Loss: 8.951e-01\n",
      "Iteration: 7540, Training Loss: 5.037e-01 , Validation Loss: 8.951e-01\n",
      "Iteration: 7541, Training Loss: 3.022e+00 , Validation Loss: 3.441e+00\n",
      "Iteration: 7542, Training Loss: 1.763e+00 , Validation Loss: 4.191e+00\n",
      "Iteration: 7543, Training Loss: 2.770e+00 , Validation Loss: 1.427e+00\n",
      "Iteration: 7544, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7545, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7546, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7547, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7548, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7549, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7550, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 7551, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7552, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 7553, Training Loss: 1.007e+00 , Validation Loss: 2.722e+00\n",
      "Iteration: 7554, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7555, Training Loss: 1.511e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 7556, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 7557, Training Loss: 1.259e+00 , Validation Loss: 2.413e+00\n",
      "Iteration: 7558, Training Loss: 2.518e-01 , Validation Loss: 1.113e+00\n",
      "Iteration: 7559, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 7560, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7561, Training Loss: -1.000e-07 , Validation Loss: 2.480e+00\n",
      "Iteration: 7562, Training Loss: 7.555e-01 , Validation Loss: 2.480e+00\n",
      "Iteration: 7563, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7564, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7565, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7566, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7567, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7568, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7569, Training Loss: 5.037e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 7570, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7571, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7572, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7573, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7574, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7575, Training Loss: 7.555e-01 , Validation Loss: 2.504e+00\n",
      "Iteration: 7576, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7577, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 7578, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7579, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 7580, Training Loss: 2.015e+00 , Validation Loss: 3.707e+00\n",
      "Iteration: 7581, Training Loss: 2.267e+00 , Validation Loss: 1.754e+00\n",
      "Iteration: 7582, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 7583, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7584, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7585, Training Loss: 1.007e+00 , Validation Loss: 1.252e+00\n",
      "Iteration: 7586, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7587, Training Loss: 2.518e-01 , Validation Loss: 1.071e+00\n",
      "Iteration: 7588, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 7589, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 7590, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 7591, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 7592, Training Loss: 7.555e-01 , Validation Loss: 3.611e+00\n",
      "Iteration: 7593, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 7594, Training Loss: 1.007e+00 , Validation Loss: 2.135e+00\n",
      "Iteration: 7595, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 7596, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 7597, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 7598, Training Loss: 2.518e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 7599, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 7600, Training Loss: 1.511e+00 , Validation Loss: 9.254e-01\n",
      "Iteration: 7601, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 7602, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 7603, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 7604, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7605, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7606, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7607, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7608, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7609, Training Loss: 7.555e-01 , Validation Loss: 1.935e+00\n",
      "Iteration: 7610, Training Loss: 3.274e+00 , Validation Loss: 5.002e+00\n",
      "Iteration: 7611, Training Loss: 2.267e+00 , Validation Loss: 3.538e+00\n",
      "Iteration: 7612, Training Loss: 1.007e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 7613, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7614, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7615, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 7616, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 7617, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 7618, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 7619, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 7620, Training Loss: 2.518e-01 , Validation Loss: 1.131e+00\n",
      "Iteration: 7621, Training Loss: 1.259e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 7622, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 7623, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7624, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7625, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7626, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7627, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 7628, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 7629, Training Loss: 1.511e+00 , Validation Loss: 2.982e+00\n",
      "Iteration: 7630, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 7631, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7632, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 7633, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 7634, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7635, Training Loss: 5.037e-01 , Validation Loss: 1.028e+00\n",
      "Iteration: 7636, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 7637, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7638, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7639, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 7640, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 7641, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 7642, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 7643, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 7644, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 7645, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7646, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7647, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7648, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7649, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 7650, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 7651, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7652, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 7653, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7654, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7655, Training Loss: 1.259e+00 , Validation Loss: 4.476e-01\n",
      "Iteration: 7656, Training Loss: 2.770e+00 , Validation Loss: 5.056e+00\n",
      "Iteration: 7657, Training Loss: 2.015e+00 , Validation Loss: 1.857e+00\n",
      "Iteration: 7658, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7659, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7660, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7661, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 7662, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7663, Training Loss: 7.555e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 7664, Training Loss: 2.518e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 7665, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 7666, Training Loss: 1.511e+00 , Validation Loss: 2.026e+00\n",
      "Iteration: 7667, Training Loss: 1.259e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 7668, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7669, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7670, Training Loss: 2.518e-01 , Validation Loss: 2.728e+00\n",
      "Iteration: 7671, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 7672, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7673, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7674, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 7675, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7676, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 7677, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 7678, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7679, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7680, Training Loss: 1.007e+00 , Validation Loss: 4.778e-01\n",
      "Iteration: 7681, Training Loss: 7.555e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 7682, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 7683, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 7684, Training Loss: -1.000e-07 , Validation Loss: 8.528e-01\n",
      "Iteration: 7685, Training Loss: 2.518e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 7686, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 7687, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 7688, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 7689, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 7690, Training Loss: 1.259e+00 , Validation Loss: 1.966e+00\n",
      "Iteration: 7691, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 7692, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 7693, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7694, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 7695, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7696, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7697, Training Loss: 7.555e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 7698, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 7699, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 7700, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 7701, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 7702, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7703, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 7704, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7705, Training Loss: 7.555e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 7706, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7707, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 7708, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7709, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7710, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 7711, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 7712, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7713, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 7714, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 7715, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 7716, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7717, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 7718, Training Loss: 1.007e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 7719, Training Loss: 2.518e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 7720, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 7721, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 7722, Training Loss: 1.259e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 7723, Training Loss: 4.030e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 7724, Training Loss: 3.022e+00 , Validation Loss: 3.998e+00\n",
      "Iteration: 7725, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 7726, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7727, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 7728, Training Loss: -1.000e-07 , Validation Loss: 1.204e+00\n",
      "Iteration: 7729, Training Loss: 7.555e-01 , Validation Loss: 1.204e+00\n",
      "Iteration: 7730, Training Loss: -1.000e-07 , Validation Loss: 6.774e-01\n",
      "Iteration: 7731, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 7732, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7733, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 7734, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7735, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 7736, Training Loss: 1.511e+00 , Validation Loss: 4.276e+00\n",
      "Iteration: 7737, Training Loss: 1.007e+00 , Validation Loss: 4.838e-01\n",
      "Iteration: 7738, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 7739, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7740, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7741, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7742, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7743, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7744, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 7745, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 7746, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 7747, Training Loss: 5.037e-01 , Validation Loss: 9.193e-01\n",
      "Iteration: 7748, Training Loss: 1.259e+00 , Validation Loss: 1.857e+00\n",
      "Iteration: 7749, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7750, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7751, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7752, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7753, Training Loss: 1.259e+00 , Validation Loss: 9.012e-01\n",
      "Iteration: 7754, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7755, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7756, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7757, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7758, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7759, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 7760, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7761, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7762, Training Loss: 2.518e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 7763, Training Loss: 7.555e-01 , Validation Loss: 3.242e+00\n",
      "Iteration: 7764, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7765, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7766, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7767, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7768, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 7769, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7770, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7771, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7772, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7773, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7774, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 7775, Training Loss: 1.007e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 7776, Training Loss: 1.007e+00 , Validation Loss: 1.935e+00\n",
      "Iteration: 7777, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 7778, Training Loss: 5.037e-01 , Validation Loss: 9.133e-01\n",
      "Iteration: 7779, Training Loss: 5.037e-01 , Validation Loss: 1.131e+00\n",
      "Iteration: 7780, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 7781, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 7782, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 7783, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 7784, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 7785, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7786, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7787, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7788, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7789, Training Loss: -1.000e-07 , Validation Loss: 1.276e+00\n",
      "Iteration: 7790, Training Loss: 5.037e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 7791, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 7792, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 7793, Training Loss: -1.000e-07 , Validation Loss: 5.020e-01\n",
      "Iteration: 7794, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 7795, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7796, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7797, Training Loss: 7.555e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 7798, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 7799, Training Loss: 2.015e+00 , Validation Loss: 1.210e+00\n",
      "Iteration: 7800, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7801, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7802, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7803, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7804, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7805, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7806, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 7807, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 7808, Training Loss: 1.007e+00 , Validation Loss: 1.954e+00\n",
      "Iteration: 7809, Training Loss: 7.555e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 7810, Training Loss: 1.007e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 7811, Training Loss: 5.037e-01 , Validation Loss: 1.288e+00\n",
      "Iteration: 7812, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 7813, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7814, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7815, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7816, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 7817, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 7818, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7819, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7820, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 7821, Training Loss: 1.259e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 7822, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7823, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 7824, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7825, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7826, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 7827, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7828, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7829, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 7830, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7831, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 7832, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7833, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7834, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7835, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 7836, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7837, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7838, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 7839, Training Loss: 7.555e-01 , Validation Loss: 1.258e+00\n",
      "Iteration: 7840, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7841, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7842, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7843, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7844, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7845, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 7846, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 7847, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7848, Training Loss: 2.518e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 7849, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 7850, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 7851, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 7852, Training Loss: -1.000e-07 , Validation Loss: 9.133e-01\n",
      "Iteration: 7853, Training Loss: 5.037e-01 , Validation Loss: 9.133e-01\n",
      "Iteration: 7854, Training Loss: 1.511e+00 , Validation Loss: 1.131e+00\n",
      "Iteration: 7855, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 7856, Training Loss: 1.007e+00 , Validation Loss: 4.597e-01\n",
      "Iteration: 7857, Training Loss: -1.000e-07 , Validation Loss: 1.288e+00\n",
      "Iteration: 7858, Training Loss: 5.037e-01 , Validation Loss: 1.288e+00\n",
      "Iteration: 7859, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 7860, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7861, Training Loss: 1.007e+00 , Validation Loss: 9.254e-01\n",
      "Iteration: 7862, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7863, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7864, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7865, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 7866, Training Loss: 5.037e-01 , Validation Loss: 2.093e+00\n",
      "Iteration: 7867, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 7868, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7869, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 7870, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7871, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 7872, Training Loss: 2.518e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 7873, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 7874, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 7875, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7876, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7877, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7878, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 7879, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7880, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7881, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 7882, Training Loss: 1.007e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 7883, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 7884, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 7885, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7886, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 7887, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 7888, Training Loss: 2.518e-01 , Validation Loss: 9.193e-01\n",
      "Iteration: 7889, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 7890, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 7891, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 7892, Training Loss: 1.259e+00 , Validation Loss: 2.897e+00\n",
      "Iteration: 7893, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7894, Training Loss: 1.007e+00 , Validation Loss: 4.597e-01\n",
      "Iteration: 7895, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 7896, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7897, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 7898, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 7899, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7900, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7901, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 7902, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 7903, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 7904, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7905, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 7906, Training Loss: 2.518e-01 , Validation Loss: 1.397e+00\n",
      "Iteration: 7907, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 7908, Training Loss: 2.518e+00 , Validation Loss: 1.379e+00\n",
      "Iteration: 7909, Training Loss: 1.007e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 7910, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 7911, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 7912, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 7913, Training Loss: 1.763e+00 , Validation Loss: 3.072e+00\n",
      "Iteration: 7914, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 7915, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7916, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 7917, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 7918, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 7919, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7920, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7921, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7922, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7923, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7924, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 7925, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7926, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 7927, Training Loss: 2.267e+00 , Validation Loss: 3.587e+00\n",
      "Iteration: 7928, Training Loss: 2.518e-01 , Validation Loss: 8.588e-01\n",
      "Iteration: 7929, Training Loss: 1.511e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 7930, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7931, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 7932, Training Loss: 2.015e+00 , Validation Loss: 3.629e+00\n",
      "Iteration: 7933, Training Loss: 1.763e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 7934, Training Loss: 1.511e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 7935, Training Loss: 2.015e+00 , Validation Loss: 3.254e+00\n",
      "Iteration: 7936, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7937, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 7938, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7939, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 7940, Training Loss: -1.000e-07 , Validation Loss: 7.923e-01\n",
      "Iteration: 7941, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 7942, Training Loss: 2.518e-01 , Validation Loss: 1.161e+00\n",
      "Iteration: 7943, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 7944, Training Loss: 7.555e-01 , Validation Loss: 1.312e+00\n",
      "Iteration: 7945, Training Loss: 2.518e-01 , Validation Loss: 1.083e+00\n",
      "Iteration: 7946, Training Loss: 2.015e+00 , Validation Loss: 3.695e+00\n",
      "Iteration: 7947, Training Loss: 1.007e+00 , Validation Loss: 9.798e-01\n",
      "Iteration: 7948, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7949, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7950, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7951, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 7952, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7953, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 7954, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 7955, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 7956, Training Loss: 1.007e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 7957, Training Loss: 1.259e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 7958, Training Loss: 2.770e+00 , Validation Loss: 3.653e+00\n",
      "Iteration: 7959, Training Loss: 1.763e+00 , Validation Loss: 1.343e+00\n",
      "Iteration: 7960, Training Loss: 1.007e+00 , Validation Loss: 4.838e-01\n",
      "Iteration: 7961, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7962, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7963, Training Loss: 2.770e+00 , Validation Loss: 3.078e+00\n",
      "Iteration: 7964, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 7965, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 7966, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7967, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7968, Training Loss: 2.518e-01 , Validation Loss: 9.314e-01\n",
      "Iteration: 7969, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7970, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7971, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7972, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7973, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7974, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7975, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7976, Training Loss: 4.281e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 7977, Training Loss: 2.770e+00 , Validation Loss: 4.409e+00\n",
      "Iteration: 7978, Training Loss: 1.259e+00 , Validation Loss: 9.254e-01\n",
      "Iteration: 7979, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 7980, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7981, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 7982, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 7983, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7984, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7985, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7986, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7987, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7988, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7989, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7990, Training Loss: 7.555e-01 , Validation Loss: 1.004e+00\n",
      "Iteration: 7991, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7992, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7993, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 7994, Training Loss: 2.267e+00 , Validation Loss: 3.937e+00\n",
      "Iteration: 7995, Training Loss: 1.511e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 7996, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 7997, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 7998, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 7999, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8000, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8001, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8002, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8003, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 8004, Training Loss: -1.000e-07 , Validation Loss: 7.983e-01\n",
      "Iteration: 8005, Training Loss: 7.555e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 8006, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8007, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8008, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8009, Training Loss: 1.763e+00 , Validation Loss: 3.798e+00\n",
      "Iteration: 8010, Training Loss: 1.259e+00 , Validation Loss: 6.653e-01\n",
      "Iteration: 8011, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 8012, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8013, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8014, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8015, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8016, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8017, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 8018, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8019, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 8020, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8021, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8022, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8023, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8024, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 8025, Training Loss: 1.007e+00 , Validation Loss: 2.250e+00\n",
      "Iteration: 8026, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8027, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8028, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8029, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8030, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8031, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8032, Training Loss: 1.007e+00 , Validation Loss: 8.407e-01\n",
      "Iteration: 8033, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8034, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 8035, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 8036, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8037, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 8038, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 8039, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 8040, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8041, Training Loss: 7.555e-01 , Validation Loss: 1.470e+00\n",
      "Iteration: 8042, Training Loss: 2.267e+00 , Validation Loss: 4.385e+00\n",
      "Iteration: 8043, Training Loss: 1.007e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 8044, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8045, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8046, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8047, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8048, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8049, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 8050, Training Loss: 1.259e+00 , Validation Loss: 3.230e+00\n",
      "Iteration: 8051, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 8052, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8053, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8054, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8055, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8056, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8057, Training Loss: 7.555e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 8058, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 8059, Training Loss: 7.555e-01 , Validation Loss: 2.897e+00\n",
      "Iteration: 8060, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8061, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8062, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8063, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 8064, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 8065, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 8066, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8067, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8068, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8069, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8070, Training Loss: 4.785e+00 , Validation Loss: 4.965e+00\n",
      "Iteration: 8071, Training Loss: 9.318e+00 , Validation Loss: 1.015e+01\n",
      "Iteration: 8072, Training Loss: 1.511e+00 , Validation Loss: 1.990e+00\n",
      "Iteration: 8073, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 8074, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8075, Training Loss: 1.007e+00 , Validation Loss: 2.486e+00\n",
      "Iteration: 8076, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 8077, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 8078, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8079, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8080, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8081, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8082, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8083, Training Loss: 2.770e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 8084, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 8085, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8086, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8087, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8088, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8089, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8090, Training Loss: 1.763e+00 , Validation Loss: 3.399e+00\n",
      "Iteration: 8091, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 8092, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8093, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8094, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8095, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8096, Training Loss: 1.259e+00 , Validation Loss: 2.673e+00\n",
      "Iteration: 8097, Training Loss: 1.007e+00 , Validation Loss: 4.778e-01\n",
      "Iteration: 8098, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 8099, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 8100, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8101, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8102, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8103, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8104, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8105, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 8106, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 8107, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 8108, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8109, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8110, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8111, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 8112, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 8113, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 8114, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 8115, Training Loss: 2.518e+00 , Validation Loss: 4.113e+00\n",
      "Iteration: 8116, Training Loss: 7.555e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 8117, Training Loss: 1.007e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 8118, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 8119, Training Loss: 1.007e+00 , Validation Loss: 4.046e+00\n",
      "Iteration: 8120, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8121, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 8122, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8123, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8124, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8125, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8126, Training Loss: 7.555e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 8127, Training Loss: 3.022e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 8128, Training Loss: 4.030e+00 , Validation Loss: 4.258e+00\n",
      "Iteration: 8129, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 8130, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 8131, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 8132, Training Loss: 3.526e+00 , Validation Loss: 4.494e+00\n",
      "Iteration: 8133, Training Loss: 2.770e+00 , Validation Loss: 3.538e+00\n",
      "Iteration: 8134, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 8135, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8136, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8137, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8138, Training Loss: 5.037e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 8139, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 8140, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8141, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8142, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8143, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8144, Training Loss: 2.770e+00 , Validation Loss: 4.814e+00\n",
      "Iteration: 8145, Training Loss: 1.763e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 8146, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8147, Training Loss: 2.518e+00 , Validation Loss: 2.413e+00\n",
      "Iteration: 8148, Training Loss: 2.267e+00 , Validation Loss: 1.572e+00\n",
      "Iteration: 8149, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 8150, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8151, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8152, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8153, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 8154, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8155, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8156, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8157, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8158, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 8159, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8160, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8161, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8162, Training Loss: -1.000e-07 , Validation Loss: 4.778e-01\n",
      "Iteration: 8163, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 8164, Training Loss: 3.022e+00 , Validation Loss: 3.490e+00\n",
      "Iteration: 8165, Training Loss: 1.259e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 8166, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 8167, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 8168, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8169, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8170, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8171, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 8172, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8173, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8174, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8175, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8176, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8177, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8178, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8179, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8180, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 8181, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8182, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8183, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 8184, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8185, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8186, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8187, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 8188, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8189, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8190, Training Loss: 3.022e+00 , Validation Loss: 5.389e+00\n",
      "Iteration: 8191, Training Loss: 1.259e+00 , Validation Loss: 1.518e+00\n",
      "Iteration: 8192, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8193, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8194, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8195, Training Loss: 5.037e-01 , Validation Loss: 1.379e+00\n",
      "Iteration: 8196, Training Loss: 7.555e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 8197, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 8198, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8199, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8200, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 8201, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 8202, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 8203, Training Loss: 1.007e+00 , Validation Loss: 4.373e+00\n",
      "Iteration: 8204, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8205, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8206, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8207, Training Loss: -1.000e-07 , Validation Loss: 6.955e-01\n",
      "Iteration: 8208, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 8209, Training Loss: 7.555e-01 , Validation Loss: 2.117e+00\n",
      "Iteration: 8210, Training Loss: 7.555e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 8211, Training Loss: 1.007e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 8212, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8213, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8214, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8215, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8216, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8217, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8218, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8219, Training Loss: -1.000e-07 , Validation Loss: 6.532e-01\n",
      "Iteration: 8220, Training Loss: 7.555e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 8221, Training Loss: 1.763e+00 , Validation Loss: 3.834e+00\n",
      "Iteration: 8222, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 8223, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 8224, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8225, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 8226, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 8227, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8228, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 8229, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8230, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8231, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8232, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 8233, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8234, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 8235, Training Loss: 3.778e+00 , Validation Loss: 5.117e+00\n",
      "Iteration: 8236, Training Loss: 3.778e+00 , Validation Loss: 4.476e+00\n",
      "Iteration: 8237, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 8238, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8239, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 8240, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8241, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8242, Training Loss: 2.015e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 8243, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8244, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8245, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8246, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 8247, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 8248, Training Loss: 7.555e-01 , Validation Loss: 2.111e+00\n",
      "Iteration: 8249, Training Loss: 7.555e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 8250, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8251, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8252, Training Loss: 7.555e-01 , Validation Loss: 1.010e+00\n",
      "Iteration: 8253, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 8254, Training Loss: 1.763e+00 , Validation Loss: 3.472e+00\n",
      "Iteration: 8255, Training Loss: 1.007e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 8256, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8257, Training Loss: -1.000e-07 , Validation Loss: 1.681e+00\n",
      "Iteration: 8258, Training Loss: 5.037e-01 , Validation Loss: 1.681e+00\n",
      "Iteration: 8259, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8260, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8261, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8262, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8263, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8264, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8265, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8266, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8267, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8268, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 8269, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8270, Training Loss: 2.770e+00 , Validation Loss: 4.034e+00\n",
      "Iteration: 8271, Training Loss: 7.555e-01 , Validation Loss: 1.778e+00\n",
      "Iteration: 8272, Training Loss: 7.555e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 8273, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 8274, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 8275, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8276, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 8277, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8278, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8279, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8280, Training Loss: 5.037e-01 , Validation Loss: 2.141e+00\n",
      "Iteration: 8281, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8282, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8283, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8284, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8285, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8286, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8287, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8288, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 8289, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8290, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8291, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8292, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 8293, Training Loss: 1.007e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 8294, Training Loss: 1.007e+00 , Validation Loss: 1.500e+00\n",
      "Iteration: 8295, Training Loss: 3.778e+00 , Validation Loss: 5.213e+00\n",
      "Iteration: 8296, Training Loss: 5.541e+00 , Validation Loss: 6.653e+00\n",
      "Iteration: 8297, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 8298, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8299, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8300, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8301, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8302, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8303, Training Loss: 1.259e+00 , Validation Loss: 1.857e+00\n",
      "Iteration: 8304, Training Loss: 1.259e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 8305, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 8306, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8307, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8308, Training Loss: 2.267e+00 , Validation Loss: 4.113e+00\n",
      "Iteration: 8309, Training Loss: 1.259e+00 , Validation Loss: 1.784e+00\n",
      "Iteration: 8310, Training Loss: 5.037e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 8311, Training Loss: 7.555e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 8312, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8313, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 8314, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8315, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 8316, Training Loss: 7.555e-01 , Validation Loss: 1.415e+00\n",
      "Iteration: 8317, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8318, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8319, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8320, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8321, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8322, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8323, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8324, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8325, Training Loss: 1.259e+00 , Validation Loss: 1.881e+00\n",
      "Iteration: 8326, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 8327, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8328, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8329, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8330, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8331, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8332, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8333, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 8334, Training Loss: 1.007e+00 , Validation Loss: 8.528e-01\n",
      "Iteration: 8335, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 8336, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 8337, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 8338, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 8339, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 8340, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8341, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8342, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 8343, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 8344, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 8345, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8346, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8347, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8348, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8349, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8350, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 8351, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8352, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 8353, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 8354, Training Loss: -1.000e-07 , Validation Loss: 7.379e-01\n",
      "Iteration: 8355, Training Loss: -1.000e-07 , Validation Loss: 7.379e-01\n",
      "Iteration: 8356, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 8357, Training Loss: 2.518e-01 , Validation Loss: 1.790e+00\n",
      "Iteration: 8358, Training Loss: -1.000e-07 , Validation Loss: 8.649e-01\n",
      "Iteration: 8359, Training Loss: 1.007e+00 , Validation Loss: 8.649e-01\n",
      "Iteration: 8360, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 8361, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8362, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8363, Training Loss: 2.518e-01 , Validation Loss: 2.268e+00\n",
      "Iteration: 8364, Training Loss: 1.259e+00 , Validation Loss: 8.588e-01\n",
      "Iteration: 8365, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8366, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 8367, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 8368, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8369, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 8370, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 8371, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8372, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8373, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8374, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 8375, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8376, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8377, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 8378, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 8379, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8380, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8381, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 8382, Training Loss: 2.015e+00 , Validation Loss: 2.649e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8383, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 8384, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 8385, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 8386, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 8387, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 8388, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 8389, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 8390, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 8391, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 8392, Training Loss: 1.007e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 8393, Training Loss: 5.037e-01 , Validation Loss: 1.234e+00\n",
      "Iteration: 8394, Training Loss: 7.555e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 8395, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 8396, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8397, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 8398, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 8399, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 8400, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 8401, Training Loss: 5.037e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 8402, Training Loss: 7.555e-01 , Validation Loss: 1.548e+00\n",
      "Iteration: 8403, Training Loss: 7.555e-01 , Validation Loss: 9.677e-01\n",
      "Iteration: 8404, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 8405, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 8406, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 8407, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 8408, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8409, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 8410, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8411, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 8412, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 8413, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8414, Training Loss: 2.518e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 8415, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 8416, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 8417, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 8418, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8419, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8420, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8421, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8422, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8423, Training Loss: 1.259e+00 , Validation Loss: 9.193e-01\n",
      "Iteration: 8424, Training Loss: 5.037e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 8425, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8426, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 8427, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 8428, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 8429, Training Loss: 5.037e-01 , Validation Loss: 8.588e-01\n",
      "Iteration: 8430, Training Loss: 2.518e+00 , Validation Loss: 4.330e+00\n",
      "Iteration: 8431, Training Loss: 1.511e+00 , Validation Loss: 1.325e+00\n",
      "Iteration: 8432, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8433, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8434, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8435, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8436, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8437, Training Loss: 2.518e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 8438, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 8439, Training Loss: -1.000e-07 , Validation Loss: 1.222e+00\n",
      "Iteration: 8440, Training Loss: 1.007e+00 , Validation Loss: 1.222e+00\n",
      "Iteration: 8441, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 8442, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8443, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 8444, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8445, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 8446, Training Loss: 5.037e-01 , Validation Loss: 1.252e+00\n",
      "Iteration: 8447, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8448, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8449, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 8450, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8451, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8452, Training Loss: 7.555e-01 , Validation Loss: 2.153e+00\n",
      "Iteration: 8453, Training Loss: -1.000e-07 , Validation Loss: 8.407e-01\n",
      "Iteration: 8454, Training Loss: 2.518e-01 , Validation Loss: 8.407e-01\n",
      "Iteration: 8455, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 8456, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 8457, Training Loss: 1.007e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 8458, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 8459, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 8460, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8461, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8462, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8463, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8464, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 8465, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 8466, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8467, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8468, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 8469, Training Loss: 2.518e-01 , Validation Loss: 1.173e+00\n",
      "Iteration: 8470, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 8471, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8472, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8473, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8474, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8475, Training Loss: 7.555e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 8476, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8477, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8478, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 8479, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8480, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8481, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 8482, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8483, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8484, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8485, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8486, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8487, Training Loss: 1.259e+00 , Validation Loss: 3.345e+00\n",
      "Iteration: 8488, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 8489, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8490, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 8491, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8492, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8493, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8494, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 8495, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 8496, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8497, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8498, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8499, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8500, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 8501, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 8502, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 8503, Training Loss: 1.007e+00 , Validation Loss: 2.335e+00\n",
      "Iteration: 8504, Training Loss: 2.518e-01 , Validation Loss: 1.125e+00\n",
      "Iteration: 8505, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 8506, Training Loss: -1.000e-07 , Validation Loss: 7.379e-01\n",
      "Iteration: 8507, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 8508, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8509, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8510, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8511, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 8512, Training Loss: 7.555e-01 , Validation Loss: 2.613e+00\n",
      "Iteration: 8513, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8514, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8515, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 8516, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 8517, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8518, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8519, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8520, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8521, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8522, Training Loss: 5.037e-01 , Validation Loss: 1.240e+00\n",
      "Iteration: 8523, Training Loss: 7.555e-01 , Validation Loss: 1.560e+00\n",
      "Iteration: 8524, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8525, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8526, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8527, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8528, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8529, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8530, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8531, Training Loss: 2.015e+00 , Validation Loss: 4.016e+00\n",
      "Iteration: 8532, Training Loss: 1.259e+00 , Validation Loss: 1.252e+00\n",
      "Iteration: 8533, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8534, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8535, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8536, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8537, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 8538, Training Loss: 7.555e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 8539, Training Loss: 1.007e+00 , Validation Loss: 3.193e+00\n",
      "Iteration: 8540, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8541, Training Loss: -1.000e-07 , Validation Loss: 7.379e-01\n",
      "Iteration: 8542, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 8543, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8544, Training Loss: 1.259e+00 , Validation Loss: 3.593e+00\n",
      "Iteration: 8545, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8546, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8547, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8548, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8549, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 8550, Training Loss: 1.007e+00 , Validation Loss: 2.595e+00\n",
      "Iteration: 8551, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 8552, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8553, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 8554, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 8555, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8556, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8557, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8558, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8559, Training Loss: 7.555e-01 , Validation Loss: 2.601e+00\n",
      "Iteration: 8560, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8561, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8562, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 8563, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 8564, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8565, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8566, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 8567, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8568, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8569, Training Loss: 1.007e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 8570, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8571, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 8572, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 8573, Training Loss: 2.770e+00 , Validation Loss: 4.222e+00\n",
      "Iteration: 8574, Training Loss: 3.022e+00 , Validation Loss: 3.695e+00\n",
      "Iteration: 8575, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 8576, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 8577, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 8578, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 8579, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8580, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8581, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 8582, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8583, Training Loss: 2.518e-01 , Validation Loss: 2.504e+00\n",
      "Iteration: 8584, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 8585, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 8586, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8587, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8588, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8589, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8590, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 8591, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 8592, Training Loss: 2.518e-01 , Validation Loss: 1.216e+00\n",
      "Iteration: 8593, Training Loss: 3.022e+00 , Validation Loss: 4.143e+00\n",
      "Iteration: 8594, Training Loss: 9.318e+00 , Validation Loss: 8.231e+00\n",
      "Iteration: 8595, Training Loss: 4.030e+00 , Validation Loss: 5.153e+00\n",
      "Iteration: 8596, Training Loss: 2.015e+00 , Validation Loss: 2.220e+00\n",
      "Iteration: 8597, Training Loss: 5.037e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 8598, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 8599, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 8600, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8601, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8602, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 8603, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 8604, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 8605, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8606, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8607, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8608, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8609, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8610, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8611, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 8612, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 8613, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 8614, Training Loss: 1.007e+00 , Validation Loss: 2.201e+00\n",
      "Iteration: 8615, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8616, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8617, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8618, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8619, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8620, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 8621, Training Loss: 1.259e+00 , Validation Loss: 3.302e+00\n",
      "Iteration: 8622, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 8623, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8624, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8625, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8626, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8627, Training Loss: 7.555e-01 , Validation Loss: 1.185e+00\n",
      "Iteration: 8628, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 8629, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8630, Training Loss: 7.555e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 8631, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8632, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8633, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8634, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8635, Training Loss: 5.037e-01 , Validation Loss: 1.107e+00\n",
      "Iteration: 8636, Training Loss: 5.037e-01 , Validation Loss: 1.917e+00\n",
      "Iteration: 8637, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8638, Training Loss: 5.037e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 8639, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 8640, Training Loss: 1.259e+00 , Validation Loss: 8.951e-01\n",
      "Iteration: 8641, Training Loss: 3.274e+00 , Validation Loss: 4.808e+00\n",
      "Iteration: 8642, Training Loss: 1.511e+00 , Validation Loss: 1.349e+00\n",
      "Iteration: 8643, Training Loss: -1.000e-07 , Validation Loss: 5.262e-01\n",
      "Iteration: 8644, Training Loss: 1.259e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 8645, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8646, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8647, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8648, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8649, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8650, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8651, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 8652, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8653, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8654, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 8655, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 8656, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 8657, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8658, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8659, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8660, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8661, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 8662, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8663, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 8664, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 8665, Training Loss: 3.022e+00 , Validation Loss: 5.691e+00\n",
      "Iteration: 8666, Training Loss: 3.022e+00 , Validation Loss: 3.459e+00\n",
      "Iteration: 8667, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 8668, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 8669, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 8670, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 8671, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 8672, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 8673, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 8674, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8675, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8676, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8677, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 8678, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 8679, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 8680, Training Loss: 5.037e-01 , Validation Loss: 2.008e+00\n",
      "Iteration: 8681, Training Loss: 2.015e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 8682, Training Loss: 4.281e+00 , Validation Loss: 5.570e+00\n",
      "Iteration: 8683, Training Loss: 1.108e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 8684, Training Loss: 2.518e+00 , Validation Loss: 4.052e+00\n",
      "Iteration: 8685, Training Loss: 2.015e+00 , Validation Loss: 1.252e+00\n",
      "Iteration: 8686, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 8687, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 8688, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8689, Training Loss: 5.037e-01 , Validation Loss: 1.790e+00\n",
      "Iteration: 8690, Training Loss: 1.007e+00 , Validation Loss: 2.359e+00\n",
      "Iteration: 8691, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8692, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8693, Training Loss: 1.511e+00 , Validation Loss: 2.081e+00\n",
      "Iteration: 8694, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8695, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8696, Training Loss: 2.015e+00 , Validation Loss: 3.484e+00\n",
      "Iteration: 8697, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 8698, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8699, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8700, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 8701, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8702, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 8703, Training Loss: 7.555e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 8704, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 8705, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 8706, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8707, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8708, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8709, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8710, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8711, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8712, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8713, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 8714, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8715, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8716, Training Loss: 1.763e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 8717, Training Loss: 1.007e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 8718, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 8719, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8720, Training Loss: 7.555e-01 , Validation Loss: 1.325e+00\n",
      "Iteration: 8721, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 8722, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 8723, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8724, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 8725, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 8726, Training Loss: 4.030e+00 , Validation Loss: 4.972e+00\n",
      "Iteration: 8727, Training Loss: 7.052e+00 , Validation Loss: 1.018e+01\n",
      "Iteration: 8728, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8729, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8730, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8731, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8732, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8733, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8734, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8735, Training Loss: 1.259e+00 , Validation Loss: 1.034e+00\n",
      "Iteration: 8736, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8737, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8738, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8739, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8740, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8741, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8742, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8743, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 8744, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8745, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8746, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8747, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 8748, Training Loss: 1.511e+00 , Validation Loss: 3.834e+00\n",
      "Iteration: 8749, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 8750, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8751, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8752, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8753, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 8754, Training Loss: 1.511e+00 , Validation Loss: 2.280e+00\n",
      "Iteration: 8755, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8756, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 8757, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8758, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8759, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8760, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8761, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8762, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8763, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8764, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 8765, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8766, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8767, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8768, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8769, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 8770, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 8771, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 8772, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8773, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 8774, Training Loss: 1.511e+00 , Validation Loss: 3.810e+00\n",
      "Iteration: 8775, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8776, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 8777, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8778, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8779, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8780, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 8781, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 8782, Training Loss: 2.267e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 8783, Training Loss: 1.007e+00 , Validation Loss: 1.669e+00\n",
      "Iteration: 8784, Training Loss: 2.015e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 8785, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8786, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8787, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8788, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8789, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8790, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8791, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8792, Training Loss: 7.555e-01 , Validation Loss: 1.240e+00\n",
      "Iteration: 8793, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8794, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 8795, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 8796, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 8797, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 8798, Training Loss: 3.022e+00 , Validation Loss: 3.774e+00\n",
      "Iteration: 8799, Training Loss: 1.511e+00 , Validation Loss: 2.407e+00\n",
      "Iteration: 8800, Training Loss: 1.763e+00 , Validation Loss: 8.588e-01\n",
      "Iteration: 8801, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 8802, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8803, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8804, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8805, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 8806, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 8807, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8808, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8809, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8810, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8811, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 8812, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8813, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8814, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8815, Training Loss: 2.518e-01 , Validation Loss: 1.669e+00\n",
      "Iteration: 8816, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 8817, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8818, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8819, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8820, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8821, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 8822, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8823, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8824, Training Loss: -1.000e-07 , Validation Loss: 6.230e-01\n",
      "Iteration: 8825, Training Loss: 1.007e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 8826, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8827, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 8828, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 8829, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8830, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8831, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 8832, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 8833, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 8834, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 8835, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 8836, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8837, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8838, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8839, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8840, Training Loss: 7.555e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 8841, Training Loss: 1.763e+00 , Validation Loss: 4.052e+00\n",
      "Iteration: 8842, Training Loss: 1.007e+00 , Validation Loss: 8.044e-01\n",
      "Iteration: 8843, Training Loss: 1.511e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8844, Training Loss: 2.015e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 8845, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8846, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 8847, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8848, Training Loss: 1.511e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 8849, Training Loss: 1.007e+00 , Validation Loss: 3.175e+00\n",
      "Iteration: 8850, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 8851, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8852, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8853, Training Loss: 2.518e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 8854, Training Loss: 2.518e-01 , Validation Loss: 3.163e+00\n",
      "Iteration: 8855, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 8856, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8857, Training Loss: 7.555e-01 , Validation Loss: 2.038e+00\n",
      "Iteration: 8858, Training Loss: 2.518e-01 , Validation Loss: 9.193e-01\n",
      "Iteration: 8859, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 8860, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8861, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 8862, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8863, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8864, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 8865, Training Loss: 1.007e+00 , Validation Loss: 1.228e+00\n",
      "Iteration: 8866, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 8867, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8868, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 8869, Training Loss: 2.518e-01 , Validation Loss: 1.343e+00\n",
      "Iteration: 8870, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 8871, Training Loss: 2.015e+00 , Validation Loss: 4.318e+00\n",
      "Iteration: 8872, Training Loss: 1.511e+00 , Validation Loss: 1.258e+00\n",
      "Iteration: 8873, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 8874, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 8875, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8876, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8877, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8878, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 8879, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 8880, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8881, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8882, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 8883, Training Loss: 2.015e+00 , Validation Loss: 4.197e+00\n",
      "Iteration: 8884, Training Loss: 7.555e-01 , Validation Loss: 1.107e+00\n",
      "Iteration: 8885, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 8886, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 8887, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 8888, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 8889, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 8890, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8891, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8892, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8893, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8894, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 8895, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 8896, Training Loss: 5.037e-01 , Validation Loss: 2.141e+00\n",
      "Iteration: 8897, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 8898, Training Loss: 2.518e-01 , Validation Loss: 1.718e+00\n",
      "Iteration: 8899, Training Loss: 5.037e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 8900, Training Loss: 1.007e+00 , Validation Loss: 7.742e-01\n",
      "Iteration: 8901, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8902, Training Loss: 3.022e+00 , Validation Loss: 4.361e+00\n",
      "Iteration: 8903, Training Loss: 1.259e+00 , Validation Loss: 1.941e+00\n",
      "Iteration: 8904, Training Loss: 1.763e+00 , Validation Loss: 8.588e-01\n",
      "Iteration: 8905, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8906, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8907, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8908, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 8909, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 8910, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8911, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8912, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8913, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8914, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8915, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8916, Training Loss: 2.518e-01 , Validation Loss: 2.613e+00\n",
      "Iteration: 8917, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8918, Training Loss: 1.259e+00 , Validation Loss: 3.133e+00\n",
      "Iteration: 8919, Training Loss: 1.511e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 8920, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 8921, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 8922, Training Loss: 2.015e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 8923, Training Loss: 7.555e-01 , Validation Loss: 9.435e-01\n",
      "Iteration: 8924, Training Loss: 1.007e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 8925, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8926, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8927, Training Loss: 1.259e+00 , Validation Loss: 2.976e+00\n",
      "Iteration: 8928, Training Loss: 1.259e+00 , Validation Loss: 6.471e-01\n",
      "Iteration: 8929, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8930, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 8931, Training Loss: -1.000e-07 , Validation Loss: 6.532e-01\n",
      "Iteration: 8932, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 8933, Training Loss: 2.015e+00 , Validation Loss: 4.264e+00\n",
      "Iteration: 8934, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 8935, Training Loss: 1.511e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 8936, Training Loss: 1.259e+00 , Validation Loss: 2.135e+00\n",
      "Iteration: 8937, Training Loss: 3.526e+00 , Validation Loss: 4.488e+00\n",
      "Iteration: 8938, Training Loss: 1.763e+00 , Validation Loss: 1.518e+00\n",
      "Iteration: 8939, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 8940, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 8941, Training Loss: 5.037e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 8942, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8943, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8944, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8945, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8946, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8947, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 8948, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 8949, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 8950, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 8951, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 8952, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8953, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8954, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 8955, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8956, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8957, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8958, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 8959, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8960, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8961, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 8962, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8963, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8964, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8965, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8966, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 8967, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 8968, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 8969, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8970, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 8971, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8972, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 8973, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 8974, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 8975, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 8976, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8977, Training Loss: 1.511e+00 , Validation Loss: 2.631e+00\n",
      "Iteration: 8978, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 8979, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 8980, Training Loss: 1.007e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 8981, Training Loss: 2.015e+00 , Validation Loss: 3.998e+00\n",
      "Iteration: 8982, Training Loss: 1.007e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 8983, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 8984, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8985, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8986, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8987, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8988, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8989, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 8990, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8991, Training Loss: 1.007e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 8992, Training Loss: 1.763e+00 , Validation Loss: 3.290e+00\n",
      "Iteration: 8993, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 8994, Training Loss: 7.555e-01 , Validation Loss: 2.262e+00\n",
      "Iteration: 8995, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 8996, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 8997, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 8998, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 8999, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9000, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9001, Training Loss: -1.000e-07 , Validation Loss: 1.167e+00\n",
      "Iteration: 9002, Training Loss: 2.518e-01 , Validation Loss: 1.167e+00\n",
      "Iteration: 9003, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9004, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9005, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 9006, Training Loss: 2.770e+00 , Validation Loss: 4.095e+00\n",
      "Iteration: 9007, Training Loss: 2.518e-01 , Validation Loss: 1.603e+00\n",
      "Iteration: 9008, Training Loss: 1.511e+00 , Validation Loss: 1.427e+00\n",
      "Iteration: 9009, Training Loss: 7.555e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 9010, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9011, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9012, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9013, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9014, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9015, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 9016, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9017, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9018, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9019, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 9020, Training Loss: 1.763e+00 , Validation Loss: 3.496e+00\n",
      "Iteration: 9021, Training Loss: 1.259e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 9022, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 9023, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 9024, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9025, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9026, Training Loss: 5.037e-01 , Validation Loss: 1.488e+00\n",
      "Iteration: 9027, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9028, Training Loss: 7.555e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 9029, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 9030, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9031, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9032, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 9033, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9034, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9035, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9036, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9037, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9038, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9039, Training Loss: -1.000e-07 , Validation Loss: 4.597e-01\n",
      "Iteration: 9040, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9041, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9042, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9043, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 9044, Training Loss: 1.259e+00 , Validation Loss: 3.593e+00\n",
      "Iteration: 9045, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 9046, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9047, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9048, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 9049, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9050, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9051, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 9052, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9053, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 9054, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 9055, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9056, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9057, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9058, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9059, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 9060, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9061, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9062, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 9063, Training Loss: 1.763e+00 , Validation Loss: 2.849e+00\n",
      "Iteration: 9064, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9065, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 9066, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 9067, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9068, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9069, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 9070, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9071, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9072, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 9073, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9074, Training Loss: 7.555e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 9075, Training Loss: 5.037e-01 , Validation Loss: 2.619e+00\n",
      "Iteration: 9076, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 9077, Training Loss: 5.037e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 9078, Training Loss: 1.511e+00 , Validation Loss: 4.355e+00\n",
      "Iteration: 9079, Training Loss: 7.555e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 9080, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9081, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 9082, Training Loss: 7.555e-01 , Validation Loss: 1.687e+00\n",
      "Iteration: 9083, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9084, Training Loss: 2.518e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 9085, Training Loss: 1.511e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 9086, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9087, Training Loss: 5.037e-01 , Validation Loss: 9.737e-01\n",
      "Iteration: 9088, Training Loss: 1.511e+00 , Validation Loss: 2.171e+00\n",
      "Iteration: 9089, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 9090, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9091, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9092, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9093, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9094, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 9095, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9096, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9097, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 9098, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9099, Training Loss: 5.037e-01 , Validation Loss: 1.579e+00\n",
      "Iteration: 9100, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9101, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 9102, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9103, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9104, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9105, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9106, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 9107, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9108, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9109, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9110, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9111, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9112, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9113, Training Loss: 1.259e+00 , Validation Loss: 1.651e+00\n",
      "Iteration: 9114, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 9115, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9116, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9117, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9118, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9119, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9120, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 9121, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9122, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 9123, Training Loss: 1.007e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 9124, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9125, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9126, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9127, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9128, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9129, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9130, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9131, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9132, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9133, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9134, Training Loss: 2.518e+00 , Validation Loss: 3.508e+00\n",
      "Iteration: 9135, Training Loss: 2.015e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 9136, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9137, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9138, Training Loss: 1.007e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 9139, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9140, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9141, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 9142, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 9143, Training Loss: 1.259e+00 , Validation Loss: 2.655e+00\n",
      "Iteration: 9144, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 9145, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 9146, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 9147, Training Loss: 1.007e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 9148, Training Loss: 1.007e+00 , Validation Loss: 2.171e+00\n",
      "Iteration: 9149, Training Loss: 4.030e+00 , Validation Loss: 4.028e+00\n",
      "Iteration: 9150, Training Loss: 8.059e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 9151, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 9152, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9153, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9154, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9155, Training Loss: 1.511e+00 , Validation Loss: 3.423e+00\n",
      "Iteration: 9156, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9157, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 9158, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9159, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9160, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9161, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9162, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9163, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 9164, Training Loss: 1.007e+00 , Validation Loss: 1.687e+00\n",
      "Iteration: 9165, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 9166, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9167, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9168, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9169, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9170, Training Loss: 1.259e+00 , Validation Loss: 2.214e+00\n",
      "Iteration: 9171, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9172, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9173, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9174, Training Loss: 2.267e+00 , Validation Loss: 3.453e+00\n",
      "Iteration: 9175, Training Loss: 1.007e+00 , Validation Loss: 8.951e-01\n",
      "Iteration: 9176, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 9177, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9178, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9179, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9180, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9181, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9182, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9183, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 9184, Training Loss: 2.015e+00 , Validation Loss: 4.052e+00\n",
      "Iteration: 9185, Training Loss: 1.511e+00 , Validation Loss: 1.004e+00\n",
      "Iteration: 9186, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 9187, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9188, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 9189, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9190, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9191, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9192, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9193, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9194, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9195, Training Loss: 1.511e+00 , Validation Loss: 3.816e+00\n",
      "Iteration: 9196, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 9197, Training Loss: 1.007e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 9198, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9199, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9200, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9201, Training Loss: 2.518e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 9202, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9203, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9204, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9205, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 9206, Training Loss: 3.022e+00 , Validation Loss: 4.252e+00\n",
      "Iteration: 9207, Training Loss: 1.511e+00 , Validation Loss: 2.093e+00\n",
      "Iteration: 9208, Training Loss: 1.259e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 9209, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 9210, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9211, Training Loss: 3.778e+00 , Validation Loss: 3.726e+00\n",
      "Iteration: 9212, Training Loss: 5.792e+00 , Validation Loss: 8.286e+00\n",
      "Iteration: 9213, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9214, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9215, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9216, Training Loss: 1.007e+00 , Validation Loss: 3.085e+00\n",
      "Iteration: 9217, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 9218, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9219, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9220, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9221, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9222, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9223, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9224, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9225, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9226, Training Loss: 1.259e+00 , Validation Loss: 2.347e+00\n",
      "Iteration: 9227, Training Loss: 1.259e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 9228, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 9229, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 9230, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9231, Training Loss: 2.770e+00 , Validation Loss: 3.810e+00\n",
      "Iteration: 9232, Training Loss: 3.526e+00 , Validation Loss: 2.522e+00\n",
      "Iteration: 9233, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9234, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9235, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 9236, Training Loss: 2.267e+00 , Validation Loss: 3.562e+00\n",
      "Iteration: 9237, Training Loss: 7.555e-01 , Validation Loss: 8.830e-01\n",
      "Iteration: 9238, Training Loss: 1.511e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 9239, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9240, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9241, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9242, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 9243, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 9244, Training Loss: 2.518e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 9245, Training Loss: 1.259e+00 , Validation Loss: 1.258e+00\n",
      "Iteration: 9246, Training Loss: 7.555e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 9247, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 9248, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9249, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9250, Training Loss: 4.281e+00 , Validation Loss: 4.088e+00\n",
      "Iteration: 9251, Training Loss: 5.792e+00 , Validation Loss: 8.274e+00\n",
      "Iteration: 9252, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 9253, Training Loss: 1.007e+00 , Validation Loss: 4.476e-01\n",
      "Iteration: 9254, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9255, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9256, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9257, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9258, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9259, Training Loss: 7.555e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 9260, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9261, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9262, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9263, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9264, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9265, Training Loss: 7.555e-01 , Validation Loss: 9.072e-01\n",
      "Iteration: 9266, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 9267, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9268, Training Loss: 1.007e+00 , Validation Loss: 4.355e-01\n",
      "Iteration: 9269, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9270, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9271, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9272, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9273, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9274, Training Loss: 1.259e+00 , Validation Loss: 1.923e+00\n",
      "Iteration: 9275, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9276, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 9277, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9278, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 9279, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9280, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9281, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9282, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9283, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9284, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9285, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9286, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9287, Training Loss: 1.511e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 9288, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9289, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9290, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9291, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 9292, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 9293, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 9294, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9295, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9296, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 9297, Training Loss: 3.778e+00 , Validation Loss: 4.010e+00\n",
      "Iteration: 9298, Training Loss: 9.318e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 9299, Training Loss: 2.015e+00 , Validation Loss: 1.935e+00\n",
      "Iteration: 9300, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 9301, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9302, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 9303, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 9304, Training Loss: 1.511e+00 , Validation Loss: 3.943e+00\n",
      "Iteration: 9305, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 9306, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 9307, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9308, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9309, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9310, Training Loss: 2.518e+00 , Validation Loss: 4.857e+00\n",
      "Iteration: 9311, Training Loss: 1.007e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 9312, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9313, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 9314, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 9315, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9316, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9317, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9318, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 9319, Training Loss: 2.267e+00 , Validation Loss: 3.453e+00\n",
      "Iteration: 9320, Training Loss: 1.007e+00 , Validation Loss: 2.074e+00\n",
      "Iteration: 9321, Training Loss: 1.259e+00 , Validation Loss: 1.016e+00\n",
      "Iteration: 9322, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9323, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 9324, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9325, Training Loss: 1.763e+00 , Validation Loss: 2.915e+00\n",
      "Iteration: 9326, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 9327, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 9328, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 9329, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 9330, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9331, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 9332, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 9333, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 9334, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9335, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 9336, Training Loss: 5.037e-01 , Validation Loss: 1.651e+00\n",
      "Iteration: 9337, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 9338, Training Loss: 1.763e+00 , Validation Loss: 4.040e+00\n",
      "Iteration: 9339, Training Loss: 1.259e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 9340, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9341, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9342, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9343, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9344, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 9345, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9346, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 9347, Training Loss: 1.007e+00 , Validation Loss: 3.042e+00\n",
      "Iteration: 9348, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 9349, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 9350, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 9351, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9352, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9353, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9354, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9355, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9356, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9357, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9358, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9359, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9360, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9361, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9362, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9363, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 9364, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 9365, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9366, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 9367, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 9368, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 9369, Training Loss: 1.007e+00 , Validation Loss: 4.415e-01\n",
      "Iteration: 9370, Training Loss: -1.000e-07 , Validation Loss: 7.621e-01\n",
      "Iteration: 9371, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 9372, Training Loss: 5.037e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 9373, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9374, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 9375, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9376, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 9377, Training Loss: 4.281e+00 , Validation Loss: 4.784e+00\n",
      "Iteration: 9378, Training Loss: 1.007e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 9379, Training Loss: 3.022e+00 , Validation Loss: 5.050e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9380, Training Loss: 1.007e+00 , Validation Loss: 9.254e-01\n",
      "Iteration: 9381, Training Loss: 7.555e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 9382, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 9383, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9384, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9385, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9386, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9387, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9388, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9389, Training Loss: -1.000e-07 , Validation Loss: 6.290e-01\n",
      "Iteration: 9390, Training Loss: 5.037e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 9391, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9392, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9393, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9394, Training Loss: -1.000e-07 , Validation Loss: 8.407e-01\n",
      "Iteration: 9395, Training Loss: 5.037e-01 , Validation Loss: 8.407e-01\n",
      "Iteration: 9396, Training Loss: 7.555e-01 , Validation Loss: 2.625e+00\n",
      "Iteration: 9397, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 9398, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 9399, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 9400, Training Loss: 2.518e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 9401, Training Loss: 2.267e+00 , Validation Loss: 4.161e+00\n",
      "Iteration: 9402, Training Loss: 1.511e+00 , Validation Loss: 1.325e+00\n",
      "Iteration: 9403, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9404, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9405, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 9406, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 9407, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 9408, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 9409, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 9410, Training Loss: 2.770e+00 , Validation Loss: 4.222e+00\n",
      "Iteration: 9411, Training Loss: 1.259e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 9412, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9413, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9414, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9415, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9416, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 9417, Training Loss: 7.555e-01 , Validation Loss: 1.415e+00\n",
      "Iteration: 9418, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9419, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9420, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9421, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9422, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 9423, Training Loss: 3.274e+00 , Validation Loss: 4.318e+00\n",
      "Iteration: 9424, Training Loss: 3.022e+00 , Validation Loss: 1.657e+00\n",
      "Iteration: 9425, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9426, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9427, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9428, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9429, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9430, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9431, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9432, Training Loss: -1.000e-07 , Validation Loss: 6.592e-01\n",
      "Iteration: 9433, Training Loss: 5.037e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 9434, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9435, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9436, Training Loss: 1.511e+00 , Validation Loss: 3.296e+00\n",
      "Iteration: 9437, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 9438, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 9439, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9440, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 9441, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9442, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9443, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9444, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9445, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9446, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9447, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9448, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9449, Training Loss: 7.555e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 9450, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9451, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9452, Training Loss: 5.037e-01 , Validation Loss: 1.318e+00\n",
      "Iteration: 9453, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9454, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9455, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9456, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9457, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 9458, Training Loss: 5.037e-01 , Validation Loss: 9.314e-01\n",
      "Iteration: 9459, Training Loss: 5.037e-01 , Validation Loss: 1.095e+00\n",
      "Iteration: 9460, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 9461, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9462, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9463, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 9464, Training Loss: 7.555e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 9465, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9466, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9467, Training Loss: 2.518e-01 , Validation Loss: 2.195e+00\n",
      "Iteration: 9468, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 9469, Training Loss: -1.000e-07 , Validation Loss: 2.129e+00\n",
      "Iteration: 9470, Training Loss: 1.259e+00 , Validation Loss: 2.129e+00\n",
      "Iteration: 9471, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9472, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 9473, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9474, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9475, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9476, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9477, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9478, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9479, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 9480, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 9481, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 9482, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 9483, Training Loss: 7.555e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 9484, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9485, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 9486, Training Loss: 5.037e-01 , Validation Loss: 8.588e-01\n",
      "Iteration: 9487, Training Loss: 2.518e-01 , Validation Loss: 1.512e+00\n",
      "Iteration: 9488, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 9489, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 9490, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9491, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 9492, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 9493, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9494, Training Loss: 3.022e+00 , Validation Loss: 4.088e+00\n",
      "Iteration: 9495, Training Loss: 2.518e+00 , Validation Loss: 3.593e+00\n",
      "Iteration: 9496, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 9497, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 9498, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 9499, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9500, Training Loss: 2.015e+00 , Validation Loss: 2.649e+00\n",
      "Iteration: 9501, Training Loss: 7.555e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 9502, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 9503, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9504, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 9505, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 9506, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9507, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9508, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9509, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9510, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9511, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 9512, Training Loss: 1.259e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 9513, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9514, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9515, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9516, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9517, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 9518, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 9519, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9520, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9521, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9522, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9523, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 9524, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9525, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9526, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9527, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9528, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9529, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 9530, Training Loss: 7.555e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 9531, Training Loss: 3.022e+00 , Validation Loss: 4.070e+00\n",
      "Iteration: 9532, Training Loss: 1.511e+00 , Validation Loss: 2.008e+00\n",
      "Iteration: 9533, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9534, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 9535, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9536, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9537, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9538, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9539, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9540, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9541, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9542, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9543, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 9544, Training Loss: 2.267e+00 , Validation Loss: 1.681e+00\n",
      "Iteration: 9545, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9546, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9547, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9548, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 9549, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9550, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 9551, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9552, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9553, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9554, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 9555, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9556, Training Loss: 2.015e+00 , Validation Loss: 4.143e+00\n",
      "Iteration: 9557, Training Loss: 2.015e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 9558, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9559, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 9560, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 9561, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 9562, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9563, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9564, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 9565, Training Loss: 1.259e+00 , Validation Loss: 1.712e+00\n",
      "Iteration: 9566, Training Loss: 5.037e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 9567, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9568, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9569, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9570, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9571, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9572, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9573, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9574, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 9575, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 9576, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 9577, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 9578, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 9579, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 9580, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9581, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 9582, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 9583, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 9584, Training Loss: 5.037e-01 , Validation Loss: 1.579e+00\n",
      "Iteration: 9585, Training Loss: 1.511e+00 , Validation Loss: 2.032e+00\n",
      "Iteration: 9586, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9587, Training Loss: 1.511e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 9588, Training Loss: 1.511e+00 , Validation Loss: 4.004e+00\n",
      "Iteration: 9589, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 9590, Training Loss: 1.259e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 9591, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9592, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9593, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 9594, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 9595, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9596, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9597, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9598, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9599, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9600, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 9601, Training Loss: 5.037e-01 , Validation Loss: 2.298e+00\n",
      "Iteration: 9602, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 9603, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9604, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9605, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 9606, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 9607, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 9608, Training Loss: 1.007e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 9609, Training Loss: 1.511e+00 , Validation Loss: 2.020e+00\n",
      "Iteration: 9610, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9611, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 9612, Training Loss: 7.555e-01 , Validation Loss: 9.556e-01\n",
      "Iteration: 9613, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9614, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9615, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9616, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 9617, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 9618, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 9619, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9620, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9621, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9622, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9623, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9624, Training Loss: 5.037e-01 , Validation Loss: 1.191e+00\n",
      "Iteration: 9625, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9626, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 9627, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9628, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9629, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9630, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9631, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9632, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9633, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9634, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9635, Training Loss: 2.518e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 9636, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 9637, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 9638, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 9639, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 9640, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9641, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9642, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9643, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9644, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9645, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 9646, Training Loss: 2.518e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 9647, Training Loss: 1.007e+00 , Validation Loss: 2.183e+00\n",
      "Iteration: 9648, Training Loss: -1.000e-07 , Validation Loss: 5.322e-01\n",
      "Iteration: 9649, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 9650, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9651, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 9652, Training Loss: 7.555e-01 , Validation Loss: 9.979e-01\n",
      "Iteration: 9653, Training Loss: 3.274e+00 , Validation Loss: 4.173e+00\n",
      "Iteration: 9654, Training Loss: 3.022e+00 , Validation Loss: 4.512e+00\n",
      "Iteration: 9655, Training Loss: 1.511e+00 , Validation Loss: 9.737e-01\n",
      "Iteration: 9656, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 9657, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9658, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9659, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9660, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9661, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9662, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9663, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 9664, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 9665, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9666, Training Loss: 2.518e-01 , Validation Loss: 1.361e+00\n",
      "Iteration: 9667, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 9668, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9669, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9670, Training Loss: 7.555e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 9671, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9672, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9673, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9674, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9675, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9676, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9677, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 9678, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 9679, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9680, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 9681, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 9682, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 9683, Training Loss: 2.770e+00 , Validation Loss: 4.101e+00\n",
      "Iteration: 9684, Training Loss: 1.763e+00 , Validation Loss: 3.151e+00\n",
      "Iteration: 9685, Training Loss: 1.511e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 9686, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9687, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9688, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9689, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9690, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9691, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9692, Training Loss: 1.763e+00 , Validation Loss: 3.931e+00\n",
      "Iteration: 9693, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 9694, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9695, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9696, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9697, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9698, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9699, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9700, Training Loss: 1.259e+00 , Validation Loss: 3.617e+00\n",
      "Iteration: 9701, Training Loss: -1.000e-07 , Validation Loss: 4.597e-01\n",
      "Iteration: 9702, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 9703, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9704, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9705, Training Loss: 7.555e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 9706, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 9707, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9708, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 9709, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 9710, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9711, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9712, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9713, Training Loss: 5.037e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 9714, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9715, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9716, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9717, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9718, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 9719, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9720, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9721, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 9722, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 9723, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 9724, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 9725, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 9726, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 9727, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9728, Training Loss: 1.259e+00 , Validation Loss: 4.899e-01\n",
      "Iteration: 9729, Training Loss: 7.555e-01 , Validation Loss: 2.147e+00\n",
      "Iteration: 9730, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9731, Training Loss: 2.518e-01 , Validation Loss: 1.754e+00\n",
      "Iteration: 9732, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 9733, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 9734, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 9735, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 9736, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 9737, Training Loss: 2.518e-01 , Validation Loss: 1.808e+00\n",
      "Iteration: 9738, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 9739, Training Loss: 1.763e+00 , Validation Loss: 4.228e+00\n",
      "Iteration: 9740, Training Loss: 1.259e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 9741, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 9742, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 9743, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 9744, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 9745, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 9746, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 9747, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9748, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9749, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 9750, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 9751, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9752, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9753, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 9754, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9755, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9756, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9757, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9758, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9759, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9760, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9761, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9762, Training Loss: 1.259e+00 , Validation Loss: 4.222e+00\n",
      "Iteration: 9763, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9764, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9765, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9766, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9767, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9768, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 9769, Training Loss: -1.000e-07 , Validation Loss: 1.415e+00\n",
      "Iteration: 9770, Training Loss: -1.000e-07 , Validation Loss: 1.415e+00\n",
      "Iteration: 9771, Training Loss: 7.555e-01 , Validation Loss: 1.415e+00\n",
      "Iteration: 9772, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9773, Training Loss: 2.518e-01 , Validation Loss: 8.407e-01\n",
      "Iteration: 9774, Training Loss: 2.015e+00 , Validation Loss: 2.903e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9775, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 9776, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 9777, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9778, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9779, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9780, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9781, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9782, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9783, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9784, Training Loss: 5.037e-01 , Validation Loss: 3.266e+00\n",
      "Iteration: 9785, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 9786, Training Loss: 1.007e+00 , Validation Loss: 1.712e+00\n",
      "Iteration: 9787, Training Loss: 1.763e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 9788, Training Loss: 2.518e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 9789, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9790, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9791, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9792, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9793, Training Loss: 1.007e+00 , Validation Loss: 4.415e-01\n",
      "Iteration: 9794, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9795, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9796, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9797, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9798, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9799, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 9800, Training Loss: 2.518e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 9801, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 9802, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 9803, Training Loss: 7.555e-01 , Validation Loss: 1.022e+00\n",
      "Iteration: 9804, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 9805, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 9806, Training Loss: 5.037e-01 , Validation Loss: 8.588e-01\n",
      "Iteration: 9807, Training Loss: 1.007e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 9808, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 9809, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 9810, Training Loss: 1.007e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 9811, Training Loss: 3.778e+00 , Validation Loss: 3.937e+00\n",
      "Iteration: 9812, Training Loss: 1.259e+00 , Validation Loss: 2.716e+00\n",
      "Iteration: 9813, Training Loss: 2.015e+00 , Validation Loss: 9.012e-01\n",
      "Iteration: 9814, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9815, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9816, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9817, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9818, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 9819, Training Loss: 1.007e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 9820, Training Loss: 7.555e-01 , Validation Loss: 1.185e+00\n",
      "Iteration: 9821, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 9822, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 9823, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9824, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9825, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9826, Training Loss: 5.037e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 9827, Training Loss: 7.555e-01 , Validation Loss: 1.518e+00\n",
      "Iteration: 9828, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9829, Training Loss: 1.007e+00 , Validation Loss: 1.972e+00\n",
      "Iteration: 9830, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9831, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 9832, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9833, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9834, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9835, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9836, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9837, Training Loss: 2.518e-01 , Validation Loss: 1.712e+00\n",
      "Iteration: 9838, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 9839, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9840, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9841, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9842, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9843, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9844, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9845, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 9846, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 9847, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 9848, Training Loss: 2.267e+00 , Validation Loss: 4.034e+00\n",
      "Iteration: 9849, Training Loss: 1.511e+00 , Validation Loss: 1.923e+00\n",
      "Iteration: 9850, Training Loss: 1.259e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 9851, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 9852, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9853, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9854, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9855, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9856, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9857, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9858, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9859, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 9860, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9861, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 9862, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9863, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9864, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 9865, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9866, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 9867, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 9868, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 9869, Training Loss: 1.259e+00 , Validation Loss: 2.679e+00\n",
      "Iteration: 9870, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9871, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 9872, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9873, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9874, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 9875, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 9876, Training Loss: 7.555e-01 , Validation Loss: 2.516e+00\n",
      "Iteration: 9877, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 9878, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9879, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 9880, Training Loss: 2.518e-01 , Validation Loss: 1.058e+00\n",
      "Iteration: 9881, Training Loss: 1.763e+00 , Validation Loss: 3.834e+00\n",
      "Iteration: 9882, Training Loss: 1.259e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 9883, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9884, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 9885, Training Loss: 2.770e+00 , Validation Loss: 4.996e+00\n",
      "Iteration: 9886, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 9887, Training Loss: 1.007e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 9888, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9889, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9890, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 9891, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9892, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9893, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 9894, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 9895, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 9896, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9897, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9898, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9899, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 9900, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 9901, Training Loss: -1.000e-07 , Validation Loss: 6.290e-01\n",
      "Iteration: 9902, Training Loss: 5.037e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 9903, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9904, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9905, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9906, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9907, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9908, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 9909, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9910, Training Loss: 5.037e-01 , Validation Loss: 1.693e+00\n",
      "Iteration: 9911, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9912, Training Loss: 1.007e+00 , Validation Loss: 2.806e+00\n",
      "Iteration: 9913, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 9914, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9915, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9916, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9917, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9918, Training Loss: 7.555e-01 , Validation Loss: 2.443e+00\n",
      "Iteration: 9919, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9920, Training Loss: 1.007e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 9921, Training Loss: 1.007e+00 , Validation Loss: 1.452e+00\n",
      "Iteration: 9922, Training Loss: 2.770e+00 , Validation Loss: 3.738e+00\n",
      "Iteration: 9923, Training Loss: 1.007e+00 , Validation Loss: 1.077e+00\n",
      "Iteration: 9924, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 9925, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9926, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9927, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9928, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9929, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 9930, Training Loss: 1.511e+00 , Validation Loss: 3.587e+00\n",
      "Iteration: 9931, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 9932, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9933, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9934, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9935, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9936, Training Loss: 7.555e-01 , Validation Loss: 1.869e+00\n",
      "Iteration: 9937, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 9938, Training Loss: 1.763e+00 , Validation Loss: 4.004e+00\n",
      "Iteration: 9939, Training Loss: 1.007e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 9940, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9941, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9942, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9943, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9944, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9945, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9946, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9947, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9948, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 9949, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 9950, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 9951, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9952, Training Loss: 5.037e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 9953, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9954, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 9955, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 9956, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 9957, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 9958, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 9959, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9960, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9961, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9962, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 9963, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9964, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 9965, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 9966, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9967, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9968, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 9969, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9970, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 9971, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9972, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9973, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 9974, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 9975, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 9976, Training Loss: 2.770e+00 , Validation Loss: 5.171e+00\n",
      "Iteration: 9977, Training Loss: 2.518e+00 , Validation Loss: 1.452e+00\n",
      "Iteration: 9978, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9979, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9980, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 9981, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9982, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9983, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9984, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9985, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 9986, Training Loss: 7.555e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 9987, Training Loss: 3.274e+00 , Validation Loss: 3.792e+00\n",
      "Iteration: 9988, Training Loss: 1.763e+00 , Validation Loss: 1.004e+00\n",
      "Iteration: 9989, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9990, Training Loss: 7.555e-01 , Validation Loss: 2.020e+00\n",
      "Iteration: 9991, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 9992, Training Loss: 2.770e+00 , Validation Loss: 4.276e+00\n",
      "Iteration: 9993, Training Loss: 1.763e+00 , Validation Loss: 1.155e+00\n",
      "Iteration: 9994, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9995, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9996, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 9997, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 9998, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 9999, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10000, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 10001, Training Loss: 2.518e-01 , Validation Loss: 3.133e+00\n",
      "Iteration: 10002, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 10003, Training Loss: 1.259e+00 , Validation Loss: 3.974e+00\n",
      "Iteration: 10004, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 10005, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10006, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 10007, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 10008, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10009, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10010, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 10011, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 10012, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 10013, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 10014, Training Loss: 1.007e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 10015, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10016, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 10017, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10018, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10019, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10020, Training Loss: 7.555e-01 , Validation Loss: 1.107e+00\n",
      "Iteration: 10021, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10022, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10023, Training Loss: 5.037e-01 , Validation Loss: 2.631e+00\n",
      "Iteration: 10024, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10025, Training Loss: 7.555e-01 , Validation Loss: 1.524e+00\n",
      "Iteration: 10026, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10027, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10028, Training Loss: 7.555e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 10029, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10030, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10031, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10032, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10033, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10034, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10035, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 10036, Training Loss: 1.259e+00 , Validation Loss: 8.044e-01\n",
      "Iteration: 10037, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10038, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10039, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10040, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10041, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10042, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10043, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10044, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10045, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10046, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10047, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10048, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10049, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10050, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10051, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 10052, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 10053, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 10054, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10055, Training Loss: 5.037e-01 , Validation Loss: 2.093e+00\n",
      "Iteration: 10056, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 10057, Training Loss: 7.555e-01 , Validation Loss: 1.845e+00\n",
      "Iteration: 10058, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10059, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 10060, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 10061, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 10062, Training Loss: 1.007e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 10063, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 10064, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 10065, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 10066, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10067, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 10068, Training Loss: 1.007e+00 , Validation Loss: 2.643e+00\n",
      "Iteration: 10069, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10070, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10071, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 10072, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10073, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10074, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10075, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10076, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10077, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10078, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10079, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10080, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 10081, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 10082, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 10083, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 10084, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 10085, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 10086, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 10087, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 10088, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10089, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10090, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 10091, Training Loss: 2.518e+00 , Validation Loss: 3.695e+00\n",
      "Iteration: 10092, Training Loss: 1.511e+00 , Validation Loss: 1.488e+00\n",
      "Iteration: 10093, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10094, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 10095, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10096, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10097, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10098, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 10099, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 10100, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 10101, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10102, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 10103, Training Loss: 5.037e-01 , Validation Loss: 1.651e+00\n",
      "Iteration: 10104, Training Loss: 2.518e-01 , Validation Loss: 2.335e+00\n",
      "Iteration: 10105, Training Loss: 1.259e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 10106, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 10107, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10108, Training Loss: 1.511e+00 , Validation Loss: 3.762e+00\n",
      "Iteration: 10109, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 10110, Training Loss: 1.511e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 10111, Training Loss: 2.518e-01 , Validation Loss: 1.300e+00\n",
      "Iteration: 10112, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 10113, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10114, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10115, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10116, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 10117, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10118, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10119, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10120, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10121, Training Loss: 1.007e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 10122, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10123, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10124, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10125, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10126, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10127, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10128, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10129, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10130, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10131, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10132, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 10133, Training Loss: 2.518e+00 , Validation Loss: 3.859e+00\n",
      "Iteration: 10134, Training Loss: 1.259e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 10135, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10136, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10137, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10138, Training Loss: 5.037e-01 , Validation Loss: 1.246e+00\n",
      "Iteration: 10139, Training Loss: 1.259e+00 , Validation Loss: 2.286e+00\n",
      "Iteration: 10140, Training Loss: 5.037e-01 , Validation Loss: 1.536e+00\n",
      "Iteration: 10141, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 10142, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 10143, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10144, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10145, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10146, Training Loss: 5.037e-01 , Validation Loss: 2.583e+00\n",
      "Iteration: 10147, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 10148, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 10149, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10150, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10151, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 10152, Training Loss: 1.511e+00 , Validation Loss: 2.697e+00\n",
      "Iteration: 10153, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10154, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10155, Training Loss: 7.555e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 10156, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10157, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10158, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10159, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 10160, Training Loss: 4.030e+00 , Validation Loss: 4.022e+00\n",
      "Iteration: 10161, Training Loss: 1.158e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 10162, Training Loss: 3.778e+00 , Validation Loss: 4.028e+00\n",
      "Iteration: 10163, Training Loss: 3.778e+00 , Validation Loss: 4.367e+00\n",
      "Iteration: 10164, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 10165, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 10166, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10167, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10168, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10169, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 10170, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10171, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10172, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10173, Training Loss: 3.274e+00 , Validation Loss: 2.873e+00\n",
      "Iteration: 10174, Training Loss: 3.274e+00 , Validation Loss: 2.316e+00\n",
      "Iteration: 10175, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 10176, Training Loss: 1.259e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 10177, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10178, Training Loss: 5.037e-01 , Validation Loss: 2.697e+00\n",
      "Iteration: 10179, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10180, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10181, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10182, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10183, Training Loss: 1.259e+00 , Validation Loss: 2.722e+00\n",
      "Iteration: 10184, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10185, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10186, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 10187, Training Loss: 3.022e+00 , Validation Loss: 3.115e+00\n",
      "Iteration: 10188, Training Loss: 2.770e+00 , Validation Loss: 2.534e+00\n",
      "Iteration: 10189, Training Loss: 1.259e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 10190, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 10191, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 10192, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10193, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10194, Training Loss: 1.259e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 10195, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10196, Training Loss: 1.259e+00 , Validation Loss: 2.522e+00\n",
      "Iteration: 10197, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 10198, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10199, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10200, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10201, Training Loss: 2.518e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 10202, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10203, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10204, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 10205, Training Loss: -1.000e-07 , Validation Loss: 1.137e+00\n",
      "Iteration: 10206, Training Loss: 2.518e-01 , Validation Loss: 1.137e+00\n",
      "Iteration: 10207, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 10208, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 10209, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 10210, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10211, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10212, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 10213, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10214, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 10215, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 10216, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10217, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10218, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10219, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 10220, Training Loss: 2.518e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 10221, Training Loss: 7.555e-01 , Validation Loss: 2.431e+00\n",
      "Iteration: 10222, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10223, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10224, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10225, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10226, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10227, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 10228, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 10229, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 10230, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 10231, Training Loss: 2.518e+00 , Validation Loss: 3.816e+00\n",
      "Iteration: 10232, Training Loss: 1.259e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 10233, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10234, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10235, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10236, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 10237, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10238, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10239, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10240, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10241, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10242, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10243, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10244, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10245, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 10246, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 10247, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10248, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 10249, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10250, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 10251, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 10252, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10253, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10254, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10255, Training Loss: 7.555e-01 , Validation Loss: 1.264e+00\n",
      "Iteration: 10256, Training Loss: 2.267e+00 , Validation Loss: 4.155e+00\n",
      "Iteration: 10257, Training Loss: 1.259e+00 , Validation Loss: 1.270e+00\n",
      "Iteration: 10258, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 10259, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 10260, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10261, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10262, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 10263, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10264, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10265, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10266, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10267, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10268, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10269, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10270, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 10271, Training Loss: 1.007e+00 , Validation Loss: 6.048e-01\n",
      "Iteration: 10272, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10273, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 10274, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10275, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 10276, Training Loss: 1.007e+00 , Validation Loss: 1.155e+00\n",
      "Iteration: 10277, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 10278, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 10279, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10280, Training Loss: 1.259e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 10281, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10282, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10283, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 10284, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10285, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10286, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 10287, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10288, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10289, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10290, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 10291, Training Loss: 5.037e-01 , Validation Loss: 1.572e+00\n",
      "Iteration: 10292, Training Loss: 1.259e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 10293, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10294, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10295, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10296, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10297, Training Loss: 7.555e-01 , Validation Loss: 1.258e+00\n",
      "Iteration: 10298, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 10299, Training Loss: 1.259e+00 , Validation Loss: 1.004e+00\n",
      "Iteration: 10300, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10301, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10302, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10303, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10304, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10305, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 10306, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 10307, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 10308, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10309, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 10310, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 10311, Training Loss: 3.274e+00 , Validation Loss: 4.209e+00\n",
      "Iteration: 10312, Training Loss: 7.555e-01 , Validation Loss: 2.032e+00\n",
      "Iteration: 10313, Training Loss: 1.259e+00 , Validation Loss: 1.343e+00\n",
      "Iteration: 10314, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 10315, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 10316, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 10317, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 10318, Training Loss: 5.037e-01 , Validation Loss: 1.179e+00\n",
      "Iteration: 10319, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10320, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10321, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10322, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 10323, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10324, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10325, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10326, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 10327, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10328, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10329, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 10330, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 10331, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10332, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10333, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10334, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 10335, Training Loss: 1.007e+00 , Validation Loss: 1.893e+00\n",
      "Iteration: 10336, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10337, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10338, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 10339, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10340, Training Loss: 1.259e+00 , Validation Loss: 2.964e+00\n",
      "Iteration: 10341, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 10342, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 10343, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 10344, Training Loss: 2.518e-01 , Validation Loss: 1.760e+00\n",
      "Iteration: 10345, Training Loss: 7.555e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 10346, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10347, Training Loss: 7.555e-01 , Validation Loss: 2.843e+00\n",
      "Iteration: 10348, Training Loss: 2.518e-01 , Validation Loss: 1.415e+00\n",
      "Iteration: 10349, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 10350, Training Loss: 2.267e+00 , Validation Loss: 2.806e+00\n",
      "Iteration: 10351, Training Loss: 1.259e+00 , Validation Loss: 1.004e+00\n",
      "Iteration: 10352, Training Loss: 2.015e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 10353, Training Loss: 2.015e+00 , Validation Loss: 4.125e+00\n",
      "Iteration: 10354, Training Loss: 1.259e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 10355, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10356, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10357, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 10358, Training Loss: 5.037e-01 , Validation Loss: 1.331e+00\n",
      "Iteration: 10359, Training Loss: 5.037e-01 , Validation Loss: 1.978e+00\n",
      "Iteration: 10360, Training Loss: 2.518e-01 , Validation Loss: 2.147e+00\n",
      "Iteration: 10361, Training Loss: 7.555e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 10362, Training Loss: 1.511e+00 , Validation Loss: 2.655e+00\n",
      "Iteration: 10363, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 10364, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 10365, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10366, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10367, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10368, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10369, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10370, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10371, Training Loss: 1.511e+00 , Validation Loss: 1.131e+00\n",
      "Iteration: 10372, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 10373, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10374, Training Loss: 7.555e-01 , Validation Loss: 1.222e+00\n",
      "Iteration: 10375, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 10376, Training Loss: 5.037e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 10377, Training Loss: 2.518e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 10378, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 10379, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10380, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 10381, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10382, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10383, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 10384, Training Loss: -1.000e-07 , Validation Loss: 9.737e-01\n",
      "Iteration: 10385, Training Loss: 7.555e-01 , Validation Loss: 9.737e-01\n",
      "Iteration: 10386, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10387, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10388, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10389, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10390, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10391, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10392, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 10393, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10394, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10395, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10396, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 10397, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10398, Training Loss: 2.518e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 10399, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 10400, Training Loss: 1.259e+00 , Validation Loss: 2.685e+00\n",
      "Iteration: 10401, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10402, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10403, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10404, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10405, Training Loss: 1.259e+00 , Validation Loss: 2.685e+00\n",
      "Iteration: 10406, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 10407, Training Loss: 7.555e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 10408, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 10409, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 10410, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 10411, Training Loss: 3.022e+00 , Validation Loss: 3.659e+00\n",
      "Iteration: 10412, Training Loss: 1.763e+00 , Validation Loss: 1.222e+00\n",
      "Iteration: 10413, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 10414, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 10415, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10416, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 10417, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 10418, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10419, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10420, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 10421, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10422, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10423, Training Loss: 2.518e-01 , Validation Loss: 1.439e+00\n",
      "Iteration: 10424, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10425, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10426, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 10427, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10428, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10429, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10430, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10431, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 10432, Training Loss: -1.000e-07 , Validation Loss: 4.778e-01\n",
      "Iteration: 10433, Training Loss: 1.511e+00 , Validation Loss: 4.778e-01\n",
      "Iteration: 10434, Training Loss: 3.274e+00 , Validation Loss: 5.195e+00\n",
      "Iteration: 10435, Training Loss: 7.555e-01 , Validation Loss: 2.413e+00\n",
      "Iteration: 10436, Training Loss: 2.518e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 10437, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10438, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10439, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10440, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10441, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 10442, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10443, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10444, Training Loss: 2.267e+00 , Validation Loss: 3.623e+00\n",
      "Iteration: 10445, Training Loss: 2.015e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 10446, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10447, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10448, Training Loss: 1.259e+00 , Validation Loss: 2.734e+00\n",
      "Iteration: 10449, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10450, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10451, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 10452, Training Loss: 3.274e+00 , Validation Loss: 4.010e+00\n",
      "Iteration: 10453, Training Loss: 1.511e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 10454, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 10455, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10456, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 10457, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10458, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 10459, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 10460, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10461, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 10462, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 10463, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10464, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10465, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10466, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10467, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10468, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10469, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10470, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10471, Training Loss: 1.259e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 10472, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 10473, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 10474, Training Loss: 1.007e+00 , Validation Loss: 2.117e+00\n",
      "Iteration: 10475, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10476, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10477, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10478, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10479, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10480, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10481, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10482, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 10483, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10484, Training Loss: 7.555e-01 , Validation Loss: 1.385e+00\n",
      "Iteration: 10485, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10486, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10487, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 10488, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10489, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 10490, Training Loss: 1.007e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 10491, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10492, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10493, Training Loss: 1.259e+00 , Validation Loss: 1.693e+00\n",
      "Iteration: 10494, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10495, Training Loss: -1.000e-07 , Validation Loss: 1.258e+00\n",
      "Iteration: 10496, Training Loss: 5.037e-01 , Validation Loss: 1.258e+00\n",
      "Iteration: 10497, Training Loss: 5.037e-01 , Validation Loss: 1.645e+00\n",
      "Iteration: 10498, Training Loss: 7.555e-01 , Validation Loss: 1.681e+00\n",
      "Iteration: 10499, Training Loss: 3.526e+00 , Validation Loss: 5.340e+00\n",
      "Iteration: 10500, Training Loss: 2.015e+00 , Validation Loss: 2.462e+00\n",
      "Iteration: 10501, Training Loss: 7.555e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 10502, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 10503, Training Loss: 1.511e+00 , Validation Loss: 4.778e-01\n",
      "Iteration: 10504, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10505, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10506, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10507, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10508, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10509, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10510, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10511, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10512, Training Loss: 1.007e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 10513, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10514, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10515, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 10516, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 10517, Training Loss: 7.555e-01 , Validation Loss: 3.169e+00\n",
      "Iteration: 10518, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10519, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10520, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10521, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10522, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10523, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10524, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10525, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10526, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 10527, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10528, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10529, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10530, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10531, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10532, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10533, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10534, Training Loss: 2.518e-01 , Validation Loss: 1.500e+00\n",
      "Iteration: 10535, Training Loss: 2.267e+00 , Validation Loss: 4.046e+00\n",
      "Iteration: 10536, Training Loss: 2.015e+00 , Validation Loss: 1.161e+00\n",
      "Iteration: 10537, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10538, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10539, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10540, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10541, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10542, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10543, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10544, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10545, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10546, Training Loss: 7.555e-01 , Validation Loss: 2.226e+00\n",
      "Iteration: 10547, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10548, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10549, Training Loss: 7.555e-01 , Validation Loss: 1.675e+00\n",
      "Iteration: 10550, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10551, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10552, Training Loss: 7.555e-01 , Validation Loss: 1.403e+00\n",
      "Iteration: 10553, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 10554, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10555, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10556, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10557, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10558, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10559, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10560, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10561, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10562, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10563, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10564, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10565, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10566, Training Loss: 7.555e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 10567, Training Loss: -1.000e-07 , Validation Loss: 5.322e-01\n",
      "Iteration: 10568, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 10569, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10570, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10571, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10572, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10573, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10574, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10575, Training Loss: 1.007e+00 , Validation Loss: 2.601e+00\n",
      "Iteration: 10576, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10577, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10578, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10579, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 10580, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 10581, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10582, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10583, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10584, Training Loss: 2.518e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 10585, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10586, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10587, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10588, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10589, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10590, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10591, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 10592, Training Loss: 1.511e+00 , Validation Loss: 3.030e+00\n",
      "Iteration: 10593, Training Loss: 1.763e+00 , Validation Loss: 3.653e+00\n",
      "Iteration: 10594, Training Loss: 1.511e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 10595, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10596, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10597, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10598, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10599, Training Loss: 1.007e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 10600, Training Loss: 1.007e+00 , Validation Loss: 1.464e+00\n",
      "Iteration: 10601, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10602, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10603, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 10604, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 10605, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10606, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10607, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10608, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10609, Training Loss: 1.007e+00 , Validation Loss: 3.199e+00\n",
      "Iteration: 10610, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 10611, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10612, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10613, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 10614, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 10615, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 10616, Training Loss: 5.037e-01 , Validation Loss: 1.349e+00\n",
      "Iteration: 10617, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10618, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 10619, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10620, Training Loss: 2.518e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 10621, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 10622, Training Loss: 7.555e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 10623, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10624, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10625, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10626, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10627, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10628, Training Loss: -1.000e-07 , Validation Loss: 5.020e-01\n",
      "Iteration: 10629, Training Loss: -1.000e-07 , Validation Loss: 5.020e-01\n",
      "Iteration: 10630, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 10631, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10632, Training Loss: 5.037e-01 , Validation Loss: 9.979e-01\n",
      "Iteration: 10633, Training Loss: 5.037e-01 , Validation Loss: 1.058e+00\n",
      "Iteration: 10634, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10635, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10636, Training Loss: 3.274e+00 , Validation Loss: 4.470e+00\n",
      "Iteration: 10637, Training Loss: 4.281e+00 , Validation Loss: 5.008e+00\n",
      "Iteration: 10638, Training Loss: 1.763e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 10639, Training Loss: 4.030e+00 , Validation Loss: 4.790e+00\n",
      "Iteration: 10640, Training Loss: 5.289e+00 , Validation Loss: 6.036e+00\n",
      "Iteration: 10641, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10642, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10643, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10644, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 10645, Training Loss: 1.511e+00 , Validation Loss: 3.133e+00\n",
      "Iteration: 10646, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 10647, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10648, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10649, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10650, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10651, Training Loss: 5.037e-01 , Validation Loss: 3.187e+00\n",
      "Iteration: 10652, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10653, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10654, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10655, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 10656, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10657, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10658, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10659, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10660, Training Loss: 5.037e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 10661, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10662, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 10663, Training Loss: 5.037e-01 , Validation Loss: 1.718e+00\n",
      "Iteration: 10664, Training Loss: 2.518e-01 , Validation Loss: 2.335e+00\n",
      "Iteration: 10665, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 10666, Training Loss: 2.267e+00 , Validation Loss: 3.871e+00\n",
      "Iteration: 10667, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 10668, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 10669, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10670, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 10671, Training Loss: 1.259e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 10672, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10673, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10674, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10675, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10676, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10677, Training Loss: 3.274e+00 , Validation Loss: 4.101e+00\n",
      "Iteration: 10678, Training Loss: 2.267e+00 , Validation Loss: 3.272e+00\n",
      "Iteration: 10679, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10680, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10681, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10682, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10683, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10684, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10685, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10686, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10687, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10688, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10689, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10690, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10691, Training Loss: 7.555e-01 , Validation Loss: 1.572e+00\n",
      "Iteration: 10692, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 10693, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10694, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10695, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10696, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10697, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10698, Training Loss: -1.000e-07 , Validation Loss: 9.919e-01\n",
      "Iteration: 10699, Training Loss: 7.555e-01 , Validation Loss: 9.919e-01\n",
      "Iteration: 10700, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 10701, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10702, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10703, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 10704, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10705, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 10706, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10707, Training Loss: 1.763e+00 , Validation Loss: 3.695e+00\n",
      "Iteration: 10708, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 10709, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 10710, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10711, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10712, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10713, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10714, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10715, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10716, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10717, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10718, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10719, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10720, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 10721, Training Loss: 2.518e-01 , Validation Loss: 2.407e+00\n",
      "Iteration: 10722, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 10723, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 10724, Training Loss: 1.259e+00 , Validation Loss: 9.435e-01\n",
      "Iteration: 10725, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10726, Training Loss: 2.267e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 10727, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10728, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10729, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10730, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10731, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 10732, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 10733, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10734, Training Loss: 7.555e-01 , Validation Loss: 1.149e+00\n",
      "Iteration: 10735, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 10736, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10737, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10738, Training Loss: 7.555e-01 , Validation Loss: 1.312e+00\n",
      "Iteration: 10739, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 10740, Training Loss: 1.007e+00 , Validation Loss: 2.443e+00\n",
      "Iteration: 10741, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10742, Training Loss: 1.259e+00 , Validation Loss: 2.601e+00\n",
      "Iteration: 10743, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10744, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10745, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 10746, Training Loss: 7.555e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 10747, Training Loss: 4.030e+00 , Validation Loss: 5.207e+00\n",
      "Iteration: 10748, Training Loss: 5.541e+00 , Validation Loss: 6.538e+00\n",
      "Iteration: 10749, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10750, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 10751, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10752, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10753, Training Loss: 3.778e+00 , Validation Loss: 4.294e+00\n",
      "Iteration: 10754, Training Loss: 3.778e+00 , Validation Loss: 4.421e+00\n",
      "Iteration: 10755, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 10756, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 10757, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10758, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 10759, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10760, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10761, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10762, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10763, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10764, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10765, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 10766, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10767, Training Loss: 5.037e-01 , Validation Loss: 9.616e-01\n",
      "Iteration: 10768, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10769, Training Loss: 2.015e+00 , Validation Loss: 3.750e+00\n",
      "Iteration: 10770, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 10771, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10772, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10773, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10774, Training Loss: 1.511e+00 , Validation Loss: 2.716e+00\n",
      "Iteration: 10775, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10776, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 10777, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 10778, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 10779, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10780, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10781, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10782, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 10783, Training Loss: 7.555e-01 , Validation Loss: 2.008e+00\n",
      "Iteration: 10784, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10785, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10786, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10787, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10788, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10789, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 10790, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 10791, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10792, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10793, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10794, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10795, Training Loss: 7.555e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 10796, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10797, Training Loss: 2.015e+00 , Validation Loss: 3.750e+00\n",
      "Iteration: 10798, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 10799, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10800, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 10801, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10802, Training Loss: 1.511e+00 , Validation Loss: 4.415e-01\n",
      "Iteration: 10803, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10804, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10805, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10806, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10807, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10808, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 10809, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10810, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10811, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10812, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10813, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 10814, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10815, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10816, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10817, Training Loss: 5.037e-01 , Validation Loss: 7.862e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10818, Training Loss: 1.007e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 10819, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10820, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 10821, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 10822, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 10823, Training Loss: 2.770e+00 , Validation Loss: 4.415e+00\n",
      "Iteration: 10824, Training Loss: 2.015e+00 , Validation Loss: 2.117e+00\n",
      "Iteration: 10825, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 10826, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 10827, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10828, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 10829, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10830, Training Loss: 7.555e-01 , Validation Loss: 2.074e+00\n",
      "Iteration: 10831, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10832, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10833, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 10834, Training Loss: 2.518e-01 , Validation Loss: 2.764e+00\n",
      "Iteration: 10835, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 10836, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10837, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10838, Training Loss: 7.555e-01 , Validation Loss: 1.343e+00\n",
      "Iteration: 10839, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10840, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10841, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 10842, Training Loss: 1.763e+00 , Validation Loss: 3.707e+00\n",
      "Iteration: 10843, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 10844, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10845, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10846, Training Loss: -1.000e-07 , Validation Loss: 8.104e-01\n",
      "Iteration: 10847, Training Loss: 1.007e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 10848, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10849, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10850, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10851, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10852, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10853, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10854, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 10855, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10856, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10857, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10858, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 10859, Training Loss: 1.007e+00 , Validation Loss: 2.183e+00\n",
      "Iteration: 10860, Training Loss: 7.555e-01 , Validation Loss: 1.748e+00\n",
      "Iteration: 10861, Training Loss: 2.015e+00 , Validation Loss: 4.451e+00\n",
      "Iteration: 10862, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 10863, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10864, Training Loss: 2.015e+00 , Validation Loss: 3.798e+00\n",
      "Iteration: 10865, Training Loss: 2.015e+00 , Validation Loss: 6.592e-01\n",
      "Iteration: 10866, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 10867, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10868, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10869, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 10870, Training Loss: 1.259e+00 , Validation Loss: 3.671e+00\n",
      "Iteration: 10871, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 10872, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10873, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10874, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10875, Training Loss: -1.000e-07 , Validation Loss: 8.346e-01\n",
      "Iteration: 10876, Training Loss: 5.037e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 10877, Training Loss: 7.555e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 10878, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 10879, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 10880, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10881, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 10882, Training Loss: 2.015e+00 , Validation Loss: 4.088e+00\n",
      "Iteration: 10883, Training Loss: 5.037e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 10884, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 10885, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10886, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10887, Training Loss: 2.518e-01 , Validation Loss: 2.074e+00\n",
      "Iteration: 10888, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 10889, Training Loss: 4.030e+00 , Validation Loss: 4.113e+00\n",
      "Iteration: 10890, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 10891, Training Loss: 3.526e+00 , Validation Loss: 4.959e+00\n",
      "Iteration: 10892, Training Loss: 2.267e+00 , Validation Loss: 2.087e+00\n",
      "Iteration: 10893, Training Loss: 1.007e+00 , Validation Loss: 6.048e-01\n",
      "Iteration: 10894, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 10895, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10896, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 10897, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 10898, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 10899, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10900, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10901, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10902, Training Loss: 2.518e+00 , Validation Loss: 4.010e+00\n",
      "Iteration: 10903, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 10904, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 10905, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 10906, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 10907, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10908, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10909, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10910, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10911, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10912, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 10913, Training Loss: 2.518e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 10914, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10915, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10916, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10917, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10918, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10919, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 10920, Training Loss: 3.274e+00 , Validation Loss: 4.978e+00\n",
      "Iteration: 10921, Training Loss: 1.007e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 10922, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 10923, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10924, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10925, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10926, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10927, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 10928, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10929, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10930, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10931, Training Loss: 7.555e-01 , Validation Loss: 1.718e+00\n",
      "Iteration: 10932, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10933, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10934, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10935, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10936, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10937, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10938, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10939, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10940, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10941, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10942, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 10943, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 10944, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10945, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 10946, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10947, Training Loss: 1.007e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 10948, Training Loss: 2.015e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10949, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10950, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10951, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10952, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 10953, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10954, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10955, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 10956, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10957, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10958, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10959, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10960, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 10961, Training Loss: 3.022e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 10962, Training Loss: 3.022e+00 , Validation Loss: 4.367e+00\n",
      "Iteration: 10963, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 10964, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 10965, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10966, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10967, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10968, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10969, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 10970, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 10971, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 10972, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10973, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 10974, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10975, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 10976, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10977, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 10978, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 10979, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10980, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10981, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10982, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10983, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10984, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10985, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10986, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 10987, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 10988, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 10989, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 10990, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 10991, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10992, Training Loss: 7.555e-01 , Validation Loss: 1.633e+00\n",
      "Iteration: 10993, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 10994, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10995, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 10996, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 10997, Training Loss: 3.022e+00 , Validation Loss: 4.990e+00\n",
      "Iteration: 10998, Training Loss: 1.259e+00 , Validation Loss: 1.331e+00\n",
      "Iteration: 10999, Training Loss: 1.007e+00 , Validation Loss: 4.838e-01\n",
      "Iteration: 11000, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11001, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11002, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11003, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11004, Training Loss: 2.770e+00 , Validation Loss: 4.306e+00\n",
      "Iteration: 11005, Training Loss: 1.259e+00 , Validation Loss: 1.119e+00\n",
      "Iteration: 11006, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 11007, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 11008, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11009, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11010, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11011, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 11012, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 11013, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 11014, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 11015, Training Loss: 2.518e+00 , Validation Loss: 3.145e+00\n",
      "Iteration: 11016, Training Loss: 1.007e+00 , Validation Loss: 1.476e+00\n",
      "Iteration: 11017, Training Loss: 1.511e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 11018, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11019, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11020, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11021, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 11022, Training Loss: 5.037e-01 , Validation Loss: 1.385e+00\n",
      "Iteration: 11023, Training Loss: 2.518e-01 , Validation Loss: 1.736e+00\n",
      "Iteration: 11024, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 11025, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 11026, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 11027, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 11028, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 11029, Training Loss: 1.007e+00 , Validation Loss: 3.332e+00\n",
      "Iteration: 11030, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11031, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11032, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 11033, Training Loss: 4.030e+00 , Validation Loss: 3.883e+00\n",
      "Iteration: 11034, Training Loss: 4.281e+00 , Validation Loss: 4.463e+00\n",
      "Iteration: 11035, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 11036, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 11037, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11038, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11039, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 11040, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 11041, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11042, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11043, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11044, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11045, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11046, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11047, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 11048, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11049, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 11050, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 11051, Training Loss: 7.555e-01 , Validation Loss: 2.401e+00\n",
      "Iteration: 11052, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 11053, Training Loss: 5.037e-01 , Validation Loss: 1.566e+00\n",
      "Iteration: 11054, Training Loss: 5.037e-01 , Validation Loss: 1.482e+00\n",
      "Iteration: 11055, Training Loss: 1.511e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 11056, Training Loss: 1.259e+00 , Validation Loss: 3.339e+00\n",
      "Iteration: 11057, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11058, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11059, Training Loss: 1.007e+00 , Validation Loss: 1.083e+00\n",
      "Iteration: 11060, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11061, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11062, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11063, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 11064, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 11065, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 11066, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11067, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11068, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 11069, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11070, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11071, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11072, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11073, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11074, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11075, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11076, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11077, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11078, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 11079, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11080, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11081, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 11082, Training Loss: 1.259e+00 , Validation Loss: 1.179e+00\n",
      "Iteration: 11083, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11084, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11085, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11086, Training Loss: 1.511e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 11087, Training Loss: 5.037e-01 , Validation Loss: 1.016e+00\n",
      "Iteration: 11088, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11089, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11090, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11091, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 11092, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 11093, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 11094, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 11095, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 11096, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 11097, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11098, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11099, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11100, Training Loss: 1.007e+00 , Validation Loss: 8.225e-01\n",
      "Iteration: 11101, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11102, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11103, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11104, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11105, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11106, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11107, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11108, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11109, Training Loss: 2.518e-01 , Validation Loss: 1.615e+00\n",
      "Iteration: 11110, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 11111, Training Loss: 2.518e+00 , Validation Loss: 4.494e+00\n",
      "Iteration: 11112, Training Loss: 1.763e+00 , Validation Loss: 2.353e+00\n",
      "Iteration: 11113, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 11114, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11115, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 11116, Training Loss: 7.555e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 11117, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11118, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11119, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11120, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 11121, Training Loss: 1.007e+00 , Validation Loss: 8.649e-01\n",
      "Iteration: 11122, Training Loss: 2.770e+00 , Validation Loss: 4.506e+00\n",
      "Iteration: 11123, Training Loss: 1.763e+00 , Validation Loss: 1.923e+00\n",
      "Iteration: 11124, Training Loss: 7.555e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 11125, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 11126, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 11127, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11128, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11129, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 11130, Training Loss: 7.555e-01 , Validation Loss: 1.125e+00\n",
      "Iteration: 11131, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11132, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11133, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11134, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11135, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11136, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11137, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11138, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11139, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11140, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 11141, Training Loss: -1.000e-07 , Validation Loss: 1.131e+00\n",
      "Iteration: 11142, Training Loss: 1.007e+00 , Validation Loss: 1.131e+00\n",
      "Iteration: 11143, Training Loss: 2.518e-01 , Validation Loss: 1.500e+00\n",
      "Iteration: 11144, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 11145, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 11146, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11147, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11148, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 11149, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 11150, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11151, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11152, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11153, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11154, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 11155, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11156, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 11157, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 11158, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11159, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11160, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 11161, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11162, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 11163, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11164, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 11165, Training Loss: 5.037e-01 , Validation Loss: 2.183e+00\n",
      "Iteration: 11166, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 11167, Training Loss: 5.037e-01 , Validation Loss: 1.046e+00\n",
      "Iteration: 11168, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11169, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11170, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11171, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11172, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 11173, Training Loss: 1.259e+00 , Validation Loss: 1.149e+00\n",
      "Iteration: 11174, Training Loss: 2.518e-01 , Validation Loss: 1.748e+00\n",
      "Iteration: 11175, Training Loss: 1.511e+00 , Validation Loss: 4.167e+00\n",
      "Iteration: 11176, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 11177, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11178, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11179, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 11180, Training Loss: 2.518e-01 , Validation Loss: 1.730e+00\n",
      "Iteration: 11181, Training Loss: 2.518e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 11182, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 11183, Training Loss: 5.037e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 11184, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11185, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11186, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11187, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11188, Training Loss: 2.518e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 11189, Training Loss: 7.555e-01 , Validation Loss: 3.780e+00\n",
      "Iteration: 11190, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11191, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11192, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 11193, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11194, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11195, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 11196, Training Loss: 1.007e+00 , Validation Loss: 2.758e+00\n",
      "Iteration: 11197, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11198, Training Loss: 2.518e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 11199, Training Loss: 1.511e+00 , Validation Loss: 2.359e+00\n",
      "Iteration: 11200, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 11201, Training Loss: 1.259e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 11202, Training Loss: 5.037e-01 , Validation Loss: 1.476e+00\n",
      "Iteration: 11203, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 11204, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11205, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11206, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 11207, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11208, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11209, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11210, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 11211, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 11212, Training Loss: 1.259e+00 , Validation Loss: 1.222e+00\n",
      "Iteration: 11213, Training Loss: 7.555e-01 , Validation Loss: 1.155e+00\n",
      "Iteration: 11214, Training Loss: 4.281e+00 , Validation Loss: 5.528e+00\n",
      "Iteration: 11215, Training Loss: 8.311e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 11216, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11217, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11218, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11219, Training Loss: 5.037e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 11220, Training Loss: 3.022e+00 , Validation Loss: 4.851e+00\n",
      "Iteration: 11221, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 11222, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 11223, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 11224, Training Loss: 2.518e-01 , Validation Loss: 1.204e+00\n",
      "Iteration: 11225, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11226, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11227, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11228, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11229, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11230, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11231, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11232, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11233, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11234, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11235, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11236, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11237, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11238, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11239, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11240, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11241, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11242, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11243, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11244, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11245, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11246, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11247, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11248, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11249, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11250, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11251, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11252, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11253, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11254, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11255, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11256, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11257, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11258, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11259, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11260, Training Loss: 5.037e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 11261, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11262, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11263, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11264, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11265, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11266, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11267, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11268, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11269, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11270, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11271, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 11272, Training Loss: 2.770e+00 , Validation Loss: 4.736e+00\n",
      "Iteration: 11273, Training Loss: 1.259e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 11274, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11275, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11276, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11277, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11278, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 11279, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11280, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11281, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11282, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11283, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 11284, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 11285, Training Loss: 1.259e+00 , Validation Loss: 3.653e+00\n",
      "Iteration: 11286, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 11287, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11288, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11289, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11290, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11291, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11292, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 11293, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11294, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11295, Training Loss: 2.015e+00 , Validation Loss: 2.873e+00\n",
      "Iteration: 11296, Training Loss: 1.511e+00 , Validation Loss: 9.254e-01\n",
      "Iteration: 11297, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 11298, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 11299, Training Loss: 1.259e+00 , Validation Loss: 1.530e+00\n",
      "Iteration: 11300, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11301, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 11302, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 11303, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 11304, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 11305, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11306, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11307, Training Loss: -1.000e-07 , Validation Loss: 7.137e-01\n",
      "Iteration: 11308, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 11309, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 11310, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11311, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11312, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 11313, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11314, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11315, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11316, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11317, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11318, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11319, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11320, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11321, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11322, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 11323, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11324, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11325, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11326, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11327, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11328, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 11329, Training Loss: 2.770e+00 , Validation Loss: 4.173e+00\n",
      "Iteration: 11330, Training Loss: 2.267e+00 , Validation Loss: 1.572e+00\n",
      "Iteration: 11331, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11332, Training Loss: 1.259e+00 , Validation Loss: 6.350e-01\n",
      "Iteration: 11333, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 11334, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11335, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11336, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11337, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11338, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11339, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11340, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11341, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 11342, Training Loss: 1.007e+00 , Validation Loss: 9.556e-01\n",
      "Iteration: 11343, Training Loss: -1.000e-07 , Validation Loss: 1.954e+00\n",
      "Iteration: 11344, Training Loss: 7.555e-01 , Validation Loss: 1.954e+00\n",
      "Iteration: 11345, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11346, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11347, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11348, Training Loss: 7.555e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 11349, Training Loss: 1.007e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 11350, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11351, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11352, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11353, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 11354, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11355, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11356, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11357, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11358, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11359, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11360, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 11361, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 11362, Training Loss: 1.007e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 11363, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11364, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 11365, Training Loss: 2.267e+00 , Validation Loss: 3.774e+00\n",
      "Iteration: 11366, Training Loss: 1.763e+00 , Validation Loss: 1.548e+00\n",
      "Iteration: 11367, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 11368, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 11369, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11370, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11371, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11372, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 11373, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 11374, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 11375, Training Loss: 5.037e-01 , Validation Loss: 1.167e+00\n",
      "Iteration: 11376, Training Loss: 7.555e-01 , Validation Loss: 1.409e+00\n",
      "Iteration: 11377, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11378, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11379, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 11380, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 11381, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 11382, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11383, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11384, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 11385, Training Loss: 1.259e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 11386, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11387, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11388, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11389, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11390, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 11391, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11392, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 11393, Training Loss: 7.555e-01 , Validation Loss: 1.615e+00\n",
      "Iteration: 11394, Training Loss: 1.763e+00 , Validation Loss: 3.078e+00\n",
      "Iteration: 11395, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 11396, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 11397, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11398, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 11399, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11400, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11401, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11402, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11403, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11404, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11405, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11406, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 11407, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 11408, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11409, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 11410, Training Loss: 7.555e-01 , Validation Loss: 1.427e+00\n",
      "Iteration: 11411, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11412, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 11413, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 11414, Training Loss: 5.037e-01 , Validation Loss: 1.210e+00\n",
      "Iteration: 11415, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 11416, Training Loss: 2.518e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 11417, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 11418, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11419, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11420, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11421, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11422, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11423, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11424, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 11425, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 11426, Training Loss: 1.259e+00 , Validation Loss: 2.939e+00\n",
      "Iteration: 11427, Training Loss: 1.511e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 11428, Training Loss: 2.518e-01 , Validation Loss: 9.737e-01\n",
      "Iteration: 11429, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 11430, Training Loss: 7.555e-01 , Validation Loss: 1.318e+00\n",
      "Iteration: 11431, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11432, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11433, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 11434, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 11435, Training Loss: 2.518e-01 , Validation Loss: 3.520e+00\n",
      "Iteration: 11436, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 11437, Training Loss: 2.518e-01 , Validation Loss: 1.306e+00\n",
      "Iteration: 11438, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 11439, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11440, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11441, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11442, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11443, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11444, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 11445, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 11446, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 11447, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 11448, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 11449, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11450, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11451, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11452, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11453, Training Loss: 1.259e+00 , Validation Loss: 9.556e-01\n",
      "Iteration: 11454, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 11455, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11456, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11457, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11458, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11459, Training Loss: 1.511e+00 , Validation Loss: 3.435e+00\n",
      "Iteration: 11460, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 11461, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 11462, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 11463, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11464, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11465, Training Loss: 1.007e+00 , Validation Loss: 1.234e+00\n",
      "Iteration: 11466, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 11467, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 11468, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 11469, Training Loss: 2.518e-01 , Validation Loss: 1.530e+00\n",
      "Iteration: 11470, Training Loss: 1.259e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 11471, Training Loss: 3.526e+00 , Validation Loss: 4.336e+00\n",
      "Iteration: 11472, Training Loss: 2.015e+00 , Validation Loss: 4.016e+00\n",
      "Iteration: 11473, Training Loss: 1.259e+00 , Validation Loss: 1.101e+00\n",
      "Iteration: 11474, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 11475, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11476, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11477, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11478, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11479, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 11480, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11481, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11482, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 11483, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 11484, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11485, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11486, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 11487, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11488, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11489, Training Loss: 5.037e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 11490, Training Loss: 7.555e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 11491, Training Loss: 1.259e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 11492, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 11493, Training Loss: -1.000e-07 , Validation Loss: 1.089e+00\n",
      "Iteration: 11494, Training Loss: 5.037e-01 , Validation Loss: 1.089e+00\n",
      "Iteration: 11495, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 11496, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11497, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11498, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11499, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11500, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 11501, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11502, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11503, Training Loss: 5.037e-01 , Validation Loss: 1.373e+00\n",
      "Iteration: 11504, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 11505, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11506, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 11507, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 11508, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11509, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11510, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11511, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11512, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11513, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11514, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11515, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11516, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 11517, Training Loss: 2.518e-01 , Validation Loss: 1.234e+00\n",
      "Iteration: 11518, Training Loss: 5.037e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 11519, Training Loss: 1.763e+00 , Validation Loss: 4.500e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11520, Training Loss: 1.763e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 11521, Training Loss: 1.259e+00 , Validation Loss: 7.802e-01\n",
      "Iteration: 11522, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 11523, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 11524, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 11525, Training Loss: 7.555e-01 , Validation Loss: 1.899e+00\n",
      "Iteration: 11526, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 11527, Training Loss: 3.022e+00 , Validation Loss: 3.998e+00\n",
      "Iteration: 11528, Training Loss: 1.511e+00 , Validation Loss: 1.923e+00\n",
      "Iteration: 11529, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 11530, Training Loss: 1.259e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 11531, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 11532, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 11533, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11534, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 11535, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 11536, Training Loss: 2.770e+00 , Validation Loss: 3.859e+00\n",
      "Iteration: 11537, Training Loss: 1.259e+00 , Validation Loss: 1.101e+00\n",
      "Iteration: 11538, Training Loss: 1.259e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 11539, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11540, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11541, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11542, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11543, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 11544, Training Loss: 1.259e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 11545, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 11546, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11547, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 11548, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11549, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11550, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11551, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11552, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11553, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 11554, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 11555, Training Loss: -1.000e-07 , Validation Loss: 8.286e-01\n",
      "Iteration: 11556, Training Loss: 5.037e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 11557, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11558, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 11559, Training Loss: 1.007e+00 , Validation Loss: 1.954e+00\n",
      "Iteration: 11560, Training Loss: 5.037e-01 , Validation Loss: 2.335e+00\n",
      "Iteration: 11561, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 11562, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11563, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11564, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11565, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11566, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11567, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11568, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11569, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11570, Training Loss: 2.518e-01 , Validation Loss: 1.101e+00\n",
      "Iteration: 11571, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 11572, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 11573, Training Loss: 4.030e+00 , Validation Loss: 4.070e+00\n",
      "Iteration: 11574, Training Loss: 1.033e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 11575, Training Loss: 2.770e+00 , Validation Loss: 4.209e+00\n",
      "Iteration: 11576, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 11577, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 11578, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 11579, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11580, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11581, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11582, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11583, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11584, Training Loss: 1.259e+00 , Validation Loss: 2.262e+00\n",
      "Iteration: 11585, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 11586, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11587, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11588, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11589, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11590, Training Loss: 7.555e-01 , Validation Loss: 1.137e+00\n",
      "Iteration: 11591, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11592, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11593, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11594, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11595, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11596, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11597, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11598, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11599, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11600, Training Loss: 1.763e+00 , Validation Loss: 1.790e+00\n",
      "Iteration: 11601, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 11602, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11603, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11604, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 11605, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 11606, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11607, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11608, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 11609, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11610, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11611, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11612, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11613, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11614, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11615, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11616, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 11617, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11618, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 11619, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 11620, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 11621, Training Loss: 4.785e+00 , Validation Loss: 4.512e+00\n",
      "Iteration: 11622, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 11623, Training Loss: 1.007e+00 , Validation Loss: 1.452e+00\n",
      "Iteration: 11624, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11625, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11626, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11627, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 11628, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 11629, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 11630, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11631, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11632, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11633, Training Loss: 1.007e+00 , Validation Loss: 1.869e+00\n",
      "Iteration: 11634, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11635, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11636, Training Loss: 1.007e+00 , Validation Loss: 1.736e+00\n",
      "Iteration: 11637, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11638, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11639, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11640, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11641, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11642, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11643, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 11644, Training Loss: 2.770e+00 , Validation Loss: 3.871e+00\n",
      "Iteration: 11645, Training Loss: 3.778e+00 , Validation Loss: 3.659e+00\n",
      "Iteration: 11646, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11647, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11648, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11649, Training Loss: -1.000e-07 , Validation Loss: 8.044e-01\n",
      "Iteration: 11650, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 11651, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11652, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11653, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11654, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11655, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 11656, Training Loss: 1.511e+00 , Validation Loss: 3.097e+00\n",
      "Iteration: 11657, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 11658, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 11659, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 11660, Training Loss: 7.555e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 11661, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11662, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11663, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11664, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11665, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11666, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11667, Training Loss: 2.518e+00 , Validation Loss: 3.828e+00\n",
      "Iteration: 11668, Training Loss: 7.555e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 11669, Training Loss: 1.007e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 11670, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11671, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11672, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11673, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11674, Training Loss: 5.037e-01 , Validation Loss: 2.274e+00\n",
      "Iteration: 11675, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11676, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11677, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11678, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11679, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11680, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 11681, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11682, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11683, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 11684, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11685, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 11686, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 11687, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11688, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11689, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 11690, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 11691, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 11692, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11693, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 11694, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11695, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11696, Training Loss: 5.037e-01 , Validation Loss: 1.155e+00\n",
      "Iteration: 11697, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11698, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 11699, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 11700, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11701, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 11702, Training Loss: 3.274e+00 , Validation Loss: 5.129e+00\n",
      "Iteration: 11703, Training Loss: 1.007e+00 , Validation Loss: 2.710e+00\n",
      "Iteration: 11704, Training Loss: 1.007e+00 , Validation Loss: 1.101e+00\n",
      "Iteration: 11705, Training Loss: 1.007e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 11706, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 11707, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11708, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11709, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11710, Training Loss: 1.259e+00 , Validation Loss: 2.625e+00\n",
      "Iteration: 11711, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11712, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11713, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11714, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11715, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11716, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11717, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 11718, Training Loss: 7.555e-01 , Validation Loss: 1.766e+00\n",
      "Iteration: 11719, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11720, Training Loss: 1.007e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 11721, Training Loss: 7.555e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 11722, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11723, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11724, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11725, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11726, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11727, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11728, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11729, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 11730, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 11731, Training Loss: 2.518e+00 , Validation Loss: 3.587e+00\n",
      "Iteration: 11732, Training Loss: 7.555e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 11733, Training Loss: 1.007e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 11734, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11735, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11736, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11737, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11738, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11739, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11740, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11741, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11742, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 11743, Training Loss: 2.267e+00 , Validation Loss: 2.824e+00\n",
      "Iteration: 11744, Training Loss: 5.037e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 11745, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 11746, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11747, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11748, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11749, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 11750, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11751, Training Loss: -1.000e-07 , Validation Loss: 8.225e-01\n",
      "Iteration: 11752, Training Loss: 7.555e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 11753, Training Loss: 1.007e+00 , Validation Loss: 7.802e-01\n",
      "Iteration: 11754, Training Loss: 1.007e+00 , Validation Loss: 1.512e+00\n",
      "Iteration: 11755, Training Loss: 5.037e-01 , Validation Loss: 2.074e+00\n",
      "Iteration: 11756, Training Loss: 1.511e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 11757, Training Loss: 7.555e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 11758, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 11759, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11760, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11761, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11762, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11763, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11764, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 11765, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 11766, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 11767, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 11768, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 11769, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 11770, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11771, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11772, Training Loss: 5.037e-01 , Validation Loss: 1.228e+00\n",
      "Iteration: 11773, Training Loss: 5.037e-01 , Validation Loss: 1.863e+00\n",
      "Iteration: 11774, Training Loss: 2.518e+00 , Validation Loss: 3.750e+00\n",
      "Iteration: 11775, Training Loss: 1.763e+00 , Validation Loss: 2.746e+00\n",
      "Iteration: 11776, Training Loss: -1.000e-07 , Validation Loss: 8.104e-01\n",
      "Iteration: 11777, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 11778, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 11779, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11780, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 11781, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11782, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 11783, Training Loss: 7.555e-01 , Validation Loss: 1.802e+00\n",
      "Iteration: 11784, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11785, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11786, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11787, Training Loss: 1.763e+00 , Validation Loss: 3.514e+00\n",
      "Iteration: 11788, Training Loss: 1.763e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 11789, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11790, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11791, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11792, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 11793, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11794, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11795, Training Loss: 5.037e-01 , Validation Loss: 8.407e-01\n",
      "Iteration: 11796, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11797, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11798, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11799, Training Loss: -1.000e-07 , Validation Loss: 1.276e+00\n",
      "Iteration: 11800, Training Loss: 2.518e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 11801, Training Loss: 2.518e+00 , Validation Loss: 3.943e+00\n",
      "Iteration: 11802, Training Loss: 2.015e+00 , Validation Loss: 1.427e+00\n",
      "Iteration: 11803, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 11804, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 11805, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11806, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 11807, Training Loss: 1.007e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 11808, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11809, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11810, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11811, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 11812, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 11813, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11814, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 11815, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 11816, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 11817, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11818, Training Loss: 2.518e-01 , Validation Loss: 1.004e+00\n",
      "Iteration: 11819, Training Loss: 3.022e+00 , Validation Loss: 3.974e+00\n",
      "Iteration: 11820, Training Loss: 3.022e+00 , Validation Loss: 1.947e+00\n",
      "Iteration: 11821, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 11822, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11823, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11824, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11825, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11826, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11827, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11828, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11829, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 11830, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11831, Training Loss: 7.555e-01 , Validation Loss: 2.129e+00\n",
      "Iteration: 11832, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 11833, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11834, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11835, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 11836, Training Loss: 3.022e+00 , Validation Loss: 3.671e+00\n",
      "Iteration: 11837, Training Loss: 2.015e+00 , Validation Loss: 2.117e+00\n",
      "Iteration: 11838, Training Loss: 1.007e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 11839, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11840, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11841, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 11842, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 11843, Training Loss: 7.555e-01 , Validation Loss: 3.205e+00\n",
      "Iteration: 11844, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 11845, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11846, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11847, Training Loss: 7.555e-01 , Validation Loss: 2.449e+00\n",
      "Iteration: 11848, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11849, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 11850, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11851, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11852, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11853, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11854, Training Loss: 7.555e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 11855, Training Loss: 2.267e+00 , Validation Loss: 3.351e+00\n",
      "Iteration: 11856, Training Loss: 1.511e+00 , Validation Loss: 8.588e-01\n",
      "Iteration: 11857, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 11858, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11859, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11860, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 11861, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 11862, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11863, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 11864, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 11865, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 11866, Training Loss: 2.770e+00 , Validation Loss: 3.998e+00\n",
      "Iteration: 11867, Training Loss: 2.267e+00 , Validation Loss: 1.929e+00\n",
      "Iteration: 11868, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 11869, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 11870, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 11871, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 11872, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11873, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 11874, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 11875, Training Loss: 5.037e-01 , Validation Loss: 1.627e+00\n",
      "Iteration: 11876, Training Loss: 5.037e-01 , Validation Loss: 2.347e+00\n",
      "Iteration: 11877, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11878, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11879, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 11880, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11881, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11882, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11883, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11884, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11885, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11886, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11887, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 11888, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 11889, Training Loss: 2.518e-01 , Validation Loss: 1.482e+00\n",
      "Iteration: 11890, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 11891, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 11892, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 11893, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11894, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11895, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11896, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11897, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11898, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11899, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 11900, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 11901, Training Loss: 5.037e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 11902, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 11903, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11904, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11905, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11906, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11907, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 11908, Training Loss: 1.763e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 11909, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 11910, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 11911, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 11912, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11913, Training Loss: -1.000e-07 , Validation Loss: 6.592e-01\n",
      "Iteration: 11914, Training Loss: 1.007e+00 , Validation Loss: 6.592e-01\n",
      "Iteration: 11915, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 11916, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 11917, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11918, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 11919, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11920, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 11921, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 11922, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11923, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 11924, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11925, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11926, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11927, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11928, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 11929, Training Loss: 3.022e+00 , Validation Loss: 4.173e+00\n",
      "Iteration: 11930, Training Loss: 2.770e+00 , Validation Loss: 2.837e+00\n",
      "Iteration: 11931, Training Loss: 7.555e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 11932, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 11933, Training Loss: 2.518e+00 , Validation Loss: 4.216e+00\n",
      "Iteration: 11934, Training Loss: 7.555e-01 , Validation Loss: 2.147e+00\n",
      "Iteration: 11935, Training Loss: 2.015e+00 , Validation Loss: 1.210e+00\n",
      "Iteration: 11936, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11937, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11938, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11939, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 11940, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 11941, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 11942, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11943, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 11944, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11945, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11946, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11947, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11948, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11949, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 11950, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 11951, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 11952, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 11953, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11954, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11955, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11956, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11957, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11958, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 11959, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 11960, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 11961, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11962, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 11963, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11964, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11965, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 11966, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 11967, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 11968, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 11969, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 11970, Training Loss: 5.037e-01 , Validation Loss: 1.839e+00\n",
      "Iteration: 11971, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11972, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11973, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11974, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11975, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11976, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11977, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 11978, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11979, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 11980, Training Loss: 3.022e+00 , Validation Loss: 4.191e+00\n",
      "Iteration: 11981, Training Loss: 2.770e+00 , Validation Loss: 2.002e+00\n",
      "Iteration: 11982, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11983, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11984, Training Loss: 1.511e+00 , Validation Loss: 1.288e+00\n",
      "Iteration: 11985, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11986, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 11987, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 11988, Training Loss: 1.007e+00 , Validation Loss: 1.960e+00\n",
      "Iteration: 11989, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 11990, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11991, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11992, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11993, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 11994, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 11995, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 11996, Training Loss: -1.000e-07 , Validation Loss: 6.895e-01\n",
      "Iteration: 11997, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 11998, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 11999, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 12000, Training Loss: 1.511e+00 , Validation Loss: 2.238e+00\n",
      "Iteration: 12001, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12002, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12003, Training Loss: 1.259e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 12004, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 12005, Training Loss: 7.555e-01 , Validation Loss: 1.312e+00\n",
      "Iteration: 12006, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 12007, Training Loss: 1.763e+00 , Validation Loss: 3.726e+00\n",
      "Iteration: 12008, Training Loss: 1.259e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 12009, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 12010, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12011, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12012, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 12013, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12014, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12015, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12016, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12017, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12018, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 12019, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 12020, Training Loss: 2.518e-01 , Validation Loss: 1.155e+00\n",
      "Iteration: 12021, Training Loss: 2.015e+00 , Validation Loss: 3.931e+00\n",
      "Iteration: 12022, Training Loss: 2.015e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 12023, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12024, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12025, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12026, Training Loss: 3.274e+00 , Validation Loss: 4.088e+00\n",
      "Iteration: 12027, Training Loss: 2.015e+00 , Validation Loss: 2.552e+00\n",
      "Iteration: 12028, Training Loss: 1.259e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 12029, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 12030, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 12031, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12032, Training Loss: 7.555e-01 , Validation Loss: 2.443e+00\n",
      "Iteration: 12033, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12034, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 12035, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 12036, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 12037, Training Loss: 2.518e-01 , Validation Loss: 1.784e+00\n",
      "Iteration: 12038, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 12039, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 12040, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12041, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 12042, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 12043, Training Loss: 2.518e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 12044, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 12045, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 12046, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 12047, Training Loss: 1.511e+00 , Validation Loss: 3.139e+00\n",
      "Iteration: 12048, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12049, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 12050, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12051, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12052, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 12053, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12054, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12055, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12056, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 12057, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 12058, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12059, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 12060, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12061, Training Loss: 7.555e-01 , Validation Loss: 2.099e+00\n",
      "Iteration: 12062, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 12063, Training Loss: 1.007e+00 , Validation Loss: 2.395e+00\n",
      "Iteration: 12064, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12065, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12066, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12067, Training Loss: 1.763e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 12068, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12069, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 12070, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12071, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12072, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12073, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12074, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12075, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 12076, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 12077, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 12078, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 12079, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 12080, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12081, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 12082, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12083, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12084, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 12085, Training Loss: 1.259e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 12086, Training Loss: 3.274e+00 , Validation Loss: 5.159e+00\n",
      "Iteration: 12087, Training Loss: 1.763e+00 , Validation Loss: 1.137e+00\n",
      "Iteration: 12088, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 12089, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 12090, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12091, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12092, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12093, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12094, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12095, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12096, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 12097, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12098, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12099, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12100, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12101, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12102, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12103, Training Loss: 2.518e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 12104, Training Loss: 7.555e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 12105, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 12106, Training Loss: 1.007e+00 , Validation Loss: 2.232e+00\n",
      "Iteration: 12107, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12108, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 12109, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12110, Training Loss: 7.555e-01 , Validation Loss: 2.032e+00\n",
      "Iteration: 12111, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 12112, Training Loss: 4.533e+00 , Validation Loss: 4.385e+00\n",
      "Iteration: 12113, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 12114, Training Loss: 3.274e+00 , Validation Loss: 4.482e+00\n",
      "Iteration: 12115, Training Loss: 1.511e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 12116, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 12117, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12118, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 12119, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12120, Training Loss: 1.259e+00 , Validation Loss: 2.595e+00\n",
      "Iteration: 12121, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12122, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12123, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12124, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12125, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12126, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12127, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12128, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12129, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12130, Training Loss: 2.015e+00 , Validation Loss: 3.580e+00\n",
      "Iteration: 12131, Training Loss: 1.007e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 12132, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 12133, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12134, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12135, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12136, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12137, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12138, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12139, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12140, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12141, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 12142, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12143, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12144, Training Loss: 7.555e-01 , Validation Loss: 1.470e+00\n",
      "Iteration: 12145, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 12146, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12147, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12148, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 12149, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12150, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12151, Training Loss: 1.007e+00 , Validation Loss: 1.071e+00\n",
      "Iteration: 12152, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 12153, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 12154, Training Loss: 1.007e+00 , Validation Loss: 3.532e+00\n",
      "Iteration: 12155, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12156, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12157, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12158, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12159, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12160, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12161, Training Loss: 7.555e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 12162, Training Loss: 2.770e+00 , Validation Loss: 3.955e+00\n",
      "Iteration: 12163, Training Loss: 1.259e+00 , Validation Loss: 8.951e-01\n",
      "Iteration: 12164, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12165, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12166, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12167, Training Loss: 5.037e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 12168, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12169, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12170, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12171, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12172, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12173, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12174, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12175, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12176, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 12177, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12178, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12179, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12180, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12181, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12182, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12183, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12184, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12185, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12186, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12187, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12188, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12189, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12190, Training Loss: 2.015e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 12191, Training Loss: 1.259e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 12192, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 12193, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12194, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12195, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12196, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12197, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12198, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12199, Training Loss: 1.763e+00 , Validation Loss: 3.629e+00\n",
      "Iteration: 12200, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12201, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 12202, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12203, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 12204, Training Loss: -1.000e-07 , Validation Loss: 2.244e+00\n",
      "Iteration: 12205, Training Loss: 1.763e+00 , Validation Loss: 2.244e+00\n",
      "Iteration: 12206, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12207, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 12208, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12209, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12210, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12211, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12212, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12213, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12214, Training Loss: 5.037e-01 , Validation Loss: 2.552e+00\n",
      "Iteration: 12215, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12216, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 12217, Training Loss: 1.511e+00 , Validation Loss: 3.617e+00\n",
      "Iteration: 12218, Training Loss: 1.763e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 12219, Training Loss: 2.267e+00 , Validation Loss: 3.931e+00\n",
      "Iteration: 12220, Training Loss: 7.555e-01 , Validation Loss: 1.149e+00\n",
      "Iteration: 12221, Training Loss: 1.511e+00 , Validation Loss: 6.592e-01\n",
      "Iteration: 12222, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12223, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12224, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12225, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12226, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12227, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12228, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12229, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12230, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12231, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12232, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12233, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12234, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12235, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 12236, Training Loss: 7.555e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 12237, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 12238, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12239, Training Loss: 2.518e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 12240, Training Loss: 7.555e-01 , Validation Loss: 1.064e+00\n",
      "Iteration: 12241, Training Loss: 2.518e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 12242, Training Loss: 1.259e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 12243, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12244, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12245, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12246, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12247, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12248, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12249, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12250, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 12251, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12252, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12253, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12254, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12255, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12256, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12257, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12258, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12259, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12260, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12261, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12262, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12263, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12264, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12265, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12266, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12267, Training Loss: 1.511e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 12268, Training Loss: 1.511e+00 , Validation Loss: 3.272e+00\n",
      "Iteration: 12269, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12270, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12271, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12272, Training Loss: 1.007e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 12273, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 12274, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12275, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 12276, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 12277, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 12278, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12279, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12280, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12281, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12282, Training Loss: 2.518e-01 , Validation Loss: 1.802e+00\n",
      "Iteration: 12283, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 12284, Training Loss: 5.037e-01 , Validation Loss: 1.101e+00\n",
      "Iteration: 12285, Training Loss: 5.037e-01 , Validation Loss: 1.476e+00\n",
      "Iteration: 12286, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12287, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12288, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12289, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12290, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12291, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 12292, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 12293, Training Loss: 5.037e-01 , Validation Loss: 2.474e+00\n",
      "Iteration: 12294, Training Loss: 1.511e+00 , Validation Loss: 3.508e+00\n",
      "Iteration: 12295, Training Loss: 1.007e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 12296, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 12297, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 12298, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12299, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12300, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12301, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12302, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12303, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12304, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12305, Training Loss: 2.518e-01 , Validation Loss: 1.216e+00\n",
      "Iteration: 12306, Training Loss: 1.007e+00 , Validation Loss: 3.447e+00\n",
      "Iteration: 12307, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 12308, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12309, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12310, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12311, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12312, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12313, Training Loss: 1.259e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 12314, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 12315, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 12316, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12317, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12318, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12319, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 12320, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 12321, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12322, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 12323, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 12324, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 12325, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12326, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12327, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 12328, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 12329, Training Loss: 7.555e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 12330, Training Loss: 2.267e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 12331, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 12332, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 12333, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12334, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 12335, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 12336, Training Loss: -1.000e-07 , Validation Loss: 1.603e+00\n",
      "Iteration: 12337, Training Loss: 1.763e+00 , Validation Loss: 1.603e+00\n",
      "Iteration: 12338, Training Loss: 1.259e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 12339, Training Loss: 1.259e+00 , Validation Loss: 2.776e+00\n",
      "Iteration: 12340, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 12341, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12342, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12343, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 12344, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12345, Training Loss: 7.555e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 12346, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12347, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 12348, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12349, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12350, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12351, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 12352, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 12353, Training Loss: 2.015e+00 , Validation Loss: 4.191e+00\n",
      "Iteration: 12354, Training Loss: 1.259e+00 , Validation Loss: 9.435e-01\n",
      "Iteration: 12355, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12356, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12357, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 12358, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 12359, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12360, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 12361, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 12362, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 12363, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 12364, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 12365, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12366, Training Loss: 2.518e-01 , Validation Loss: 9.133e-01\n",
      "Iteration: 12367, Training Loss: 1.007e+00 , Validation Loss: 3.272e+00\n",
      "Iteration: 12368, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 12369, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12370, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12371, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12372, Training Loss: 7.555e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 12373, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12374, Training Loss: 1.511e+00 , Validation Loss: 3.466e+00\n",
      "Iteration: 12375, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 12376, Training Loss: -1.000e-07 , Validation Loss: 4.597e-01\n",
      "Iteration: 12377, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 12378, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12379, Training Loss: 4.281e+00 , Validation Loss: 4.107e+00\n",
      "Iteration: 12380, Training Loss: 9.822e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 12381, Training Loss: 1.511e+00 , Validation Loss: 3.157e+00\n",
      "Iteration: 12382, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12383, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12384, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 12385, Training Loss: 2.267e+00 , Validation Loss: 2.970e+00\n",
      "Iteration: 12386, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 12387, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12388, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12389, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12390, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12391, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12392, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12393, Training Loss: 1.511e+00 , Validation Loss: 3.139e+00\n",
      "Iteration: 12394, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12395, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12396, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12397, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12398, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12399, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12400, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12401, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12402, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12403, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12404, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12405, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12406, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 12407, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 12408, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 12409, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12410, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 12411, Training Loss: 2.015e+00 , Validation Loss: 3.103e+00\n",
      "Iteration: 12412, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 12413, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12414, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 12415, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 12416, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12417, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12418, Training Loss: 1.259e+00 , Validation Loss: 1.929e+00\n",
      "Iteration: 12419, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12420, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12421, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12422, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12423, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12424, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12425, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 12426, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12427, Training Loss: 1.007e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 12428, Training Loss: 7.555e-01 , Validation Loss: 1.143e+00\n",
      "Iteration: 12429, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 12430, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12431, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 12432, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12433, Training Loss: 7.555e-01 , Validation Loss: 2.927e+00\n",
      "Iteration: 12434, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12435, Training Loss: 1.259e+00 , Validation Loss: 1.210e+00\n",
      "Iteration: 12436, Training Loss: 1.007e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 12437, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12438, Training Loss: -1.000e-07 , Validation Loss: 4.597e-01\n",
      "Iteration: 12439, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 12440, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 12441, Training Loss: 7.555e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 12442, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12443, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12444, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12445, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 12446, Training Loss: 2.770e+00 , Validation Loss: 4.209e+00\n",
      "Iteration: 12447, Training Loss: 1.007e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 12448, Training Loss: 1.007e+00 , Validation Loss: 1.839e+00\n",
      "Iteration: 12449, Training Loss: 1.007e+00 , Validation Loss: 9.133e-01\n",
      "Iteration: 12450, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12451, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12452, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12453, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12454, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12455, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12456, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 12457, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 12458, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 12459, Training Loss: 1.007e+00 , Validation Loss: 1.040e+00\n",
      "Iteration: 12460, Training Loss: 1.007e+00 , Validation Loss: 1.627e+00\n",
      "Iteration: 12461, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12462, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12463, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 12464, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12465, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12466, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 12467, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12468, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12469, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 12470, Training Loss: 1.007e+00 , Validation Loss: 2.208e+00\n",
      "Iteration: 12471, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12472, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12473, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 12474, Training Loss: 2.015e+00 , Validation Loss: 2.740e+00\n",
      "Iteration: 12475, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 12476, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12477, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12478, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 12479, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12480, Training Loss: 1.007e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 12481, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12482, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12483, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 12484, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12485, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12486, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12487, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12488, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12489, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 12490, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 12491, Training Loss: 5.037e-01 , Validation Loss: 1.337e+00\n",
      "Iteration: 12492, Training Loss: 5.037e-01 , Validation Loss: 2.105e+00\n",
      "Iteration: 12493, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 12494, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12495, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 12496, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 12497, Training Loss: 3.526e+00 , Validation Loss: 5.056e+00\n",
      "Iteration: 12498, Training Loss: 6.296e+00 , Validation Loss: 5.873e+00\n",
      "Iteration: 12499, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12500, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12501, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12502, Training Loss: -1.000e-07 , Validation Loss: 6.532e-01\n",
      "Iteration: 12503, Training Loss: 7.555e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 12504, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 12505, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 12506, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12507, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12508, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12509, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 12510, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12511, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12512, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12513, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12514, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12515, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12516, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12517, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12518, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12519, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12520, Training Loss: 1.763e+00 , Validation Loss: 3.187e+00\n",
      "Iteration: 12521, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 12522, Training Loss: 1.007e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 12523, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12524, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12525, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12526, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12527, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12528, Training Loss: 1.259e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 12529, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12530, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12531, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 12532, Training Loss: 5.037e-01 , Validation Loss: 3.024e+00\n",
      "Iteration: 12533, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12534, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12535, Training Loss: 5.037e-01 , Validation Loss: 2.201e+00\n",
      "Iteration: 12536, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12537, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12538, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12539, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12540, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12541, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12542, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12543, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12544, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12545, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 12546, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 12547, Training Loss: -1.000e-07 , Validation Loss: 1.064e+00\n",
      "Iteration: 12548, Training Loss: 1.007e+00 , Validation Loss: 1.064e+00\n",
      "Iteration: 12549, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12550, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12551, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 12552, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 12553, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12554, Training Loss: 1.007e+00 , Validation Loss: 2.619e+00\n",
      "Iteration: 12555, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12556, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12557, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12558, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12559, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12560, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12561, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 12562, Training Loss: 1.007e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 12563, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12564, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12565, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12566, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12567, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 12568, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 12569, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12570, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12571, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 12572, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 12573, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 12574, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12575, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12576, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12577, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12578, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12579, Training Loss: 1.007e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 12580, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 12581, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 12582, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 12583, Training Loss: 2.518e+00 , Validation Loss: 3.762e+00\n",
      "Iteration: 12584, Training Loss: 1.511e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 12585, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12586, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12587, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 12588, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12589, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12590, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12591, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 12592, Training Loss: 5.037e-01 , Validation Loss: 1.397e+00\n",
      "Iteration: 12593, Training Loss: 2.518e-01 , Validation Loss: 2.081e+00\n",
      "Iteration: 12594, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 12595, Training Loss: 7.555e-01 , Validation Loss: 3.423e+00\n",
      "Iteration: 12596, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12597, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12598, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12599, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12600, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 12601, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12602, Training Loss: 1.511e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 12603, Training Loss: 2.518e+00 , Validation Loss: 4.022e+00\n",
      "Iteration: 12604, Training Loss: 1.763e+00 , Validation Loss: 2.026e+00\n",
      "Iteration: 12605, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 12606, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 12607, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12608, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 12609, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 12610, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 12611, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 12612, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 12613, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 12614, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12615, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12616, Training Loss: 7.555e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 12617, Training Loss: 3.274e+00 , Validation Loss: 5.371e+00\n",
      "Iteration: 12618, Training Loss: 2.015e+00 , Validation Loss: 1.972e+00\n",
      "Iteration: 12619, Training Loss: 1.007e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 12620, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 12621, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 12622, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 12623, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 12624, Training Loss: 1.007e+00 , Validation Loss: 1.421e+00\n",
      "Iteration: 12625, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12626, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 12627, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12628, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12629, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12630, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12631, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12632, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12633, Training Loss: 5.037e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 12634, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12635, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 12636, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12637, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12638, Training Loss: 7.555e-01 , Validation Loss: 1.143e+00\n",
      "Iteration: 12639, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12640, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12641, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 12642, Training Loss: -1.000e-07 , Validation Loss: 8.830e-01\n",
      "Iteration: 12643, Training Loss: 5.037e-01 , Validation Loss: 8.830e-01\n",
      "Iteration: 12644, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12645, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12646, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12647, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12648, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12649, Training Loss: 5.037e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 12650, Training Loss: 1.007e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 12651, Training Loss: 5.037e-01 , Validation Loss: 1.524e+00\n",
      "Iteration: 12652, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 12653, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 12654, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12655, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 12656, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 12657, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12658, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12659, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 12660, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12661, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 12662, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12663, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 12664, Training Loss: 4.533e+00 , Validation Loss: 4.869e+00\n",
      "Iteration: 12665, Training Loss: 1.058e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 12666, Training Loss: 3.526e+00 , Validation Loss: 5.147e+00\n",
      "Iteration: 12667, Training Loss: 2.015e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 12668, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 12669, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 12670, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12671, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12672, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12673, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12674, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12675, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 12676, Training Loss: 1.511e+00 , Validation Loss: 3.387e+00\n",
      "Iteration: 12677, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 12678, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12679, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12680, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12681, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 12682, Training Loss: 2.267e+00 , Validation Loss: 3.302e+00\n",
      "Iteration: 12683, Training Loss: 1.259e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 12684, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12685, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12686, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12687, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12688, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12689, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 12690, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12691, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12692, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 12693, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12694, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12695, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12696, Training Loss: -1.000e-07 , Validation Loss: 1.566e+00\n",
      "Iteration: 12697, Training Loss: 5.037e-01 , Validation Loss: 1.566e+00\n",
      "Iteration: 12698, Training Loss: 1.007e+00 , Validation Loss: 2.734e+00\n",
      "Iteration: 12699, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12700, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12701, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12702, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12703, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12704, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 12705, Training Loss: 1.007e+00 , Validation Loss: 2.462e+00\n",
      "Iteration: 12706, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12707, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12708, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 12709, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12710, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12711, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12712, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12713, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12714, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12715, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12716, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12717, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12718, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12719, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 12720, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12721, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 12722, Training Loss: 2.518e-01 , Validation Loss: 1.827e+00\n",
      "Iteration: 12723, Training Loss: 7.555e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 12724, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 12725, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 12726, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12727, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12728, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12729, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 12730, Training Loss: 7.555e-01 , Validation Loss: 1.560e+00\n",
      "Iteration: 12731, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12732, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12733, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12734, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12735, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12736, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12737, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 12738, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 12739, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12740, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12741, Training Loss: 1.007e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 12742, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 12743, Training Loss: 1.763e+00 , Validation Loss: 3.714e+00\n",
      "Iteration: 12744, Training Loss: 1.259e+00 , Validation Loss: 6.350e-01\n",
      "Iteration: 12745, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 12746, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 12747, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12748, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12749, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12750, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12751, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12752, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 12753, Training Loss: 7.555e-01 , Validation Loss: 1.700e+00\n",
      "Iteration: 12754, Training Loss: 2.770e+00 , Validation Loss: 4.391e+00\n",
      "Iteration: 12755, Training Loss: 1.259e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 12756, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 12757, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 12758, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12759, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12760, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12761, Training Loss: 2.518e-01 , Validation Loss: 1.343e+00\n",
      "Iteration: 12762, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 12763, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12764, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12765, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 12766, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12767, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12768, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 12769, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 12770, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12771, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12772, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12773, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 12774, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12775, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12776, Training Loss: 1.763e+00 , Validation Loss: 4.312e+00\n",
      "Iteration: 12777, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12778, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12779, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12780, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12781, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 12782, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 12783, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 12784, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12785, Training Loss: 1.007e+00 , Validation Loss: 2.081e+00\n",
      "Iteration: 12786, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12787, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12788, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12789, Training Loss: 1.007e+00 , Validation Loss: 1.058e+00\n",
      "Iteration: 12790, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12791, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12792, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12793, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12794, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12795, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12796, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12797, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12798, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12799, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12800, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12801, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12802, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12803, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12804, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 12805, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 12806, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12807, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12808, Training Loss: 5.037e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 12809, Training Loss: 2.015e+00 , Validation Loss: 4.101e+00\n",
      "Iteration: 12810, Training Loss: 1.007e+00 , Validation Loss: 9.435e-01\n",
      "Iteration: 12811, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 12812, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12813, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 12814, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12815, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12816, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12817, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12818, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 12819, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 12820, Training Loss: 1.511e+00 , Validation Loss: 3.659e+00\n",
      "Iteration: 12821, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 12822, Training Loss: 2.518e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 12823, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 12824, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12825, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 12826, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 12827, Training Loss: 1.511e+00 , Validation Loss: 9.677e-01\n",
      "Iteration: 12828, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12829, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 12830, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 12831, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 12832, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12833, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12834, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12835, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12836, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12837, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12838, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12839, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12840, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 12841, Training Loss: 5.037e-01 , Validation Loss: 1.712e+00\n",
      "Iteration: 12842, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 12843, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 12844, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12845, Training Loss: -1.000e-07 , Validation Loss: 4.597e-01\n",
      "Iteration: 12846, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 12847, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12848, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12849, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12850, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12851, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12852, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12853, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 12854, Training Loss: 7.555e-01 , Validation Loss: 1.355e+00\n",
      "Iteration: 12855, Training Loss: 5.037e-01 , Validation Loss: 1.107e+00\n",
      "Iteration: 12856, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12857, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 12858, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 12859, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 12860, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 12861, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 12862, Training Loss: 2.518e-01 , Validation Loss: 1.137e+00\n",
      "Iteration: 12863, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 12864, Training Loss: -1.000e-07 , Validation Loss: 7.318e-01\n",
      "Iteration: 12865, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 12866, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 12867, Training Loss: 2.518e-01 , Validation Loss: 1.155e+00\n",
      "Iteration: 12868, Training Loss: 4.030e+00 , Validation Loss: 3.859e+00\n",
      "Iteration: 12869, Training Loss: 9.822e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 12870, Training Loss: 1.259e+00 , Validation Loss: 2.068e+00\n",
      "Iteration: 12871, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12872, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12873, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 12874, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12875, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12876, Training Loss: 7.555e-01 , Validation Loss: 2.710e+00\n",
      "Iteration: 12877, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12878, Training Loss: 7.555e-01 , Validation Loss: 1.385e+00\n",
      "Iteration: 12879, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12880, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12881, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12882, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 12883, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12884, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12885, Training Loss: 1.259e+00 , Validation Loss: 2.982e+00\n",
      "Iteration: 12886, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12887, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 12888, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12889, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12890, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12891, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 12892, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12893, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12894, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12895, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12896, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12897, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12898, Training Loss: 7.555e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 12899, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12900, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12901, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12902, Training Loss: 1.763e+00 , Validation Loss: 3.750e+00\n",
      "Iteration: 12903, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12904, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12905, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 12906, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12907, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12908, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12909, Training Loss: 2.518e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 12910, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 12911, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12912, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12913, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 12914, Training Loss: 7.555e-01 , Validation Loss: 2.534e+00\n",
      "Iteration: 12915, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12916, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 12917, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12918, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 12919, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12920, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12921, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12922, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12923, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12924, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12925, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12926, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 12927, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 12928, Training Loss: 2.518e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 12929, Training Loss: 2.015e+00 , Validation Loss: 3.925e+00\n",
      "Iteration: 12930, Training Loss: 1.259e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 12931, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 12932, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12933, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12934, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12935, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12936, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12937, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12938, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12939, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12940, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12941, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 12942, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 12943, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 12944, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 12945, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12946, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 12947, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 12948, Training Loss: -1.000e-07 , Validation Loss: 7.439e-01\n",
      "Iteration: 12949, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 12950, Training Loss: 1.007e+00 , Validation Loss: 3.647e+00\n",
      "Iteration: 12951, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 12952, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12953, Training Loss: 1.007e+00 , Validation Loss: 4.234e-01\n",
      "Iteration: 12954, Training Loss: 5.037e-01 , Validation Loss: 2.806e+00\n",
      "Iteration: 12955, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 12956, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 12957, Training Loss: 2.267e+00 , Validation Loss: 4.064e+00\n",
      "Iteration: 12958, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 12959, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 12960, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12961, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12962, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12963, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12964, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12965, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12966, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 12967, Training Loss: 5.289e+00 , Validation Loss: 5.165e+00\n",
      "Iteration: 12968, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 12969, Training Loss: 1.511e+00 , Validation Loss: 4.603e+00\n",
      "Iteration: 12970, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12971, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12972, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 12973, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12974, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12975, Training Loss: 2.015e+00 , Validation Loss: 2.564e+00\n",
      "Iteration: 12976, Training Loss: 2.015e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 12977, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12978, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 12979, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12980, Training Loss: 2.267e+00 , Validation Loss: 4.143e+00\n",
      "Iteration: 12981, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 12982, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12983, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12984, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12985, Training Loss: 2.518e-01 , Validation Loss: 8.951e-01\n",
      "Iteration: 12986, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12987, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12988, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12989, Training Loss: 3.022e+00 , Validation Loss: 3.248e+00\n",
      "Iteration: 12990, Training Loss: 5.037e-01 , Validation Loss: 1.270e+00\n",
      "Iteration: 12991, Training Loss: 1.259e+00 , Validation Loss: 9.435e-01\n",
      "Iteration: 12992, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 12993, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 12994, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12995, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 12996, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12997, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 12998, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 12999, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13000, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13001, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13002, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13003, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13004, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13005, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13006, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13007, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 13008, Training Loss: 2.015e+00 , Validation Loss: 2.389e+00\n",
      "Iteration: 13009, Training Loss: 2.518e-01 , Validation Loss: 1.071e+00\n",
      "Iteration: 13010, Training Loss: 2.015e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 13011, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13012, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13013, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 13014, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 13015, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 13016, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13017, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 13018, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13019, Training Loss: 7.555e-01 , Validation Loss: 1.911e+00\n",
      "Iteration: 13020, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13021, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13022, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13023, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13024, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 13025, Training Loss: 3.274e+00 , Validation Loss: 3.943e+00\n",
      "Iteration: 13026, Training Loss: 2.770e+00 , Validation Loss: 1.960e+00\n",
      "Iteration: 13027, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13028, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13029, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13030, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13031, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13032, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13033, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13034, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13035, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13036, Training Loss: 7.555e-01 , Validation Loss: 1.627e+00\n",
      "Iteration: 13037, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 13038, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13039, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13040, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13041, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13042, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 13043, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13044, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 13045, Training Loss: 1.763e+00 , Validation Loss: 4.082e+00\n",
      "Iteration: 13046, Training Loss: 2.015e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 13047, Training Loss: 1.259e+00 , Validation Loss: 5.443e-01\n",
      "Iteration: 13048, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13049, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 13050, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13051, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13052, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 13053, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13054, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13055, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13056, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13057, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13058, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13059, Training Loss: 1.511e+00 , Validation Loss: 3.435e+00\n",
      "Iteration: 13060, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 13061, Training Loss: 2.015e+00 , Validation Loss: 2.480e+00\n",
      "Iteration: 13062, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13063, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 13064, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13065, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13066, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13067, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13068, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 13069, Training Loss: 2.518e-01 , Validation Loss: 2.032e+00\n",
      "Iteration: 13070, Training Loss: 5.037e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 13071, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 13072, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13073, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13074, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13075, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13076, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13077, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13078, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13079, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13080, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13081, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13082, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13083, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13084, Training Loss: 2.518e-01 , Validation Loss: 2.292e+00\n",
      "Iteration: 13085, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 13086, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13087, Training Loss: 2.518e+00 , Validation Loss: 3.931e+00\n",
      "Iteration: 13088, Training Loss: 2.518e+00 , Validation Loss: 1.421e+00\n",
      "Iteration: 13089, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13090, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13091, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13092, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13093, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13094, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13095, Training Loss: 1.763e+00 , Validation Loss: 2.322e+00\n",
      "Iteration: 13096, Training Loss: 2.518e+00 , Validation Loss: 1.101e+00\n",
      "Iteration: 13097, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13098, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13099, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 13100, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13101, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 13102, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 13103, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13104, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13105, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13106, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13107, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 13108, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 13109, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 13110, Training Loss: 5.037e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 13111, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13112, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13113, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 13114, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 13115, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 13116, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13117, Training Loss: 3.022e+00 , Validation Loss: 3.828e+00\n",
      "Iteration: 13118, Training Loss: 1.259e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 13119, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13120, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13121, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13122, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13123, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13124, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13125, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13126, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13127, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13128, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13129, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13130, Training Loss: 2.267e+00 , Validation Loss: 3.822e+00\n",
      "Iteration: 13131, Training Loss: 1.511e+00 , Validation Loss: 1.161e+00\n",
      "Iteration: 13132, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13133, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13134, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 13135, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13136, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13137, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13138, Training Loss: 1.007e+00 , Validation Loss: 1.754e+00\n",
      "Iteration: 13139, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13140, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 13141, Training Loss: 2.770e+00 , Validation Loss: 3.732e+00\n",
      "Iteration: 13142, Training Loss: 1.511e+00 , Validation Loss: 2.595e+00\n",
      "Iteration: 13143, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 13144, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13145, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 13146, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13147, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 13148, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13149, Training Loss: 5.037e-01 , Validation Loss: 9.677e-01\n",
      "Iteration: 13150, Training Loss: 1.259e+00 , Validation Loss: 1.796e+00\n",
      "Iteration: 13151, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 13152, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13153, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13154, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13155, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13156, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 13157, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13158, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13159, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13160, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13161, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13162, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13163, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 13164, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13165, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13166, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13167, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 13168, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13169, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13170, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13171, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13172, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13173, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13174, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13175, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13176, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13177, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13178, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13179, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13180, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13181, Training Loss: 5.037e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 13182, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13183, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13184, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13185, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 13186, Training Loss: 5.037e-01 , Validation Loss: 1.712e+00\n",
      "Iteration: 13187, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13188, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13189, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13190, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13191, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13192, Training Loss: 7.555e-01 , Validation Loss: 2.032e+00\n",
      "Iteration: 13193, Training Loss: 1.259e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 13194, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 13195, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13196, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13197, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13198, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13199, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13200, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 13201, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 13202, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13203, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13204, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13205, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13206, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13207, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13208, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 13209, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 13210, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 13211, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 13212, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13213, Training Loss: 1.007e+00 , Validation Loss: 1.802e+00\n",
      "Iteration: 13214, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13215, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13216, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13217, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13218, Training Loss: 1.007e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 13219, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 13220, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 13221, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13222, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 13223, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 13224, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 13225, Training Loss: 1.007e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 13226, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13227, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13228, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 13229, Training Loss: 2.518e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 13230, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 13231, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13232, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 13233, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 13234, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 13235, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13236, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 13237, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 13238, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 13239, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 13240, Training Loss: 3.274e+00 , Validation Loss: 4.385e+00\n",
      "Iteration: 13241, Training Loss: 2.770e+00 , Validation Loss: 2.322e+00\n",
      "Iteration: 13242, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13243, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13244, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 13245, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13246, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13247, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13248, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13249, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13250, Training Loss: 2.518e-01 , Validation Loss: 1.300e+00\n",
      "Iteration: 13251, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 13252, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13253, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13254, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 13255, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 13256, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13257, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13258, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 13259, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13260, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 13261, Training Loss: 4.030e+00 , Validation Loss: 4.403e+00\n",
      "Iteration: 13262, Training Loss: 1.385e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 13263, Training Loss: 2.518e+00 , Validation Loss: 5.855e+00\n",
      "Iteration: 13264, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13265, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13266, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13267, Training Loss: 3.022e+00 , Validation Loss: 3.417e+00\n",
      "Iteration: 13268, Training Loss: 1.763e+00 , Validation Loss: 1.064e+00\n",
      "Iteration: 13269, Training Loss: 1.511e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 13270, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13271, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13272, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 13273, Training Loss: 3.022e+00 , Validation Loss: 4.959e+00\n",
      "Iteration: 13274, Training Loss: 1.259e+00 , Validation Loss: 1.149e+00\n",
      "Iteration: 13275, Training Loss: 1.763e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 13276, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13277, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13278, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13279, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13280, Training Loss: 7.555e-01 , Validation Loss: 2.468e+00\n",
      "Iteration: 13281, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13282, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13283, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13284, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 13285, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 13286, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 13287, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 13288, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13289, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13290, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13291, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 13292, Training Loss: 3.526e+00 , Validation Loss: 5.129e+00\n",
      "Iteration: 13293, Training Loss: 1.511e+00 , Validation Loss: 2.298e+00\n",
      "Iteration: 13294, Training Loss: 7.555e-01 , Validation Loss: 9.133e-01\n",
      "Iteration: 13295, Training Loss: 1.259e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 13296, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13297, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13298, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13299, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13300, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13301, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13302, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 13303, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 13304, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 13305, Training Loss: 2.518e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 13306, Training Loss: 3.022e+00 , Validation Loss: 3.732e+00\n",
      "Iteration: 13307, Training Loss: 2.770e+00 , Validation Loss: 2.601e+00\n",
      "Iteration: 13308, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13309, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13310, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13311, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13312, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13313, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13314, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 13315, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 13316, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13317, Training Loss: 3.022e+00 , Validation Loss: 3.822e+00\n",
      "Iteration: 13318, Training Loss: 2.518e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 13319, Training Loss: 1.259e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 13320, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 13321, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13322, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 13323, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13324, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 13325, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13326, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 13327, Training Loss: 5.037e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 13328, Training Loss: 1.007e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 13329, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13330, Training Loss: -1.000e-07 , Validation Loss: 6.411e-01\n",
      "Iteration: 13331, Training Loss: 2.518e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 13332, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13333, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13334, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13335, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13336, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13337, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13338, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13339, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13340, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 13341, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 13342, Training Loss: 1.007e+00 , Validation Loss: 3.103e+00\n",
      "Iteration: 13343, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13344, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 13345, Training Loss: 2.518e-01 , Validation Loss: 8.588e-01\n",
      "Iteration: 13346, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 13347, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13348, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13349, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13350, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13351, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 13352, Training Loss: 2.518e-01 , Validation Loss: 2.044e+00\n",
      "Iteration: 13353, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 13354, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13355, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13356, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 13357, Training Loss: 1.511e+00 , Validation Loss: 3.762e+00\n",
      "Iteration: 13358, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13359, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 13360, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13361, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13362, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13363, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13364, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13365, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 13366, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13367, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 13368, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 13369, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13370, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13371, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13372, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13373, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 13374, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 13375, Training Loss: 1.007e+00 , Validation Loss: 2.171e+00\n",
      "Iteration: 13376, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13377, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13378, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13379, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13380, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13381, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13382, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13383, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13384, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13385, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 13386, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 13387, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13388, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13389, Training Loss: 1.259e+00 , Validation Loss: 1.990e+00\n",
      "Iteration: 13390, Training Loss: 1.511e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13391, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 13392, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13393, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13394, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13395, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13396, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 13397, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 13398, Training Loss: 1.259e+00 , Validation Loss: 2.764e+00\n",
      "Iteration: 13399, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 13400, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 13401, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13402, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13403, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13404, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13405, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13406, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 13407, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 13408, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13409, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13410, Training Loss: 1.007e+00 , Validation Loss: 3.302e+00\n",
      "Iteration: 13411, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13412, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13413, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13414, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13415, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13416, Training Loss: 1.259e+00 , Validation Loss: 2.443e+00\n",
      "Iteration: 13417, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 13418, Training Loss: 1.259e+00 , Validation Loss: 2.165e+00\n",
      "Iteration: 13419, Training Loss: 1.259e+00 , Validation Loss: 1.482e+00\n",
      "Iteration: 13420, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 13421, Training Loss: 1.007e+00 , Validation Loss: 3.689e+00\n",
      "Iteration: 13422, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 13423, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13424, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13425, Training Loss: 1.511e+00 , Validation Loss: 3.411e+00\n",
      "Iteration: 13426, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 13427, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 13428, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 13429, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13430, Training Loss: 1.259e+00 , Validation Loss: 8.165e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13431, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13432, Training Loss: 1.259e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 13433, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 13434, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 13435, Training Loss: 7.555e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 13436, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13437, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13438, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13439, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13440, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 13441, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 13442, Training Loss: 2.770e+00 , Validation Loss: 4.155e+00\n",
      "Iteration: 13443, Training Loss: 1.511e+00 , Validation Loss: 1.143e+00\n",
      "Iteration: 13444, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13445, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13446, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 13447, Training Loss: 7.555e-01 , Validation Loss: 2.195e+00\n",
      "Iteration: 13448, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13449, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13450, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13451, Training Loss: 1.007e+00 , Validation Loss: 8.649e-01\n",
      "Iteration: 13452, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13453, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13454, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13455, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13456, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13457, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13458, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13459, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13460, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13461, Training Loss: 5.037e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 13462, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13463, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13464, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 13465, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 13466, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13467, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13468, Training Loss: -1.000e-07 , Validation Loss: 4.778e-01\n",
      "Iteration: 13469, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 13470, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13471, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 13472, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13473, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 13474, Training Loss: 3.778e+00 , Validation Loss: 4.367e+00\n",
      "Iteration: 13475, Training Loss: 6.044e+00 , Validation Loss: 4.911e+00\n",
      "Iteration: 13476, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 13477, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 13478, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13479, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13480, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13481, Training Loss: 5.037e-01 , Validation Loss: 2.147e+00\n",
      "Iteration: 13482, Training Loss: 5.037e-01 , Validation Loss: 1.095e+00\n",
      "Iteration: 13483, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13484, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13485, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13486, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13487, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13488, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 13489, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13490, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13491, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13492, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13493, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13494, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13495, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 13496, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 13497, Training Loss: 1.007e+00 , Validation Loss: 2.171e+00\n",
      "Iteration: 13498, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 13499, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13500, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13501, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 13502, Training Loss: 2.015e+00 , Validation Loss: 3.707e+00\n",
      "Iteration: 13503, Training Loss: 7.555e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 13504, Training Loss: 1.007e+00 , Validation Loss: 5.988e-01\n",
      "Iteration: 13505, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 13506, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13507, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13508, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13509, Training Loss: 2.518e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 13510, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13511, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13512, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13513, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 13514, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 13515, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13516, Training Loss: 1.259e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 13517, Training Loss: 1.259e+00 , Validation Loss: 1.621e+00\n",
      "Iteration: 13518, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13519, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13520, Training Loss: 1.007e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 13521, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 13522, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 13523, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13524, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13525, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13526, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13527, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13528, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13529, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13530, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 13531, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 13532, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 13533, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 13534, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 13535, Training Loss: 1.763e+00 , Validation Loss: 3.629e+00\n",
      "Iteration: 13536, Training Loss: 1.259e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 13537, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 13538, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 13539, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 13540, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 13541, Training Loss: 1.511e+00 , Validation Loss: 2.897e+00\n",
      "Iteration: 13542, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13543, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13544, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13545, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 13546, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 13547, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 13548, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13549, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 13550, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 13551, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13552, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13553, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13554, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13555, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 13556, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13557, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 13558, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 13559, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13560, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13561, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 13562, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13563, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13564, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13565, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13566, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13567, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13568, Training Loss: 7.555e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 13569, Training Loss: 5.037e-01 , Validation Loss: 1.669e+00\n",
      "Iteration: 13570, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13571, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13572, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13573, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13574, Training Loss: 7.555e-01 , Validation Loss: 1.524e+00\n",
      "Iteration: 13575, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13576, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 13577, Training Loss: 1.763e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 13578, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 13579, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13580, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13581, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 13582, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 13583, Training Loss: 2.518e-01 , Validation Loss: 1.984e+00\n",
      "Iteration: 13584, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 13585, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13586, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13587, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13588, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13589, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 13590, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13591, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 13592, Training Loss: 1.511e+00 , Validation Loss: 3.750e+00\n",
      "Iteration: 13593, Training Loss: 1.511e+00 , Validation Loss: 4.778e-01\n",
      "Iteration: 13594, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 13595, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13596, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13597, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13598, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13599, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13600, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13601, Training Loss: 5.037e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 13602, Training Loss: 5.037e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 13603, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13604, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13605, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13606, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13607, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13608, Training Loss: 1.007e+00 , Validation Loss: 4.597e-01\n",
      "Iteration: 13609, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13610, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 13611, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13612, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 13613, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 13614, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 13615, Training Loss: 1.007e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 13616, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13617, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13618, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13619, Training Loss: -1.000e-07 , Validation Loss: 1.748e+00\n",
      "Iteration: 13620, Training Loss: 1.007e+00 , Validation Loss: 1.748e+00\n",
      "Iteration: 13621, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13622, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13623, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 13624, Training Loss: 3.022e+00 , Validation Loss: 3.889e+00\n",
      "Iteration: 13625, Training Loss: 1.007e+00 , Validation Loss: 2.111e+00\n",
      "Iteration: 13626, Training Loss: 1.763e+00 , Validation Loss: 1.300e+00\n",
      "Iteration: 13627, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13628, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13629, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 13630, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 13631, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13632, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13633, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 13634, Training Loss: 5.037e-01 , Validation Loss: 1.693e+00\n",
      "Iteration: 13635, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13636, Training Loss: -1.000e-07 , Validation Loss: 1.542e+00\n",
      "Iteration: 13637, Training Loss: -1.000e-07 , Validation Loss: 1.542e+00\n",
      "Iteration: 13638, Training Loss: 5.037e-01 , Validation Loss: 1.542e+00\n",
      "Iteration: 13639, Training Loss: 1.007e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 13640, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13641, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13642, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13643, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13644, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13645, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13646, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13647, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13648, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13649, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13650, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 13651, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 13652, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 13653, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13654, Training Loss: 1.007e+00 , Validation Loss: 2.927e+00\n",
      "Iteration: 13655, Training Loss: 7.555e-01 , Validation Loss: 3.441e+00\n",
      "Iteration: 13656, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 13657, Training Loss: 2.770e+00 , Validation Loss: 4.088e+00\n",
      "Iteration: 13658, Training Loss: 1.007e+00 , Validation Loss: 2.504e+00\n",
      "Iteration: 13659, Training Loss: 5.037e-01 , Validation Loss: 1.034e+00\n",
      "Iteration: 13660, Training Loss: 1.763e+00 , Validation Loss: 8.588e-01\n",
      "Iteration: 13661, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13662, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13663, Training Loss: 2.267e+00 , Validation Loss: 4.088e+00\n",
      "Iteration: 13664, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13665, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13666, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13667, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13668, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13669, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13670, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 13671, Training Loss: 1.259e+00 , Validation Loss: 4.155e+00\n",
      "Iteration: 13672, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13673, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13674, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 13675, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13676, Training Loss: 1.007e+00 , Validation Loss: 4.899e-01\n",
      "Iteration: 13677, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13678, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13679, Training Loss: 1.007e+00 , Validation Loss: 2.244e+00\n",
      "Iteration: 13680, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13681, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 13682, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13683, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13684, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13685, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13686, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13687, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13688, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13689, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13690, Training Loss: 1.511e+00 , Validation Loss: 3.738e+00\n",
      "Iteration: 13691, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13692, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13693, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13694, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13695, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 13696, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13697, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13698, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13699, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13700, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 13701, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13702, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13703, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13704, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13705, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13706, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13707, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13708, Training Loss: 1.259e+00 , Validation Loss: 1.433e+00\n",
      "Iteration: 13709, Training Loss: 2.770e+00 , Validation Loss: 5.195e+00\n",
      "Iteration: 13710, Training Loss: 7.555e-01 , Validation Loss: 1.458e+00\n",
      "Iteration: 13711, Training Loss: 1.511e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 13712, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 13713, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13714, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13715, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13716, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13717, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13718, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 13719, Training Loss: 2.770e+00 , Validation Loss: 3.804e+00\n",
      "Iteration: 13720, Training Loss: 1.763e+00 , Validation Loss: 1.494e+00\n",
      "Iteration: 13721, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 13722, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13723, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13724, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13725, Training Loss: 5.037e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 13726, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 13727, Training Loss: 1.511e+00 , Validation Loss: 3.720e+00\n",
      "Iteration: 13728, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 13729, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13730, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 13731, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13732, Training Loss: 1.007e+00 , Validation Loss: 2.558e+00\n",
      "Iteration: 13733, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 13734, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 13735, Training Loss: 1.511e+00 , Validation Loss: 3.308e+00\n",
      "Iteration: 13736, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 13737, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13738, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13739, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13740, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13741, Training Loss: 1.007e+00 , Validation Loss: 7.802e-01\n",
      "Iteration: 13742, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 13743, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13744, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13745, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13746, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13747, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13748, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13749, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13750, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 13751, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13752, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13753, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13754, Training Loss: 7.555e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 13755, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 13756, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13757, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13758, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 13759, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13760, Training Loss: 1.007e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 13761, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 13762, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13763, Training Loss: 1.511e+00 , Validation Loss: 1.899e+00\n",
      "Iteration: 13764, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13765, Training Loss: 5.037e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 13766, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13767, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13768, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13769, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 13770, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 13771, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13772, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 13773, Training Loss: 1.007e+00 , Validation Loss: 4.415e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13774, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13775, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 13776, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 13777, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13778, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 13779, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13780, Training Loss: 1.259e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 13781, Training Loss: 1.007e+00 , Validation Loss: 1.028e+00\n",
      "Iteration: 13782, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 13783, Training Loss: 3.274e+00 , Validation Loss: 3.955e+00\n",
      "Iteration: 13784, Training Loss: 2.015e+00 , Validation Loss: 3.091e+00\n",
      "Iteration: 13785, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 13786, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13787, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13788, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 13789, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 13790, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 13791, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13792, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 13793, Training Loss: 1.763e+00 , Validation Loss: 1.216e+00\n",
      "Iteration: 13794, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13795, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13796, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 13797, Training Loss: 1.007e+00 , Validation Loss: 2.921e+00\n",
      "Iteration: 13798, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13799, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13800, Training Loss: 7.555e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 13801, Training Loss: -1.000e-07 , Validation Loss: 6.230e-01\n",
      "Iteration: 13802, Training Loss: 2.518e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 13803, Training Loss: 2.518e-01 , Validation Loss: 2.038e+00\n",
      "Iteration: 13804, Training Loss: 3.022e+00 , Validation Loss: 4.082e+00\n",
      "Iteration: 13805, Training Loss: 7.555e-01 , Validation Loss: 1.161e+00\n",
      "Iteration: 13806, Training Loss: 1.763e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 13807, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 13808, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13809, Training Loss: 1.511e+00 , Validation Loss: 3.762e+00\n",
      "Iteration: 13810, Training Loss: 1.763e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 13811, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 13812, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13813, Training Loss: 2.015e+00 , Validation Loss: 3.913e+00\n",
      "Iteration: 13814, Training Loss: 1.511e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 13815, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 13816, Training Loss: 2.518e-01 , Validation Loss: 2.661e+00\n",
      "Iteration: 13817, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 13818, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 13819, Training Loss: 1.007e+00 , Validation Loss: 1.566e+00\n",
      "Iteration: 13820, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13821, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13822, Training Loss: 1.007e+00 , Validation Loss: 1.621e+00\n",
      "Iteration: 13823, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13824, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13825, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13826, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13827, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 13828, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 13829, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13830, Training Loss: 2.518e-01 , Validation Loss: 1.191e+00\n",
      "Iteration: 13831, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 13832, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13833, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13834, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 13835, Training Loss: 2.770e+00 , Validation Loss: 3.556e+00\n",
      "Iteration: 13836, Training Loss: 1.259e+00 , Validation Loss: 1.337e+00\n",
      "Iteration: 13837, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 13838, Training Loss: 2.267e+00 , Validation Loss: 6.653e-01\n",
      "Iteration: 13839, Training Loss: 7.555e-01 , Validation Loss: 2.159e+00\n",
      "Iteration: 13840, Training Loss: 1.259e+00 , Validation Loss: 4.076e+00\n",
      "Iteration: 13841, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 13842, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 13843, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 13844, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13845, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13846, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13847, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13848, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 13849, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 13850, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 13851, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13852, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13853, Training Loss: -1.000e-07 , Validation Loss: 7.983e-01\n",
      "Iteration: 13854, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 13855, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13856, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13857, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 13858, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 13859, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 13860, Training Loss: 3.526e+00 , Validation Loss: 4.524e+00\n",
      "Iteration: 13861, Training Loss: 2.015e+00 , Validation Loss: 2.147e+00\n",
      "Iteration: 13862, Training Loss: 1.259e+00 , Validation Loss: 8.346e-01\n",
      "Iteration: 13863, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13864, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13865, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13866, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13867, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13868, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13869, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13870, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13871, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13872, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 13873, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13874, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13875, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 13876, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13877, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13878, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13879, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 13880, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13881, Training Loss: 1.259e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 13882, Training Loss: -1.000e-07 , Validation Loss: 6.955e-01\n",
      "Iteration: 13883, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 13884, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 13885, Training Loss: 5.037e-01 , Validation Loss: 1.597e+00\n",
      "Iteration: 13886, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13887, Training Loss: 1.259e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 13888, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13889, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13890, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 13891, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13892, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13893, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13894, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 13895, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13896, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 13897, Training Loss: 2.267e+00 , Validation Loss: 4.343e+00\n",
      "Iteration: 13898, Training Loss: 2.770e+00 , Validation Loss: 1.542e+00\n",
      "Iteration: 13899, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 13900, Training Loss: 1.511e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 13901, Training Loss: 2.518e+00 , Validation Loss: 5.564e+00\n",
      "Iteration: 13902, Training Loss: 2.015e+00 , Validation Loss: 1.790e+00\n",
      "Iteration: 13903, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 13904, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 13905, Training Loss: 1.259e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 13906, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 13907, Training Loss: 1.259e+00 , Validation Loss: 1.210e+00\n",
      "Iteration: 13908, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13909, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13910, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13911, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13912, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13913, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13914, Training Loss: 1.511e+00 , Validation Loss: 2.177e+00\n",
      "Iteration: 13915, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 13916, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 13917, Training Loss: 5.037e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 13918, Training Loss: 2.518e-01 , Validation Loss: 1.778e+00\n",
      "Iteration: 13919, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 13920, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 13921, Training Loss: 5.037e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 13922, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13923, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 13924, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13925, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13926, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 13927, Training Loss: 2.518e-01 , Validation Loss: 9.858e-01\n",
      "Iteration: 13928, Training Loss: 1.259e+00 , Validation Loss: 5.020e-01\n",
      "Iteration: 13929, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 13930, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13931, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13932, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13933, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13934, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 13935, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13936, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 13937, Training Loss: 1.007e+00 , Validation Loss: 1.107e+00\n",
      "Iteration: 13938, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13939, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13940, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13941, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 13942, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 13943, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 13944, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13945, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 13946, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 13947, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 13948, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 13949, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 13950, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 13951, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 13952, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 13953, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13954, Training Loss: 1.007e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 13955, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 13956, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 13957, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13958, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13959, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13960, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13961, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 13962, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 13963, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 13964, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13965, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13966, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13967, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13968, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 13969, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13970, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13971, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13972, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 13973, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13974, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 13975, Training Loss: 7.555e-01 , Validation Loss: 1.530e+00\n",
      "Iteration: 13976, Training Loss: 1.007e+00 , Validation Loss: 1.149e+00\n",
      "Iteration: 13977, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13978, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 13979, Training Loss: 7.555e-01 , Validation Loss: 1.343e+00\n",
      "Iteration: 13980, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13981, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13982, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13983, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13984, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 13985, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 13986, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13987, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 13988, Training Loss: 5.037e-01 , Validation Loss: 9.314e-01\n",
      "Iteration: 13989, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 13990, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13991, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 13992, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 13993, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13994, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13995, Training Loss: 1.007e+00 , Validation Loss: 1.778e+00\n",
      "Iteration: 13996, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 13997, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 13998, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 13999, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 14000, Training Loss: 5.037e-01 , Validation Loss: 2.111e+00\n",
      "Iteration: 14001, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14002, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14003, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 14004, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14005, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14006, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14007, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14008, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14009, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 14010, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14011, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14012, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14013, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14014, Training Loss: 1.007e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 14015, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14016, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14017, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14018, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 14019, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14020, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14021, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14022, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 14023, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 14024, Training Loss: 5.037e-01 , Validation Loss: 9.556e-01\n",
      "Iteration: 14025, Training Loss: 2.518e-01 , Validation Loss: 1.899e+00\n",
      "Iteration: 14026, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 14027, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 14028, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14029, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14030, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14031, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14032, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14033, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14034, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 14035, Training Loss: 5.037e-01 , Validation Loss: 1.318e+00\n",
      "Iteration: 14036, Training Loss: 1.007e+00 , Validation Loss: 2.437e+00\n",
      "Iteration: 14037, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14038, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 14039, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 14040, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14041, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 14042, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14043, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14044, Training Loss: -1.000e-07 , Validation Loss: 7.802e-01\n",
      "Iteration: 14045, Training Loss: 5.037e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 14046, Training Loss: 2.518e-01 , Validation Loss: 1.458e+00\n",
      "Iteration: 14047, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 14048, Training Loss: 2.518e-01 , Validation Loss: 1.796e+00\n",
      "Iteration: 14049, Training Loss: 3.022e+00 , Validation Loss: 4.179e+00\n",
      "Iteration: 14050, Training Loss: 4.281e+00 , Validation Loss: 4.784e+00\n",
      "Iteration: 14051, Training Loss: 1.007e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 14052, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14053, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14054, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 14055, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 14056, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14057, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 14058, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14059, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14060, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 14061, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 14062, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14063, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14064, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14065, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14066, Training Loss: 1.259e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 14067, Training Loss: 3.274e+00 , Validation Loss: 5.322e+00\n",
      "Iteration: 14068, Training Loss: 2.770e+00 , Validation Loss: 2.431e+00\n",
      "Iteration: 14069, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14070, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 14071, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14072, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14073, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 14074, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14075, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14076, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14077, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 14078, Training Loss: 5.037e-01 , Validation Loss: 1.482e+00\n",
      "Iteration: 14079, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 14080, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 14081, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 14082, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 14083, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14084, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14085, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14086, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14087, Training Loss: 2.267e+00 , Validation Loss: 4.288e+00\n",
      "Iteration: 14088, Training Loss: 2.518e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 14089, Training Loss: 1.763e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 14090, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14091, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14092, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14093, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14094, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14095, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14096, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 14097, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 14098, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 14099, Training Loss: -1.000e-07 , Validation Loss: 7.802e-01\n",
      "Iteration: 14100, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 14101, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 14102, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 14103, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14104, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14105, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 14106, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14107, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14108, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14109, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14110, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14111, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14112, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14113, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14114, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14115, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14116, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14117, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14118, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14119, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14120, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 14121, Training Loss: 7.555e-01 , Validation Loss: 1.198e+00\n",
      "Iteration: 14122, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 14123, Training Loss: -1.000e-07 , Validation Loss: 7.439e-01\n",
      "Iteration: 14124, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 14125, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14126, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14127, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14128, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14129, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 14130, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14131, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14132, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 14133, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14134, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14135, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 14136, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14137, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14138, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14139, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14140, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14141, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14142, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14143, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14144, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14145, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 14146, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14147, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14148, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14149, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14150, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14151, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14152, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14153, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14154, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14155, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14156, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 14157, Training Loss: 1.763e+00 , Validation Loss: 3.647e+00\n",
      "Iteration: 14158, Training Loss: 1.511e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 14159, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 14160, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14161, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14162, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14163, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 14164, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 14165, Training Loss: 1.007e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 14166, Training Loss: 2.518e-01 , Validation Loss: 1.246e+00\n",
      "Iteration: 14167, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 14168, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14169, Training Loss: 7.555e-01 , Validation Loss: 3.653e+00\n",
      "Iteration: 14170, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14171, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14172, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 14173, Training Loss: 2.518e-01 , Validation Loss: 1.621e+00\n",
      "Iteration: 14174, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 14175, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14176, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14177, Training Loss: 5.037e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 14178, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14179, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 14180, Training Loss: 7.555e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 14181, Training Loss: 2.518e+00 , Validation Loss: 3.623e+00\n",
      "Iteration: 14182, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 14183, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 14184, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14185, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14186, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 14187, Training Loss: 7.555e-01 , Validation Loss: 1.040e+00\n",
      "Iteration: 14188, Training Loss: 2.518e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 14189, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 14190, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 14191, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14192, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 14193, Training Loss: 5.037e-01 , Validation Loss: 2.093e+00\n",
      "Iteration: 14194, Training Loss: 1.007e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 14195, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 14196, Training Loss: 2.518e-01 , Validation Loss: 1.095e+00\n",
      "Iteration: 14197, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 14198, Training Loss: 1.259e+00 , Validation Loss: 2.619e+00\n",
      "Iteration: 14199, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 14200, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14201, Training Loss: 1.763e+00 , Validation Loss: 3.381e+00\n",
      "Iteration: 14202, Training Loss: 1.007e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 14203, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 14204, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 14205, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 14206, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 14207, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14208, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14209, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14210, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14211, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14212, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14213, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14214, Training Loss: 1.007e+00 , Validation Loss: 1.700e+00\n",
      "Iteration: 14215, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14216, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14217, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14218, Training Loss: 3.022e+00 , Validation Loss: 4.754e+00\n",
      "Iteration: 14219, Training Loss: 1.259e+00 , Validation Loss: 1.566e+00\n",
      "Iteration: 14220, Training Loss: 7.555e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 14221, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14222, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14223, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14224, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14225, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14226, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 14227, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14228, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14229, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14230, Training Loss: 2.015e+00 , Validation Loss: 3.974e+00\n",
      "Iteration: 14231, Training Loss: 1.763e+00 , Validation Loss: 8.891e-01\n",
      "Iteration: 14232, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 14233, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 14234, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 14235, Training Loss: 5.037e-01 , Validation Loss: 2.462e+00\n",
      "Iteration: 14236, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14237, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 14238, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 14239, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14240, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 14241, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14242, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14243, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 14244, Training Loss: 1.007e+00 , Validation Loss: 1.028e+00\n",
      "Iteration: 14245, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 14246, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14247, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 14248, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14249, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14250, Training Loss: 5.037e-01 , Validation Loss: 1.318e+00\n",
      "Iteration: 14251, Training Loss: 1.259e+00 , Validation Loss: 2.824e+00\n",
      "Iteration: 14252, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 14253, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 14254, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14255, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14256, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14257, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 14258, Training Loss: 1.259e+00 , Validation Loss: 9.556e-01\n",
      "Iteration: 14259, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 14260, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 14261, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14262, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14263, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14264, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 14265, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14266, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14267, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14268, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14269, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14270, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14271, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14272, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14273, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14274, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14275, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 14276, Training Loss: 7.555e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 14277, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14278, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 14279, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14280, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14281, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14282, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 14283, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 14284, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14285, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14286, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14287, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 14288, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14289, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14290, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14291, Training Loss: 1.007e+00 , Validation Loss: 2.365e+00\n",
      "Iteration: 14292, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14293, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 14294, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 14295, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 14296, Training Loss: -1.000e-07 , Validation Loss: 8.528e-01\n",
      "Iteration: 14297, Training Loss: 2.518e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 14298, Training Loss: 1.259e+00 , Validation Loss: 3.538e+00\n",
      "Iteration: 14299, Training Loss: 7.555e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 14300, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 14301, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 14302, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 14303, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 14304, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 14305, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14306, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14307, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14308, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14309, Training Loss: 1.007e+00 , Validation Loss: 1.028e+00\n",
      "Iteration: 14310, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 14311, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14312, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14313, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 14314, Training Loss: 2.518e-01 , Validation Loss: 9.314e-01\n",
      "Iteration: 14315, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 14316, Training Loss: 7.555e-01 , Validation Loss: 2.081e+00\n",
      "Iteration: 14317, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14318, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14319, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 14320, Training Loss: 1.763e+00 , Validation Loss: 1.627e+00\n",
      "Iteration: 14321, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14322, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 14323, Training Loss: 1.259e+00 , Validation Loss: 3.085e+00\n",
      "Iteration: 14324, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 14325, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14326, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14327, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14328, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14329, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14330, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14331, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 14332, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14333, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 14334, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14335, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 14336, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 14337, Training Loss: 7.555e-01 , Validation Loss: 2.056e+00\n",
      "Iteration: 14338, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14339, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14340, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 14341, Training Loss: 7.555e-01 , Validation Loss: 1.064e+00\n",
      "Iteration: 14342, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14343, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14344, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 14345, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14346, Training Loss: 1.511e+00 , Validation Loss: 3.326e+00\n",
      "Iteration: 14347, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 14348, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14349, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 14350, Training Loss: 5.037e-01 , Validation Loss: 1.494e+00\n",
      "Iteration: 14351, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14352, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14353, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14354, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14355, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14356, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14357, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14358, Training Loss: 2.518e-01 , Validation Loss: 1.119e+00\n",
      "Iteration: 14359, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 14360, Training Loss: 5.037e-01 , Validation Loss: 1.899e+00\n",
      "Iteration: 14361, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 14362, Training Loss: 7.555e-01 , Validation Loss: 1.028e+00\n",
      "Iteration: 14363, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14364, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14365, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14366, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 14367, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14368, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14369, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 14370, Training Loss: 3.022e+00 , Validation Loss: 4.131e+00\n",
      "Iteration: 14371, Training Loss: 1.511e+00 , Validation Loss: 2.776e+00\n",
      "Iteration: 14372, Training Loss: 1.007e+00 , Validation Loss: 8.346e-01\n",
      "Iteration: 14373, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14374, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 14375, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 14376, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14377, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 14378, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 14379, Training Loss: 5.037e-01 , Validation Loss: 1.058e+00\n",
      "Iteration: 14380, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14381, Training Loss: 1.763e+00 , Validation Loss: 2.764e+00\n",
      "Iteration: 14382, Training Loss: 1.259e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 14383, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14384, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14385, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14386, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14387, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14388, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14389, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 14390, Training Loss: 1.763e+00 , Validation Loss: 3.707e+00\n",
      "Iteration: 14391, Training Loss: 1.763e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 14392, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14393, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14394, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 14395, Training Loss: 2.015e+00 , Validation Loss: 3.714e+00\n",
      "Iteration: 14396, Training Loss: 2.267e+00 , Validation Loss: 1.064e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14397, Training Loss: 1.007e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 14398, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14399, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14400, Training Loss: 1.007e+00 , Validation Loss: 2.105e+00\n",
      "Iteration: 14401, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14402, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14403, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 14404, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14405, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14406, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14407, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14408, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14409, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14410, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14411, Training Loss: 5.037e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 14412, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14413, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14414, Training Loss: 3.022e+00 , Validation Loss: 4.095e+00\n",
      "Iteration: 14415, Training Loss: 2.267e+00 , Validation Loss: 1.349e+00\n",
      "Iteration: 14416, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 14417, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 14418, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14419, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14420, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14421, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14422, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14423, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 14424, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 14425, Training Loss: 2.518e+00 , Validation Loss: 3.925e+00\n",
      "Iteration: 14426, Training Loss: 1.259e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 14427, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14428, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14429, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 14430, Training Loss: 1.511e+00 , Validation Loss: 2.274e+00\n",
      "Iteration: 14431, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 14432, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14433, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 14434, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 14435, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 14436, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 14437, Training Loss: 1.259e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 14438, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14439, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 14440, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 14441, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 14442, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 14443, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 14444, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 14445, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14446, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14447, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14448, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 14449, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14450, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14451, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14452, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 14453, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14454, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14455, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 14456, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14457, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14458, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 14459, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 14460, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 14461, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14462, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14463, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 14464, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14465, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 14466, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 14467, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14468, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 14469, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14470, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 14471, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14472, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14473, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14474, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14475, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14476, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 14477, Training Loss: -1.000e-07 , Validation Loss: 1.294e+00\n",
      "Iteration: 14478, Training Loss: 2.518e-01 , Validation Loss: 1.294e+00\n",
      "Iteration: 14479, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 14480, Training Loss: 7.555e-01 , Validation Loss: 1.669e+00\n",
      "Iteration: 14481, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14482, Training Loss: 2.518e-01 , Validation Loss: 1.234e+00\n",
      "Iteration: 14483, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 14484, Training Loss: 1.259e+00 , Validation Loss: 1.161e+00\n",
      "Iteration: 14485, Training Loss: 1.007e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 14486, Training Loss: 2.518e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 14487, Training Loss: 1.763e+00 , Validation Loss: 2.770e+00\n",
      "Iteration: 14488, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 14489, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14490, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14491, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14492, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14493, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14494, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 14495, Training Loss: 1.259e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 14496, Training Loss: 1.259e+00 , Validation Loss: 1.706e+00\n",
      "Iteration: 14497, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14498, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14499, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14500, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 14501, Training Loss: 5.037e-01 , Validation Loss: 3.738e+00\n",
      "Iteration: 14502, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14503, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14504, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 14505, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14506, Training Loss: 7.555e-01 , Validation Loss: 1.137e+00\n",
      "Iteration: 14507, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 14508, Training Loss: 7.555e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 14509, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14510, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14511, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14512, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14513, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14514, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 14515, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14516, Training Loss: 5.037e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 14517, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14518, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14519, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 14520, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 14521, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14522, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 14523, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14524, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14525, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14526, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14527, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 14528, Training Loss: 3.778e+00 , Validation Loss: 4.415e+00\n",
      "Iteration: 14529, Training Loss: 4.533e+00 , Validation Loss: 5.050e+00\n",
      "Iteration: 14530, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14531, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 14532, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14533, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 14534, Training Loss: 1.007e+00 , Validation Loss: 2.286e+00\n",
      "Iteration: 14535, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14536, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14537, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14538, Training Loss: -1.000e-07 , Validation Loss: 4.899e-01\n",
      "Iteration: 14539, Training Loss: -1.000e-07 , Validation Loss: 4.899e-01\n",
      "Iteration: 14540, Training Loss: 1.007e+00 , Validation Loss: 4.899e-01\n",
      "Iteration: 14541, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 14542, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 14543, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14544, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14545, Training Loss: 1.259e+00 , Validation Loss: 1.839e+00\n",
      "Iteration: 14546, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 14547, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14548, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14549, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14550, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14551, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14552, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 14553, Training Loss: 1.511e+00 , Validation Loss: 2.661e+00\n",
      "Iteration: 14554, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 14555, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 14556, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14557, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 14558, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14559, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 14560, Training Loss: 1.007e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 14561, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14562, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14563, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14564, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14565, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14566, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14567, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14568, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14569, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14570, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 14571, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 14572, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 14573, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14574, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 14575, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14576, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14577, Training Loss: 1.007e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 14578, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 14579, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 14580, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 14581, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14582, Training Loss: 1.259e+00 , Validation Loss: 1.814e+00\n",
      "Iteration: 14583, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14584, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 14585, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14586, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14587, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14588, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14589, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14590, Training Loss: 5.037e-01 , Validation Loss: 1.258e+00\n",
      "Iteration: 14591, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14592, Training Loss: 2.518e-01 , Validation Loss: 2.746e+00\n",
      "Iteration: 14593, Training Loss: 5.037e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 14594, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 14595, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14596, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 14597, Training Loss: 7.555e-01 , Validation Loss: 3.816e+00\n",
      "Iteration: 14598, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14599, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14600, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14601, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14602, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14603, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 14604, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14605, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 14606, Training Loss: 1.259e+00 , Validation Loss: 1.155e+00\n",
      "Iteration: 14607, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14608, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 14609, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14610, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14611, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14612, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14613, Training Loss: 2.518e-01 , Validation Loss: 1.748e+00\n",
      "Iteration: 14614, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 14615, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 14616, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14617, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14618, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 14619, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 14620, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 14621, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 14622, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14623, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14624, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14625, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14626, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14627, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14628, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14629, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14630, Training Loss: 1.259e+00 , Validation Loss: 2.105e+00\n",
      "Iteration: 14631, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 14632, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14633, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 14634, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14635, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 14636, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 14637, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 14638, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14639, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14640, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14641, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14642, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 14643, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14644, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14645, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14646, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 14647, Training Loss: 1.259e+00 , Validation Loss: 1.657e+00\n",
      "Iteration: 14648, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 14649, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 14650, Training Loss: 5.037e-01 , Validation Loss: 1.754e+00\n",
      "Iteration: 14651, Training Loss: 3.526e+00 , Validation Loss: 5.195e+00\n",
      "Iteration: 14652, Training Loss: 2.267e+00 , Validation Loss: 2.068e+00\n",
      "Iteration: 14653, Training Loss: 7.555e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 14654, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14655, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14656, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14657, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14658, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14659, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14660, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14661, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14662, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 14663, Training Loss: 3.022e+00 , Validation Loss: 4.947e+00\n",
      "Iteration: 14664, Training Loss: 1.007e+00 , Validation Loss: 2.250e+00\n",
      "Iteration: 14665, Training Loss: 7.555e-01 , Validation Loss: 9.012e-01\n",
      "Iteration: 14666, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 14667, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 14668, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14669, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 14670, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14671, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14672, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14673, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14674, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14675, Training Loss: 7.555e-01 , Validation Loss: 2.050e+00\n",
      "Iteration: 14676, Training Loss: 2.518e-01 , Validation Loss: 1.252e+00\n",
      "Iteration: 14677, Training Loss: 1.259e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 14678, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14679, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 14680, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 14681, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 14682, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 14683, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 14684, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 14685, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 14686, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14687, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14688, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14689, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14690, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14691, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14692, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14693, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14694, Training Loss: 1.007e+00 , Validation Loss: 1.101e+00\n",
      "Iteration: 14695, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 14696, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 14697, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 14698, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14699, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14700, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14701, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14702, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14703, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14704, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14705, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14706, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14707, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14708, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14709, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 14710, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 14711, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 14712, Training Loss: 7.555e-01 , Validation Loss: 2.776e+00\n",
      "Iteration: 14713, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14714, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 14715, Training Loss: 3.022e+00 , Validation Loss: 5.183e+00\n",
      "Iteration: 14716, Training Loss: 2.015e+00 , Validation Loss: 1.917e+00\n",
      "Iteration: 14717, Training Loss: 1.763e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 14718, Training Loss: 2.518e-01 , Validation Loss: 1.929e+00\n",
      "Iteration: 14719, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 14720, Training Loss: 2.518e-01 , Validation Loss: 9.858e-01\n",
      "Iteration: 14721, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 14722, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 14723, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14724, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14725, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14726, Training Loss: -1.000e-07 , Validation Loss: 6.955e-01\n",
      "Iteration: 14727, Training Loss: -1.000e-07 , Validation Loss: 6.955e-01\n",
      "Iteration: 14728, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 14729, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14730, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14731, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14732, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14733, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 14734, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14735, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 14736, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14737, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14738, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14739, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 14740, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 14741, Training Loss: 2.267e+00 , Validation Loss: 3.732e+00\n",
      "Iteration: 14742, Training Loss: 1.511e+00 , Validation Loss: 1.300e+00\n",
      "Iteration: 14743, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 14744, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14745, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14746, Training Loss: -1.000e-07 , Validation Loss: 6.653e-01\n",
      "Iteration: 14747, Training Loss: 2.518e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 14748, Training Loss: 1.763e+00 , Validation Loss: 3.683e+00\n",
      "Iteration: 14749, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14750, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 14751, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 14752, Training Loss: 1.259e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 14753, Training Loss: 1.007e+00 , Validation Loss: 2.359e+00\n",
      "Iteration: 14754, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14755, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14756, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14757, Training Loss: 2.518e+00 , Validation Loss: 4.095e+00\n",
      "Iteration: 14758, Training Loss: 1.259e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 14759, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14760, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 14761, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 14762, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14763, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14764, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14765, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14766, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14767, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14768, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14769, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14770, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14771, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14772, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14773, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14774, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 14775, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14776, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14777, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14778, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14779, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 14780, Training Loss: 1.007e+00 , Validation Loss: 3.877e+00\n",
      "Iteration: 14781, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 14782, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14783, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14784, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 14785, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14786, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14787, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14788, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14789, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14790, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 14791, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 14792, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14793, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14794, Training Loss: 1.511e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 14795, Training Loss: -1.000e-07 , Validation Loss: 5.141e-01\n",
      "Iteration: 14796, Training Loss: 7.555e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 14797, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14798, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14799, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 14800, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14801, Training Loss: 7.555e-01 , Validation Loss: 2.068e+00\n",
      "Iteration: 14802, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 14803, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14804, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 14805, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 14806, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 14807, Training Loss: 1.007e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 14808, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 14809, Training Loss: 5.037e-01 , Validation Loss: 1.216e+00\n",
      "Iteration: 14810, Training Loss: 2.518e-01 , Validation Loss: 2.516e+00\n",
      "Iteration: 14811, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 14812, Training Loss: 2.015e+00 , Validation Loss: 4.125e+00\n",
      "Iteration: 14813, Training Loss: 5.037e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 14814, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14815, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14816, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 14817, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 14818, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14819, Training Loss: 1.007e+00 , Validation Loss: 1.524e+00\n",
      "Iteration: 14820, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14821, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 14822, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 14823, Training Loss: 7.555e-01 , Validation Loss: 2.570e+00\n",
      "Iteration: 14824, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14825, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14826, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14827, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14828, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 14829, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 14830, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 14831, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14832, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14833, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14834, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 14835, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14836, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 14837, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 14838, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 14839, Training Loss: 2.518e+00 , Validation Loss: 3.689e+00\n",
      "Iteration: 14840, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 14841, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14842, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 14843, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 14844, Training Loss: 2.518e-01 , Validation Loss: 9.919e-01\n",
      "Iteration: 14845, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 14846, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14847, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14848, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14849, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14850, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 14851, Training Loss: 1.007e+00 , Validation Loss: 3.653e+00\n",
      "Iteration: 14852, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14853, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14854, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 14855, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 14856, Training Loss: 1.007e+00 , Validation Loss: 2.389e+00\n",
      "Iteration: 14857, Training Loss: 2.518e+00 , Validation Loss: 4.028e+00\n",
      "Iteration: 14858, Training Loss: 5.037e-01 , Validation Loss: 9.616e-01\n",
      "Iteration: 14859, Training Loss: 1.511e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 14860, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14861, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14862, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 14863, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14864, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14865, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14866, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 14867, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 14868, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14869, Training Loss: 1.007e+00 , Validation Loss: 1.415e+00\n",
      "Iteration: 14870, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14871, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 14872, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 14873, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 14874, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14875, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14876, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 14877, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 14878, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 14879, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 14880, Training Loss: 1.007e+00 , Validation Loss: 1.966e+00\n",
      "Iteration: 14881, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 14882, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 14883, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 14884, Training Loss: 2.015e+00 , Validation Loss: 4.258e+00\n",
      "Iteration: 14885, Training Loss: 5.037e-01 , Validation Loss: 1.198e+00\n",
      "Iteration: 14886, Training Loss: 2.518e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 14887, Training Loss: 1.763e+00 , Validation Loss: 6.350e-01\n",
      "Iteration: 14888, Training Loss: 1.763e+00 , Validation Loss: 2.413e+00\n",
      "Iteration: 14889, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 14890, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 14891, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14892, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14893, Training Loss: 7.555e-01 , Validation Loss: 2.256e+00\n",
      "Iteration: 14894, Training Loss: 5.037e-01 , Validation Loss: 1.246e+00\n",
      "Iteration: 14895, Training Loss: -1.000e-07 , Validation Loss: 1.524e+00\n",
      "Iteration: 14896, Training Loss: -1.000e-07 , Validation Loss: 1.524e+00\n",
      "Iteration: 14897, Training Loss: -1.000e-07 , Validation Loss: 1.524e+00\n",
      "Iteration: 14898, Training Loss: -1.000e-07 , Validation Loss: 1.524e+00\n",
      "Iteration: 14899, Training Loss: 1.259e+00 , Validation Loss: 1.524e+00\n",
      "Iteration: 14900, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 14901, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 14902, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 14903, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14904, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 14905, Training Loss: 1.007e+00 , Validation Loss: 1.337e+00\n",
      "Iteration: 14906, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 14907, Training Loss: 1.259e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 14908, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14909, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 14910, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 14911, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14912, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 14913, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 14914, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14915, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 14916, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 14917, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 14918, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14919, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14920, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14921, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14922, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14923, Training Loss: 1.007e+00 , Validation Loss: 5.504e-01\n",
      "Iteration: 14924, Training Loss: 2.015e+00 , Validation Loss: 4.088e+00\n",
      "Iteration: 14925, Training Loss: 1.007e+00 , Validation Loss: 1.228e+00\n",
      "Iteration: 14926, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 14927, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14928, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14929, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 14930, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 14931, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14932, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14933, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14934, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14935, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 14936, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14937, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14938, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14939, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 14940, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14941, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14942, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14943, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 14944, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14945, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14946, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14947, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14948, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14949, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14950, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 14951, Training Loss: 4.030e+00 , Validation Loss: 3.949e+00\n",
      "Iteration: 14952, Training Loss: 8.815e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 14953, Training Loss: 2.518e+00 , Validation Loss: 4.463e+00\n",
      "Iteration: 14954, Training Loss: 2.267e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 14955, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14956, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14957, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14958, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14959, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14960, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14961, Training Loss: 5.037e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 14962, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14963, Training Loss: 2.015e+00 , Validation Loss: 2.667e+00\n",
      "Iteration: 14964, Training Loss: 1.511e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 14965, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 14966, Training Loss: 7.555e-01 , Validation Loss: 1.252e+00\n",
      "Iteration: 14967, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14968, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14969, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 14970, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14971, Training Loss: 2.267e+00 , Validation Loss: 3.925e+00\n",
      "Iteration: 14972, Training Loss: 2.267e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 14973, Training Loss: 2.518e+00 , Validation Loss: 4.717e+00\n",
      "Iteration: 14974, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 14975, Training Loss: 1.763e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 14976, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14977, Training Loss: 7.555e-01 , Validation Loss: 1.004e+00\n",
      "Iteration: 14978, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 14979, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 14980, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14981, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14982, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14983, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14984, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14985, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 14986, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14987, Training Loss: 3.274e+00 , Validation Loss: 3.810e+00\n",
      "Iteration: 14988, Training Loss: 2.015e+00 , Validation Loss: 1.857e+00\n",
      "Iteration: 14989, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14990, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 14991, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14992, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14993, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14994, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 14995, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 14996, Training Loss: 1.007e+00 , Validation Loss: 1.954e+00\n",
      "Iteration: 14997, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 14998, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 14999, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15000, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15001, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15002, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15003, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15004, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15005, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15006, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 15007, Training Loss: 4.030e+00 , Validation Loss: 4.137e+00\n",
      "Iteration: 15008, Training Loss: 2.015e+00 , Validation Loss: 4.542e+00\n",
      "Iteration: 15009, Training Loss: 2.015e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 15010, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15011, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15012, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15013, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15014, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15015, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15016, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 15017, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15018, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15019, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 15020, Training Loss: 1.259e+00 , Validation Loss: 3.720e+00\n",
      "Iteration: 15021, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 15022, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 15023, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15024, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15025, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 15026, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15027, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 15028, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 15029, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 15030, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15031, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 15032, Training Loss: 5.037e-01 , Validation Loss: 1.270e+00\n",
      "Iteration: 15033, Training Loss: 5.037e-01 , Validation Loss: 1.869e+00\n",
      "Iteration: 15034, Training Loss: 7.555e-01 , Validation Loss: 2.649e+00\n",
      "Iteration: 15035, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15036, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15037, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15038, Training Loss: 5.037e-01 , Validation Loss: 8.951e-01\n",
      "Iteration: 15039, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15040, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 15041, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15042, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 15043, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 15044, Training Loss: 2.518e-01 , Validation Loss: 9.677e-01\n",
      "Iteration: 15045, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 15046, Training Loss: 7.555e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 15047, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15048, Training Loss: 2.518e+00 , Validation Loss: 3.847e+00\n",
      "Iteration: 15049, Training Loss: 7.555e-01 , Validation Loss: 1.651e+00\n",
      "Iteration: 15050, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 15051, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 15052, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15053, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15054, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15055, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15056, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15057, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15058, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15059, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15060, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15061, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15062, Training Loss: 1.007e+00 , Validation Loss: 8.528e-01\n",
      "Iteration: 15063, Training Loss: 2.015e+00 , Validation Loss: 3.635e+00\n",
      "Iteration: 15064, Training Loss: 1.007e+00 , Validation Loss: 1.421e+00\n",
      "Iteration: 15065, Training Loss: 1.511e+00 , Validation Loss: 8.951e-01\n",
      "Iteration: 15066, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 15067, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 15068, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 15069, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 15070, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 15071, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15072, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15073, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 15074, Training Loss: 5.037e-01 , Validation Loss: 1.839e+00\n",
      "Iteration: 15075, Training Loss: 1.763e+00 , Validation Loss: 3.532e+00\n",
      "Iteration: 15076, Training Loss: 1.007e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 15077, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 15078, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15079, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15080, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15081, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 15082, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 15083, Training Loss: 1.259e+00 , Validation Loss: 2.377e+00\n",
      "Iteration: 15084, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 15085, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15086, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 15087, Training Loss: 3.274e+00 , Validation Loss: 3.641e+00\n",
      "Iteration: 15088, Training Loss: 2.770e+00 , Validation Loss: 3.169e+00\n",
      "Iteration: 15089, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 15090, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 15091, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 15092, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 15093, Training Loss: 1.259e+00 , Validation Loss: 1.113e+00\n",
      "Iteration: 15094, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15095, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15096, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15097, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15098, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15099, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 15100, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15101, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15102, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15103, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15104, Training Loss: -1.000e-07 , Validation Loss: 6.834e-01\n",
      "Iteration: 15105, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 15106, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15107, Training Loss: 1.763e+00 , Validation Loss: 3.774e+00\n",
      "Iteration: 15108, Training Loss: -1.000e-07 , Validation Loss: 8.830e-01\n",
      "Iteration: 15109, Training Loss: 1.259e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 15110, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 15111, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15112, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15113, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 15114, Training Loss: 1.511e+00 , Validation Loss: 3.810e+00\n",
      "Iteration: 15115, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15116, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15117, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15118, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15119, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15120, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15121, Training Loss: 1.259e+00 , Validation Loss: 1.234e+00\n",
      "Iteration: 15122, Training Loss: 5.037e-01 , Validation Loss: 1.312e+00\n",
      "Iteration: 15123, Training Loss: 3.022e+00 , Validation Loss: 5.177e+00\n",
      "Iteration: 15124, Training Loss: 1.259e+00 , Validation Loss: 1.361e+00\n",
      "Iteration: 15125, Training Loss: 1.259e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 15126, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15127, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15128, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15129, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15130, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15131, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 15132, Training Loss: 2.267e+00 , Validation Loss: 4.222e+00\n",
      "Iteration: 15133, Training Loss: 1.259e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 15134, Training Loss: 1.511e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 15135, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15136, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15137, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15138, Training Loss: 5.037e-01 , Validation Loss: 1.343e+00\n",
      "Iteration: 15139, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15140, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 15141, Training Loss: 2.518e+00 , Validation Loss: 3.593e+00\n",
      "Iteration: 15142, Training Loss: 2.015e+00 , Validation Loss: 1.482e+00\n",
      "Iteration: 15143, Training Loss: 1.259e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 15144, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15145, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15146, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 15147, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 15148, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15149, Training Loss: 1.511e+00 , Validation Loss: 3.248e+00\n",
      "Iteration: 15150, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15151, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 15152, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15153, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 15154, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 15155, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 15156, Training Loss: 1.259e+00 , Validation Loss: 1.736e+00\n",
      "Iteration: 15157, Training Loss: 5.037e-01 , Validation Loss: 1.760e+00\n",
      "Iteration: 15158, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15159, Training Loss: 2.518e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 15160, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 15161, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 15162, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 15163, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 15164, Training Loss: 5.037e-01 , Validation Loss: 2.159e+00\n",
      "Iteration: 15165, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15166, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15167, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15168, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15169, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15170, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15171, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15172, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15173, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 15174, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15175, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 15176, Training Loss: 1.007e+00 , Validation Loss: 4.046e+00\n",
      "Iteration: 15177, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15178, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15179, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15180, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15181, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 15182, Training Loss: 1.763e+00 , Validation Loss: 8.528e-01\n",
      "Iteration: 15183, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15184, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15185, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15186, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15187, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15188, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15189, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15190, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15191, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15192, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 15193, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 15194, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 15195, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15196, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15197, Training Loss: 1.259e+00 , Validation Loss: 2.921e+00\n",
      "Iteration: 15198, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 15199, Training Loss: 7.555e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 15200, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 15201, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 15202, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 15203, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15204, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15205, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 15206, Training Loss: 1.007e+00 , Validation Loss: 2.044e+00\n",
      "Iteration: 15207, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 15208, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15209, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15210, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15211, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15212, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15213, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 15214, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15215, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15216, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 15217, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 15218, Training Loss: 2.518e-01 , Validation Loss: 2.171e+00\n",
      "Iteration: 15219, Training Loss: 2.770e+00 , Validation Loss: 4.330e+00\n",
      "Iteration: 15220, Training Loss: 1.511e+00 , Validation Loss: 1.210e+00\n",
      "Iteration: 15221, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15222, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 15223, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15224, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 15225, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 15226, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15227, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15228, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 15229, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15230, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15231, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 15232, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15233, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 15234, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 15235, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15236, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15237, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15238, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15239, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15240, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 15241, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 15242, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 15243, Training Loss: 1.259e+00 , Validation Loss: 3.762e+00\n",
      "Iteration: 15244, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 15245, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15246, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15247, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15248, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15249, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15250, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 15251, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15252, Training Loss: 1.259e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 15253, Training Loss: 5.037e+00 , Validation Loss: 4.246e+00\n",
      "Iteration: 15254, Training Loss: 1.184e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 15255, Training Loss: 1.007e+00 , Validation Loss: 1.996e+00\n",
      "Iteration: 15256, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 15257, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 15258, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 15259, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15260, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15261, Training Loss: 1.763e+00 , Validation Loss: 1.984e+00\n",
      "Iteration: 15262, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 15263, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15264, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15265, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15266, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15267, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15268, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15269, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15270, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15271, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15272, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15273, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15274, Training Loss: 2.518e-01 , Validation Loss: 1.264e+00\n",
      "Iteration: 15275, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 15276, Training Loss: -1.000e-07 , Validation Loss: 1.597e+00\n",
      "Iteration: 15277, Training Loss: 5.037e-01 , Validation Loss: 1.597e+00\n",
      "Iteration: 15278, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15279, Training Loss: 1.007e+00 , Validation Loss: 1.300e+00\n",
      "Iteration: 15280, Training Loss: 2.267e+00 , Validation Loss: 3.296e+00\n",
      "Iteration: 15281, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15282, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 15283, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15284, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15285, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15286, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 15287, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15288, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15289, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15290, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15291, Training Loss: 1.007e+00 , Validation Loss: 1.101e+00\n",
      "Iteration: 15292, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15293, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15294, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15295, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15296, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15297, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 15298, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 15299, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15300, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15301, Training Loss: 1.007e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 15302, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15303, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15304, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15305, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15306, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15307, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15308, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15309, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15310, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15311, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15312, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15313, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 15314, Training Loss: -1.000e-07 , Validation Loss: 5.262e-01\n",
      "Iteration: 15315, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 15316, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 15317, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15318, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 15319, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 15320, Training Loss: 5.037e-01 , Validation Loss: 1.488e+00\n",
      "Iteration: 15321, Training Loss: 2.518e+00 , Validation Loss: 3.544e+00\n",
      "Iteration: 15322, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 15323, Training Loss: 1.511e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 15324, Training Loss: 5.037e-01 , Validation Loss: 1.597e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15325, Training Loss: 7.555e-01 , Validation Loss: 2.891e+00\n",
      "Iteration: 15326, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15327, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 15328, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 15329, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 15330, Training Loss: 7.555e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 15331, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 15332, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15333, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15334, Training Loss: 7.555e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 15335, Training Loss: 2.518e-01 , Validation Loss: 1.947e+00\n",
      "Iteration: 15336, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 15337, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 15338, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15339, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 15340, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 15341, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15342, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15343, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15344, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 15345, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 15346, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15347, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 15348, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 15349, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 15350, Training Loss: 7.555e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 15351, Training Loss: 2.267e+00 , Validation Loss: 5.074e+00\n",
      "Iteration: 15352, Training Loss: 1.007e+00 , Validation Loss: 1.246e+00\n",
      "Iteration: 15353, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 15354, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15355, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15356, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 15357, Training Loss: 1.007e+00 , Validation Loss: 3.072e+00\n",
      "Iteration: 15358, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15359, Training Loss: 7.555e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 15360, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 15361, Training Loss: -1.000e-07 , Validation Loss: 7.802e-01\n",
      "Iteration: 15362, Training Loss: -1.000e-07 , Validation Loss: 7.802e-01\n",
      "Iteration: 15363, Training Loss: 7.555e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 15364, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 15365, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 15366, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 15367, Training Loss: 2.267e+00 , Validation Loss: 3.792e+00\n",
      "Iteration: 15368, Training Loss: 7.555e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 15369, Training Loss: 1.511e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 15370, Training Loss: 2.518e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 15371, Training Loss: 3.274e+00 , Validation Loss: 3.895e+00\n",
      "Iteration: 15372, Training Loss: 1.259e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 15373, Training Loss: 1.007e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 15374, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15375, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15376, Training Loss: -1.000e-07 , Validation Loss: 4.899e-01\n",
      "Iteration: 15377, Training Loss: 5.037e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 15378, Training Loss: 5.037e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 15379, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15380, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 15381, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15382, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15383, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 15384, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15385, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 15386, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15387, Training Loss: 7.555e-01 , Validation Loss: 2.195e+00\n",
      "Iteration: 15388, Training Loss: 3.022e+00 , Validation Loss: 4.246e+00\n",
      "Iteration: 15389, Training Loss: 2.015e+00 , Validation Loss: 2.020e+00\n",
      "Iteration: 15390, Training Loss: 1.259e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 15391, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15392, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15393, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15394, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15395, Training Loss: 1.259e+00 , Validation Loss: 1.808e+00\n",
      "Iteration: 15396, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15397, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15398, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15399, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15400, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15401, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15402, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15403, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15404, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15405, Training Loss: 2.518e-01 , Validation Loss: 1.282e+00\n",
      "Iteration: 15406, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15407, Training Loss: 2.770e+00 , Validation Loss: 4.167e+00\n",
      "Iteration: 15408, Training Loss: 1.511e+00 , Validation Loss: 1.784e+00\n",
      "Iteration: 15409, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 15410, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15411, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 15412, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15413, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15414, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15415, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15416, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 15417, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 15418, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 15419, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 15420, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15421, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15422, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 15423, Training Loss: 2.518e-01 , Validation Loss: 1.409e+00\n",
      "Iteration: 15424, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 15425, Training Loss: 5.037e-01 , Validation Loss: 4.046e+00\n",
      "Iteration: 15426, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15427, Training Loss: 7.555e-01 , Validation Loss: 1.222e+00\n",
      "Iteration: 15428, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 15429, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 15430, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15431, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15432, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15433, Training Loss: 7.555e-01 , Validation Loss: 9.314e-01\n",
      "Iteration: 15434, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15435, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 15436, Training Loss: 5.037e-01 , Validation Loss: 1.312e+00\n",
      "Iteration: 15437, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15438, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15439, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15440, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15441, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15442, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15443, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 15444, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 15445, Training Loss: 2.518e+00 , Validation Loss: 5.473e+00\n",
      "Iteration: 15446, Training Loss: 2.518e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 15447, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 15448, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15449, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15450, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15451, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15452, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15453, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15454, Training Loss: 5.037e-01 , Validation Loss: 1.264e+00\n",
      "Iteration: 15455, Training Loss: 1.007e+00 , Validation Loss: 3.048e+00\n",
      "Iteration: 15456, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15457, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15458, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15459, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15460, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15461, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 15462, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 15463, Training Loss: 7.555e-01 , Validation Loss: 2.528e+00\n",
      "Iteration: 15464, Training Loss: 1.259e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 15465, Training Loss: 1.259e+00 , Validation Loss: 1.452e+00\n",
      "Iteration: 15466, Training Loss: 1.259e+00 , Validation Loss: 1.179e+00\n",
      "Iteration: 15467, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 15468, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 15469, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 15470, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 15471, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 15472, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 15473, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15474, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15475, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15476, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15477, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 15478, Training Loss: 1.763e+00 , Validation Loss: 4.439e+00\n",
      "Iteration: 15479, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 15480, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 15481, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15482, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15483, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 15484, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 15485, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 15486, Training Loss: 1.259e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 15487, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 15488, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 15489, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 15490, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15491, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15492, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15493, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15494, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15495, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15496, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15497, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 15498, Training Loss: 2.518e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 15499, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 15500, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 15501, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 15502, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 15503, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 15504, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 15505, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15506, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15507, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15508, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15509, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15510, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15511, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 15512, Training Loss: -1.000e-07 , Validation Loss: 7.379e-01\n",
      "Iteration: 15513, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 15514, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 15515, Training Loss: 1.511e+00 , Validation Loss: 2.546e+00\n",
      "Iteration: 15516, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15517, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 15518, Training Loss: 5.037e-01 , Validation Loss: 2.667e+00\n",
      "Iteration: 15519, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 15520, Training Loss: 3.022e+00 , Validation Loss: 5.413e+00\n",
      "Iteration: 15521, Training Loss: 1.259e+00 , Validation Loss: 1.748e+00\n",
      "Iteration: 15522, Training Loss: 1.007e+00 , Validation Loss: 6.653e-01\n",
      "Iteration: 15523, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 15524, Training Loss: 1.259e+00 , Validation Loss: 3.714e+00\n",
      "Iteration: 15525, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15526, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15527, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 15528, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 15529, Training Loss: 1.259e+00 , Validation Loss: 8.225e-01\n",
      "Iteration: 15530, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15531, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15532, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15533, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 15534, Training Loss: 1.007e+00 , Validation Loss: 8.467e-01\n",
      "Iteration: 15535, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 15536, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 15537, Training Loss: 7.555e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 15538, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 15539, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15540, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 15541, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15542, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 15543, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 15544, Training Loss: 1.007e+00 , Validation Loss: 1.391e+00\n",
      "Iteration: 15545, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 15546, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 15547, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 15548, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15549, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 15550, Training Loss: 5.037e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 15551, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 15552, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15553, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15554, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15555, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 15556, Training Loss: 1.007e+00 , Validation Loss: 2.105e+00\n",
      "Iteration: 15557, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 15558, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 15559, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 15560, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 15561, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 15562, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15563, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15564, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15565, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 15566, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15567, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 15568, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 15569, Training Loss: 2.015e+00 , Validation Loss: 3.568e+00\n",
      "Iteration: 15570, Training Loss: 1.763e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 15571, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 15572, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 15573, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 15574, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15575, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15576, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 15577, Training Loss: 2.518e-01 , Validation Loss: 1.204e+00\n",
      "Iteration: 15578, Training Loss: 2.518e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 15579, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 15580, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 15581, Training Loss: 7.555e-01 , Validation Loss: 2.740e+00\n",
      "Iteration: 15582, Training Loss: 2.518e-01 , Validation Loss: 1.149e+00\n",
      "Iteration: 15583, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 15584, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 15585, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15586, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 15587, Training Loss: 1.511e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 15588, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 15589, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 15590, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 15591, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 15592, Training Loss: 1.763e+00 , Validation Loss: 3.695e+00\n",
      "Iteration: 15593, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15594, Training Loss: 2.518e-01 , Validation Loss: 1.857e+00\n",
      "Iteration: 15595, Training Loss: 2.267e+00 , Validation Loss: 4.379e+00\n",
      "Iteration: 15596, Training Loss: 1.007e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 15597, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15598, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 15599, Training Loss: -1.000e-07 , Validation Loss: 9.677e-01\n",
      "Iteration: 15600, Training Loss: 7.555e-01 , Validation Loss: 9.677e-01\n",
      "Iteration: 15601, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 15602, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 15603, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15604, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 15605, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15606, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 15607, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 15608, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 15609, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15610, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15611, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 15612, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15613, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15614, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 15615, Training Loss: -1.000e-07 , Validation Loss: 6.774e-01\n",
      "Iteration: 15616, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 15617, Training Loss: 1.259e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 15618, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 15619, Training Loss: 7.555e-01 , Validation Loss: 1.905e+00\n",
      "Iteration: 15620, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15621, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 15622, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15623, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 15624, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15625, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 15626, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15627, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15628, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15629, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15630, Training Loss: 2.518e-01 , Validation Loss: 1.252e+00\n",
      "Iteration: 15631, Training Loss: 2.770e+00 , Validation Loss: 3.986e+00\n",
      "Iteration: 15632, Training Loss: 1.763e+00 , Validation Loss: 1.536e+00\n",
      "Iteration: 15633, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 15634, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15635, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15636, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 15637, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15638, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15639, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15640, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15641, Training Loss: 2.267e+00 , Validation Loss: 4.125e+00\n",
      "Iteration: 15642, Training Loss: 2.015e+00 , Validation Loss: 2.540e+00\n",
      "Iteration: 15643, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 15644, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 15645, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 15646, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15647, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15648, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15649, Training Loss: 5.037e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 15650, Training Loss: 7.555e-01 , Validation Loss: 1.996e+00\n",
      "Iteration: 15651, Training Loss: 1.259e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 15652, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15653, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15654, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15655, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15656, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15657, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15658, Training Loss: 1.007e+00 , Validation Loss: 2.341e+00\n",
      "Iteration: 15659, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 15660, Training Loss: 7.555e-01 , Validation Loss: 1.204e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15661, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 15662, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 15663, Training Loss: 1.007e+00 , Validation Loss: 1.536e+00\n",
      "Iteration: 15664, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15665, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 15666, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 15667, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 15668, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15669, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 15670, Training Loss: 3.022e+00 , Validation Loss: 5.207e+00\n",
      "Iteration: 15671, Training Loss: 1.511e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 15672, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15673, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15674, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15675, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15676, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15677, Training Loss: 1.259e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 15678, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 15679, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 15680, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15681, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 15682, Training Loss: 7.555e-01 , Validation Loss: 3.574e+00\n",
      "Iteration: 15683, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15684, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15685, Training Loss: -1.000e-07 , Validation Loss: 5.080e-01\n",
      "Iteration: 15686, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 15687, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15688, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 15689, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 15690, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15691, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15692, Training Loss: 5.037e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 15693, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15694, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15695, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15696, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15697, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 15698, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15699, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15700, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15701, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 15702, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 15703, Training Loss: 1.007e+00 , Validation Loss: 5.080e-01\n",
      "Iteration: 15704, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 15705, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 15706, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 15707, Training Loss: 1.259e+00 , Validation Loss: 2.395e+00\n",
      "Iteration: 15708, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15709, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15710, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15711, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15712, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 15713, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15714, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15715, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15716, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15717, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 15718, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 15719, Training Loss: 7.555e-01 , Validation Loss: 3.054e+00\n",
      "Iteration: 15720, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15721, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15722, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15723, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 15724, Training Loss: -1.000e-07 , Validation Loss: 7.923e-01\n",
      "Iteration: 15725, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 15726, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15727, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15728, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 15729, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15730, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 15731, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 15732, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15733, Training Loss: -1.000e-07 , Validation Loss: 7.621e-01\n",
      "Iteration: 15734, Training Loss: 7.555e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 15735, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15736, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15737, Training Loss: 1.511e+00 , Validation Loss: 3.804e+00\n",
      "Iteration: 15738, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15739, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15740, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15741, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15742, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15743, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15744, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15745, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15746, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15747, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15748, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15749, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15750, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15751, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15752, Training Loss: 7.555e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 15753, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15754, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15755, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15756, Training Loss: 1.007e+00 , Validation Loss: 1.161e+00\n",
      "Iteration: 15757, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 15758, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 15759, Training Loss: 5.037e-01 , Validation Loss: 1.167e+00\n",
      "Iteration: 15760, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15761, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 15762, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15763, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15764, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15765, Training Loss: 3.274e+00 , Validation Loss: 4.167e+00\n",
      "Iteration: 15766, Training Loss: 3.274e+00 , Validation Loss: 1.984e+00\n",
      "Iteration: 15767, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15768, Training Loss: 1.259e+00 , Validation Loss: 4.476e-01\n",
      "Iteration: 15769, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 15770, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15771, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15772, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15773, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15774, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15775, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 15776, Training Loss: 7.555e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 15777, Training Loss: 1.511e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 15778, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 15779, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15780, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15781, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15782, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 15783, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 15784, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15785, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15786, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15787, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 15788, Training Loss: 2.518e-01 , Validation Loss: 1.010e+00\n",
      "Iteration: 15789, Training Loss: 1.007e+00 , Validation Loss: 3.472e+00\n",
      "Iteration: 15790, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 15791, Training Loss: 1.007e+00 , Validation Loss: 6.532e-01\n",
      "Iteration: 15792, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15793, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 15794, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 15795, Training Loss: 7.555e-01 , Validation Loss: 1.379e+00\n",
      "Iteration: 15796, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15797, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15798, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15799, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15800, Training Loss: 2.518e-01 , Validation Loss: 9.072e-01\n",
      "Iteration: 15801, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 15802, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 15803, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 15804, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15805, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 15806, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15807, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15808, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15809, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15810, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15811, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 15812, Training Loss: 1.007e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 15813, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15814, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15815, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15816, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15817, Training Loss: 2.518e-01 , Validation Loss: 1.010e+00\n",
      "Iteration: 15818, Training Loss: 1.259e+00 , Validation Loss: 2.377e+00\n",
      "Iteration: 15819, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15820, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 15821, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15822, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 15823, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15824, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15825, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 15826, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 15827, Training Loss: 5.037e-01 , Validation Loss: 1.173e+00\n",
      "Iteration: 15828, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 15829, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 15830, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15831, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15832, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 15833, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15834, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15835, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15836, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 15837, Training Loss: 1.259e+00 , Validation Loss: 2.256e+00\n",
      "Iteration: 15838, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15839, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 15840, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 15841, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 15842, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 15843, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 15844, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 15845, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 15846, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 15847, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15848, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15849, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 15850, Training Loss: 1.511e+00 , Validation Loss: 2.540e+00\n",
      "Iteration: 15851, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15852, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15853, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15854, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15855, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15856, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15857, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15858, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15859, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15860, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15861, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15862, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 15863, Training Loss: 3.778e+00 , Validation Loss: 4.173e+00\n",
      "Iteration: 15864, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 15865, Training Loss: 3.778e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 15866, Training Loss: 1.259e+00 , Validation Loss: 1.143e+00\n",
      "Iteration: 15867, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 15868, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15869, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 15870, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15871, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15872, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 15873, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15874, Training Loss: 7.555e-01 , Validation Loss: 2.189e+00\n",
      "Iteration: 15875, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15876, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15877, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15878, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15879, Training Loss: 1.763e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 15880, Training Loss: 2.267e+00 , Validation Loss: 4.584e+00\n",
      "Iteration: 15881, Training Loss: 1.259e+00 , Validation Loss: 1.204e+00\n",
      "Iteration: 15882, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 15883, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15884, Training Loss: 1.259e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 15885, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15886, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 15887, Training Loss: 2.015e+00 , Validation Loss: 4.131e+00\n",
      "Iteration: 15888, Training Loss: 1.511e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 15889, Training Loss: 1.259e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 15890, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15891, Training Loss: 1.259e+00 , Validation Loss: 1.494e+00\n",
      "Iteration: 15892, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 15893, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15894, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15895, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15896, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15897, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15898, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15899, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15900, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15901, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15902, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15903, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15904, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 15905, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 15906, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15907, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15908, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15909, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15910, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 15911, Training Loss: 1.763e+00 , Validation Loss: 2.201e+00\n",
      "Iteration: 15912, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 15913, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15914, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15915, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15916, Training Loss: 7.555e-01 , Validation Loss: 3.459e+00\n",
      "Iteration: 15917, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15918, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15919, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 15920, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15921, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 15922, Training Loss: 1.007e+00 , Validation Loss: 2.395e+00\n",
      "Iteration: 15923, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15924, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 15925, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15926, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15927, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15928, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15929, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 15930, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15931, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 15932, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15933, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 15934, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15935, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15936, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15937, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15938, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 15939, Training Loss: 7.555e-01 , Validation Loss: 2.244e+00\n",
      "Iteration: 15940, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 15941, Training Loss: 3.778e+00 , Validation Loss: 4.349e+00\n",
      "Iteration: 15942, Training Loss: 5.541e+00 , Validation Loss: 5.086e+00\n",
      "Iteration: 15943, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 15944, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15945, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15946, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15947, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15948, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15949, Training Loss: -1.000e-07 , Validation Loss: 6.774e-01\n",
      "Iteration: 15950, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 15951, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15952, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 15953, Training Loss: 1.259e+00 , Validation Loss: 2.758e+00\n",
      "Iteration: 15954, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 15955, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 15956, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15957, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15958, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15959, Training Loss: 5.037e-01 , Validation Loss: 1.337e+00\n",
      "Iteration: 15960, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15961, Training Loss: 2.015e+00 , Validation Loss: 3.853e+00\n",
      "Iteration: 15962, Training Loss: 7.555e-01 , Validation Loss: 1.004e+00\n",
      "Iteration: 15963, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 15964, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 15965, Training Loss: 5.037e-01 , Validation Loss: 1.724e+00\n",
      "Iteration: 15966, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15967, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15968, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15969, Training Loss: 2.015e+00 , Validation Loss: 2.740e+00\n",
      "Iteration: 15970, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 15971, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15972, Training Loss: 7.555e-01 , Validation Loss: 1.204e+00\n",
      "Iteration: 15973, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15974, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15975, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15976, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15977, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15978, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15979, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 15980, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 15981, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15982, Training Loss: 5.037e-01 , Validation Loss: 1.935e+00\n",
      "Iteration: 15983, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 15984, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 15985, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 15986, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 15987, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15988, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15989, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15990, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15991, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 15992, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15993, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 15994, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15995, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 15996, Training Loss: 7.555e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 15997, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 15998, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 15999, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16000, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16001, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16002, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 16003, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 16004, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16005, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16006, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 16007, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16008, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 16009, Training Loss: 7.555e-01 , Validation Loss: 1.010e+00\n",
      "Iteration: 16010, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 16011, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 16012, Training Loss: 7.555e-01 , Validation Loss: 1.270e+00\n",
      "Iteration: 16013, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 16014, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16015, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16016, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16017, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 16018, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16019, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16020, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 16021, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 16022, Training Loss: 7.555e-01 , Validation Loss: 1.385e+00\n",
      "Iteration: 16023, Training Loss: 5.037e-01 , Validation Loss: 1.198e+00\n",
      "Iteration: 16024, Training Loss: 2.518e-01 , Validation Loss: 1.917e+00\n",
      "Iteration: 16025, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 16026, Training Loss: 5.037e-01 , Validation Loss: 1.439e+00\n",
      "Iteration: 16027, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16028, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16029, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16030, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16031, Training Loss: 1.763e+00 , Validation Loss: 2.274e+00\n",
      "Iteration: 16032, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 16033, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 16034, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16035, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16036, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16037, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 16038, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16039, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 16040, Training Loss: 5.037e-01 , Validation Loss: 1.331e+00\n",
      "Iteration: 16041, Training Loss: 1.511e+00 , Validation Loss: 3.345e+00\n",
      "Iteration: 16042, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16043, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 16044, Training Loss: 5.037e-01 , Validation Loss: 2.135e+00\n",
      "Iteration: 16045, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 16046, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16047, Training Loss: 1.007e+00 , Validation Loss: 1.597e+00\n",
      "Iteration: 16048, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 16049, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16050, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16051, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 16052, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16053, Training Loss: 5.037e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 16054, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16055, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16056, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16057, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16058, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16059, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 16060, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16061, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16062, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16063, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16064, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16065, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16066, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 16067, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16068, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 16069, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16070, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16071, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 16072, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16073, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16074, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16075, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 16076, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16077, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16078, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16079, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16080, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16081, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16082, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 16083, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 16084, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 16085, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 16086, Training Loss: 1.007e+00 , Validation Loss: 1.010e+00\n",
      "Iteration: 16087, Training Loss: 1.007e+00 , Validation Loss: 1.941e+00\n",
      "Iteration: 16088, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16089, Training Loss: 1.007e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 16090, Training Loss: 2.267e+00 , Validation Loss: 3.187e+00\n",
      "Iteration: 16091, Training Loss: 1.511e+00 , Validation Loss: 9.979e-01\n",
      "Iteration: 16092, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 16093, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16094, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16095, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16096, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 16097, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 16098, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16099, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 16100, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16101, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16102, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 16103, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16104, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16105, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16106, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16107, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16108, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16109, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 16110, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 16111, Training Loss: 1.259e+00 , Validation Loss: 1.941e+00\n",
      "Iteration: 16112, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 16113, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 16114, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16115, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16116, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16117, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 16118, Training Loss: 1.511e+00 , Validation Loss: 1.621e+00\n",
      "Iteration: 16119, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 16120, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16121, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16122, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 16123, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16124, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16125, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 16126, Training Loss: 1.511e+00 , Validation Loss: 1.954e+00\n",
      "Iteration: 16127, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 16128, Training Loss: 4.030e+00 , Validation Loss: 5.153e+00\n",
      "Iteration: 16129, Training Loss: 3.022e+00 , Validation Loss: 3.526e+00\n",
      "Iteration: 16130, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16131, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16132, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16133, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16134, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16135, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 16136, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16137, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16138, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16139, Training Loss: 1.763e+00 , Validation Loss: 3.998e+00\n",
      "Iteration: 16140, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 16141, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 16142, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 16143, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16144, Training Loss: 2.518e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 16145, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 16146, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16147, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16148, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16149, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 16150, Training Loss: 3.022e+00 , Validation Loss: 5.056e+00\n",
      "Iteration: 16151, Training Loss: 1.511e+00 , Validation Loss: 1.071e+00\n",
      "Iteration: 16152, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16153, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 16154, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16155, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16156, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16157, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 16158, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16159, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 16160, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 16161, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 16162, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 16163, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16164, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16165, Training Loss: 5.037e-01 , Validation Loss: 1.264e+00\n",
      "Iteration: 16166, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16167, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 16168, Training Loss: 7.555e-01 , Validation Loss: 2.074e+00\n",
      "Iteration: 16169, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16170, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16171, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 16172, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 16173, Training Loss: 1.259e+00 , Validation Loss: 1.095e+00\n",
      "Iteration: 16174, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 16175, Training Loss: 1.511e+00 , Validation Loss: 2.885e+00\n",
      "Iteration: 16176, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 16177, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 16178, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 16179, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 16180, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 16181, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16182, Training Loss: 1.259e+00 , Validation Loss: 5.504e-01\n",
      "Iteration: 16183, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 16184, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16185, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 16186, Training Loss: 1.259e+00 , Validation Loss: 4.234e-01\n",
      "Iteration: 16187, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16188, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16189, Training Loss: 7.555e-01 , Validation Loss: 1.131e+00\n",
      "Iteration: 16190, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16191, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16192, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16193, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16194, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16195, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16196, Training Loss: 7.555e-01 , Validation Loss: 1.869e+00\n",
      "Iteration: 16197, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16198, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16199, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16200, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16201, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 16202, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 16203, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 16204, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16205, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16206, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 16207, Training Loss: 7.555e-01 , Validation Loss: 1.137e+00\n",
      "Iteration: 16208, Training Loss: 4.281e+00 , Validation Loss: 4.209e+00\n",
      "Iteration: 16209, Training Loss: 8.815e+00 , Validation Loss: 8.304e+00\n",
      "Iteration: 16210, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16211, Training Loss: 1.511e+00 , Validation Loss: 2.607e+00\n",
      "Iteration: 16212, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16213, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16214, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16215, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16216, Training Loss: 2.015e+00 , Validation Loss: 2.885e+00\n",
      "Iteration: 16217, Training Loss: 1.259e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 16218, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16219, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 16220, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16221, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16222, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16223, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16224, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 16225, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 16226, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16227, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16228, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16229, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16230, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16231, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16232, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 16233, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 16234, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16235, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 16236, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 16237, Training Loss: 3.022e+00 , Validation Loss: 4.917e+00\n",
      "Iteration: 16238, Training Loss: 2.770e+00 , Validation Loss: 1.355e+00\n",
      "Iteration: 16239, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16240, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16241, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16242, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16243, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16244, Training Loss: 2.267e+00 , Validation Loss: 3.260e+00\n",
      "Iteration: 16245, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16246, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16247, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 16248, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 16249, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16250, Training Loss: 1.259e+00 , Validation Loss: 2.322e+00\n",
      "Iteration: 16251, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16252, Training Loss: 1.259e+00 , Validation Loss: 2.189e+00\n",
      "Iteration: 16253, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 16254, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16255, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16256, Training Loss: 1.511e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 16257, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 16258, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16259, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16260, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16261, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 16262, Training Loss: 2.770e+00 , Validation Loss: 3.955e+00\n",
      "Iteration: 16263, Training Loss: 1.763e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 16264, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 16265, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16266, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16267, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16268, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16269, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16270, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16271, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 16272, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16273, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16274, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16275, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16276, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16277, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16278, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16279, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16280, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16281, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 16282, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16283, Training Loss: -1.000e-07 , Validation Loss: 7.742e-01\n",
      "Iteration: 16284, Training Loss: -1.000e-07 , Validation Loss: 7.742e-01\n",
      "Iteration: 16285, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 16286, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 16287, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16288, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16289, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16290, Training Loss: 7.555e-01 , Validation Loss: 1.155e+00\n",
      "Iteration: 16291, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16292, Training Loss: -1.000e-07 , Validation Loss: 6.713e-01\n",
      "Iteration: 16293, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 16294, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16295, Training Loss: 7.555e-01 , Validation Loss: 1.718e+00\n",
      "Iteration: 16296, Training Loss: 1.259e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 16297, Training Loss: 3.274e+00 , Validation Loss: 5.165e+00\n",
      "Iteration: 16298, Training Loss: 1.259e+00 , Validation Loss: 2.431e+00\n",
      "Iteration: 16299, Training Loss: 7.555e-01 , Validation Loss: 1.107e+00\n",
      "Iteration: 16300, Training Loss: 7.555e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 16301, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16302, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 16303, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16304, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 16305, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 16306, Training Loss: 1.259e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 16307, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16308, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16309, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16310, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 16311, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16312, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 16313, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 16314, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 16315, Training Loss: 4.281e+00 , Validation Loss: 4.258e+00\n",
      "Iteration: 16316, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 16317, Training Loss: 4.030e+00 , Validation Loss: 5.522e+00\n",
      "Iteration: 16318, Training Loss: 3.022e+00 , Validation Loss: 1.839e+00\n",
      "Iteration: 16319, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 16320, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 16321, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 16322, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16323, Training Loss: 2.015e+00 , Validation Loss: 4.488e+00\n",
      "Iteration: 16324, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 16325, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16326, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 16327, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16328, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16329, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16330, Training Loss: 1.007e+00 , Validation Loss: 9.133e-01\n",
      "Iteration: 16331, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 16332, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16333, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16334, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16335, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16336, Training Loss: 2.770e+00 , Validation Loss: 3.205e+00\n",
      "Iteration: 16337, Training Loss: 2.267e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 16338, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16339, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16340, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16341, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16342, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16343, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16344, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16345, Training Loss: 7.555e-01 , Validation Loss: 3.992e+00\n",
      "Iteration: 16346, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16347, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16348, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16349, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16350, Training Loss: 1.511e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 16351, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16352, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 16353, Training Loss: 1.007e+00 , Validation Loss: 3.345e+00\n",
      "Iteration: 16354, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16355, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16356, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16357, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16358, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 16359, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16360, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 16361, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16362, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 16363, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16364, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 16365, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16366, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16367, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16368, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16369, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16370, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16371, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 16372, Training Loss: 2.518e+00 , Validation Loss: 5.129e+00\n",
      "Iteration: 16373, Training Loss: 7.555e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 16374, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 16375, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 16376, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16377, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16378, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16379, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16380, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16381, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16382, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16383, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16384, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16385, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16386, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16387, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16388, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16389, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 16390, Training Loss: 1.259e+00 , Validation Loss: 4.415e-01\n",
      "Iteration: 16391, Training Loss: 7.555e-01 , Validation Loss: 3.157e+00\n",
      "Iteration: 16392, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16393, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16394, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 16395, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 16396, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16397, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16398, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16399, Training Loss: 5.037e-01 , Validation Loss: 3.012e+00\n",
      "Iteration: 16400, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 16401, Training Loss: 5.037e-01 , Validation Loss: 2.456e+00\n",
      "Iteration: 16402, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16403, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16404, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16405, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16406, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16407, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 16408, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16409, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16410, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 16411, Training Loss: 7.555e-01 , Validation Loss: 1.500e+00\n",
      "Iteration: 16412, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16413, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16414, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 16415, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 16416, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16417, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16418, Training Loss: 3.022e+00 , Validation Loss: 5.099e+00\n",
      "Iteration: 16419, Training Loss: 1.763e+00 , Validation Loss: 1.700e+00\n",
      "Iteration: 16420, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 16421, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 16422, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16423, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16424, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16425, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16426, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16427, Training Loss: 5.037e-01 , Validation Loss: 2.740e+00\n",
      "Iteration: 16428, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 16429, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16430, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 16431, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 16432, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16433, Training Loss: 1.007e+00 , Validation Loss: 2.020e+00\n",
      "Iteration: 16434, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16435, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16436, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16437, Training Loss: 7.555e-01 , Validation Loss: 1.155e+00\n",
      "Iteration: 16438, Training Loss: 1.763e+00 , Validation Loss: 5.195e+00\n",
      "Iteration: 16439, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 16440, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 16441, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16442, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 16443, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16444, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 16445, Training Loss: 7.555e-01 , Validation Loss: 2.189e+00\n",
      "Iteration: 16446, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16447, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 16448, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16449, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16450, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 16451, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 16452, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16453, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 16454, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 16455, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16456, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 16457, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 16458, Training Loss: 2.518e+00 , Validation Loss: 4.088e+00\n",
      "Iteration: 16459, Training Loss: 1.511e+00 , Validation Loss: 1.179e+00\n",
      "Iteration: 16460, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 16461, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16462, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 16463, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 16464, Training Loss: 2.518e-01 , Validation Loss: 1.246e+00\n",
      "Iteration: 16465, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 16466, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16467, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 16468, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16469, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 16470, Training Loss: 2.518e-01 , Validation Loss: 9.375e-01\n",
      "Iteration: 16471, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 16472, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 16473, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 16474, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 16475, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 16476, Training Loss: 1.511e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 16477, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 16478, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 16479, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16480, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16481, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16482, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16483, Training Loss: 1.007e+00 , Validation Loss: 2.673e+00\n",
      "Iteration: 16484, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 16485, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16486, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16487, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 16488, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16489, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16490, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16491, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16492, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16493, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16494, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16495, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16496, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 16497, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 16498, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16499, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16500, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 16501, Training Loss: 1.259e+00 , Validation Loss: 2.238e+00\n",
      "Iteration: 16502, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16503, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16504, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16505, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 16506, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16507, Training Loss: 2.267e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 16508, Training Loss: 5.037e-01 , Validation Loss: 1.004e+00\n",
      "Iteration: 16509, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 16510, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16511, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16512, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 16513, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16514, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16515, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16516, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 16517, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 16518, Training Loss: 7.555e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 16519, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16520, Training Loss: 5.037e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 16521, Training Loss: 1.007e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 16522, Training Loss: 1.259e+00 , Validation Loss: 2.685e+00\n",
      "Iteration: 16523, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 16524, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 16525, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 16526, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16527, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16528, Training Loss: 1.259e+00 , Validation Loss: 7.681e-01\n",
      "Iteration: 16529, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16530, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16531, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 16532, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16533, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16534, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16535, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16536, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16537, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 16538, Training Loss: 5.037e-01 , Validation Loss: 1.657e+00\n",
      "Iteration: 16539, Training Loss: 1.007e+00 , Validation Loss: 1.966e+00\n",
      "Iteration: 16540, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16541, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16542, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16543, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 16544, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16545, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16546, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16547, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16548, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16549, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16550, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16551, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 16552, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16553, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 16554, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16555, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 16556, Training Loss: 2.770e+00 , Validation Loss: 4.796e+00\n",
      "Iteration: 16557, Training Loss: 2.267e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 16558, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 16559, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 16560, Training Loss: 7.555e-01 , Validation Loss: 1.669e+00\n",
      "Iteration: 16561, Training Loss: 5.037e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 16562, Training Loss: 7.555e-01 , Validation Loss: 1.857e+00\n",
      "Iteration: 16563, Training Loss: 1.007e+00 , Validation Loss: 7.802e-01\n",
      "Iteration: 16564, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16565, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16566, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 16567, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16568, Training Loss: 7.555e-01 , Validation Loss: 9.133e-01\n",
      "Iteration: 16569, Training Loss: 1.259e+00 , Validation Loss: 5.988e-01\n",
      "Iteration: 16570, Training Loss: 1.763e+00 , Validation Loss: 3.689e+00\n",
      "Iteration: 16571, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 16572, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 16573, Training Loss: 5.037e-01 , Validation Loss: 2.093e+00\n",
      "Iteration: 16574, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16575, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 16576, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16577, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16578, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16579, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16580, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16581, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16582, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16583, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16584, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16585, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16586, Training Loss: 1.259e+00 , Validation Loss: 2.679e+00\n",
      "Iteration: 16587, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16588, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16589, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 16590, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16591, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 16592, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16593, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16594, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 16595, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 16596, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16597, Training Loss: 3.778e+00 , Validation Loss: 4.155e+00\n",
      "Iteration: 16598, Training Loss: 2.015e+00 , Validation Loss: 3.641e+00\n",
      "Iteration: 16599, Training Loss: 7.555e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 16600, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 16601, Training Loss: 2.518e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 16602, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16603, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16604, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 16605, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16606, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16607, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16608, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16609, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16610, Training Loss: 2.015e+00 , Validation Loss: 3.006e+00\n",
      "Iteration: 16611, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 16612, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16613, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16614, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16615, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16616, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16617, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 16618, Training Loss: 2.518e-01 , Validation Loss: 9.435e-01\n",
      "Iteration: 16619, Training Loss: 1.259e+00 , Validation Loss: 3.744e+00\n",
      "Iteration: 16620, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 16621, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 16622, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16623, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 16624, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 16625, Training Loss: 5.037e-01 , Validation Loss: 1.990e+00\n",
      "Iteration: 16626, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 16627, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 16628, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 16629, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16630, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 16631, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16632, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16633, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 16634, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16635, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 16636, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16637, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16638, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16639, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16640, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16641, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16642, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16643, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16644, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16645, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16646, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16647, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 16648, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 16649, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16650, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16651, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 16652, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16653, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16654, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 16655, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 16656, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 16657, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 16658, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16659, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16660, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16661, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16662, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16663, Training Loss: 5.037e-01 , Validation Loss: 1.064e+00\n",
      "Iteration: 16664, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 16665, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 16666, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 16667, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 16668, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 16669, Training Loss: 7.555e-01 , Validation Loss: 2.280e+00\n",
      "Iteration: 16670, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16671, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16672, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 16673, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 16674, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 16675, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16676, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 16677, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 16678, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 16679, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16680, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16681, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16682, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16683, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16684, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 16685, Training Loss: 5.037e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 16686, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16687, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16688, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 16689, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16690, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16691, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16692, Training Loss: 1.007e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 16693, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16694, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16695, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16696, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 16697, Training Loss: 1.259e+00 , Validation Loss: 2.008e+00\n",
      "Iteration: 16698, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 16699, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16700, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16701, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16702, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16703, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 16704, Training Loss: 2.518e-01 , Validation Loss: 1.875e+00\n",
      "Iteration: 16705, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 16706, Training Loss: 1.511e+00 , Validation Loss: 2.782e+00\n",
      "Iteration: 16707, Training Loss: 1.259e+00 , Validation Loss: 5.443e-01\n",
      "Iteration: 16708, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16709, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16710, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 16711, Training Loss: 1.007e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 16712, Training Loss: 1.007e+00 , Validation Loss: 1.784e+00\n",
      "Iteration: 16713, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 16714, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 16715, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16716, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 16717, Training Loss: 2.518e-01 , Validation Loss: 1.500e+00\n",
      "Iteration: 16718, Training Loss: 1.259e+00 , Validation Loss: 8.407e-01\n",
      "Iteration: 16719, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16720, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 16721, Training Loss: 5.037e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 16722, Training Loss: 7.555e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 16723, Training Loss: 1.007e+00 , Validation Loss: 5.988e-01\n",
      "Iteration: 16724, Training Loss: 5.037e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 16725, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16726, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 16727, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 16728, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 16729, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 16730, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 16731, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 16732, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 16733, Training Loss: 1.007e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 16734, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 16735, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 16736, Training Loss: 7.555e-01 , Validation Loss: 1.476e+00\n",
      "Iteration: 16737, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16738, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16739, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 16740, Training Loss: -1.000e-07 , Validation Loss: 6.653e-01\n",
      "Iteration: 16741, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 16742, Training Loss: 1.763e+00 , Validation Loss: 2.691e+00\n",
      "Iteration: 16743, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 16744, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16745, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 16746, Training Loss: -1.000e-07 , Validation Loss: 4.778e-01\n",
      "Iteration: 16747, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 16748, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 16749, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16750, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16751, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 16752, Training Loss: 5.037e-01 , Validation Loss: 1.101e+00\n",
      "Iteration: 16753, Training Loss: 1.007e+00 , Validation Loss: 1.246e+00\n",
      "Iteration: 16754, Training Loss: 5.037e-01 , Validation Loss: 2.316e+00\n",
      "Iteration: 16755, Training Loss: 1.007e+00 , Validation Loss: 8.044e-01\n",
      "Iteration: 16756, Training Loss: 7.555e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 16757, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 16758, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 16759, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 16760, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 16761, Training Loss: -1.000e-07 , Validation Loss: 4.838e-01\n",
      "Iteration: 16762, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 16763, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 16764, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16765, Training Loss: 1.007e+00 , Validation Loss: 4.959e-01\n",
      "Iteration: 16766, Training Loss: 4.281e+00 , Validation Loss: 5.159e+00\n",
      "Iteration: 16767, Training Loss: 1.083e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 16768, Training Loss: 3.778e+00 , Validation Loss: 4.965e+00\n",
      "Iteration: 16769, Training Loss: 2.770e+00 , Validation Loss: 2.074e+00\n",
      "Iteration: 16770, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 16771, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 16772, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16773, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 16774, Training Loss: 1.259e+00 , Validation Loss: 3.411e+00\n",
      "Iteration: 16775, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16776, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 16777, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16778, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16779, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16780, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16781, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16782, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 16783, Training Loss: 1.763e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 16784, Training Loss: 1.511e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 16785, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16786, Training Loss: 7.555e-01 , Validation Loss: 1.875e+00\n",
      "Iteration: 16787, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16788, Training Loss: 2.518e-01 , Validation Loss: 1.071e+00\n",
      "Iteration: 16789, Training Loss: 2.770e+00 , Validation Loss: 3.889e+00\n",
      "Iteration: 16790, Training Loss: 1.511e+00 , Validation Loss: 3.169e+00\n",
      "Iteration: 16791, Training Loss: 2.015e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 16792, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16793, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16794, Training Loss: 2.770e+00 , Validation Loss: 3.441e+00\n",
      "Iteration: 16795, Training Loss: 1.511e+00 , Validation Loss: 1.929e+00\n",
      "Iteration: 16796, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 16797, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 16798, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16799, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16800, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 16801, Training Loss: 2.015e+00 , Validation Loss: 3.816e+00\n",
      "Iteration: 16802, Training Loss: 1.259e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 16803, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16804, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16805, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16806, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16807, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16808, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16809, Training Loss: 1.511e+00 , Validation Loss: 1.718e+00\n",
      "Iteration: 16810, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16811, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16812, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16813, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16814, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16815, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16816, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16817, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16818, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16819, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16820, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16821, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 16822, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 16823, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16824, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16825, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16826, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16827, Training Loss: 1.007e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 16828, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 16829, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16830, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16831, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16832, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16833, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 16834, Training Loss: -1.000e-07 , Validation Loss: 7.318e-01\n",
      "Iteration: 16835, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 16836, Training Loss: 7.555e-01 , Validation Loss: 1.941e+00\n",
      "Iteration: 16837, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 16838, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16839, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16840, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 16841, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16842, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16843, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16844, Training Loss: 2.518e+00 , Validation Loss: 4.355e+00\n",
      "Iteration: 16845, Training Loss: 1.259e+00 , Validation Loss: 4.838e-01\n",
      "Iteration: 16846, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16847, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16848, Training Loss: 7.555e-01 , Validation Loss: 1.089e+00\n",
      "Iteration: 16849, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16850, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 16851, Training Loss: 3.778e+00 , Validation Loss: 4.476e+00\n",
      "Iteration: 16852, Training Loss: 1.511e+00 , Validation Loss: 2.401e+00\n",
      "Iteration: 16853, Training Loss: 5.037e-01 , Validation Loss: 9.616e-01\n",
      "Iteration: 16854, Training Loss: 2.518e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 16855, Training Loss: 1.259e+00 , Validation Loss: 6.350e-01\n",
      "Iteration: 16856, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16857, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16858, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16859, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16860, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16861, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16862, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 16863, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16864, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16865, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16866, Training Loss: 1.259e+00 , Validation Loss: 1.185e+00\n",
      "Iteration: 16867, Training Loss: 5.037e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 16868, Training Loss: 1.007e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 16869, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16870, Training Loss: 2.267e+00 , Validation Loss: 3.774e+00\n",
      "Iteration: 16871, Training Loss: 1.259e+00 , Validation Loss: 1.083e+00\n",
      "Iteration: 16872, Training Loss: 1.007e+00 , Validation Loss: 5.080e-01\n",
      "Iteration: 16873, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 16874, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 16875, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16876, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16877, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 16878, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 16879, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16880, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16881, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16882, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16883, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16884, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16885, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16886, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16887, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16888, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16889, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16890, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16891, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16892, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16893, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16894, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16895, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16896, Training Loss: 1.007e+00 , Validation Loss: 1.730e+00\n",
      "Iteration: 16897, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16898, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16899, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16900, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16901, Training Loss: 5.037e-01 , Validation Loss: 1.996e+00\n",
      "Iteration: 16902, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16903, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 16904, Training Loss: 1.259e+00 , Validation Loss: 3.212e+00\n",
      "Iteration: 16905, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 16906, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16907, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16908, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 16909, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 16910, Training Loss: 7.555e-01 , Validation Loss: 1.893e+00\n",
      "Iteration: 16911, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16912, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 16913, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 16914, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16915, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16916, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 16917, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 16918, Training Loss: 5.037e-01 , Validation Loss: 1.458e+00\n",
      "Iteration: 16919, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16920, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16921, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16922, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 16923, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16924, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 16925, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 16926, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 16927, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16928, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16929, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 16930, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16931, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16932, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16933, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16934, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16935, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16936, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16937, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16938, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16939, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16940, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16941, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16942, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16943, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16944, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16945, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 16946, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 16947, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 16948, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16949, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16950, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 16951, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16952, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16953, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16954, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16955, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 16956, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 16957, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 16958, Training Loss: 2.518e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 16959, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 16960, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 16961, Training Loss: 2.518e-01 , Validation Loss: 1.663e+00\n",
      "Iteration: 16962, Training Loss: 3.778e+00 , Validation Loss: 4.076e+00\n",
      "Iteration: 16963, Training Loss: 7.555e+00 , Validation Loss: 8.352e+00\n",
      "Iteration: 16964, Training Loss: 1.763e+00 , Validation Loss: 3.326e+00\n",
      "Iteration: 16965, Training Loss: 1.007e+00 , Validation Loss: 8.467e-01\n",
      "Iteration: 16966, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 16967, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16968, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16969, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16970, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16971, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16972, Training Loss: 1.259e+00 , Validation Loss: 1.077e+00\n",
      "Iteration: 16973, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 16974, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16975, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16976, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 16977, Training Loss: 5.037e-01 , Validation Loss: 2.014e+00\n",
      "Iteration: 16978, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 16979, Training Loss: 2.518e+00 , Validation Loss: 4.095e+00\n",
      "Iteration: 16980, Training Loss: 2.015e+00 , Validation Loss: 9.254e-01\n",
      "Iteration: 16981, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16982, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16983, Training Loss: 5.037e-01 , Validation Loss: 1.845e+00\n",
      "Iteration: 16984, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16985, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16986, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16987, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 16988, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16989, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 16990, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 16991, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16992, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16993, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 16994, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16995, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16996, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 16997, Training Loss: 1.007e+00 , Validation Loss: 4.355e-01\n",
      "Iteration: 16998, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 16999, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 17000, Training Loss: 2.267e+00 , Validation Loss: 4.349e+00\n",
      "Iteration: 17001, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 17002, Training Loss: 1.763e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 17003, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17004, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17005, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17006, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17007, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 17008, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17009, Training Loss: 1.511e+00 , Validation Loss: 2.673e+00\n",
      "Iteration: 17010, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17011, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17012, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 17013, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 17014, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17015, Training Loss: 1.007e+00 , Validation Loss: 3.199e+00\n",
      "Iteration: 17016, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17017, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17018, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17019, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17020, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17021, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 17022, Training Loss: 1.511e+00 , Validation Loss: 3.677e+00\n",
      "Iteration: 17023, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 17024, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 17025, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17026, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17027, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17028, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17029, Training Loss: 7.555e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 17030, Training Loss: 1.763e+00 , Validation Loss: 3.532e+00\n",
      "Iteration: 17031, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 17032, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17033, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17034, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17035, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 17036, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 17037, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 17038, Training Loss: 2.015e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 17039, Training Loss: -1.000e-07 , Validation Loss: 6.774e-01\n",
      "Iteration: 17040, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 17041, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17042, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17043, Training Loss: 7.555e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 17044, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 17045, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17046, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17047, Training Loss: 1.007e+00 , Validation Loss: 3.514e+00\n",
      "Iteration: 17048, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17049, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17050, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 17051, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17052, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17053, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17054, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 17055, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17056, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17057, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17058, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 17059, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 17060, Training Loss: 2.267e+00 , Validation Loss: 4.004e+00\n",
      "Iteration: 17061, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 17062, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17063, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17064, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 17065, Training Loss: 1.007e+00 , Validation Loss: 2.891e+00\n",
      "Iteration: 17066, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17067, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17068, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17069, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17070, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17071, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17072, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 17073, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 17074, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 17075, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 17076, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 17077, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 17078, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17079, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17080, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 17081, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17082, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17083, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 17084, Training Loss: 4.533e+00 , Validation Loss: 5.165e+00\n",
      "Iteration: 17085, Training Loss: 1.108e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 17086, Training Loss: 2.518e+00 , Validation Loss: 5.226e+00\n",
      "Iteration: 17087, Training Loss: 1.511e+00 , Validation Loss: 9.435e-01\n",
      "Iteration: 17088, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17089, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17090, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17091, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17092, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 17093, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17094, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17095, Training Loss: 5.037e-01 , Validation Loss: 1.155e+00\n",
      "Iteration: 17096, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17097, Training Loss: 3.274e+00 , Validation Loss: 3.798e+00\n",
      "Iteration: 17098, Training Loss: 2.770e+00 , Validation Loss: 3.187e+00\n",
      "Iteration: 17099, Training Loss: 1.259e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 17100, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17101, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17102, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17103, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17104, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17105, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17106, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17107, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17108, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17109, Training Loss: 2.015e+00 , Validation Loss: 3.889e+00\n",
      "Iteration: 17110, Training Loss: 1.511e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 17111, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17112, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17113, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17114, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 17115, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 17116, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17117, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17118, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 17119, Training Loss: 3.526e+00 , Validation Loss: 4.445e+00\n",
      "Iteration: 17120, Training Loss: 2.770e+00 , Validation Loss: 3.248e+00\n",
      "Iteration: 17121, Training Loss: 7.555e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 17122, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 17123, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17124, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17125, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17126, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17127, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 17128, Training Loss: 1.511e+00 , Validation Loss: 2.855e+00\n",
      "Iteration: 17129, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 17130, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 17131, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 17132, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17133, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17134, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17135, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17136, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17137, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 17138, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17139, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17140, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17141, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 17142, Training Loss: 3.022e+00 , Validation Loss: 3.689e+00\n",
      "Iteration: 17143, Training Loss: 1.511e+00 , Validation Loss: 1.790e+00\n",
      "Iteration: 17144, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 17145, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17146, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17147, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17148, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17149, Training Loss: 1.763e+00 , Validation Loss: 3.538e+00\n",
      "Iteration: 17150, Training Loss: 1.763e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 17151, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17152, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17153, Training Loss: 2.015e+00 , Validation Loss: 2.649e+00\n",
      "Iteration: 17154, Training Loss: 2.518e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 17155, Training Loss: 5.037e-01 , Validation Loss: 2.891e+00\n",
      "Iteration: 17156, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17157, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17158, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17159, Training Loss: 5.037e-01 , Validation Loss: 9.133e-01\n",
      "Iteration: 17160, Training Loss: 2.518e-01 , Validation Loss: 1.845e+00\n",
      "Iteration: 17161, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 17162, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17163, Training Loss: -1.000e-07 , Validation Loss: 6.955e-01\n",
      "Iteration: 17164, Training Loss: 1.007e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 17165, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17166, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17167, Training Loss: 2.518e-01 , Validation Loss: 1.439e+00\n",
      "Iteration: 17168, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 17169, Training Loss: 1.511e+00 , Validation Loss: 2.607e+00\n",
      "Iteration: 17170, Training Loss: 1.259e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 17171, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17172, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17173, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17174, Training Loss: -1.000e-07 , Validation Loss: 5.080e-01\n",
      "Iteration: 17175, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 17176, Training Loss: 3.022e+00 , Validation Loss: 4.161e+00\n",
      "Iteration: 17177, Training Loss: 3.274e+00 , Validation Loss: 3.157e+00\n",
      "Iteration: 17178, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 17179, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17180, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17181, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17182, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17183, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17184, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17185, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17186, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 17187, Training Loss: 2.267e+00 , Validation Loss: 4.566e+00\n",
      "Iteration: 17188, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 17189, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 17190, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17191, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17192, Training Loss: 1.763e+00 , Validation Loss: 2.643e+00\n",
      "Iteration: 17193, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 17194, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 17195, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 17196, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17197, Training Loss: 5.037e-01 , Validation Loss: 8.951e-01\n",
      "Iteration: 17198, Training Loss: 7.555e-01 , Validation Loss: 1.706e+00\n",
      "Iteration: 17199, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17200, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17201, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 17202, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 17203, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17204, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 17205, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 17206, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17207, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17208, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17209, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17210, Training Loss: 2.518e-01 , Validation Loss: 1.016e+00\n",
      "Iteration: 17211, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 17212, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17213, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17214, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17215, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17216, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17217, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17218, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17219, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 17220, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 17221, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17222, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17223, Training Loss: 5.037e-01 , Validation Loss: 1.742e+00\n",
      "Iteration: 17224, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17225, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 17226, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17227, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17228, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17229, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17230, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17231, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17232, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 17233, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 17234, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 17235, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 17236, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 17237, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17238, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 17239, Training Loss: 1.259e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 17240, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17241, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17242, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17243, Training Loss: 1.007e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 17244, Training Loss: 2.518e-01 , Validation Loss: 2.546e+00\n",
      "Iteration: 17245, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 17246, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 17247, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17248, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 17249, Training Loss: 5.037e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 17250, Training Loss: 2.015e+00 , Validation Loss: 4.609e+00\n",
      "Iteration: 17251, Training Loss: 1.259e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 17252, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 17253, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17254, Training Loss: 7.555e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 17255, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17256, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17257, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 17258, Training Loss: 7.555e-01 , Validation Loss: 1.131e+00\n",
      "Iteration: 17259, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17260, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17261, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17262, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17263, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17264, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 17265, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17266, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 17267, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17268, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17269, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 17270, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 17271, Training Loss: 2.518e-01 , Validation Loss: 2.208e+00\n",
      "Iteration: 17272, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 17273, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 17274, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17275, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17276, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 17277, Training Loss: 5.037e-01 , Validation Loss: 3.490e+00\n",
      "Iteration: 17278, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 17279, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17280, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17281, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17282, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17283, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17284, Training Loss: 1.259e+00 , Validation Loss: 3.756e+00\n",
      "Iteration: 17285, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 17286, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 17287, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17288, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 17289, Training Loss: 5.037e-01 , Validation Loss: 2.220e+00\n",
      "Iteration: 17290, Training Loss: 7.555e-01 , Validation Loss: 2.093e+00\n",
      "Iteration: 17291, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17292, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17293, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 17294, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 17295, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 17296, Training Loss: 1.259e+00 , Validation Loss: 2.964e+00\n",
      "Iteration: 17297, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17298, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17299, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17300, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 17301, Training Loss: 7.555e-01 , Validation Loss: 1.609e+00\n",
      "Iteration: 17302, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17303, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 17304, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 17305, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17306, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17307, Training Loss: 2.518e-01 , Validation Loss: 1.548e+00\n",
      "Iteration: 17308, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 17309, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17310, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17311, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17312, Training Loss: 2.518e+00 , Validation Loss: 3.695e+00\n",
      "Iteration: 17313, Training Loss: 1.511e+00 , Validation Loss: 1.778e+00\n",
      "Iteration: 17314, Training Loss: 2.518e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 17315, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 17316, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 17317, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 17318, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 17319, Training Loss: 1.007e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 17320, Training Loss: 7.555e-01 , Validation Loss: 1.869e+00\n",
      "Iteration: 17321, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17322, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17323, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 17324, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17325, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17326, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17327, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17328, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 17329, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 17330, Training Loss: 1.511e+00 , Validation Loss: 2.595e+00\n",
      "Iteration: 17331, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 17332, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 17333, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17334, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17335, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17336, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17337, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17338, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 17339, Training Loss: 7.555e-01 , Validation Loss: 2.389e+00\n",
      "Iteration: 17340, Training Loss: 5.037e-01 , Validation Loss: 1.155e+00\n",
      "Iteration: 17341, Training Loss: 7.555e-01 , Validation Loss: 2.147e+00\n",
      "Iteration: 17342, Training Loss: -1.000e-07 , Validation Loss: 1.161e+00\n",
      "Iteration: 17343, Training Loss: 2.518e-01 , Validation Loss: 1.161e+00\n",
      "Iteration: 17344, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 17345, Training Loss: 3.274e+00 , Validation Loss: 4.439e+00\n",
      "Iteration: 17346, Training Loss: 1.259e+00 , Validation Loss: 2.601e+00\n",
      "Iteration: 17347, Training Loss: 1.259e+00 , Validation Loss: 1.391e+00\n",
      "Iteration: 17348, Training Loss: 1.007e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 17349, Training Loss: 1.259e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 17350, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 17351, Training Loss: 5.037e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 17352, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17353, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17354, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17355, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17356, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17357, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 17358, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 17359, Training Loss: 2.267e+00 , Validation Loss: 4.107e+00\n",
      "Iteration: 17360, Training Loss: 5.037e-01 , Validation Loss: 1.790e+00\n",
      "Iteration: 17361, Training Loss: 5.037e-01 , Validation Loss: 1.427e+00\n",
      "Iteration: 17362, Training Loss: 1.763e+00 , Validation Loss: 1.034e+00\n",
      "Iteration: 17363, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 17364, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 17365, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 17366, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 17367, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17368, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17369, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 17370, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 17371, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 17372, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 17373, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17374, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 17375, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 17376, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17377, Training Loss: 1.007e+00 , Validation Loss: 2.419e+00\n",
      "Iteration: 17378, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17379, Training Loss: 1.511e+00 , Validation Loss: 1.210e+00\n",
      "Iteration: 17380, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 17381, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17382, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17383, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17384, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 17385, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 17386, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 17387, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17388, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17389, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17390, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17391, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17392, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17393, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 17394, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 17395, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17396, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17397, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 17398, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 17399, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 17400, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 17401, Training Loss: 7.555e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 17402, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 17403, Training Loss: 7.555e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 17404, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17405, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17406, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17407, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17408, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17409, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17410, Training Loss: 7.555e-01 , Validation Loss: 8.951e-01\n",
      "Iteration: 17411, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17412, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17413, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17414, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 17415, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 17416, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17417, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17418, Training Loss: 7.555e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 17419, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 17420, Training Loss: 7.555e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 17421, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17422, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 17423, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 17424, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 17425, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17426, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 17427, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 17428, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 17429, Training Loss: 2.518e-01 , Validation Loss: 1.198e+00\n",
      "Iteration: 17430, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 17431, Training Loss: 1.007e+00 , Validation Loss: 8.407e-01\n",
      "Iteration: 17432, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 17433, Training Loss: 3.022e+00 , Validation Loss: 4.064e+00\n",
      "Iteration: 17434, Training Loss: 1.007e+00 , Validation Loss: 1.687e+00\n",
      "Iteration: 17435, Training Loss: 1.763e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 17436, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17437, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17438, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17439, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17440, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17441, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17442, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17443, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17444, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17445, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17446, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 17447, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17448, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 17449, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 17450, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17451, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17452, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 17453, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17454, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17455, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17456, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17457, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17458, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17459, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17460, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 17461, Training Loss: 2.015e+00 , Validation Loss: 4.324e+00\n",
      "Iteration: 17462, Training Loss: 1.007e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 17463, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 17464, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17465, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17466, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 17467, Training Loss: 1.259e+00 , Validation Loss: 1.621e+00\n",
      "Iteration: 17468, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17469, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 17470, Training Loss: 1.259e+00 , Validation Loss: 3.157e+00\n",
      "Iteration: 17471, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 17472, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 17473, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17474, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17475, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17476, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17477, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 17478, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 17479, Training Loss: 5.037e-01 , Validation Loss: 1.016e+00\n",
      "Iteration: 17480, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17481, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 17482, Training Loss: 5.037e-01 , Validation Loss: 2.044e+00\n",
      "Iteration: 17483, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 17484, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17485, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 17486, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 17487, Training Loss: 1.259e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 17488, Training Loss: 1.511e+00 , Validation Loss: 3.466e+00\n",
      "Iteration: 17489, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 17490, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17491, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17492, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 17493, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17494, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17495, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 17496, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 17497, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17498, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17499, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17500, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17501, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17502, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17503, Training Loss: 5.037e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 17504, Training Loss: -1.000e-07 , Validation Loss: 1.210e+00\n",
      "Iteration: 17505, Training Loss: 7.555e-01 , Validation Loss: 1.210e+00\n",
      "Iteration: 17506, Training Loss: 5.037e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 17507, Training Loss: 2.518e-01 , Validation Loss: 1.778e+00\n",
      "Iteration: 17508, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 17509, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 17510, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17511, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 17512, Training Loss: 7.555e-01 , Validation Loss: 1.173e+00\n",
      "Iteration: 17513, Training Loss: 1.259e+00 , Validation Loss: 9.435e-01\n",
      "Iteration: 17514, Training Loss: 1.007e+00 , Validation Loss: 1.488e+00\n",
      "Iteration: 17515, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 17516, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 17517, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 17518, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 17519, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17520, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17521, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 17522, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17523, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17524, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17525, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 17526, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 17527, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 17528, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17529, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17530, Training Loss: -1.000e-07 , Validation Loss: 5.020e-01\n",
      "Iteration: 17531, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 17532, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17533, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17534, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17535, Training Loss: -1.000e-07 , Validation Loss: 6.290e-01\n",
      "Iteration: 17536, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 17537, Training Loss: 5.037e-01 , Validation Loss: 2.044e+00\n",
      "Iteration: 17538, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17539, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17540, Training Loss: 1.007e+00 , Validation Loss: 5.504e-01\n",
      "Iteration: 17541, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17542, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17543, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 17544, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 17545, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 17546, Training Loss: 1.007e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 17547, Training Loss: 1.763e+00 , Validation Loss: 3.810e+00\n",
      "Iteration: 17548, Training Loss: 1.007e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 17549, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 17550, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17551, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 17552, Training Loss: 1.259e+00 , Validation Loss: 1.016e+00\n",
      "Iteration: 17553, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17554, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 17555, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 17556, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17557, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 17558, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17559, Training Loss: 5.037e-01 , Validation Loss: 1.034e+00\n",
      "Iteration: 17560, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 17561, Training Loss: 5.037e-01 , Validation Loss: 2.232e+00\n",
      "Iteration: 17562, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17563, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17564, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17565, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17566, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17567, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17568, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17569, Training Loss: 5.037e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 17570, Training Loss: -1.000e-07 , Validation Loss: 7.621e-01\n",
      "Iteration: 17571, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 17572, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17573, Training Loss: 7.555e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 17574, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17575, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17576, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 17577, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17578, Training Loss: 1.259e+00 , Validation Loss: 2.274e+00\n",
      "Iteration: 17579, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 17580, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 17581, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 17582, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17583, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17584, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17585, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17586, Training Loss: 1.259e+00 , Validation Loss: 7.802e-01\n",
      "Iteration: 17587, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 17588, Training Loss: 2.015e+00 , Validation Loss: 4.294e+00\n",
      "Iteration: 17589, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 17590, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17591, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 17592, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17593, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 17594, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17595, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17596, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17597, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17598, Training Loss: 1.259e+00 , Validation Loss: 1.240e+00\n",
      "Iteration: 17599, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17600, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 17601, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17602, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17603, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 17604, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17605, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 17606, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17607, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17608, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17609, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 17610, Training Loss: 2.770e+00 , Validation Loss: 3.931e+00\n",
      "Iteration: 17611, Training Loss: 1.511e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 17612, Training Loss: 1.007e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 17613, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17614, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 17615, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17616, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17617, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17618, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17619, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17620, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17621, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 17622, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17623, Training Loss: 5.037e-01 , Validation Loss: 2.020e+00\n",
      "Iteration: 17624, Training Loss: 5.037e-01 , Validation Loss: 2.087e+00\n",
      "Iteration: 17625, Training Loss: 2.267e+00 , Validation Loss: 3.514e+00\n",
      "Iteration: 17626, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 17627, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 17628, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 17629, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17630, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17631, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17632, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 17633, Training Loss: 1.007e+00 , Validation Loss: 6.350e-01\n",
      "Iteration: 17634, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17635, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 17636, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 17637, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17638, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17639, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 17640, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 17641, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 17642, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 17643, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 17644, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17645, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 17646, Training Loss: 1.007e+00 , Validation Loss: 1.687e+00\n",
      "Iteration: 17647, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17648, Training Loss: 1.007e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 17649, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17650, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17651, Training Loss: 5.037e-01 , Validation Loss: 1.627e+00\n",
      "Iteration: 17652, Training Loss: 2.518e-01 , Validation Loss: 1.863e+00\n",
      "Iteration: 17653, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 17654, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17655, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17656, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17657, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17658, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 17659, Training Loss: 1.511e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 17660, Training Loss: 4.030e+00 , Validation Loss: 5.044e+00\n",
      "Iteration: 17661, Training Loss: 2.770e+00 , Validation Loss: 4.010e+00\n",
      "Iteration: 17662, Training Loss: 1.511e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 17663, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17664, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17665, Training Loss: 5.037e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 17666, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17667, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17668, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17669, Training Loss: 1.007e+00 , Validation Loss: 8.528e-01\n",
      "Iteration: 17670, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17671, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17672, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17673, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17674, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17675, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17676, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 17677, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 17678, Training Loss: 1.259e+00 , Validation Loss: 3.193e+00\n",
      "Iteration: 17679, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17680, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17681, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17682, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17683, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17684, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 17685, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17686, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17687, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17688, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17689, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17690, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17691, Training Loss: 1.259e+00 , Validation Loss: 3.060e+00\n",
      "Iteration: 17692, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 17693, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17694, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17695, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17696, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17697, Training Loss: 1.259e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 17698, Training Loss: 1.259e+00 , Validation Loss: 6.532e-01\n",
      "Iteration: 17699, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 17700, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 17701, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17702, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 17703, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 17704, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17705, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17706, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17707, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 17708, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17709, Training Loss: 1.259e+00 , Validation Loss: 3.580e+00\n",
      "Iteration: 17710, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 17711, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17712, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17713, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17714, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 17715, Training Loss: 2.518e-01 , Validation Loss: 2.359e+00\n",
      "Iteration: 17716, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 17717, Training Loss: 2.770e+00 , Validation Loss: 4.203e+00\n",
      "Iteration: 17718, Training Loss: 1.763e+00 , Validation Loss: 1.766e+00\n",
      "Iteration: 17719, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 17720, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17721, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17722, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17723, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17724, Training Loss: 2.518e-01 , Validation Loss: 1.210e+00\n",
      "Iteration: 17725, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 17726, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17727, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 17728, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 17729, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 17730, Training Loss: 1.007e+00 , Validation Loss: 4.415e-01\n",
      "Iteration: 17731, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17732, Training Loss: 1.259e+00 , Validation Loss: 2.183e+00\n",
      "Iteration: 17733, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 17734, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17735, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17736, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17737, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17738, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17739, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 17740, Training Loss: 7.555e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 17741, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17742, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 17743, Training Loss: 2.518e-01 , Validation Loss: 1.306e+00\n",
      "Iteration: 17744, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 17745, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 17746, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17747, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17748, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 17749, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 17750, Training Loss: 2.518e+00 , Validation Loss: 2.522e+00\n",
      "Iteration: 17751, Training Loss: 1.511e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 17752, Training Loss: 1.007e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 17753, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17754, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17755, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 17756, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17757, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17758, Training Loss: 2.770e+00 , Validation Loss: 4.101e+00\n",
      "Iteration: 17759, Training Loss: 1.763e+00 , Validation Loss: 1.064e+00\n",
      "Iteration: 17760, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 17761, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17762, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17763, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17764, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17765, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17766, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 17767, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17768, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17769, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17770, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17771, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17772, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17773, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17774, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17775, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 17776, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17777, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17778, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17779, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17780, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17781, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17782, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17783, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17784, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 17785, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17786, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17787, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17788, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 17789, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 17790, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17791, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17792, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17793, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17794, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17795, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17796, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 17797, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17798, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 17799, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 17800, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17801, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 17802, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17803, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17804, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17805, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17806, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 17807, Training Loss: 1.259e+00 , Validation Loss: 3.163e+00\n",
      "Iteration: 17808, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17809, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17810, Training Loss: 5.037e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 17811, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 17812, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 17813, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 17814, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17815, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17816, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 17817, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17818, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17819, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17820, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 17821, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 17822, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17823, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17824, Training Loss: 5.037e-01 , Validation Loss: 9.012e-01\n",
      "Iteration: 17825, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17826, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17827, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 17828, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17829, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17830, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 17831, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17832, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17833, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 17834, Training Loss: 7.555e-01 , Validation Loss: 3.556e+00\n",
      "Iteration: 17835, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 17836, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 17837, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 17838, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17839, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 17840, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 17841, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 17842, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 17843, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17844, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17845, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17846, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17847, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17848, Training Loss: 2.518e-01 , Validation Loss: 9.435e-01\n",
      "Iteration: 17849, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 17850, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17851, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17852, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17853, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 17854, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 17855, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 17856, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17857, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17858, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17859, Training Loss: 7.555e-01 , Validation Loss: 3.532e+00\n",
      "Iteration: 17860, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17861, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 17862, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17863, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17864, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 17865, Training Loss: 3.778e+00 , Validation Loss: 4.669e+00\n",
      "Iteration: 17866, Training Loss: 2.518e+00 , Validation Loss: 4.367e+00\n",
      "Iteration: 17867, Training Loss: 5.037e-01 , Validation Loss: 1.004e+00\n",
      "Iteration: 17868, Training Loss: 1.007e+00 , Validation Loss: 8.467e-01\n",
      "Iteration: 17869, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 17870, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 17871, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 17872, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17873, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17874, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 17875, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 17876, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17877, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 17878, Training Loss: 2.267e+00 , Validation Loss: 4.028e+00\n",
      "Iteration: 17879, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 17880, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 17881, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17882, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17883, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17884, Training Loss: -1.000e-07 , Validation Loss: 7.197e-01\n",
      "Iteration: 17885, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 17886, Training Loss: -1.000e-07 , Validation Loss: 5.020e-01\n",
      "Iteration: 17887, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 17888, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17889, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17890, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17891, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 17892, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 17893, Training Loss: 5.037e-01 , Validation Loss: 2.123e+00\n",
      "Iteration: 17894, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17895, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 17896, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17897, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 17898, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17899, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17900, Training Loss: 2.518e-01 , Validation Loss: 1.210e+00\n",
      "Iteration: 17901, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 17902, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17903, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 17904, Training Loss: 2.770e+00 , Validation Loss: 4.082e+00\n",
      "Iteration: 17905, Training Loss: 1.259e+00 , Validation Loss: 1.675e+00\n",
      "Iteration: 17906, Training Loss: 1.007e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 17907, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 17908, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17909, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 17910, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 17911, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17912, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 17913, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 17914, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17915, Training Loss: 5.037e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 17916, Training Loss: 1.007e+00 , Validation Loss: 1.579e+00\n",
      "Iteration: 17917, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 17918, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17919, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 17920, Training Loss: 3.022e+00 , Validation Loss: 4.415e+00\n",
      "Iteration: 17921, Training Loss: 2.015e+00 , Validation Loss: 1.113e+00\n",
      "Iteration: 17922, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 17923, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17924, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17925, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17926, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 17927, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 17928, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17929, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17930, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17931, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 17932, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17933, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 17934, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17935, Training Loss: 1.007e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 17936, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 17937, Training Loss: 5.037e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 17938, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17939, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17940, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 17941, Training Loss: -1.000e-07 , Validation Loss: 7.197e-01\n",
      "Iteration: 17942, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 17943, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 17944, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 17945, Training Loss: 1.007e+00 , Validation Loss: 4.234e-01\n",
      "Iteration: 17946, Training Loss: -1.000e-07 , Validation Loss: 8.830e-01\n",
      "Iteration: 17947, Training Loss: 7.555e-01 , Validation Loss: 8.830e-01\n",
      "Iteration: 17948, Training Loss: 3.274e+00 , Validation Loss: 5.461e+00\n",
      "Iteration: 17949, Training Loss: 2.770e+00 , Validation Loss: 4.137e+00\n",
      "Iteration: 17950, Training Loss: 2.015e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 17951, Training Loss: 1.007e+00 , Validation Loss: 2.220e+00\n",
      "Iteration: 17952, Training Loss: 4.281e+00 , Validation Loss: 4.276e+00\n",
      "Iteration: 17953, Training Loss: 7.807e+00 , Validation Loss: 8.897e+00\n",
      "Iteration: 17954, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17955, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17956, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17957, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17958, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17959, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17960, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17961, Training Loss: 7.555e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 17962, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17963, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17964, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17965, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17966, Training Loss: 5.037e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 17967, Training Loss: -1.000e-07 , Validation Loss: 1.113e+00\n",
      "Iteration: 17968, Training Loss: 5.037e-01 , Validation Loss: 1.113e+00\n",
      "Iteration: 17969, Training Loss: 1.259e+00 , Validation Loss: 3.212e+00\n",
      "Iteration: 17970, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 17971, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17972, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17973, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17974, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 17975, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17976, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17977, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17978, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17979, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17980, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17981, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 17982, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17983, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17984, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 17985, Training Loss: 2.267e+00 , Validation Loss: 4.046e+00\n",
      "Iteration: 17986, Training Loss: 1.511e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 17987, Training Loss: 1.763e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 17988, Training Loss: -1.000e-07 , Validation Loss: 6.592e-01\n",
      "Iteration: 17989, Training Loss: 7.555e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 17990, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17991, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17992, Training Loss: 3.274e+00 , Validation Loss: 4.724e+00\n",
      "Iteration: 17993, Training Loss: 1.259e+00 , Validation Loss: 2.516e+00\n",
      "Iteration: 17994, Training Loss: 2.015e+00 , Validation Loss: 1.361e+00\n",
      "Iteration: 17995, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17996, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 17997, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 17998, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 17999, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 18000, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18001, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18002, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18003, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18004, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18005, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18006, Training Loss: 2.518e+00 , Validation Loss: 3.078e+00\n",
      "Iteration: 18007, Training Loss: 2.267e+00 , Validation Loss: 3.913e+00\n",
      "Iteration: 18008, Training Loss: 1.259e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 18009, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18010, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 18011, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 18012, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18013, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18014, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18015, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 18016, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18017, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18018, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 18019, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 18020, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18021, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 18022, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 18023, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18024, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 18025, Training Loss: 1.763e+00 , Validation Loss: 3.544e+00\n",
      "Iteration: 18026, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 18027, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18028, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18029, Training Loss: 1.007e+00 , Validation Loss: 5.443e-01\n",
      "Iteration: 18030, Training Loss: 3.274e+00 , Validation Loss: 4.548e+00\n",
      "Iteration: 18031, Training Loss: 1.259e+00 , Validation Loss: 3.520e+00\n",
      "Iteration: 18032, Training Loss: 1.259e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 18033, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 18034, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 18035, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18036, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 18037, Training Loss: 1.511e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18038, Training Loss: 2.518e-01 , Validation Loss: 1.173e+00\n",
      "Iteration: 18039, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 18040, Training Loss: 5.037e-01 , Validation Loss: 1.966e+00\n",
      "Iteration: 18041, Training Loss: 1.007e+00 , Validation Loss: 3.048e+00\n",
      "Iteration: 18042, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18043, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18044, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18045, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18046, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 18047, Training Loss: 1.511e+00 , Validation Loss: 1.536e+00\n",
      "Iteration: 18048, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 18049, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18050, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18051, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18052, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18053, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18054, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 18055, Training Loss: 5.037e-01 , Validation Loss: 1.657e+00\n",
      "Iteration: 18056, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 18057, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 18058, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18059, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 18060, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18061, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18062, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 18063, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18064, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18065, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18066, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18067, Training Loss: 2.518e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 18068, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 18069, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 18070, Training Loss: 2.518e-01 , Validation Loss: 3.496e+00\n",
      "Iteration: 18071, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 18072, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 18073, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18074, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18075, Training Loss: 7.555e-01 , Validation Loss: 1.506e+00\n",
      "Iteration: 18076, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18077, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18078, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18079, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 18080, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 18081, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 18082, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18083, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 18084, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 18085, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18086, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18087, Training Loss: 1.007e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 18088, Training Loss: 1.763e+00 , Validation Loss: 3.139e+00\n",
      "Iteration: 18089, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18090, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18091, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18092, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 18093, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18094, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18095, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18096, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18097, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 18098, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18099, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18100, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 18101, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18102, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18103, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 18104, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 18105, Training Loss: 7.555e-01 , Validation Loss: 1.391e+00\n",
      "Iteration: 18106, Training Loss: 2.518e+00 , Validation Loss: 4.276e+00\n",
      "Iteration: 18107, Training Loss: 1.007e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 18108, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18109, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 18110, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18111, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18112, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18113, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18114, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 18115, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 18116, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 18117, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18118, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 18119, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 18120, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 18121, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 18122, Training Loss: 7.555e-01 , Validation Loss: 1.155e+00\n",
      "Iteration: 18123, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 18124, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18125, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18126, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18127, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 18128, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 18129, Training Loss: 7.555e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 18130, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18131, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18132, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18133, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18134, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18135, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 18136, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18137, Training Loss: 1.007e+00 , Validation Loss: 9.133e-01\n",
      "Iteration: 18138, Training Loss: 2.518e-01 , Validation Loss: 1.198e+00\n",
      "Iteration: 18139, Training Loss: 3.778e+00 , Validation Loss: 4.107e+00\n",
      "Iteration: 18140, Training Loss: 5.792e+00 , Validation Loss: 7.996e+00\n",
      "Iteration: 18141, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18142, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 18143, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18144, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18145, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18146, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18147, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18148, Training Loss: -1.000e-07 , Validation Loss: 1.712e+00\n",
      "Iteration: 18149, Training Loss: 5.037e-01 , Validation Loss: 1.712e+00\n",
      "Iteration: 18150, Training Loss: 7.555e-01 , Validation Loss: 2.280e+00\n",
      "Iteration: 18151, Training Loss: 7.555e-01 , Validation Loss: 2.032e+00\n",
      "Iteration: 18152, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18153, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18154, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18155, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18156, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18157, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 18158, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 18159, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18160, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18161, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18162, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18163, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18164, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18165, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18166, Training Loss: 5.037e-01 , Validation Loss: 2.153e+00\n",
      "Iteration: 18167, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18168, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 18169, Training Loss: 1.007e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 18170, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18171, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18172, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18173, Training Loss: 2.518e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 18174, Training Loss: 2.518e+00 , Validation Loss: 4.046e+00\n",
      "Iteration: 18175, Training Loss: 1.763e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 18176, Training Loss: 1.511e+00 , Validation Loss: 5.080e-01\n",
      "Iteration: 18177, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18178, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18179, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 18180, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 18181, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 18182, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18183, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18184, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18185, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18186, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 18187, Training Loss: 3.526e+00 , Validation Loss: 5.365e+00\n",
      "Iteration: 18188, Training Loss: 1.511e+00 , Validation Loss: 1.845e+00\n",
      "Iteration: 18189, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18190, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18191, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18192, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18193, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 18194, Training Loss: 2.770e+00 , Validation Loss: 4.082e+00\n",
      "Iteration: 18195, Training Loss: 1.763e+00 , Validation Loss: 9.737e-01\n",
      "Iteration: 18196, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18197, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18198, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18199, Training Loss: 1.763e+00 , Validation Loss: 3.883e+00\n",
      "Iteration: 18200, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18201, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18202, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18203, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 18204, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 18205, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18206, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18207, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18208, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18209, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 18210, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18211, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18212, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18213, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18214, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18215, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 18216, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 18217, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 18218, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18219, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18220, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18221, Training Loss: -1.000e-07 , Validation Loss: 6.350e-01\n",
      "Iteration: 18222, Training Loss: 5.037e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 18223, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18224, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18225, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18226, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18227, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18228, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18229, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 18230, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18231, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 18232, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 18233, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18234, Training Loss: 1.007e+00 , Validation Loss: 2.546e+00\n",
      "Iteration: 18235, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 18236, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 18237, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18238, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18239, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18240, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 18241, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18242, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18243, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18244, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18245, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18246, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18247, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18248, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18249, Training Loss: 3.778e+00 , Validation Loss: 4.343e+00\n",
      "Iteration: 18250, Training Loss: 2.518e+00 , Validation Loss: 4.445e+00\n",
      "Iteration: 18251, Training Loss: 1.007e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 18252, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18253, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18254, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18255, Training Loss: -1.000e-07 , Validation Loss: 1.597e+00\n",
      "Iteration: 18256, Training Loss: 2.518e-01 , Validation Loss: 1.597e+00\n",
      "Iteration: 18257, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 18258, Training Loss: 2.770e+00 , Validation Loss: 3.955e+00\n",
      "Iteration: 18259, Training Loss: 2.518e+00 , Validation Loss: 2.147e+00\n",
      "Iteration: 18260, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 18261, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18262, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18263, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 18264, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18265, Training Loss: 1.007e+00 , Validation Loss: 1.095e+00\n",
      "Iteration: 18266, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18267, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 18268, Training Loss: 5.037e-01 , Validation Loss: 2.540e+00\n",
      "Iteration: 18269, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 18270, Training Loss: 5.037e-01 , Validation Loss: 1.040e+00\n",
      "Iteration: 18271, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18272, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18273, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 18274, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 18275, Training Loss: 7.555e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 18276, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18277, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18278, Training Loss: 7.555e-01 , Validation Loss: 1.845e+00\n",
      "Iteration: 18279, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 18280, Training Loss: 1.511e+00 , Validation Loss: 2.637e+00\n",
      "Iteration: 18281, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18282, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18283, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18284, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18285, Training Loss: 2.518e-01 , Validation Loss: 1.149e+00\n",
      "Iteration: 18286, Training Loss: -1.000e-07 , Validation Loss: 6.592e-01\n",
      "Iteration: 18287, Training Loss: 2.518e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 18288, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 18289, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 18290, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 18291, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18292, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 18293, Training Loss: 1.007e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 18294, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 18295, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18296, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18297, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18298, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18299, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18300, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18301, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18302, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 18303, Training Loss: -1.000e-07 , Validation Loss: 1.730e+00\n",
      "Iteration: 18304, Training Loss: 1.259e+00 , Validation Loss: 1.730e+00\n",
      "Iteration: 18305, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 18306, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18307, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 18308, Training Loss: 1.511e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 18309, Training Loss: 1.511e+00 , Validation Loss: 1.216e+00\n",
      "Iteration: 18310, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18311, Training Loss: 1.511e+00 , Validation Loss: 3.949e+00\n",
      "Iteration: 18312, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18313, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 18314, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 18315, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18316, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18317, Training Loss: 3.274e+00 , Validation Loss: 4.282e+00\n",
      "Iteration: 18318, Training Loss: 2.015e+00 , Validation Loss: 3.580e+00\n",
      "Iteration: 18319, Training Loss: 2.770e+00 , Validation Loss: 1.046e+00\n",
      "Iteration: 18320, Training Loss: 1.007e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 18321, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18322, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18323, Training Loss: 1.007e+00 , Validation Loss: 2.498e+00\n",
      "Iteration: 18324, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18325, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18326, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18327, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18328, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18329, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 18330, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 18331, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18332, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 18333, Training Loss: 1.259e+00 , Validation Loss: 2.540e+00\n",
      "Iteration: 18334, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 18335, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18336, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 18337, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18338, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 18339, Training Loss: 4.281e+00 , Validation Loss: 4.494e+00\n",
      "Iteration: 18340, Training Loss: 8.815e+00 , Validation Loss: 1.014e+01\n",
      "Iteration: 18341, Training Loss: 5.037e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 18342, Training Loss: 7.555e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 18343, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18344, Training Loss: 2.518e+00 , Validation Loss: 4.282e+00\n",
      "Iteration: 18345, Training Loss: 1.007e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 18346, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 18347, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18348, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18349, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18350, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18351, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18352, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18353, Training Loss: 2.518e-01 , Validation Loss: 2.274e+00\n",
      "Iteration: 18354, Training Loss: 2.770e+00 , Validation Loss: 4.222e+00\n",
      "Iteration: 18355, Training Loss: 2.518e-01 , Validation Loss: 1.010e+00\n",
      "Iteration: 18356, Training Loss: 1.511e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 18357, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18358, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18359, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18360, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18361, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18362, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18363, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18364, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18365, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 18366, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18367, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18368, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 18369, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18370, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18371, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18372, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18373, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18374, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18375, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18376, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 18377, Training Loss: 3.526e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 18378, Training Loss: 2.015e+00 , Validation Loss: 1.748e+00\n",
      "Iteration: 18379, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 18380, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 18381, Training Loss: 2.267e+00 , Validation Loss: 2.407e+00\n",
      "Iteration: 18382, Training Loss: 1.511e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 18383, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18384, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18385, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 18386, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18387, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 18388, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18389, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 18390, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 18391, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18392, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18393, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 18394, Training Loss: 7.555e-01 , Validation Loss: 2.625e+00\n",
      "Iteration: 18395, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18396, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18397, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18398, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 18399, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 18400, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 18401, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18402, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18403, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18404, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 18405, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18406, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18407, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18408, Training Loss: 5.037e-01 , Validation Loss: 1.820e+00\n",
      "Iteration: 18409, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18410, Training Loss: -1.000e-07 , Validation Loss: 8.286e-01\n",
      "Iteration: 18411, Training Loss: -1.000e-07 , Validation Loss: 8.286e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 18412, Training Loss: 1.007e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 18413, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18414, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18415, Training Loss: 1.259e+00 , Validation Loss: 1.397e+00\n",
      "Iteration: 18416, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18417, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18418, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18419, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18420, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18421, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18422, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 18423, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 18424, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 18425, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18426, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18427, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18428, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18429, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18430, Training Loss: -1.000e-07 , Validation Loss: 5.322e-01\n",
      "Iteration: 18431, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 18432, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 18433, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 18434, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 18435, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 18436, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18437, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18438, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 18439, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18440, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18441, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18442, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18443, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18444, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18445, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18446, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18447, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18448, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18449, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18450, Training Loss: 1.007e+00 , Validation Loss: 2.377e+00\n",
      "Iteration: 18451, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18452, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18453, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18454, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18455, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18456, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18457, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18458, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18459, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18460, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18461, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18462, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 18463, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 18464, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 18465, Training Loss: 1.007e+00 , Validation Loss: 2.304e+00\n",
      "Iteration: 18466, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 18467, Training Loss: 2.518e-01 , Validation Loss: 1.234e+00\n",
      "Iteration: 18468, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 18469, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18470, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18471, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 18472, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 18473, Training Loss: 1.763e+00 , Validation Loss: 3.901e+00\n",
      "Iteration: 18474, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 18475, Training Loss: 1.007e+00 , Validation Loss: 6.532e-01\n",
      "Iteration: 18476, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 18477, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18478, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18479, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18480, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18481, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18482, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 18483, Training Loss: 2.770e+00 , Validation Loss: 4.010e+00\n",
      "Iteration: 18484, Training Loss: 2.015e+00 , Validation Loss: 2.341e+00\n",
      "Iteration: 18485, Training Loss: 5.037e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 18486, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18487, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 18488, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18489, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 18490, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18491, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18492, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 18493, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18494, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18495, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18496, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 18497, Training Loss: 7.555e-01 , Validation Loss: 1.512e+00\n",
      "Iteration: 18498, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18499, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18500, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18501, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18502, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18503, Training Loss: 7.555e-01 , Validation Loss: 1.046e+00\n",
      "Iteration: 18504, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18505, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18506, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18507, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18508, Training Loss: 7.555e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 18509, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18510, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 18511, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 18512, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 18513, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18514, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18515, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 18516, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18517, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 18518, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18519, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18520, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18521, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18522, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18523, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18524, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 18525, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18526, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 18527, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18528, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 18529, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18530, Training Loss: 1.511e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 18531, Training Loss: 2.518e-01 , Validation Loss: 3.484e+00\n",
      "Iteration: 18532, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 18533, Training Loss: 1.511e+00 , Validation Loss: 3.738e+00\n",
      "Iteration: 18534, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 18535, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 18536, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18537, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18538, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 18539, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 18540, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 18541, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18542, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 18543, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18544, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18545, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18546, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18547, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18548, Training Loss: -1.000e-07 , Validation Loss: 1.095e+00\n",
      "Iteration: 18549, Training Loss: 1.007e+00 , Validation Loss: 1.095e+00\n",
      "Iteration: 18550, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18551, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18552, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18553, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18554, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18555, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18556, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18557, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18558, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18559, Training Loss: 1.511e+00 , Validation Loss: 3.943e+00\n",
      "Iteration: 18560, Training Loss: 1.259e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 18561, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 18562, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18563, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18564, Training Loss: -1.000e-07 , Validation Loss: 8.104e-01\n",
      "Iteration: 18565, Training Loss: 5.037e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 18566, Training Loss: 1.007e+00 , Validation Loss: 1.675e+00\n",
      "Iteration: 18567, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 18568, Training Loss: 7.555e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 18569, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18570, Training Loss: 2.518e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 18571, Training Loss: 1.259e+00 , Validation Loss: 1.300e+00\n",
      "Iteration: 18572, Training Loss: 2.518e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 18573, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 18574, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18575, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 18576, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 18577, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 18578, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18579, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18580, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18581, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18582, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18583, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18584, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18585, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18586, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18587, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18588, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18589, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18590, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18591, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18592, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 18593, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 18594, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18595, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18596, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 18597, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 18598, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18599, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18600, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18601, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18602, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18603, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18604, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18605, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 18606, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18607, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18608, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18609, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18610, Training Loss: -1.000e-07 , Validation Loss: 6.895e-01\n",
      "Iteration: 18611, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 18612, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18613, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18614, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 18615, Training Loss: 1.007e+00 , Validation Loss: 1.155e+00\n",
      "Iteration: 18616, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18617, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 18618, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18619, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18620, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18621, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18622, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18623, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 18624, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 18625, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 18626, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18627, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 18628, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 18629, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18630, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18631, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18632, Training Loss: 5.037e-01 , Validation Loss: 1.294e+00\n",
      "Iteration: 18633, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18634, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18635, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18636, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 18637, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 18638, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 18639, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18640, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18641, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18642, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18643, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18644, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 18645, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 18646, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18647, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18648, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18649, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18650, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18651, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18652, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18653, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18654, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18655, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 18656, Training Loss: 7.555e-01 , Validation Loss: 3.943e+00\n",
      "Iteration: 18657, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18658, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18659, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18660, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18661, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18662, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18663, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18664, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 18665, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18666, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18667, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 18668, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18669, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 18670, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 18671, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 18672, Training Loss: 3.274e+00 , Validation Loss: 4.736e+00\n",
      "Iteration: 18673, Training Loss: 2.518e+00 , Validation Loss: 2.238e+00\n",
      "Iteration: 18674, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 18675, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 18676, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 18677, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18678, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18679, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18680, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18681, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 18682, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 18683, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 18684, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18685, Training Loss: 1.007e+00 , Validation Loss: 4.899e-01\n",
      "Iteration: 18686, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 18687, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 18688, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 18689, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 18690, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18691, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 18692, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 18693, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 18694, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 18695, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 18696, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18697, Training Loss: 2.518e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 18698, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 18699, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18700, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18701, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18702, Training Loss: 1.511e+00 , Validation Loss: 5.189e+00\n",
      "Iteration: 18703, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18704, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18705, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18706, Training Loss: 1.259e+00 , Validation Loss: 4.234e-01\n",
      "Iteration: 18707, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18708, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 18709, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 18710, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18711, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 18712, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 18713, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 18714, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 18715, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 18716, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 18717, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 18718, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18719, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18720, Training Loss: 1.007e+00 , Validation Loss: 2.365e+00\n",
      "Iteration: 18721, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18722, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18723, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18724, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 18725, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 18726, Training Loss: 1.763e+00 , Validation Loss: 2.576e+00\n",
      "Iteration: 18727, Training Loss: -1.000e-07 , Validation Loss: 6.895e-01\n",
      "Iteration: 18728, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 18729, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18730, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18731, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18732, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18733, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18734, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 18735, Training Loss: 1.763e+00 , Validation Loss: 2.056e+00\n",
      "Iteration: 18736, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 18737, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 18738, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18739, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18740, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18741, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18742, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18743, Training Loss: -1.000e-07 , Validation Loss: 6.713e-01\n",
      "Iteration: 18744, Training Loss: -1.000e-07 , Validation Loss: 6.713e-01\n",
      "Iteration: 18745, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 18746, Training Loss: 2.518e-01 , Validation Loss: 1.083e+00\n",
      "Iteration: 18747, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 18748, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 18749, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 18750, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 18751, Training Loss: 1.259e+00 , Validation Loss: 2.933e+00\n",
      "Iteration: 18752, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18753, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 18754, Training Loss: 7.555e-01 , Validation Loss: 9.435e-01\n",
      "Iteration: 18755, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 18756, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 18757, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 18758, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 18759, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 18760, Training Loss: 2.518e-01 , Validation Loss: 1.343e+00\n",
      "Iteration: 18761, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 18762, Training Loss: 7.555e-01 , Validation Loss: 2.655e+00\n",
      "Iteration: 18763, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 18764, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 18765, Training Loss: 1.007e+00 , Validation Loss: 3.490e+00\n",
      "Iteration: 18766, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 18767, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18768, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18769, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 18770, Training Loss: 7.555e-01 , Validation Loss: 1.258e+00\n",
      "Iteration: 18771, Training Loss: 2.518e-01 , Validation Loss: 8.588e-01\n",
      "Iteration: 18772, Training Loss: 3.526e+00 , Validation Loss: 3.998e+00\n",
      "Iteration: 18773, Training Loss: 7.555e-01 , Validation Loss: 2.359e+00\n",
      "Iteration: 18774, Training Loss: 7.555e-01 , Validation Loss: 1.530e+00\n",
      "Iteration: 18775, Training Loss: 1.763e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 18776, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18777, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18778, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18779, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18780, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18781, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18782, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18783, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 18784, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 18785, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18786, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18787, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18788, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18789, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 18790, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18791, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 18792, Training Loss: 1.763e+00 , Validation Loss: 3.532e+00\n",
      "Iteration: 18793, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 18794, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18795, Training Loss: 5.037e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 18796, Training Loss: 5.037e-01 , Validation Loss: 2.371e+00\n",
      "Iteration: 18797, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18798, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18799, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18800, Training Loss: 5.037e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 18801, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18802, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18803, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18804, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 18805, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18806, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18807, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18808, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18809, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18810, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18811, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18812, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18813, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18814, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18815, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 18816, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18817, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18818, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18819, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18820, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18821, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18822, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18823, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18824, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18825, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18826, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 18827, Training Loss: -1.000e-07 , Validation Loss: 1.802e+00\n",
      "Iteration: 18828, Training Loss: 2.518e-01 , Validation Loss: 1.802e+00\n",
      "Iteration: 18829, Training Loss: 2.267e+00 , Validation Loss: 4.312e+00\n",
      "Iteration: 18830, Training Loss: 1.259e+00 , Validation Loss: 1.917e+00\n",
      "Iteration: 18831, Training Loss: 1.007e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 18832, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 18833, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18834, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18835, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18836, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18837, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18838, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18839, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 18840, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 18841, Training Loss: 5.037e-01 , Validation Loss: 1.349e+00\n",
      "Iteration: 18842, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18843, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18844, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 18845, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 18846, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18847, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18848, Training Loss: 2.518e-01 , Validation Loss: 1.385e+00\n",
      "Iteration: 18849, Training Loss: 3.022e+00 , Validation Loss: 3.562e+00\n",
      "Iteration: 18850, Training Loss: 2.015e+00 , Validation Loss: 1.131e+00\n",
      "Iteration: 18851, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18852, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18853, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18854, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18855, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 18856, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18857, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18858, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18859, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18860, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18861, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18862, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18863, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18864, Training Loss: -1.000e-07 , Validation Loss: 6.169e-01\n",
      "Iteration: 18865, Training Loss: 1.007e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 18866, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18867, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18868, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18869, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18870, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 18871, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18872, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18873, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18874, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18875, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18876, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18877, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18878, Training Loss: 1.511e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 18879, Training Loss: 1.763e+00 , Validation Loss: 2.607e+00\n",
      "Iteration: 18880, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 18881, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18882, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18883, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18884, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18885, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18886, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18887, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18888, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18889, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18890, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18891, Training Loss: 1.007e+00 , Validation Loss: 2.510e+00\n",
      "Iteration: 18892, Training Loss: 2.518e-01 , Validation Loss: 1.198e+00\n",
      "Iteration: 18893, Training Loss: 3.526e+00 , Validation Loss: 3.907e+00\n",
      "Iteration: 18894, Training Loss: 3.022e+00 , Validation Loss: 2.226e+00\n",
      "Iteration: 18895, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18896, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18897, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18898, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18899, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18900, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 18901, Training Loss: 4.785e+00 , Validation Loss: 4.923e+00\n",
      "Iteration: 18902, Training Loss: 6.800e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 18903, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18904, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18905, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 18906, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 18907, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18908, Training Loss: 2.518e-01 , Validation Loss: 9.858e-01\n",
      "Iteration: 18909, Training Loss: 2.770e+00 , Validation Loss: 3.714e+00\n",
      "Iteration: 18910, Training Loss: 1.511e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 18911, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 18912, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18913, Training Loss: 1.511e+00 , Validation Loss: 3.103e+00\n",
      "Iteration: 18914, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 18915, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 18916, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 18917, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18918, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18919, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18920, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18921, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18922, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 18923, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18924, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18925, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18926, Training Loss: 2.770e+00 , Validation Loss: 3.859e+00\n",
      "Iteration: 18927, Training Loss: 2.518e+00 , Validation Loss: 2.764e+00\n",
      "Iteration: 18928, Training Loss: 1.007e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 18929, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18930, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18931, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18932, Training Loss: 3.274e+00 , Validation Loss: 4.234e+00\n",
      "Iteration: 18933, Training Loss: 1.763e+00 , Validation Loss: 2.413e+00\n",
      "Iteration: 18934, Training Loss: 1.511e+00 , Validation Loss: 9.979e-01\n",
      "Iteration: 18935, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 18936, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18937, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18938, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 18939, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18940, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 18941, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18942, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18943, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18944, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18945, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18946, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18947, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 18948, Training Loss: 3.022e+00 , Validation Loss: 4.222e+00\n",
      "Iteration: 18949, Training Loss: 2.267e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 18950, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 18951, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 18952, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18953, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 18954, Training Loss: 1.259e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 18955, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 18956, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 18957, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 18958, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18959, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18960, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18961, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18962, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18963, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 18964, Training Loss: 1.259e+00 , Validation Loss: 3.895e+00\n",
      "Iteration: 18965, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 18966, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 18967, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18968, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18969, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 18970, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 18971, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 18972, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 18973, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 18974, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 18975, Training Loss: 5.037e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 18976, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 18977, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18978, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 18979, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 18980, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 18981, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 18982, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18983, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 18984, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 18985, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 18986, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 18987, Training Loss: 2.518e-01 , Validation Loss: 1.137e+00\n",
      "Iteration: 18988, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 18989, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 18990, Training Loss: 1.763e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 18991, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 18992, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 18993, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 18994, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 18995, Training Loss: 1.007e+00 , Validation Loss: 1.198e+00\n",
      "Iteration: 18996, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 18997, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 18998, Training Loss: 1.007e+00 , Validation Loss: 1.052e+00\n",
      "Iteration: 18999, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 19000, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19001, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19002, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19003, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19004, Training Loss: 1.007e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 19005, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19006, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19007, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 19008, Training Loss: -1.000e-07 , Validation Loss: 1.161e+00\n",
      "Iteration: 19009, Training Loss: 7.555e-01 , Validation Loss: 1.161e+00\n",
      "Iteration: 19010, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19011, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19012, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19013, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19014, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 19015, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 19016, Training Loss: 1.007e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 19017, Training Loss: 2.518e-01 , Validation Loss: 1.385e+00\n",
      "Iteration: 19018, Training Loss: -1.000e-07 , Validation Loss: 7.621e-01\n",
      "Iteration: 19019, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 19020, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 19021, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19022, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19023, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19024, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19025, Training Loss: 5.037e-01 , Validation Loss: 1.923e+00\n",
      "Iteration: 19026, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 19027, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19028, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 19029, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 19030, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19031, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 19032, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19033, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19034, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19035, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 19036, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19037, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 19038, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 19039, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 19040, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19041, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19042, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 19043, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19044, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 19045, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 19046, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 19047, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19048, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 19049, Training Loss: 1.007e+00 , Validation Loss: 1.004e+00\n",
      "Iteration: 19050, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19051, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19052, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19053, Training Loss: 2.518e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 19054, Training Loss: 1.007e+00 , Validation Loss: 2.316e+00\n",
      "Iteration: 19055, Training Loss: 5.037e-01 , Validation Loss: 2.371e+00\n",
      "Iteration: 19056, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 19057, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 19058, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 19059, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 19060, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 19061, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 19062, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19063, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19064, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19065, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19066, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19067, Training Loss: 1.007e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 19068, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 19069, Training Loss: 1.007e+00 , Validation Loss: 5.988e-01\n",
      "Iteration: 19070, Training Loss: 5.037e-01 , Validation Loss: 1.204e+00\n",
      "Iteration: 19071, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 19072, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19073, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19074, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19075, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19076, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19077, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 19078, Training Loss: 1.259e+00 , Validation Loss: 3.683e+00\n",
      "Iteration: 19079, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 19080, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19081, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 19082, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19083, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19084, Training Loss: 5.037e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 19085, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19086, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19087, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19088, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19089, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 19090, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 19091, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 19092, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19093, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19094, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 19095, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 19096, Training Loss: 7.555e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 19097, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19098, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19099, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19100, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 19101, Training Loss: 1.007e+00 , Validation Loss: 3.810e+00\n",
      "Iteration: 19102, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19103, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19104, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19105, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19106, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 19107, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 19108, Training Loss: 2.518e-01 , Validation Loss: 1.125e+00\n",
      "Iteration: 19109, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 19110, Training Loss: 2.518e-01 , Validation Loss: 2.062e+00\n",
      "Iteration: 19111, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 19112, Training Loss: 5.037e-01 , Validation Loss: 2.976e+00\n",
      "Iteration: 19113, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19114, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19115, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19116, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 19117, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19118, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19119, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19120, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19121, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 19122, Training Loss: 1.259e+00 , Validation Loss: 2.274e+00\n",
      "Iteration: 19123, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 19124, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19125, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19126, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 19127, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 19128, Training Loss: 7.555e-01 , Validation Loss: 2.329e+00\n",
      "Iteration: 19129, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 19130, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 19131, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19132, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 19133, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 19134, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 19135, Training Loss: 5.037e-01 , Validation Loss: 1.222e+00\n",
      "Iteration: 19136, Training Loss: 1.511e+00 , Validation Loss: 2.050e+00\n",
      "Iteration: 19137, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 19138, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19139, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19140, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19141, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19142, Training Loss: 1.259e+00 , Validation Loss: 2.304e+00\n",
      "Iteration: 19143, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 19144, Training Loss: 7.555e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 19145, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 19146, Training Loss: 5.037e-01 , Validation Loss: 2.232e+00\n",
      "Iteration: 19147, Training Loss: 5.037e-01 , Validation Loss: 1.881e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 19148, Training Loss: 1.007e+00 , Validation Loss: 2.389e+00\n",
      "Iteration: 19149, Training Loss: 1.007e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 19150, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19151, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19152, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19153, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19154, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 19155, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19156, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19157, Training Loss: 5.037e-01 , Validation Loss: 1.960e+00\n",
      "Iteration: 19158, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 19159, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19160, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19161, Training Loss: 7.555e-01 , Validation Loss: 1.391e+00\n",
      "Iteration: 19162, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 19163, Training Loss: 1.007e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 19164, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19165, Training Loss: 1.511e+00 , Validation Loss: 2.117e+00\n",
      "Iteration: 19166, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19167, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19168, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19169, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 19170, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19171, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19172, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 19173, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19174, Training Loss: 2.518e-01 , Validation Loss: 1.246e+00\n",
      "Iteration: 19175, Training Loss: 2.770e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 19176, Training Loss: 2.518e-01 , Validation Loss: 1.403e+00\n",
      "Iteration: 19177, Training Loss: 1.259e+00 , Validation Loss: 1.071e+00\n",
      "Iteration: 19178, Training Loss: 1.511e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 19179, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 19180, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 19181, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19182, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19183, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 19184, Training Loss: 1.259e+00 , Validation Loss: 1.294e+00\n",
      "Iteration: 19185, Training Loss: 2.518e-01 , Validation Loss: 1.409e+00\n",
      "Iteration: 19186, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 19187, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 19188, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 19189, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19190, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19191, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19192, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19193, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19194, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19195, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19196, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19197, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19198, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19199, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 19200, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 19201, Training Loss: 3.778e+00 , Validation Loss: 4.548e+00\n",
      "Iteration: 19202, Training Loss: 1.763e+00 , Validation Loss: 2.280e+00\n",
      "Iteration: 19203, Training Loss: 1.259e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 19204, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19205, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19206, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19207, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19208, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19209, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19210, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19211, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19212, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19213, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19214, Training Loss: 5.037e-01 , Validation Loss: 1.452e+00\n",
      "Iteration: 19215, Training Loss: 5.037e+00 , Validation Loss: 5.195e+00\n",
      "Iteration: 19216, Training Loss: 1.184e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 19217, Training Loss: 2.770e+00 , Validation Loss: 5.159e+00\n",
      "Iteration: 19218, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 19219, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 19220, Training Loss: 2.267e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 19221, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19222, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 19223, Training Loss: 2.015e+00 , Validation Loss: 2.734e+00\n",
      "Iteration: 19224, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19225, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19226, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19227, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 19228, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19229, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19230, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19231, Training Loss: 2.015e+00 , Validation Loss: 3.236e+00\n",
      "Iteration: 19232, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 19233, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 19234, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19235, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19236, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19237, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19238, Training Loss: 2.518e+00 , Validation Loss: 4.306e+00\n",
      "Iteration: 19239, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 19240, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 19241, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19242, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19243, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19244, Training Loss: 5.037e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 19245, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19246, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19247, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19248, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19249, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19250, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19251, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 19252, Training Loss: 2.518e-01 , Validation Loss: 1.506e+00\n",
      "Iteration: 19253, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19254, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19255, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19256, Training Loss: 2.518e-01 , Validation Loss: 1.857e+00\n",
      "Iteration: 19257, Training Loss: 2.770e+00 , Validation Loss: 4.312e+00\n",
      "Iteration: 19258, Training Loss: 5.037e-01 , Validation Loss: 1.240e+00\n",
      "Iteration: 19259, Training Loss: 1.763e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 19260, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19261, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19262, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19263, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 19264, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 19265, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 19266, Training Loss: 7.555e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 19267, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19268, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19269, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19270, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19271, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19272, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19273, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19274, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 19275, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19276, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19277, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19278, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 19279, Training Loss: 3.274e+00 , Validation Loss: 4.343e+00\n",
      "Iteration: 19280, Training Loss: 2.015e+00 , Validation Loss: 3.859e+00\n",
      "Iteration: 19281, Training Loss: 2.267e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 19282, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19283, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 19284, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19285, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 19286, Training Loss: 2.518e+00 , Validation Loss: 5.117e+00\n",
      "Iteration: 19287, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 19288, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19289, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19290, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19291, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19292, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19293, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19294, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19295, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19296, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 19297, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19298, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19299, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19300, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19301, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19302, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19303, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19304, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19305, Training Loss: 3.022e+00 , Validation Loss: 4.264e+00\n",
      "Iteration: 19306, Training Loss: 2.770e+00 , Validation Loss: 2.752e+00\n",
      "Iteration: 19307, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19308, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 19309, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 19310, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19311, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19312, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19313, Training Loss: 1.511e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 19314, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19315, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19316, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19317, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19318, Training Loss: -1.000e-07 , Validation Loss: 4.959e-01\n",
      "Iteration: 19319, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 19320, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 19321, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 19322, Training Loss: 5.037e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 19323, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19324, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19325, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 19326, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 19327, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 19328, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 19329, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 19330, Training Loss: 2.015e+00 , Validation Loss: 3.145e+00\n",
      "Iteration: 19331, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 19332, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 19333, Training Loss: 7.555e-01 , Validation Loss: 1.802e+00\n",
      "Iteration: 19334, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19335, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 19336, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 19337, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19338, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19339, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19340, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19341, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19342, Training Loss: 7.555e-01 , Validation Loss: 1.784e+00\n",
      "Iteration: 19343, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 19344, Training Loss: 3.526e+00 , Validation Loss: 4.941e+00\n",
      "Iteration: 19345, Training Loss: 2.770e+00 , Validation Loss: 2.673e+00\n",
      "Iteration: 19346, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19347, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19348, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19349, Training Loss: 7.555e-01 , Validation Loss: 1.881e+00\n",
      "Iteration: 19350, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 19351, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19352, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19353, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19354, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19355, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 19356, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19357, Training Loss: -1.000e-07 , Validation Loss: 5.141e-01\n",
      "Iteration: 19358, Training Loss: 7.555e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 19359, Training Loss: 2.015e+00 , Validation Loss: 3.913e+00\n",
      "Iteration: 19360, Training Loss: 1.511e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 19361, Training Loss: 1.511e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 19362, Training Loss: 1.259e+00 , Validation Loss: 3.193e+00\n",
      "Iteration: 19363, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 19364, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19365, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19366, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19367, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19368, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 19369, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19370, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19371, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19372, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19373, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19374, Training Loss: 7.555e-01 , Validation Loss: 1.331e+00\n",
      "Iteration: 19375, Training Loss: 2.518e+00 , Validation Loss: 4.336e+00\n",
      "Iteration: 19376, Training Loss: 1.511e+00 , Validation Loss: 1.300e+00\n",
      "Iteration: 19377, Training Loss: 1.259e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 19378, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19379, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19380, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19381, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19382, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19383, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19384, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19385, Training Loss: 1.259e+00 , Validation Loss: 3.447e+00\n",
      "Iteration: 19386, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 19387, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19388, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19389, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 19390, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 19391, Training Loss: 3.022e+00 , Validation Loss: 3.986e+00\n",
      "Iteration: 19392, Training Loss: 7.555e-01 , Validation Loss: 1.046e+00\n",
      "Iteration: 19393, Training Loss: 7.555e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 19394, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 19395, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19396, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19397, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19398, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 19399, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19400, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 19401, Training Loss: 7.555e-01 , Validation Loss: 1.264e+00\n",
      "Iteration: 19402, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19403, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19404, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19405, Training Loss: 1.259e+00 , Validation Loss: 2.746e+00\n",
      "Iteration: 19406, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 19407, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19408, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19409, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19410, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19411, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19412, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 19413, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19414, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19415, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19416, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19417, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19418, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19419, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19420, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 19421, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19422, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 19423, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19424, Training Loss: 1.007e+00 , Validation Loss: 1.288e+00\n",
      "Iteration: 19425, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 19426, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 19427, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19428, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19429, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 19430, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19431, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19432, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19433, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19434, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19435, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19436, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19437, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19438, Training Loss: -1.000e-07 , Validation Loss: 7.621e-01\n",
      "Iteration: 19439, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 19440, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 19441, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19442, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 19443, Training Loss: 2.518e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 19444, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 19445, Training Loss: 2.518e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 19446, Training Loss: 7.555e-01 , Validation Loss: 1.427e+00\n",
      "Iteration: 19447, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19448, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 19449, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 19450, Training Loss: 3.526e+00 , Validation Loss: 3.992e+00\n",
      "Iteration: 19451, Training Loss: 5.541e+00 , Validation Loss: 5.044e+00\n",
      "Iteration: 19452, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19453, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19454, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19455, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19456, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19457, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19458, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19459, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19460, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19461, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19462, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19463, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19464, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 19465, Training Loss: 4.030e+00 , Validation Loss: 3.738e+00\n",
      "Iteration: 19466, Training Loss: 8.563e+00 , Validation Loss: 9.254e+00\n",
      "Iteration: 19467, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19468, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19469, Training Loss: 7.555e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 19470, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19471, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19472, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19473, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19474, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19475, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 19476, Training Loss: 2.518e+00 , Validation Loss: 3.036e+00\n",
      "Iteration: 19477, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 19478, Training Loss: 1.007e+00 , Validation Loss: 6.471e-01\n",
      "Iteration: 19479, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19480, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 19481, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19482, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19483, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19484, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19485, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 19486, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19487, Training Loss: -1.000e-07 , Validation Loss: 1.191e+00\n",
      "Iteration: 19488, Training Loss: 7.555e-01 , Validation Loss: 1.191e+00\n",
      "Iteration: 19489, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19490, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19491, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19492, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19493, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19494, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19495, Training Loss: 1.511e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 19496, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19497, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19498, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19499, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19500, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19501, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19502, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 19503, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19504, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19505, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19506, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19507, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19508, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19509, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19510, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 19511, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19512, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19513, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19514, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 19515, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 19516, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 19517, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 19518, Training Loss: 7.555e-01 , Validation Loss: 1.560e+00\n",
      "Iteration: 19519, Training Loss: -1.000e-07 , Validation Loss: 1.125e+00\n",
      "Iteration: 19520, Training Loss: 2.518e-01 , Validation Loss: 1.125e+00\n",
      "Iteration: 19521, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 19522, Training Loss: 1.259e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 19523, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19524, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 19525, Training Loss: 3.022e+00 , Validation Loss: 5.062e+00\n",
      "Iteration: 19526, Training Loss: 2.015e+00 , Validation Loss: 2.437e+00\n",
      "Iteration: 19527, Training Loss: 7.555e-01 , Validation Loss: 8.588e-01\n",
      "Iteration: 19528, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19529, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 19530, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 19531, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19532, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19533, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19534, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19535, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 19536, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 19537, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 19538, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 19539, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19540, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 19541, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19542, Training Loss: 2.518e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 19543, Training Loss: 3.022e+00 , Validation Loss: 3.986e+00\n",
      "Iteration: 19544, Training Loss: -1.000e-07 , Validation Loss: 1.445e+00\n",
      "Iteration: 19545, Training Loss: 1.763e+00 , Validation Loss: 1.445e+00\n",
      "Iteration: 19546, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 19547, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19548, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19549, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 19550, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19551, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 19552, Training Loss: 1.007e+00 , Validation Loss: 3.544e+00\n",
      "Iteration: 19553, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19554, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19555, Training Loss: -1.000e-07 , Validation Loss: 5.262e-01\n",
      "Iteration: 19556, Training Loss: 2.518e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 19557, Training Loss: 1.259e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 19558, Training Loss: 2.770e+00 , Validation Loss: 4.385e+00\n",
      "Iteration: 19559, Training Loss: 3.022e+00 , Validation Loss: 2.208e+00\n",
      "Iteration: 19560, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 19561, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19562, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 19563, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19564, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19565, Training Loss: 1.511e+00 , Validation Loss: 2.286e+00\n",
      "Iteration: 19566, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19567, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19568, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19569, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19570, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 19571, Training Loss: 5.037e-01 , Validation Loss: 1.458e+00\n",
      "Iteration: 19572, Training Loss: 3.778e+00 , Validation Loss: 5.026e+00\n",
      "Iteration: 19573, Training Loss: 3.274e+00 , Validation Loss: 3.605e+00\n",
      "Iteration: 19574, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 19575, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19576, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19577, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19578, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19579, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19580, Training Loss: 2.267e+00 , Validation Loss: 4.427e+00\n",
      "Iteration: 19581, Training Loss: 1.763e+00 , Validation Loss: 1.294e+00\n",
      "Iteration: 19582, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19583, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 19584, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19585, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19586, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19587, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19588, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 19589, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 19590, Training Loss: 5.037e-01 , Validation Loss: 1.494e+00\n",
      "Iteration: 19591, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19592, Training Loss: -1.000e-07 , Validation Loss: 5.322e-01\n",
      "Iteration: 19593, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 19594, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 19595, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19596, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19597, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 19598, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19599, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19600, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19601, Training Loss: 7.555e-01 , Validation Loss: 2.951e+00\n",
      "Iteration: 19602, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 19603, Training Loss: 7.555e-01 , Validation Loss: 3.556e+00\n",
      "Iteration: 19604, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19605, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19606, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19607, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19608, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19609, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19610, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19611, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19612, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19613, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19614, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19615, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19616, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19617, Training Loss: 7.555e-01 , Validation Loss: 2.129e+00\n",
      "Iteration: 19618, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19619, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19620, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19621, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 19622, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19623, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19624, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19625, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19626, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19627, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 19628, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19629, Training Loss: -1.000e-07 , Validation Loss: 1.125e+00\n",
      "Iteration: 19630, Training Loss: 7.555e-01 , Validation Loss: 1.125e+00\n",
      "Iteration: 19631, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19632, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 19633, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 19634, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 19635, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19636, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19637, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19638, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19639, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19640, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19641, Training Loss: 1.259e+00 , Validation Loss: 1.790e+00\n",
      "Iteration: 19642, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 19643, Training Loss: 5.037e-01 , Validation Loss: 1.820e+00\n",
      "Iteration: 19644, Training Loss: 2.015e+00 , Validation Loss: 3.756e+00\n",
      "Iteration: 19645, Training Loss: 1.511e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 19646, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19647, Training Loss: 7.555e-01 , Validation Loss: 1.270e+00\n",
      "Iteration: 19648, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19649, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19650, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19651, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 19652, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19653, Training Loss: 1.259e+00 , Validation Loss: 3.472e+00\n",
      "Iteration: 19654, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19655, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 19656, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19657, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19658, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 19659, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19660, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19661, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19662, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19663, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 19664, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 19665, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19666, Training Loss: 2.518e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 19667, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19668, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19669, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19670, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19671, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 19672, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19673, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 19674, Training Loss: 5.037e-01 , Validation Loss: 1.143e+00\n",
      "Iteration: 19675, Training Loss: 5.037e-01 , Validation Loss: 2.044e+00\n",
      "Iteration: 19676, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19677, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 19678, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19679, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19680, Training Loss: 3.526e+00 , Validation Loss: 3.949e+00\n",
      "Iteration: 19681, Training Loss: 2.267e+00 , Validation Loss: 1.476e+00\n",
      "Iteration: 19682, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19683, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19684, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19685, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19686, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19687, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19688, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19689, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19690, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19691, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19692, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 19693, Training Loss: 4.281e+00 , Validation Loss: 5.074e+00\n",
      "Iteration: 19694, Training Loss: 8.815e+00 , Validation Loss: 9.647e+00\n",
      "Iteration: 19695, Training Loss: 1.007e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 19696, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 19697, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 19698, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19699, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19700, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19701, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19702, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19703, Training Loss: 2.770e+00 , Validation Loss: 3.671e+00\n",
      "Iteration: 19704, Training Loss: 1.259e+00 , Validation Loss: 1.566e+00\n",
      "Iteration: 19705, Training Loss: 1.259e+00 , Validation Loss: 6.592e-01\n",
      "Iteration: 19706, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 19707, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19708, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19709, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19710, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19711, Training Loss: 2.518e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 19712, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19713, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19714, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19715, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19716, Training Loss: 1.259e+00 , Validation Loss: 2.964e+00\n",
      "Iteration: 19717, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19718, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19719, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19720, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19721, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 19722, Training Loss: 5.037e-01 , Validation Loss: 1.675e+00\n",
      "Iteration: 19723, Training Loss: 2.518e-01 , Validation Loss: 1.700e+00\n",
      "Iteration: 19724, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 19725, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 19726, Training Loss: 1.007e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 19727, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19728, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19729, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 19730, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 19731, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 19732, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19733, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19734, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19735, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19736, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19737, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19738, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19739, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 19740, Training Loss: 1.007e+00 , Validation Loss: 4.778e-01\n",
      "Iteration: 19741, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19742, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19743, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19744, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19745, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 19746, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 19747, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19748, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19749, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 19750, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19751, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19752, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19753, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19754, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 19755, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19756, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19757, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19758, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19759, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19760, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 19761, Training Loss: 2.518e+00 , Validation Loss: 3.889e+00\n",
      "Iteration: 19762, Training Loss: 1.259e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 19763, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19764, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19765, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19766, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19767, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19768, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19769, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19770, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 19771, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 19772, Training Loss: 1.007e+00 , Validation Loss: 2.710e+00\n",
      "Iteration: 19773, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19774, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19775, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19776, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 19777, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 19778, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19779, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19780, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 19781, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19782, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19783, Training Loss: 2.518e+00 , Validation Loss: 3.889e+00\n",
      "Iteration: 19784, Training Loss: 1.259e+00 , Validation Loss: 1.790e+00\n",
      "Iteration: 19785, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 19786, Training Loss: 1.007e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 19787, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19788, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19789, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19790, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19791, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19792, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19793, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19794, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19795, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19796, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 19797, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 19798, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 19799, Training Loss: -1.000e-07 , Validation Loss: 2.201e+00\n",
      "Iteration: 19800, Training Loss: 5.037e-01 , Validation Loss: 2.201e+00\n",
      "Iteration: 19801, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19802, Training Loss: -1.000e-07 , Validation Loss: 7.197e-01\n",
      "Iteration: 19803, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 19804, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19805, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19806, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19807, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 19808, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19809, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19810, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 19811, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 19812, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19813, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 19814, Training Loss: 1.511e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 19815, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 19816, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 19817, Training Loss: 1.511e+00 , Validation Loss: 1.960e+00\n",
      "Iteration: 19818, Training Loss: 1.259e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 19819, Training Loss: 5.037e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 19820, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19821, Training Loss: 7.555e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 19822, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19823, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19824, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19825, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 19826, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 19827, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 19828, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19829, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19830, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 19831, Training Loss: 3.022e+00 , Validation Loss: 4.179e+00\n",
      "Iteration: 19832, Training Loss: 1.763e+00 , Validation Loss: 3.085e+00\n",
      "Iteration: 19833, Training Loss: 1.763e+00 , Validation Loss: 1.028e+00\n",
      "Iteration: 19834, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19835, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19836, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19837, Training Loss: 7.555e-01 , Validation Loss: 1.046e+00\n",
      "Iteration: 19838, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19839, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19840, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19841, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19842, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19843, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 19844, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19845, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19846, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19847, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 19848, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19849, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19850, Training Loss: 2.518e-01 , Validation Loss: 1.415e+00\n",
      "Iteration: 19851, Training Loss: 2.267e+00 , Validation Loss: 3.992e+00\n",
      "Iteration: 19852, Training Loss: 1.259e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 19853, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19854, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 19855, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19856, Training Loss: 2.518e-01 , Validation Loss: 1.058e+00\n",
      "Iteration: 19857, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 19858, Training Loss: 1.259e+00 , Validation Loss: 2.861e+00\n",
      "Iteration: 19859, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 19860, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19861, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19862, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19863, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 19864, Training Loss: 7.555e-01 , Validation Loss: 1.784e+00\n",
      "Iteration: 19865, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 19866, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19867, Training Loss: 1.007e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 19868, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19869, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19870, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19871, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19872, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19873, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 19874, Training Loss: 7.555e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 19875, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 19876, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 19877, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 19878, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 19879, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19880, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 19881, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19882, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19883, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 19884, Training Loss: 5.037e-01 , Validation Loss: 2.595e+00\n",
      "Iteration: 19885, Training Loss: 2.518e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 19886, Training Loss: 1.259e+00 , Validation Loss: 1.004e+00\n",
      "Iteration: 19887, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 19888, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19889, Training Loss: 7.555e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 19890, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 19891, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 19892, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 19893, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 19894, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19895, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19896, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19897, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19898, Training Loss: 7.555e-01 , Validation Loss: 1.403e+00\n",
      "Iteration: 19899, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 19900, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19901, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19902, Training Loss: 1.259e+00 , Validation Loss: 3.435e+00\n",
      "Iteration: 19903, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19904, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19905, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19906, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19907, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19908, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19909, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19910, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 19911, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19912, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19913, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19914, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19915, Training Loss: 4.030e+00 , Validation Loss: 4.790e+00\n",
      "Iteration: 19916, Training Loss: 2.015e+00 , Validation Loss: 4.082e+00\n",
      "Iteration: 19917, Training Loss: 2.267e+00 , Validation Loss: 1.476e+00\n",
      "Iteration: 19918, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 19919, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19920, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19921, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19922, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19923, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19924, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 19925, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19926, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19927, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 19928, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 19929, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19930, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19931, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 19932, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19933, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19934, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19935, Training Loss: 1.259e+00 , Validation Loss: 4.838e-01\n",
      "Iteration: 19936, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 19937, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 19938, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 19939, Training Loss: 4.281e+00 , Validation Loss: 4.518e+00\n",
      "Iteration: 19940, Training Loss: 1.133e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 19941, Training Loss: 1.511e+00 , Validation Loss: 2.437e+00\n",
      "Iteration: 19942, Training Loss: 7.555e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 19943, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19944, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 19945, Training Loss: 5.037e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 19946, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19947, Training Loss: 2.015e+00 , Validation Loss: 3.326e+00\n",
      "Iteration: 19948, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 19949, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 19950, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 19951, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19952, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19953, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19954, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19955, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19956, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19957, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19958, Training Loss: -1.000e-07 , Validation Loss: 2.510e+00\n",
      "Iteration: 19959, Training Loss: 1.511e+00 , Validation Loss: 2.510e+00\n",
      "Iteration: 19960, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19961, Training Loss: 1.763e+00 , Validation Loss: 3.472e+00\n",
      "Iteration: 19962, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 19963, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 19964, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19965, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19966, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 19967, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 19968, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19969, Training Loss: 2.267e+00 , Validation Loss: 3.387e+00\n",
      "Iteration: 19970, Training Loss: 1.007e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 19971, Training Loss: 1.007e+00 , Validation Loss: 7.802e-01\n",
      "Iteration: 19972, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19973, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19974, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19975, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19976, Training Loss: 1.259e+00 , Validation Loss: 1.179e+00\n",
      "Iteration: 19977, Training Loss: 3.526e+00 , Validation Loss: 4.603e+00\n",
      "Iteration: 19978, Training Loss: 1.259e+00 , Validation Loss: 1.911e+00\n",
      "Iteration: 19979, Training Loss: 1.763e+00 , Validation Loss: 1.071e+00\n",
      "Iteration: 19980, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 19981, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19982, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19983, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19984, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19985, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19986, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19987, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 19988, Training Loss: 3.778e+00 , Validation Loss: 4.409e+00\n",
      "Iteration: 19989, Training Loss: 1.259e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 19990, Training Loss: 7.555e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 19991, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 19992, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 19993, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 19994, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19995, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 19996, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19997, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 19998, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 19999, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20000, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20001, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 20002, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 20003, Training Loss: 2.015e+00 , Validation Loss: 4.385e+00\n",
      "Iteration: 20004, Training Loss: 1.763e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 20005, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20006, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20007, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20008, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 20009, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20010, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20011, Training Loss: 1.259e+00 , Validation Loss: 2.625e+00\n",
      "Iteration: 20012, Training Loss: 1.763e+00 , Validation Loss: 3.012e+00\n",
      "Iteration: 20013, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20014, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 20015, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20016, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20017, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 20018, Training Loss: 1.007e+00 , Validation Loss: 9.677e-01\n",
      "Iteration: 20019, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20020, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20021, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20022, Training Loss: 7.555e-01 , Validation Loss: 3.375e+00\n",
      "Iteration: 20023, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20024, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20025, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 20026, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20027, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20028, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20029, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20030, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20031, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20032, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20033, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20034, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20035, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20036, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20037, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20038, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20039, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20040, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20041, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20042, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20043, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20044, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20045, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20046, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20047, Training Loss: 2.518e-01 , Validation Loss: 1.687e+00\n",
      "Iteration: 20048, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 20049, Training Loss: 2.518e-01 , Validation Loss: 2.008e+00\n",
      "Iteration: 20050, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 20051, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20052, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20053, Training Loss: 5.037e-01 , Validation Loss: 3.151e+00\n",
      "Iteration: 20054, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20055, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20056, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 20057, Training Loss: 2.015e+00 , Validation Loss: 4.409e+00\n",
      "Iteration: 20058, Training Loss: 1.007e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 20059, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20060, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20061, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20062, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20063, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20064, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20065, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20066, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20067, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20068, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20069, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 20070, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20071, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 20072, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 20073, Training Loss: 1.259e+00 , Validation Loss: 2.425e+00\n",
      "Iteration: 20074, Training Loss: 7.555e-01 , Validation Loss: 2.250e+00\n",
      "Iteration: 20075, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 20076, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 20077, Training Loss: 1.007e+00 , Validation Loss: 4.838e-01\n",
      "Iteration: 20078, Training Loss: 5.037e-01 , Validation Loss: 2.837e+00\n",
      "Iteration: 20079, Training Loss: 5.037e-01 , Validation Loss: 3.544e+00\n",
      "Iteration: 20080, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20081, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20082, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20083, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20084, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20085, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20086, Training Loss: 7.555e-01 , Validation Loss: 1.113e+00\n",
      "Iteration: 20087, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20088, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20089, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 20090, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 20091, Training Loss: 2.015e+00 , Validation Loss: 3.538e+00\n",
      "Iteration: 20092, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 20093, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 20094, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 20095, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20096, Training Loss: -1.000e-07 , Validation Loss: 1.427e+00\n",
      "Iteration: 20097, Training Loss: 1.259e+00 , Validation Loss: 1.427e+00\n",
      "Iteration: 20098, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 20099, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20100, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 20101, Training Loss: 7.555e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 20102, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 20103, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20104, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20105, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20106, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20107, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20108, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20109, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20110, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20111, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20112, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 20113, Training Loss: 1.259e+00 , Validation Loss: 3.320e+00\n",
      "Iteration: 20114, Training Loss: 1.007e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 20115, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 20116, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20117, Training Loss: 1.007e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 20118, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20119, Training Loss: 5.037e-01 , Validation Loss: 1.210e+00\n",
      "Iteration: 20120, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 20121, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20122, Training Loss: 5.037e-01 , Validation Loss: 1.579e+00\n",
      "Iteration: 20123, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20124, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 20125, Training Loss: 2.518e-01 , Validation Loss: 9.616e-01\n",
      "Iteration: 20126, Training Loss: 1.763e+00 , Validation Loss: 3.859e+00\n",
      "Iteration: 20127, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20128, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20129, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 20130, Training Loss: 5.037e-01 , Validation Loss: 1.651e+00\n",
      "Iteration: 20131, Training Loss: 2.518e-01 , Validation Loss: 2.062e+00\n",
      "Iteration: 20132, Training Loss: 3.778e+00 , Validation Loss: 4.524e+00\n",
      "Iteration: 20133, Training Loss: 6.296e+00 , Validation Loss: 6.961e+00\n",
      "Iteration: 20134, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20135, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 20136, Training Loss: 1.511e+00 , Validation Loss: 3.974e+00\n",
      "Iteration: 20137, Training Loss: 1.511e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 20138, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20139, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20140, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 20141, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20142, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20143, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20144, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 20145, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20146, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20147, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20148, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20149, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20150, Training Loss: 1.259e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 20151, Training Loss: 2.267e+00 , Validation Loss: 4.113e+00\n",
      "Iteration: 20152, Training Loss: 1.007e+00 , Validation Loss: 1.058e+00\n",
      "Iteration: 20153, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 20154, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 20155, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20156, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20157, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20158, Training Loss: 1.007e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 20159, Training Loss: 2.518e+00 , Validation Loss: 5.002e+00\n",
      "Iteration: 20160, Training Loss: 1.763e+00 , Validation Loss: 9.979e-01\n",
      "Iteration: 20161, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 20162, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 20163, Training Loss: 2.518e+00 , Validation Loss: 3.738e+00\n",
      "Iteration: 20164, Training Loss: 7.555e-01 , Validation Loss: 1.034e+00\n",
      "Iteration: 20165, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 20166, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 20167, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 20168, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 20169, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 20170, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20171, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20172, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 20173, Training Loss: 5.037e-01 , Validation Loss: 1.778e+00\n",
      "Iteration: 20174, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20175, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20176, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20177, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 20178, Training Loss: 3.022e+00 , Validation Loss: 4.016e+00\n",
      "Iteration: 20179, Training Loss: 1.007e+00 , Validation Loss: 2.189e+00\n",
      "Iteration: 20180, Training Loss: 1.259e+00 , Validation Loss: 1.125e+00\n",
      "Iteration: 20181, Training Loss: 1.511e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 20182, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20183, Training Loss: 7.555e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 20184, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 20185, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20186, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 20187, Training Loss: 2.770e+00 , Validation Loss: 4.046e+00\n",
      "Iteration: 20188, Training Loss: 3.022e+00 , Validation Loss: 4.155e+00\n",
      "Iteration: 20189, Training Loss: 1.259e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 20190, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20191, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20192, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 20193, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 20194, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 20195, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 20196, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20197, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 20198, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20199, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20200, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 20201, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 20202, Training Loss: 7.555e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 20203, Training Loss: 1.259e+00 , Validation Loss: 2.226e+00\n",
      "Iteration: 20204, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20205, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20206, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20207, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20208, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20209, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20210, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20211, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20212, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20213, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20214, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20215, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20216, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20217, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20218, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 20219, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 20220, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20221, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 20222, Training Loss: 1.007e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 20223, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20224, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 20225, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 20226, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20227, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20228, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20229, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20230, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20231, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20232, Training Loss: 1.511e+00 , Validation Loss: 3.653e+00\n",
      "Iteration: 20233, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20234, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 20235, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 20236, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20237, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 20238, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 20239, Training Loss: 1.259e+00 , Validation Loss: 2.788e+00\n",
      "Iteration: 20240, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20241, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20242, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20243, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20244, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20245, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 20246, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20247, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20248, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20249, Training Loss: 5.037e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 20250, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20251, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20252, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20253, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20254, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20255, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20256, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20257, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 20258, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20259, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20260, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20261, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20262, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20263, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20264, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20265, Training Loss: 1.259e+00 , Validation Loss: 1.802e+00\n",
      "Iteration: 20266, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20267, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20268, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20269, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20270, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20271, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 20272, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 20273, Training Loss: 7.555e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 20274, Training Loss: 1.007e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 20275, Training Loss: 7.555e-01 , Validation Loss: 1.978e+00\n",
      "Iteration: 20276, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20277, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20278, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 20279, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 20280, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 20281, Training Loss: 1.763e+00 , Validation Loss: 3.822e+00\n",
      "Iteration: 20282, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 20283, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 20284, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 20285, Training Loss: 1.007e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 20286, Training Loss: 7.555e-01 , Validation Loss: 2.129e+00\n",
      "Iteration: 20287, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20288, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20289, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 20290, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 20291, Training Loss: 2.518e-01 , Validation Loss: 1.252e+00\n",
      "Iteration: 20292, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 20293, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 20294, Training Loss: 2.015e+00 , Validation Loss: 4.010e+00\n",
      "Iteration: 20295, Training Loss: 7.555e-01 , Validation Loss: 1.004e+00\n",
      "Iteration: 20296, Training Loss: 1.007e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 20297, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20298, Training Loss: 1.259e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 20299, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20300, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 20301, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 20302, Training Loss: 1.007e+00 , Validation Loss: 1.663e+00\n",
      "Iteration: 20303, Training Loss: 2.267e+00 , Validation Loss: 4.155e+00\n",
      "Iteration: 20304, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 20305, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20306, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 20307, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 20308, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20309, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 20310, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 20311, Training Loss: 2.770e+00 , Validation Loss: 3.665e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20312, Training Loss: 7.555e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 20313, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 20314, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20315, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20316, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20317, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20318, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20319, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20320, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20321, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 20322, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20323, Training Loss: 1.511e+00 , Validation Loss: 1.179e+00\n",
      "Iteration: 20324, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 20325, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 20326, Training Loss: 5.037e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 20327, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20328, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20329, Training Loss: 1.007e+00 , Validation Loss: 2.691e+00\n",
      "Iteration: 20330, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20331, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20332, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 20333, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20334, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20335, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20336, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20337, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20338, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20339, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20340, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 20341, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 20342, Training Loss: -1.000e-07 , Validation Loss: 5.080e-01\n",
      "Iteration: 20343, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 20344, Training Loss: -1.000e-07 , Validation Loss: 7.923e-01\n",
      "Iteration: 20345, Training Loss: 1.259e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 20346, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20347, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20348, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20349, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20350, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20351, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 20352, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 20353, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 20354, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 20355, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 20356, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 20357, Training Loss: 2.518e-01 , Validation Loss: 1.700e+00\n",
      "Iteration: 20358, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 20359, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20360, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20361, Training Loss: -1.000e-07 , Validation Loss: 7.802e-01\n",
      "Iteration: 20362, Training Loss: 5.037e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 20363, Training Loss: 7.555e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 20364, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 20365, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20366, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20367, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 20368, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 20369, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20370, Training Loss: 7.555e-01 , Validation Loss: 1.627e+00\n",
      "Iteration: 20371, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20372, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20373, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20374, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20375, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20376, Training Loss: 1.007e+00 , Validation Loss: 1.693e+00\n",
      "Iteration: 20377, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20378, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20379, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20380, Training Loss: 7.555e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 20381, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 20382, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20383, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20384, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 20385, Training Loss: 3.022e+00 , Validation Loss: 4.336e+00\n",
      "Iteration: 20386, Training Loss: 2.015e+00 , Validation Loss: 1.445e+00\n",
      "Iteration: 20387, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 20388, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20389, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 20390, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 20391, Training Loss: 2.518e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 20392, Training Loss: 1.007e+00 , Validation Loss: 2.504e+00\n",
      "Iteration: 20393, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20394, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20395, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20396, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20397, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20398, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20399, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20400, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20401, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20402, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20403, Training Loss: 2.518e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 20404, Training Loss: -1.000e-07 , Validation Loss: 5.080e-01\n",
      "Iteration: 20405, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 20406, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 20407, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 20408, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20409, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 20410, Training Loss: 5.037e-01 , Validation Loss: 2.038e+00\n",
      "Iteration: 20411, Training Loss: 2.015e+00 , Validation Loss: 3.726e+00\n",
      "Iteration: 20412, Training Loss: 1.259e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 20413, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20414, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20415, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20416, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20417, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20418, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20419, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20420, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20421, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 20422, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 20423, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 20424, Training Loss: 1.007e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 20425, Training Loss: -1.000e-07 , Validation Loss: 9.193e-01\n",
      "Iteration: 20426, Training Loss: 2.518e-01 , Validation Loss: 9.193e-01\n",
      "Iteration: 20427, Training Loss: 3.022e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 20428, Training Loss: 1.763e+00 , Validation Loss: 1.633e+00\n",
      "Iteration: 20429, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20430, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20431, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20432, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20433, Training Loss: 2.518e-01 , Validation Loss: 1.379e+00\n",
      "Iteration: 20434, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 20435, Training Loss: 7.555e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 20436, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 20437, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20438, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20439, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20440, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20441, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20442, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20443, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20444, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20445, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20446, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20447, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 20448, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20449, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20450, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 20451, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 20452, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 20453, Training Loss: 1.511e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 20454, Training Loss: 3.778e+00 , Validation Loss: 4.832e+00\n",
      "Iteration: 20455, Training Loss: 4.030e+00 , Validation Loss: 4.336e+00\n",
      "Iteration: 20456, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 20457, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20458, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 20459, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20460, Training Loss: 7.555e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 20461, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20462, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20463, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20464, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20465, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20466, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 20467, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 20468, Training Loss: 2.015e+00 , Validation Loss: 3.738e+00\n",
      "Iteration: 20469, Training Loss: 1.511e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 20470, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 20471, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20472, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 20473, Training Loss: 2.518e+00 , Validation Loss: 3.508e+00\n",
      "Iteration: 20474, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 20475, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20476, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 20477, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20478, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20479, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20480, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20481, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20482, Training Loss: 2.015e+00 , Validation Loss: 3.828e+00\n",
      "Iteration: 20483, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 20484, Training Loss: 7.555e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 20485, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 20486, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20487, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20488, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20489, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20490, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20491, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 20492, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 20493, Training Loss: 1.007e+00 , Validation Loss: 6.350e-01\n",
      "Iteration: 20494, Training Loss: 5.037e-01 , Validation Loss: 1.325e+00\n",
      "Iteration: 20495, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 20496, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20497, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20498, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20499, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20500, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20501, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20502, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20503, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20504, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20505, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20506, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20507, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20508, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20509, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20510, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20511, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20512, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20513, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20514, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20515, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20516, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20517, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20518, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20519, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20520, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20521, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20522, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20523, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20524, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20525, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20526, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20527, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20528, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20529, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20530, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20531, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20532, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20533, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20534, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20535, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20536, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20537, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20538, Training Loss: 1.007e+00 , Validation Loss: 1.766e+00\n",
      "Iteration: 20539, Training Loss: 3.778e+00 , Validation Loss: 5.195e+00\n",
      "Iteration: 20540, Training Loss: 1.259e+00 , Validation Loss: 1.917e+00\n",
      "Iteration: 20541, Training Loss: 1.259e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 20542, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20543, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20544, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20545, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20546, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20547, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20548, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20549, Training Loss: 2.267e+00 , Validation Loss: 2.637e+00\n",
      "Iteration: 20550, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 20551, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20552, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20553, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20554, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 20555, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20556, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20557, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20558, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 20559, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 20560, Training Loss: -1.000e-07 , Validation Loss: 7.681e-01\n",
      "Iteration: 20561, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 20562, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20563, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20564, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20565, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 20566, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20567, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20568, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20569, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 20570, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 20571, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20572, Training Loss: 2.518e-01 , Validation Loss: 2.081e+00\n",
      "Iteration: 20573, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20574, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 20575, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 20576, Training Loss: 2.518e-01 , Validation Loss: 2.226e+00\n",
      "Iteration: 20577, Training Loss: 1.007e+00 , Validation Loss: 4.367e+00\n",
      "Iteration: 20578, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20579, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 20580, Training Loss: 1.259e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 20581, Training Loss: 3.778e+00 , Validation Loss: 4.965e+00\n",
      "Iteration: 20582, Training Loss: 2.518e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 20583, Training Loss: 7.555e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 20584, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20585, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 20586, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20587, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20588, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20589, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 20590, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20591, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20592, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20593, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20594, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20595, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20596, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20597, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20598, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 20599, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 20600, Training Loss: 1.511e+00 , Validation Loss: 3.895e+00\n",
      "Iteration: 20601, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 20602, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 20603, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20604, Training Loss: 7.555e-01 , Validation Loss: 1.240e+00\n",
      "Iteration: 20605, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20606, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20607, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20608, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20609, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20610, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 20611, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20612, Training Loss: -1.000e-07 , Validation Loss: 7.983e-01\n",
      "Iteration: 20613, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 20614, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20615, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 20616, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20617, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20618, Training Loss: 5.037e-01 , Validation Loss: 1.355e+00\n",
      "Iteration: 20619, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 20620, Training Loss: 1.259e+00 , Validation Loss: 4.415e-01\n",
      "Iteration: 20621, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 20622, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20623, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20624, Training Loss: -1.000e-07 , Validation Loss: 5.262e-01\n",
      "Iteration: 20625, Training Loss: 7.555e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 20626, Training Loss: 2.518e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 20627, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 20628, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20629, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 20630, Training Loss: 1.007e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 20631, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20632, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 20633, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20634, Training Loss: 5.037e-01 , Validation Loss: 1.252e+00\n",
      "Iteration: 20635, Training Loss: 2.518e-01 , Validation Loss: 1.439e+00\n",
      "Iteration: 20636, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 20637, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 20638, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20639, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20640, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 20641, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20642, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 20643, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 20644, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20645, Training Loss: 1.259e+00 , Validation Loss: 1.591e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20646, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20647, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 20648, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 20649, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20650, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20651, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 20652, Training Loss: 5.037e-01 , Validation Loss: 1.911e+00\n",
      "Iteration: 20653, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 20654, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20655, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20656, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20657, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20658, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20659, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20660, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20661, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20662, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20663, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20664, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20665, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20666, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20667, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 20668, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20669, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20670, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20671, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20672, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20673, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20674, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20675, Training Loss: 5.037e-01 , Validation Loss: 1.887e+00\n",
      "Iteration: 20676, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20677, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20678, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20679, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 20680, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 20681, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20682, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20683, Training Loss: -1.000e-07 , Validation Loss: 6.955e-01\n",
      "Iteration: 20684, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 20685, Training Loss: 2.518e-01 , Validation Loss: 1.198e+00\n",
      "Iteration: 20686, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 20687, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 20688, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 20689, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20690, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20691, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20692, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20693, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 20694, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20695, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 20696, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 20697, Training Loss: 5.037e-01 , Validation Loss: 1.560e+00\n",
      "Iteration: 20698, Training Loss: 7.555e-01 , Validation Loss: 2.165e+00\n",
      "Iteration: 20699, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 20700, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 20701, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 20702, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 20703, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20704, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 20705, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 20706, Training Loss: 2.518e-01 , Validation Loss: 1.004e+00\n",
      "Iteration: 20707, Training Loss: 2.015e+00 , Validation Loss: 3.611e+00\n",
      "Iteration: 20708, Training Loss: 1.511e+00 , Validation Loss: 8.588e-01\n",
      "Iteration: 20709, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 20710, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20711, Training Loss: 1.511e+00 , Validation Loss: 3.714e+00\n",
      "Iteration: 20712, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 20713, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20714, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20715, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20716, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 20717, Training Loss: 1.007e+00 , Validation Loss: 1.784e+00\n",
      "Iteration: 20718, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 20719, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 20720, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20721, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20722, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20723, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20724, Training Loss: 2.518e-01 , Validation Loss: 1.681e+00\n",
      "Iteration: 20725, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 20726, Training Loss: -1.000e-07 , Validation Loss: 7.318e-01\n",
      "Iteration: 20727, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 20728, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 20729, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20730, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 20731, Training Loss: 7.555e-01 , Validation Loss: 2.062e+00\n",
      "Iteration: 20732, Training Loss: 1.007e+00 , Validation Loss: 1.282e+00\n",
      "Iteration: 20733, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20734, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20735, Training Loss: 2.518e-01 , Validation Loss: 1.827e+00\n",
      "Iteration: 20736, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 20737, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 20738, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20739, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20740, Training Loss: 7.555e-01 , Validation Loss: 9.193e-01\n",
      "Iteration: 20741, Training Loss: 1.511e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 20742, Training Loss: 2.518e-01 , Validation Loss: 2.050e+00\n",
      "Iteration: 20743, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 20744, Training Loss: 1.511e+00 , Validation Loss: 3.641e+00\n",
      "Iteration: 20745, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 20746, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20747, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 20748, Training Loss: 2.015e+00 , Validation Loss: 2.625e+00\n",
      "Iteration: 20749, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 20750, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 20751, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20752, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20753, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20754, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20755, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20756, Training Loss: 7.555e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 20757, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20758, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20759, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 20760, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 20761, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 20762, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20763, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20764, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20765, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20766, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20767, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20768, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20769, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20770, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 20771, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20772, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20773, Training Loss: 7.555e-01 , Validation Loss: 1.706e+00\n",
      "Iteration: 20774, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20775, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 20776, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 20777, Training Loss: 5.037e-01 , Validation Loss: 1.028e+00\n",
      "Iteration: 20778, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20779, Training Loss: 3.022e+00 , Validation Loss: 4.276e+00\n",
      "Iteration: 20780, Training Loss: 1.007e+00 , Validation Loss: 2.159e+00\n",
      "Iteration: 20781, Training Loss: 1.007e+00 , Validation Loss: 1.361e+00\n",
      "Iteration: 20782, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 20783, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 20784, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20785, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20786, Training Loss: 1.511e+00 , Validation Loss: 3.423e+00\n",
      "Iteration: 20787, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20788, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 20789, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20790, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20791, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20792, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20793, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20794, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20795, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20796, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20797, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 20798, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20799, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20800, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20801, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 20802, Training Loss: 2.518e+00 , Validation Loss: 4.355e+00\n",
      "Iteration: 20803, Training Loss: 1.511e+00 , Validation Loss: 9.979e-01\n",
      "Iteration: 20804, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20805, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 20806, Training Loss: 7.555e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 20807, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20808, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20809, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20810, Training Loss: -1.000e-07 , Validation Loss: 6.955e-01\n",
      "Iteration: 20811, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 20812, Training Loss: 2.518e-01 , Validation Loss: 3.459e+00\n",
      "Iteration: 20813, Training Loss: 1.763e+00 , Validation Loss: 1.010e+00\n",
      "Iteration: 20814, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20815, Training Loss: 5.037e-01 , Validation Loss: 1.663e+00\n",
      "Iteration: 20816, Training Loss: 2.518e-01 , Validation Loss: 2.026e+00\n",
      "Iteration: 20817, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 20818, Training Loss: 1.259e+00 , Validation Loss: 2.679e+00\n",
      "Iteration: 20819, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20820, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20821, Training Loss: -1.000e-07 , Validation Loss: 1.276e+00\n",
      "Iteration: 20822, Training Loss: 5.037e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 20823, Training Loss: 3.022e+00 , Validation Loss: 5.105e+00\n",
      "Iteration: 20824, Training Loss: 1.511e+00 , Validation Loss: 1.482e+00\n",
      "Iteration: 20825, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 20826, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20827, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 20828, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 20829, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20830, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 20831, Training Loss: 3.022e+00 , Validation Loss: 4.476e+00\n",
      "Iteration: 20832, Training Loss: 2.518e+00 , Validation Loss: 3.387e+00\n",
      "Iteration: 20833, Training Loss: 1.259e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 20834, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20835, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20836, Training Loss: 1.763e+00 , Validation Loss: 3.574e+00\n",
      "Iteration: 20837, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 20838, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20839, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20840, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20841, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20842, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20843, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 20844, Training Loss: 1.007e+00 , Validation Loss: 2.673e+00\n",
      "Iteration: 20845, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20846, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 20847, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20848, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20849, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20850, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20851, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20852, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 20853, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20854, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20855, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 20856, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20857, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20858, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 20859, Training Loss: 2.518e-01 , Validation Loss: 2.020e+00\n",
      "Iteration: 20860, Training Loss: 2.770e+00 , Validation Loss: 4.385e+00\n",
      "Iteration: 20861, Training Loss: 2.770e+00 , Validation Loss: 2.201e+00\n",
      "Iteration: 20862, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20863, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20864, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20865, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20866, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 20867, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 20868, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 20869, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 20870, Training Loss: 1.007e+00 , Validation Loss: 1.149e+00\n",
      "Iteration: 20871, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20872, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20873, Training Loss: 5.037e-01 , Validation Loss: 1.675e+00\n",
      "Iteration: 20874, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 20875, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 20876, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20877, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20878, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20879, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20880, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 20881, Training Loss: 3.526e+00 , Validation Loss: 5.032e+00\n",
      "Iteration: 20882, Training Loss: 2.267e+00 , Validation Loss: 1.899e+00\n",
      "Iteration: 20883, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 20884, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 20885, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 20886, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 20887, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 20888, Training Loss: 1.007e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 20889, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20890, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 20891, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20892, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 20893, Training Loss: 5.037e-01 , Validation Loss: 2.528e+00\n",
      "Iteration: 20894, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20895, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 20896, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 20897, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 20898, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 20899, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 20900, Training Loss: 7.555e-01 , Validation Loss: 2.522e+00\n",
      "Iteration: 20901, Training Loss: 7.555e-01 , Validation Loss: 1.318e+00\n",
      "Iteration: 20902, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20903, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20904, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20905, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20906, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20907, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20908, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20909, Training Loss: 7.555e-01 , Validation Loss: 1.391e+00\n",
      "Iteration: 20910, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20911, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20912, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20913, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20914, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20915, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20916, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20917, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20918, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 20919, Training Loss: 5.037e-01 , Validation Loss: 1.706e+00\n",
      "Iteration: 20920, Training Loss: 7.555e-01 , Validation Loss: 2.383e+00\n",
      "Iteration: 20921, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20922, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20923, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20924, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20925, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20926, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20927, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 20928, Training Loss: -1.000e-07 , Validation Loss: 1.010e+00\n",
      "Iteration: 20929, Training Loss: 1.259e+00 , Validation Loss: 1.010e+00\n",
      "Iteration: 20930, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 20931, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20932, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20933, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 20934, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 20935, Training Loss: 1.007e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 20936, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 20937, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20938, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 20939, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20940, Training Loss: 2.518e-01 , Validation Loss: 1.947e+00\n",
      "Iteration: 20941, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 20942, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20943, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 20944, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20945, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20946, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20947, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20948, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20949, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20950, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 20951, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 20952, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20953, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20954, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20955, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20956, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 20957, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 20958, Training Loss: 1.511e+00 , Validation Loss: 3.714e+00\n",
      "Iteration: 20959, Training Loss: 1.007e+00 , Validation Loss: 4.899e-01\n",
      "Iteration: 20960, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 20961, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20962, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20963, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20964, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20965, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20966, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20967, Training Loss: 4.030e+00 , Validation Loss: 4.258e+00\n",
      "Iteration: 20968, Training Loss: 6.044e+00 , Validation Loss: 5.389e+00\n",
      "Iteration: 20969, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 20970, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 20971, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20972, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20973, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20974, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20975, Training Loss: 2.518e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 20976, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20977, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20978, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20979, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20980, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20981, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20982, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20983, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 20984, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20985, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20986, Training Loss: -1.000e-07 , Validation Loss: 8.467e-01\n",
      "Iteration: 20987, Training Loss: 5.037e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 20988, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20989, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20990, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20991, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20992, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20993, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20994, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 20995, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20996, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 20997, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 20998, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 20999, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 21000, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21001, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21002, Training Loss: 1.007e+00 , Validation Loss: 1.155e+00\n",
      "Iteration: 21003, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21004, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21005, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21006, Training Loss: 5.037e-01 , Validation Loss: 1.845e+00\n",
      "Iteration: 21007, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21008, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21009, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 21010, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21011, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21012, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21013, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21014, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21015, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21016, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 21017, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21018, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21019, Training Loss: 1.007e+00 , Validation Loss: 6.048e-01\n",
      "Iteration: 21020, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21021, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21022, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 21023, Training Loss: 2.267e+00 , Validation Loss: 3.447e+00\n",
      "Iteration: 21024, Training Loss: 1.259e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 21025, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21026, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21027, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21028, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21029, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21030, Training Loss: 2.518e-01 , Validation Loss: 2.341e+00\n",
      "Iteration: 21031, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 21032, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21033, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21034, Training Loss: 1.007e+00 , Validation Loss: 2.099e+00\n",
      "Iteration: 21035, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 21036, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21037, Training Loss: 1.511e+00 , Validation Loss: 3.810e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 21038, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 21039, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 21040, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 21041, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21042, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 21043, Training Loss: 2.015e+00 , Validation Loss: 3.490e+00\n",
      "Iteration: 21044, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21045, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21046, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21047, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21048, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 21049, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21050, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 21051, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21052, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21053, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 21054, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21055, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21056, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 21057, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 21058, Training Loss: 1.007e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 21059, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21060, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21061, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21062, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21063, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21064, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21065, Training Loss: -1.000e-07 , Validation Loss: 4.838e-01\n",
      "Iteration: 21066, Training Loss: -1.000e-07 , Validation Loss: 4.838e-01\n",
      "Iteration: 21067, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 21068, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21069, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 21070, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21071, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 21072, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 21073, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21074, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 21075, Training Loss: 1.259e+00 , Validation Loss: 2.522e+00\n",
      "Iteration: 21076, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 21077, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 21078, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 21079, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 21080, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 21081, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21082, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21083, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21084, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 21085, Training Loss: 7.555e-01 , Validation Loss: 1.518e+00\n",
      "Iteration: 21086, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21087, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 21088, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 21089, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21090, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21091, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21092, Training Loss: 7.555e-01 , Validation Loss: 1.911e+00\n",
      "Iteration: 21093, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21094, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21095, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 21096, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 21097, Training Loss: 2.267e+00 , Validation Loss: 3.986e+00\n",
      "Iteration: 21098, Training Loss: 1.259e+00 , Validation Loss: 1.415e+00\n",
      "Iteration: 21099, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 21100, Training Loss: 7.555e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 21101, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21102, Training Loss: 1.763e+00 , Validation Loss: 3.786e+00\n",
      "Iteration: 21103, Training Loss: 1.007e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 21104, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21105, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 21106, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 21107, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21108, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21109, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21110, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21111, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21112, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21113, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21114, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21115, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21116, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21117, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21118, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21119, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 21120, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 21121, Training Loss: 2.518e-01 , Validation Loss: 1.427e+00\n",
      "Iteration: 21122, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 21123, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 21124, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21125, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 21126, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21127, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 21128, Training Loss: 1.763e+00 , Validation Loss: 2.951e+00\n",
      "Iteration: 21129, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 21130, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21131, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21132, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21133, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 21134, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21135, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 21136, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 21137, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 21138, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21139, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 21140, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 21141, Training Loss: 1.259e+00 , Validation Loss: 8.407e-01\n",
      "Iteration: 21142, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21143, Training Loss: -1.000e-07 , Validation Loss: 4.778e-01\n",
      "Iteration: 21144, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 21145, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 21146, Training Loss: 2.267e+00 , Validation Loss: 2.843e+00\n",
      "Iteration: 21147, Training Loss: 1.259e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 21148, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 21149, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21150, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 21151, Training Loss: -1.000e-07 , Validation Loss: 7.621e-01\n",
      "Iteration: 21152, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 21153, Training Loss: -1.000e-07 , Validation Loss: 8.044e-01\n",
      "Iteration: 21154, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 21155, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 21156, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 21157, Training Loss: 7.555e-01 , Validation Loss: 1.216e+00\n",
      "Iteration: 21158, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21159, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 21160, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 21161, Training Loss: 2.518e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 21162, Training Loss: 5.037e-01 , Validation Loss: 1.784e+00\n",
      "Iteration: 21163, Training Loss: 7.555e-01 , Validation Loss: 1.445e+00\n",
      "Iteration: 21164, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21165, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 21166, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 21167, Training Loss: 3.526e+00 , Validation Loss: 4.421e+00\n",
      "Iteration: 21168, Training Loss: 5.289e+00 , Validation Loss: 5.250e+00\n",
      "Iteration: 21169, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 21170, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21171, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21172, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 21173, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21174, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21175, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21176, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21177, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21178, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21179, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 21180, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21181, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21182, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21183, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21184, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21185, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21186, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 21187, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21188, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21189, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21190, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21191, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 21192, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21193, Training Loss: -1.000e-07 , Validation Loss: 6.532e-01\n",
      "Iteration: 21194, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 21195, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21196, Training Loss: 1.511e+00 , Validation Loss: 3.714e+00\n",
      "Iteration: 21197, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21198, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21199, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21200, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 21201, Training Loss: 5.037e-01 , Validation Loss: 1.645e+00\n",
      "Iteration: 21202, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21203, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21204, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 21205, Training Loss: -1.000e-07 , Validation Loss: 1.046e+00\n",
      "Iteration: 21206, Training Loss: 7.555e-01 , Validation Loss: 1.046e+00\n",
      "Iteration: 21207, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21208, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21209, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21210, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21211, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 21212, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21213, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21214, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21215, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21216, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21217, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21218, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21219, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 21220, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21221, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 21222, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 21223, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 21224, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 21225, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 21226, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21227, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 21228, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 21229, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 21230, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21231, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 21232, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 21233, Training Loss: 1.511e+00 , Validation Loss: 3.580e+00\n",
      "Iteration: 21234, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 21235, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21236, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 21237, Training Loss: 1.007e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 21238, Training Loss: 2.518e-01 , Validation Loss: 1.143e+00\n",
      "Iteration: 21239, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 21240, Training Loss: 5.037e-01 , Validation Loss: 1.415e+00\n",
      "Iteration: 21241, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21242, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21243, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21244, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21245, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21246, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 21247, Training Loss: 2.518e-01 , Validation Loss: 1.657e+00\n",
      "Iteration: 21248, Training Loss: 1.007e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 21249, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21250, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21251, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21252, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21253, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21254, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 21255, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 21256, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21257, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21258, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 21259, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21260, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21261, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21262, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21263, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21264, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 21265, Training Loss: 2.518e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 21266, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 21267, Training Loss: 2.518e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 21268, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 21269, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21270, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 21271, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21272, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21273, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21274, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 21275, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21276, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21277, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21278, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21279, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21280, Training Loss: -1.000e-07 , Validation Loss: 7.379e-01\n",
      "Iteration: 21281, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 21282, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21283, Training Loss: -1.000e-07 , Validation Loss: 6.109e-01\n",
      "Iteration: 21284, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 21285, Training Loss: 7.555e-01 , Validation Loss: 2.032e+00\n",
      "Iteration: 21286, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21287, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21288, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21289, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21290, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21291, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21292, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21293, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 21294, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21295, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21296, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21297, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21298, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21299, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21300, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21301, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 21302, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21303, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21304, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21305, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21306, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21307, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21308, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 21309, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 21310, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21311, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21312, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21313, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21314, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21315, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21316, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 21317, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 21318, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21319, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21320, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 21321, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 21322, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 21323, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21324, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21325, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 21326, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 21327, Training Loss: 2.015e+00 , Validation Loss: 4.028e+00\n",
      "Iteration: 21328, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 21329, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21330, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21331, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21332, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21333, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21334, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21335, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21336, Training Loss: -1.000e-07 , Validation Loss: 1.143e+00\n",
      "Iteration: 21337, Training Loss: 2.518e-01 , Validation Loss: 1.143e+00\n",
      "Iteration: 21338, Training Loss: 1.259e+00 , Validation Loss: 3.683e+00\n",
      "Iteration: 21339, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21340, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 21341, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21342, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21343, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 21344, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21345, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 21346, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21347, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 21348, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21349, Training Loss: 2.015e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 21350, Training Loss: 1.007e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 21351, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 21352, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21353, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21354, Training Loss: 5.037e-01 , Validation Loss: 1.077e+00\n",
      "Iteration: 21355, Training Loss: 7.555e-01 , Validation Loss: 1.107e+00\n",
      "Iteration: 21356, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21357, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21358, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21359, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21360, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21361, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 21362, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 21363, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21364, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 21365, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21366, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21367, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21368, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21369, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21370, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21371, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21372, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 21373, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 21374, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 21375, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21376, Training Loss: 1.511e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 21377, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 21378, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21379, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21380, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 21381, Training Loss: 2.015e+00 , Validation Loss: 3.968e+00\n",
      "Iteration: 21382, Training Loss: 1.007e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 21383, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 21384, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21385, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21386, Training Loss: 1.511e+00 , Validation Loss: 2.286e+00\n",
      "Iteration: 21387, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21388, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21389, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 21390, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 21391, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 21392, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21393, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 21394, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 21395, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 21396, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 21397, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21398, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21399, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21400, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21401, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21402, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21403, Training Loss: 5.037e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 21404, Training Loss: -1.000e-07 , Validation Loss: 1.488e+00\n",
      "Iteration: 21405, Training Loss: 5.037e-01 , Validation Loss: 1.488e+00\n",
      "Iteration: 21406, Training Loss: 1.007e+00 , Validation Loss: 1.947e+00\n",
      "Iteration: 21407, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21408, Training Loss: 5.037e-01 , Validation Loss: 1.772e+00\n",
      "Iteration: 21409, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 21410, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21411, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21412, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 21413, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21414, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21415, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21416, Training Loss: -1.000e-07 , Validation Loss: 1.137e+00\n",
      "Iteration: 21417, Training Loss: 1.007e+00 , Validation Loss: 1.137e+00\n",
      "Iteration: 21418, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21419, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 21420, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21421, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21422, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 21423, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 21424, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 21425, Training Loss: 7.555e-01 , Validation Loss: 9.737e-01\n",
      "Iteration: 21426, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 21427, Training Loss: -1.000e-07 , Validation Loss: 7.318e-01\n",
      "Iteration: 21428, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 21429, Training Loss: 4.281e+00 , Validation Loss: 4.512e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 21430, Training Loss: 7.304e+00 , Validation Loss: 9.502e+00\n",
      "Iteration: 21431, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21432, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21433, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21434, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21435, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21436, Training Loss: 1.007e+00 , Validation Loss: 2.443e+00\n",
      "Iteration: 21437, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21438, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21439, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21440, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21441, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21442, Training Loss: 1.511e+00 , Validation Loss: 2.341e+00\n",
      "Iteration: 21443, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21444, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 21445, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21446, Training Loss: -1.000e-07 , Validation Loss: 4.778e-01\n",
      "Iteration: 21447, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 21448, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 21449, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 21450, Training Loss: 1.763e+00 , Validation Loss: 3.544e+00\n",
      "Iteration: 21451, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 21452, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21453, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21454, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21455, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21456, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21457, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21458, Training Loss: 7.555e-01 , Validation Loss: 1.663e+00\n",
      "Iteration: 21459, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 21460, Training Loss: 2.267e+00 , Validation Loss: 3.974e+00\n",
      "Iteration: 21461, Training Loss: 3.274e+00 , Validation Loss: 1.355e+00\n",
      "Iteration: 21462, Training Loss: 3.022e+00 , Validation Loss: 4.463e+00\n",
      "Iteration: 21463, Training Loss: 2.015e+00 , Validation Loss: 1.071e+00\n",
      "Iteration: 21464, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21465, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21466, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 21467, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21468, Training Loss: 1.007e+00 , Validation Loss: 9.314e-01\n",
      "Iteration: 21469, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21470, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21471, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21472, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21473, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 21474, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21475, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21476, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21477, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21478, Training Loss: 1.763e+00 , Validation Loss: 3.484e+00\n",
      "Iteration: 21479, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 21480, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21481, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21482, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21483, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21484, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21485, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21486, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21487, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21488, Training Loss: -1.000e-07 , Validation Loss: 1.185e+00\n",
      "Iteration: 21489, Training Loss: 5.037e-01 , Validation Loss: 1.185e+00\n",
      "Iteration: 21490, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21491, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21492, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 21493, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 21494, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 21495, Training Loss: 1.763e+00 , Validation Loss: 2.794e+00\n",
      "Iteration: 21496, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21497, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 21498, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21499, Training Loss: 1.763e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 21500, Training Loss: 2.518e-01 , Validation Loss: 1.851e+00\n",
      "Iteration: 21501, Training Loss: 7.555e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 21502, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 21503, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 21504, Training Loss: 2.015e+00 , Validation Loss: 3.955e+00\n",
      "Iteration: 21505, Training Loss: 5.037e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 21506, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21507, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21508, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21509, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 21510, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21511, Training Loss: 7.555e-01 , Validation Loss: 1.439e+00\n",
      "Iteration: 21512, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21513, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21514, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21515, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21516, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21517, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21518, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 21519, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 21520, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 21521, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21522, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21523, Training Loss: 1.007e+00 , Validation Loss: 2.607e+00\n",
      "Iteration: 21524, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21525, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21526, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21527, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21528, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 21529, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21530, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21531, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 21532, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21533, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21534, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 21535, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 21536, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21537, Training Loss: 3.526e+00 , Validation Loss: 4.724e+00\n",
      "Iteration: 21538, Training Loss: 2.267e+00 , Validation Loss: 3.072e+00\n",
      "Iteration: 21539, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 21540, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21541, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21542, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21543, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21544, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21545, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21546, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21547, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 21548, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 21549, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21550, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 21551, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21552, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21553, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 21554, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21555, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21556, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21557, Training Loss: 3.526e+00 , Validation Loss: 4.203e+00\n",
      "Iteration: 21558, Training Loss: 2.267e+00 , Validation Loss: 2.050e+00\n",
      "Iteration: 21559, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21560, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 21561, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21562, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21563, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 21564, Training Loss: 1.511e+00 , Validation Loss: 4.137e+00\n",
      "Iteration: 21565, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 21566, Training Loss: 1.511e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 21567, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21568, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 21569, Training Loss: 3.022e+00 , Validation Loss: 4.336e+00\n",
      "Iteration: 21570, Training Loss: 1.511e+00 , Validation Loss: 1.766e+00\n",
      "Iteration: 21571, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 21572, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 21573, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21574, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21575, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21576, Training Loss: 7.555e-01 , Validation Loss: 1.222e+00\n",
      "Iteration: 21577, Training Loss: 3.526e+00 , Validation Loss: 4.070e+00\n",
      "Iteration: 21578, Training Loss: 5.289e+00 , Validation Loss: 5.655e+00\n",
      "Iteration: 21579, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 21580, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 21581, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21582, Training Loss: 2.518e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 21583, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21584, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21585, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 21586, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21587, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21588, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 21589, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 21590, Training Loss: 7.555e-01 , Validation Loss: 2.480e+00\n",
      "Iteration: 21591, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21592, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21593, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 21594, Training Loss: 5.037e-01 , Validation Loss: 1.881e+00\n",
      "Iteration: 21595, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21596, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 21597, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21598, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21599, Training Loss: -1.000e-07 , Validation Loss: 4.778e-01\n",
      "Iteration: 21600, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 21601, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 21602, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 21603, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21604, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 21605, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 21606, Training Loss: 2.770e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 21607, Training Loss: 1.007e+00 , Validation Loss: 1.403e+00\n",
      "Iteration: 21608, Training Loss: 1.007e+00 , Validation Loss: 8.588e-01\n",
      "Iteration: 21609, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21610, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21611, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21612, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21613, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 21614, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 21615, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21616, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21617, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21618, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21619, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 21620, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21621, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21622, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21623, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21624, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21625, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21626, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21627, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21628, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21629, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21630, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21631, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 21632, Training Loss: 2.518e-01 , Validation Loss: 1.161e+00\n",
      "Iteration: 21633, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 21634, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 21635, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 21636, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 21637, Training Loss: -1.000e-07 , Validation Loss: 7.439e-01\n",
      "Iteration: 21638, Training Loss: -1.000e-07 , Validation Loss: 7.439e-01\n",
      "Iteration: 21639, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 21640, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21641, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21642, Training Loss: 1.007e+00 , Validation Loss: 2.480e+00\n",
      "Iteration: 21643, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 21644, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 21645, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21646, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21647, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 21648, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21649, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 21650, Training Loss: 7.555e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 21651, Training Loss: 1.007e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 21652, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21653, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21654, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 21655, Training Loss: 7.555e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 21656, Training Loss: 1.763e+00 , Validation Loss: 2.165e+00\n",
      "Iteration: 21657, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21658, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 21659, Training Loss: 1.259e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 21660, Training Loss: 2.770e+00 , Validation Loss: 3.895e+00\n",
      "Iteration: 21661, Training Loss: 1.763e+00 , Validation Loss: 1.119e+00\n",
      "Iteration: 21662, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 21663, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21664, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 21665, Training Loss: 1.007e+00 , Validation Loss: 5.020e-01\n",
      "Iteration: 21666, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 21667, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21668, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 21669, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 21670, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21671, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21672, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21673, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 21674, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 21675, Training Loss: 1.259e+00 , Validation Loss: 3.587e+00\n",
      "Iteration: 21676, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 21677, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 21678, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21679, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21680, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21681, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 21682, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21683, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21684, Training Loss: 2.518e-01 , Validation Loss: 1.137e+00\n",
      "Iteration: 21685, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 21686, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 21687, Training Loss: 1.259e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 21688, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21689, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 21690, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 21691, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21692, Training Loss: 2.518e-01 , Validation Loss: 1.125e+00\n",
      "Iteration: 21693, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 21694, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 21695, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21696, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21697, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21698, Training Loss: 5.037e-01 , Validation Loss: 2.371e+00\n",
      "Iteration: 21699, Training Loss: 7.555e-01 , Validation Loss: 3.689e+00\n",
      "Iteration: 21700, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21701, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21702, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21703, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21704, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21705, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21706, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21707, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21708, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21709, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21710, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21711, Training Loss: -1.000e-07 , Validation Loss: 5.322e-01\n",
      "Iteration: 21712, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 21713, Training Loss: 2.518e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 21714, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 21715, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21716, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21717, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21718, Training Loss: 1.259e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 21719, Training Loss: -1.000e-07 , Validation Loss: 7.500e-01\n",
      "Iteration: 21720, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 21721, Training Loss: 5.037e-01 , Validation Loss: 2.274e+00\n",
      "Iteration: 21722, Training Loss: 2.518e+00 , Validation Loss: 3.756e+00\n",
      "Iteration: 21723, Training Loss: 1.259e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 21724, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 21725, Training Loss: 7.555e-01 , Validation Loss: 1.439e+00\n",
      "Iteration: 21726, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21727, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21728, Training Loss: 7.555e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 21729, Training Loss: 1.511e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 21730, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 21731, Training Loss: -1.000e-07 , Validation Loss: 4.597e-01\n",
      "Iteration: 21732, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 21733, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21734, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 21735, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21736, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21737, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21738, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 21739, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21740, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21741, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21742, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 21743, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 21744, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 21745, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 21746, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 21747, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21748, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 21749, Training Loss: 5.037e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 21750, Training Loss: 7.555e-01 , Validation Loss: 1.306e+00\n",
      "Iteration: 21751, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21752, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21753, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21754, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21755, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21756, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21757, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21758, Training Loss: 5.037e-01 , Validation Loss: 9.314e-01\n",
      "Iteration: 21759, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 21760, Training Loss: 3.526e+00 , Validation Loss: 4.530e+00\n",
      "Iteration: 21761, Training Loss: 2.518e+00 , Validation Loss: 3.605e+00\n",
      "Iteration: 21762, Training Loss: 2.518e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 21763, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 21764, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 21765, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21766, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21767, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21768, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 21769, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 21770, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21771, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 21772, Training Loss: 1.511e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 21773, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21774, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 21775, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21776, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21777, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21778, Training Loss: 3.778e+00 , Validation Loss: 4.149e+00\n",
      "Iteration: 21779, Training Loss: 2.518e+00 , Validation Loss: 4.336e+00\n",
      "Iteration: 21780, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 21781, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 21782, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21783, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21784, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21785, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21786, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21787, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 21788, Training Loss: 1.007e+00 , Validation Loss: 1.040e+00\n",
      "Iteration: 21789, Training Loss: 1.259e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 21790, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21791, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21792, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21793, Training Loss: 1.259e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 21794, Training Loss: 2.518e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 21795, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 21796, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 21797, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21798, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21799, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21800, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 21801, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21802, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21803, Training Loss: 1.259e+00 , Validation Loss: 2.413e+00\n",
      "Iteration: 21804, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 21805, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21806, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21807, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21808, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 21809, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21810, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21811, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21812, Training Loss: 1.763e+00 , Validation Loss: 3.786e+00\n",
      "Iteration: 21813, Training Loss: 1.007e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 21814, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 21815, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21816, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21817, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21818, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21819, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21820, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21821, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21822, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21823, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 21824, Training Loss: 2.518e-01 , Validation Loss: 2.050e+00\n",
      "Iteration: 21825, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 21826, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21827, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21828, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21829, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 21830, Training Loss: 2.267e+00 , Validation Loss: 3.798e+00\n",
      "Iteration: 21831, Training Loss: 1.259e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 21832, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21833, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 21834, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 21835, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 21836, Training Loss: 5.037e-01 , Validation Loss: 1.730e+00\n",
      "Iteration: 21837, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 21838, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21839, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21840, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 21841, Training Loss: 7.555e-01 , Validation Loss: 8.346e-01\n",
      "Iteration: 21842, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 21843, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21844, Training Loss: 2.518e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 21845, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21846, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21847, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21848, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21849, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21850, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21851, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 21852, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21853, Training Loss: 2.267e+00 , Validation Loss: 3.060e+00\n",
      "Iteration: 21854, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21855, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21856, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 21857, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21858, Training Loss: 1.259e+00 , Validation Loss: 4.355e-01\n",
      "Iteration: 21859, Training Loss: 3.274e+00 , Validation Loss: 3.925e+00\n",
      "Iteration: 21860, Training Loss: 3.022e+00 , Validation Loss: 2.861e+00\n",
      "Iteration: 21861, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 21862, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21863, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21864, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 21865, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 21866, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 21867, Training Loss: 2.770e+00 , Validation Loss: 4.203e+00\n",
      "Iteration: 21868, Training Loss: 2.267e+00 , Validation Loss: 1.748e+00\n",
      "Iteration: 21869, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21870, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21871, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21872, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21873, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21874, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21875, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 21876, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21877, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 21878, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21879, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 21880, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 21881, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 21882, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21883, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21884, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21885, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21886, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21887, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21888, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21889, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 21890, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21891, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21892, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21893, Training Loss: 2.267e+00 , Validation Loss: 3.992e+00\n",
      "Iteration: 21894, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21895, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21896, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21897, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21898, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21899, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21900, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21901, Training Loss: 1.511e+00 , Validation Loss: 4.022e+00\n",
      "Iteration: 21902, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21903, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 21904, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21905, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21906, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 21907, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 21908, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 21909, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 21910, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21911, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21912, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 21913, Training Loss: 2.518e+00 , Validation Loss: 4.869e+00\n",
      "Iteration: 21914, Training Loss: 5.037e-01 , Validation Loss: 9.072e-01\n",
      "Iteration: 21915, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 21916, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21917, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 21918, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 21919, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 21920, Training Loss: 3.778e+00 , Validation Loss: 3.895e+00\n",
      "Iteration: 21921, Training Loss: 5.037e+00 , Validation Loss: 5.873e+00\n",
      "Iteration: 21922, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21923, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21924, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21925, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21926, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21927, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21928, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21929, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21930, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21931, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21932, Training Loss: 5.037e-01 , Validation Loss: 1.264e+00\n",
      "Iteration: 21933, Training Loss: -1.000e-07 , Validation Loss: 1.827e+00\n",
      "Iteration: 21934, Training Loss: 2.518e-01 , Validation Loss: 1.827e+00\n",
      "Iteration: 21935, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 21936, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21937, Training Loss: 2.518e-01 , Validation Loss: 1.337e+00\n",
      "Iteration: 21938, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 21939, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21940, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21941, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21942, Training Loss: 7.555e-01 , Validation Loss: 3.151e+00\n",
      "Iteration: 21943, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21944, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 21945, Training Loss: 5.037e-01 , Validation Loss: 2.691e+00\n",
      "Iteration: 21946, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21947, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21948, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 21949, Training Loss: 2.267e+00 , Validation Loss: 5.026e+00\n",
      "Iteration: 21950, Training Loss: 1.259e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 21951, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21952, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21953, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21954, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21955, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21956, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21957, Training Loss: 2.518e+00 , Validation Loss: 3.514e+00\n",
      "Iteration: 21958, Training Loss: 1.763e+00 , Validation Loss: 1.228e+00\n",
      "Iteration: 21959, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 21960, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21961, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21962, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21963, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21964, Training Loss: 7.555e-01 , Validation Loss: 1.724e+00\n",
      "Iteration: 21965, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 21966, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21967, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21968, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21969, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 21970, Training Loss: 5.037e-01 , Validation Loss: 3.121e+00\n",
      "Iteration: 21971, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21972, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21973, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21974, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21975, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21976, Training Loss: 1.007e+00 , Validation Loss: 5.080e-01\n",
      "Iteration: 21977, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21978, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 21979, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21980, Training Loss: 2.518e-01 , Validation Loss: 1.228e+00\n",
      "Iteration: 21981, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21982, Training Loss: 5.037e-01 , Validation Loss: 1.458e+00\n",
      "Iteration: 21983, Training Loss: 1.259e+00 , Validation Loss: 2.056e+00\n",
      "Iteration: 21984, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21985, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21986, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21987, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21988, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21989, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21990, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21991, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 21992, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 21993, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 21994, Training Loss: 2.518e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 21995, Training Loss: 2.015e+00 , Validation Loss: 9.254e-01\n",
      "Iteration: 21996, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21997, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21998, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 21999, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 22000, Training Loss: 5.037e-01 , Validation Loss: 2.014e+00\n",
      "Iteration: 22001, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22002, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22003, Training Loss: 7.555e-01 , Validation Loss: 1.960e+00\n",
      "Iteration: 22004, Training Loss: 4.030e+00 , Validation Loss: 4.566e+00\n",
      "Iteration: 22005, Training Loss: 4.533e+00 , Validation Loss: 4.875e+00\n",
      "Iteration: 22006, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22007, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 22008, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22009, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22010, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22011, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22012, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22013, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22014, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22015, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22016, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22017, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22018, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22019, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22020, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22021, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22022, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22023, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22024, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 22025, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22026, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22027, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22028, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22029, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22030, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22031, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22032, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22033, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22034, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22035, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22036, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22037, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22038, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 22039, Training Loss: 7.555e-01 , Validation Loss: 2.443e+00\n",
      "Iteration: 22040, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 22041, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 22042, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22043, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22044, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22045, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22046, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22047, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22048, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22049, Training Loss: 1.259e+00 , Validation Loss: 2.159e+00\n",
      "Iteration: 22050, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22051, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22052, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22053, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22054, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22055, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22056, Training Loss: 5.037e-01 , Validation Loss: 1.524e+00\n",
      "Iteration: 22057, Training Loss: 1.511e+00 , Validation Loss: 2.776e+00\n",
      "Iteration: 22058, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 22059, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22060, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22061, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22062, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22063, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22064, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22065, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22066, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22067, Training Loss: 3.022e+00 , Validation Loss: 4.766e+00\n",
      "Iteration: 22068, Training Loss: 1.763e+00 , Validation Loss: 1.784e+00\n",
      "Iteration: 22069, Training Loss: 7.555e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 22070, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22071, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22072, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22073, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22074, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22075, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22076, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22077, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22078, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22079, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22080, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22081, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22082, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22083, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22084, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22085, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22086, Training Loss: 5.037e-01 , Validation Loss: 2.915e+00\n",
      "Iteration: 22087, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22088, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 22089, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22090, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 22091, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22092, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22093, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22094, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22095, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22096, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 22097, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 22098, Training Loss: 2.267e+00 , Validation Loss: 4.506e+00\n",
      "Iteration: 22099, Training Loss: 1.259e+00 , Validation Loss: 1.373e+00\n",
      "Iteration: 22100, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 22101, Training Loss: 1.259e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 22102, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 22103, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22104, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22105, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22106, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22107, Training Loss: 2.770e+00 , Validation Loss: 4.615e+00\n",
      "Iteration: 22108, Training Loss: 1.763e+00 , Validation Loss: 2.873e+00\n",
      "Iteration: 22109, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 22110, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 22111, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22112, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22113, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22114, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22115, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22116, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22117, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22118, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 22119, Training Loss: 3.022e+00 , Validation Loss: 3.593e+00\n",
      "Iteration: 22120, Training Loss: 1.007e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 22121, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 22122, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 22123, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 22124, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22125, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22126, Training Loss: 1.763e+00 , Validation Loss: 6.048e-01\n",
      "Iteration: 22127, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 22128, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22129, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22130, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22131, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22132, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22133, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22134, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22135, Training Loss: 2.518e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 22136, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 22137, Training Loss: 2.518e-01 , Validation Loss: 9.919e-01\n",
      "Iteration: 22138, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 22139, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22140, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22141, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 22142, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22143, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 22144, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 22145, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 22146, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22147, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22148, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22149, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22150, Training Loss: 2.518e-01 , Validation Loss: 1.579e+00\n",
      "Iteration: 22151, Training Loss: 2.267e+00 , Validation Loss: 4.240e+00\n",
      "Iteration: 22152, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22153, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22154, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22155, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22156, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22157, Training Loss: 7.555e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 22158, Training Loss: 5.037e-01 , Validation Loss: 2.528e+00\n",
      "Iteration: 22159, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 22160, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 22161, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 22162, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22163, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22164, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22165, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22166, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22167, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22168, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22169, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 22170, Training Loss: 1.007e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 22171, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22172, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22173, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22174, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22175, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22176, Training Loss: 5.037e-01 , Validation Loss: 2.214e+00\n",
      "Iteration: 22177, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22178, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22179, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 22180, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22181, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 22182, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 22183, Training Loss: 1.763e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 22184, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 22185, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22186, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 22187, Training Loss: 5.037e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 22188, Training Loss: 2.518e-01 , Validation Loss: 1.270e+00\n",
      "Iteration: 22189, Training Loss: 7.555e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 22190, Training Loss: 7.555e-01 , Validation Loss: 2.498e+00\n",
      "Iteration: 22191, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22192, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22193, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 22194, Training Loss: -1.000e-07 , Validation Loss: 7.439e-01\n",
      "Iteration: 22195, Training Loss: 1.007e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 22196, Training Loss: 2.518e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 22197, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 22198, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 22199, Training Loss: 2.015e+00 , Validation Loss: 2.734e+00\n",
      "Iteration: 22200, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 22201, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22202, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 22203, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22204, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22205, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22206, Training Loss: 2.518e-01 , Validation Loss: 1.331e+00\n",
      "Iteration: 22207, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 22208, Training Loss: 5.037e-01 , Validation Loss: 2.117e+00\n",
      "Iteration: 22209, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22210, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22211, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22212, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22213, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22214, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22215, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 22216, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 22217, Training Loss: 3.526e+00 , Validation Loss: 4.209e+00\n",
      "Iteration: 22218, Training Loss: 2.770e+00 , Validation Loss: 4.052e+00\n",
      "Iteration: 22219, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 22220, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 22221, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22222, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22223, Training Loss: -1.000e-07 , Validation Loss: 6.109e-01\n",
      "Iteration: 22224, Training Loss: -1.000e-07 , Validation Loss: 6.109e-01\n",
      "Iteration: 22225, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 22226, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22227, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 22228, Training Loss: 2.518e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 22229, Training Loss: 1.763e+00 , Validation Loss: 8.649e-01\n",
      "Iteration: 22230, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22231, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22232, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22233, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22234, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22235, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 22236, Training Loss: 3.526e+00 , Validation Loss: 3.901e+00\n",
      "Iteration: 22237, Training Loss: 4.533e+00 , Validation Loss: 4.760e+00\n",
      "Iteration: 22238, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 22239, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22240, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22241, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22242, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22243, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22244, Training Loss: 7.555e-01 , Validation Loss: 2.776e+00\n",
      "Iteration: 22245, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22246, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22247, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22248, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22249, Training Loss: 7.555e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 22250, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22251, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22252, Training Loss: -1.000e-07 , Validation Loss: 6.713e-01\n",
      "Iteration: 22253, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 22254, Training Loss: 2.518e+00 , Validation Loss: 3.744e+00\n",
      "Iteration: 22255, Training Loss: 2.015e+00 , Validation Loss: 2.407e+00\n",
      "Iteration: 22256, Training Loss: 2.015e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 22257, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22258, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 22259, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 22260, Training Loss: 2.518e-01 , Validation Loss: 2.341e+00\n",
      "Iteration: 22261, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22262, Training Loss: 7.555e-01 , Validation Loss: 2.885e+00\n",
      "Iteration: 22263, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22264, Training Loss: 3.274e+00 , Validation Loss: 3.726e+00\n",
      "Iteration: 22265, Training Loss: 1.763e+00 , Validation Loss: 3.671e+00\n",
      "Iteration: 22266, Training Loss: 1.763e+00 , Validation Loss: 1.010e+00\n",
      "Iteration: 22267, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22268, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 22269, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22270, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22271, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22272, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22273, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22274, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22275, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22276, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22277, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22278, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22279, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22280, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22281, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22282, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22283, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 22284, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 22285, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 22286, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 22287, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22288, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22289, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22290, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22291, Training Loss: -1.000e-07 , Validation Loss: 5.141e-01\n",
      "Iteration: 22292, Training Loss: -1.000e-07 , Validation Loss: 5.141e-01\n",
      "Iteration: 22293, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 22294, Training Loss: 3.526e+00 , Validation Loss: 4.391e+00\n",
      "Iteration: 22295, Training Loss: 2.770e+00 , Validation Loss: 2.322e+00\n",
      "Iteration: 22296, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22297, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22298, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22299, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22300, Training Loss: 7.555e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 22301, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22302, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 22303, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 22304, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22305, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22306, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22307, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22308, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22309, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22310, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22311, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22312, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22313, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22314, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22315, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22316, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 22317, Training Loss: 1.763e+00 , Validation Loss: 3.290e+00\n",
      "Iteration: 22318, Training Loss: -1.000e-07 , Validation Loss: 6.411e-01\n",
      "Iteration: 22319, Training Loss: 1.007e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 22320, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22321, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22322, Training Loss: 2.518e-01 , Validation Loss: 1.064e+00\n",
      "Iteration: 22323, Training Loss: 2.518e+00 , Validation Loss: 3.907e+00\n",
      "Iteration: 22324, Training Loss: 1.007e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 22325, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 22326, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 22327, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22328, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22329, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22330, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22331, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22332, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22333, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22334, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22335, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22336, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22337, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22338, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 22339, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 22340, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22341, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22342, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 22343, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 22344, Training Loss: 2.518e-01 , Validation Loss: 1.476e+00\n",
      "Iteration: 22345, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 22346, Training Loss: 7.555e-01 , Validation Loss: 1.839e+00\n",
      "Iteration: 22347, Training Loss: -1.000e-07 , Validation Loss: 7.742e-01\n",
      "Iteration: 22348, Training Loss: 5.037e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 22349, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22350, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22351, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22352, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 22353, Training Loss: 1.511e+00 , Validation Loss: 1.966e+00\n",
      "Iteration: 22354, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 22355, Training Loss: 5.037e-01 , Validation Loss: 1.258e+00\n",
      "Iteration: 22356, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 22357, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 22358, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 22359, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22360, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 22361, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22362, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22363, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 22364, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22365, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22366, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22367, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22368, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22369, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22370, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22371, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22372, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 22373, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22374, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22375, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22376, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 22377, Training Loss: 5.037e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 22378, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22379, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 22380, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 22381, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 22382, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 22383, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22384, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 22385, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 22386, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 22387, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 22388, Training Loss: 7.555e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 22389, Training Loss: 1.511e+00 , Validation Loss: 3.490e+00\n",
      "Iteration: 22390, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 22391, Training Loss: 1.007e+00 , Validation Loss: 5.443e-01\n",
      "Iteration: 22392, Training Loss: 1.763e+00 , Validation Loss: 3.242e+00\n",
      "Iteration: 22393, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22394, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 22395, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22396, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 22397, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 22398, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 22399, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22400, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22401, Training Loss: 7.555e-01 , Validation Loss: 1.258e+00\n",
      "Iteration: 22402, Training Loss: 2.267e+00 , Validation Loss: 3.937e+00\n",
      "Iteration: 22403, Training Loss: 2.518e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 22404, Training Loss: 1.511e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 22405, Training Loss: -1.000e-07 , Validation Loss: 1.935e+00\n",
      "Iteration: 22406, Training Loss: 7.555e-01 , Validation Loss: 1.935e+00\n",
      "Iteration: 22407, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22408, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22409, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22410, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22411, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22412, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22413, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22414, Training Loss: 7.555e-01 , Validation Loss: 1.445e+00\n",
      "Iteration: 22415, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22416, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22417, Training Loss: 2.518e+00 , Validation Loss: 4.439e+00\n",
      "Iteration: 22418, Training Loss: 1.259e+00 , Validation Loss: 1.331e+00\n",
      "Iteration: 22419, Training Loss: 1.007e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 22420, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22421, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22422, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22423, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 22424, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 22425, Training Loss: 5.037e-01 , Validation Loss: 1.046e+00\n",
      "Iteration: 22426, Training Loss: 1.259e+00 , Validation Loss: 1.941e+00\n",
      "Iteration: 22427, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 22428, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22429, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22430, Training Loss: 7.555e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 22431, Training Loss: 7.555e-01 , Validation Loss: 1.034e+00\n",
      "Iteration: 22432, Training Loss: 7.555e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 22433, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22434, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22435, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22436, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22437, Training Loss: 7.555e-01 , Validation Loss: 1.288e+00\n",
      "Iteration: 22438, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22439, Training Loss: 1.007e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 22440, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22441, Training Loss: -1.000e-07 , Validation Loss: 5.322e-01\n",
      "Iteration: 22442, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 22443, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 22444, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22445, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 22446, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 22447, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22448, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22449, Training Loss: 1.007e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 22450, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22451, Training Loss: 7.555e-01 , Validation Loss: 1.591e+00\n",
      "Iteration: 22452, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22453, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22454, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 22455, Training Loss: -1.000e-07 , Validation Loss: 7.983e-01\n",
      "Iteration: 22456, Training Loss: -1.000e-07 , Validation Loss: 7.983e-01\n",
      "Iteration: 22457, Training Loss: 5.037e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 22458, Training Loss: 2.267e+00 , Validation Loss: 2.129e+00\n",
      "Iteration: 22459, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 22460, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22461, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 22462, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22463, Training Loss: 1.007e+00 , Validation Loss: 4.476e-01\n",
      "Iteration: 22464, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 22465, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 22466, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22467, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 22468, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 22469, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 22470, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 22471, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 22472, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22473, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22474, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22475, Training Loss: 1.511e+00 , Validation Loss: 1.845e+00\n",
      "Iteration: 22476, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 22477, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22478, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22479, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22480, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 22481, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22482, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 22483, Training Loss: 3.526e+00 , Validation Loss: 5.177e+00\n",
      "Iteration: 22484, Training Loss: 1.511e+00 , Validation Loss: 3.635e+00\n",
      "Iteration: 22485, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 22486, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22487, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22488, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22489, Training Loss: 2.518e-01 , Validation Loss: 1.228e+00\n",
      "Iteration: 22490, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22491, Training Loss: 1.259e+00 , Validation Loss: 2.062e+00\n",
      "Iteration: 22492, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22493, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22494, Training Loss: 7.555e-01 , Validation Loss: 9.072e-01\n",
      "Iteration: 22495, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22496, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22497, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22498, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22499, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22500, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22501, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22502, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22503, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22504, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22505, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22506, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22507, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22508, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22509, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22510, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22511, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22512, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22513, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22514, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22515, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 22516, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22517, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22518, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22519, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22520, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 22521, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 22522, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22523, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22524, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22525, Training Loss: 1.007e+00 , Validation Loss: 1.258e+00\n",
      "Iteration: 22526, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22527, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22528, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22529, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22530, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22531, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22532, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22533, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22534, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22535, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22536, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22537, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 22538, Training Loss: 1.763e+00 , Validation Loss: 3.381e+00\n",
      "Iteration: 22539, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 22540, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 22541, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 22542, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 22543, Training Loss: 5.037e-01 , Validation Loss: 2.177e+00\n",
      "Iteration: 22544, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22545, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 22546, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 22547, Training Loss: 1.763e+00 , Validation Loss: 3.756e+00\n",
      "Iteration: 22548, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 22549, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 22550, Training Loss: 1.007e+00 , Validation Loss: 5.020e-01\n",
      "Iteration: 22551, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 22552, Training Loss: -1.000e-07 , Validation Loss: 1.191e+00\n",
      "Iteration: 22553, Training Loss: 2.518e-01 , Validation Loss: 1.191e+00\n",
      "Iteration: 22554, Training Loss: 3.022e+00 , Validation Loss: 3.738e+00\n",
      "Iteration: 22555, Training Loss: 7.555e-01 , Validation Loss: 9.979e-01\n",
      "Iteration: 22556, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22557, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22558, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 22559, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22560, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22561, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22562, Training Loss: -1.000e-07 , Validation Loss: 1.427e+00\n",
      "Iteration: 22563, Training Loss: 2.518e-01 , Validation Loss: 1.427e+00\n",
      "Iteration: 22564, Training Loss: 2.518e+00 , Validation Loss: 3.992e+00\n",
      "Iteration: 22565, Training Loss: 1.511e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 22566, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22567, Training Loss: 2.015e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 22568, Training Loss: 1.007e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 22569, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 22570, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 22571, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22572, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 22573, Training Loss: 5.037e-01 , Validation Loss: 2.643e+00\n",
      "Iteration: 22574, Training Loss: -1.000e-07 , Validation Loss: 4.899e-01\n",
      "Iteration: 22575, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 22576, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22577, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 22578, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 22579, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 22580, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22581, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22582, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 22583, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 22584, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22585, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22586, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22587, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22588, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22589, Training Loss: 7.555e-01 , Validation Loss: 1.984e+00\n",
      "Iteration: 22590, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22591, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22592, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22593, Training Loss: 1.511e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 22594, Training Loss: 2.267e+00 , Validation Loss: 4.597e+00\n",
      "Iteration: 22595, Training Loss: 1.007e+00 , Validation Loss: 8.951e-01\n",
      "Iteration: 22596, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 22597, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 22598, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22599, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22600, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22601, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 22602, Training Loss: 1.511e+00 , Validation Loss: 2.951e+00\n",
      "Iteration: 22603, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22604, Training Loss: 1.007e+00 , Validation Loss: 4.476e-01\n",
      "Iteration: 22605, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22606, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22607, Training Loss: 1.259e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 22608, Training Loss: 2.770e+00 , Validation Loss: 5.201e+00\n",
      "Iteration: 22609, Training Loss: 1.259e+00 , Validation Loss: 1.252e+00\n",
      "Iteration: 22610, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 22611, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 22612, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22613, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 22614, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22615, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22616, Training Loss: 5.037e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 22617, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22618, Training Loss: 5.037e-01 , Validation Loss: 2.800e+00\n",
      "Iteration: 22619, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22620, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22621, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22622, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22623, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22624, Training Loss: 5.037e-01 , Validation Loss: 1.820e+00\n",
      "Iteration: 22625, Training Loss: 2.518e+00 , Validation Loss: 3.580e+00\n",
      "Iteration: 22626, Training Loss: 1.511e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 22627, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 22628, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22629, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 22630, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22631, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 22632, Training Loss: 2.770e+00 , Validation Loss: 3.417e+00\n",
      "Iteration: 22633, Training Loss: 2.015e+00 , Validation Loss: 1.917e+00\n",
      "Iteration: 22634, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22635, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 22636, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22637, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22638, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 22639, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 22640, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22641, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22642, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22643, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22644, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22645, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 22646, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22647, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 22648, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22649, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22650, Training Loss: 1.511e+00 , Validation Loss: 2.613e+00\n",
      "Iteration: 22651, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22652, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22653, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22654, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22655, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 22656, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22657, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22658, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22659, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22660, Training Loss: 2.518e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 22661, Training Loss: 2.518e+00 , Validation Loss: 3.544e+00\n",
      "Iteration: 22662, Training Loss: 3.022e+00 , Validation Loss: 1.445e+00\n",
      "Iteration: 22663, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22664, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 22665, Training Loss: 1.007e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 22666, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22667, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22668, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22669, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22670, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22671, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22672, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22673, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22674, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22675, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 22676, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 22677, Training Loss: 2.015e+00 , Validation Loss: 4.197e+00\n",
      "Iteration: 22678, Training Loss: 1.259e+00 , Validation Loss: 1.216e+00\n",
      "Iteration: 22679, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 22680, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 22681, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22682, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 22683, Training Loss: 1.007e+00 , Validation Loss: 1.875e+00\n",
      "Iteration: 22684, Training Loss: 1.511e+00 , Validation Loss: 2.873e+00\n",
      "Iteration: 22685, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22686, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 22687, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 22688, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22689, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22690, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 22691, Training Loss: 3.274e+00 , Validation Loss: 4.476e+00\n",
      "Iteration: 22692, Training Loss: 2.267e+00 , Validation Loss: 3.042e+00\n",
      "Iteration: 22693, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 22694, Training Loss: 1.007e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 22695, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22696, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22697, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22698, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22699, Training Loss: 1.007e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 22700, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22701, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22702, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22703, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22704, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22705, Training Loss: -1.000e-07 , Validation Loss: 8.891e-01\n",
      "Iteration: 22706, Training Loss: -1.000e-07 , Validation Loss: 8.891e-01\n",
      "Iteration: 22707, Training Loss: 7.555e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 22708, Training Loss: -1.000e-07 , Validation Loss: 4.959e-01\n",
      "Iteration: 22709, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 22710, Training Loss: 3.526e+00 , Validation Loss: 4.306e+00\n",
      "Iteration: 22711, Training Loss: 2.015e+00 , Validation Loss: 2.637e+00\n",
      "Iteration: 22712, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 22713, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 22714, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22715, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22716, Training Loss: 2.015e+00 , Validation Loss: 3.792e+00\n",
      "Iteration: 22717, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 22718, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 22719, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22720, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22721, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22722, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22723, Training Loss: 1.007e+00 , Validation Loss: 3.562e+00\n",
      "Iteration: 22724, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22725, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22726, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22727, Training Loss: 1.259e+00 , Validation Loss: 5.988e-01\n",
      "Iteration: 22728, Training Loss: 1.763e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 22729, Training Loss: 1.007e+00 , Validation Loss: 2.008e+00\n",
      "Iteration: 22730, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 22731, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22732, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22733, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 22734, Training Loss: 2.518e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 22735, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 22736, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22737, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22738, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22739, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22740, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22741, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 22742, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22743, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22744, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22745, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22746, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22747, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22748, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 22749, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 22750, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 22751, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 22752, Training Loss: 2.015e+00 , Validation Loss: 1.409e+00\n",
      "Iteration: 22753, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22754, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22755, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22756, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22757, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22758, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22759, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22760, Training Loss: 1.763e+00 , Validation Loss: 3.822e+00\n",
      "Iteration: 22761, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 22762, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22763, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22764, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22765, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22766, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22767, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22768, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22769, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22770, Training Loss: 5.037e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 22771, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 22772, Training Loss: 3.526e+00 , Validation Loss: 5.153e+00\n",
      "Iteration: 22773, Training Loss: 3.778e+00 , Validation Loss: 4.863e+00\n",
      "Iteration: 22774, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 22775, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 22776, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22777, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22778, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22779, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22780, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22781, Training Loss: 5.037e-01 , Validation Loss: 2.117e+00\n",
      "Iteration: 22782, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 22783, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22784, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22785, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22786, Training Loss: 7.555e-01 , Validation Loss: 1.198e+00\n",
      "Iteration: 22787, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22788, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 22789, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22790, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22791, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22792, Training Loss: 7.555e-01 , Validation Loss: 1.748e+00\n",
      "Iteration: 22793, Training Loss: 1.007e+00 , Validation Loss: 1.276e+00\n",
      "Iteration: 22794, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 22795, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22796, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22797, Training Loss: 5.037e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 22798, Training Loss: 5.037e-01 , Validation Loss: 1.494e+00\n",
      "Iteration: 22799, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22800, Training Loss: 1.259e+00 , Validation Loss: 2.546e+00\n",
      "Iteration: 22801, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22802, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 22803, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 22804, Training Loss: 1.511e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 22805, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22806, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22807, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 22808, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22809, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 22810, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22811, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 22812, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 22813, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 22814, Training Loss: 7.555e-01 , Validation Loss: 1.113e+00\n",
      "Iteration: 22815, Training Loss: 3.022e+00 , Validation Loss: 4.191e+00\n",
      "Iteration: 22816, Training Loss: 1.763e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 22817, Training Loss: 1.511e+00 , Validation Loss: 5.443e-01\n",
      "Iteration: 22818, Training Loss: 1.511e+00 , Validation Loss: 3.714e+00\n",
      "Iteration: 22819, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 22820, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22821, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22822, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22823, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22824, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22825, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22826, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22827, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 22828, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22829, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22830, Training Loss: -1.000e-07 , Validation Loss: 7.318e-01\n",
      "Iteration: 22831, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 22832, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 22833, Training Loss: 2.770e+00 , Validation Loss: 3.804e+00\n",
      "Iteration: 22834, Training Loss: 1.763e+00 , Validation Loss: 1.597e+00\n",
      "Iteration: 22835, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 22836, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 22837, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 22838, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22839, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22840, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22841, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22842, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 22843, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 22844, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 22845, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 22846, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22847, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22848, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22849, Training Loss: 1.259e+00 , Validation Loss: 1.095e+00\n",
      "Iteration: 22850, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 22851, Training Loss: 7.555e-01 , Validation Loss: 1.131e+00\n",
      "Iteration: 22852, Training Loss: 7.555e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 22853, Training Loss: 7.555e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 22854, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22855, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22856, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22857, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22858, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22859, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 22860, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22861, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22862, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 22863, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 22864, Training Loss: 7.555e-01 , Validation Loss: 1.585e+00\n",
      "Iteration: 22865, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 22866, Training Loss: 3.778e+00 , Validation Loss: 4.705e+00\n",
      "Iteration: 22867, Training Loss: 5.541e+00 , Validation Loss: 5.226e+00\n",
      "Iteration: 22868, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22869, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22870, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22871, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22872, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22873, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22874, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22875, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 22876, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22877, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22878, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22879, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22880, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 22881, Training Loss: 2.518e-01 , Validation Loss: 2.353e+00\n",
      "Iteration: 22882, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 22883, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 22884, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22885, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 22886, Training Loss: 7.555e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 22887, Training Loss: 3.274e+00 , Validation Loss: 5.177e+00\n",
      "Iteration: 22888, Training Loss: 2.770e+00 , Validation Loss: 2.443e+00\n",
      "Iteration: 22889, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 22890, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 22891, Training Loss: 1.007e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 22892, Training Loss: 1.511e+00 , Validation Loss: 3.393e+00\n",
      "Iteration: 22893, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 22894, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22895, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22896, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22897, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22898, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22899, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22900, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22901, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22902, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22903, Training Loss: 5.037e-01 , Validation Loss: 1.712e+00\n",
      "Iteration: 22904, Training Loss: 3.274e+00 , Validation Loss: 3.720e+00\n",
      "Iteration: 22905, Training Loss: 3.526e+00 , Validation Loss: 4.530e+00\n",
      "Iteration: 22906, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22907, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 22908, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 22909, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22910, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22911, Training Loss: 1.007e+00 , Validation Loss: 9.556e-01\n",
      "Iteration: 22912, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22913, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22914, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22915, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22916, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 22917, Training Loss: 1.259e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 22918, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22919, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22920, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22921, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 22922, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 22923, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22924, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22925, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22926, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22927, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22928, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22929, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22930, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22931, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22932, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22933, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 22934, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 22935, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 22936, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 22937, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 22938, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22939, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22940, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22941, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22942, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22943, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22944, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22945, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22946, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22947, Training Loss: 1.007e+00 , Validation Loss: 3.774e+00\n",
      "Iteration: 22948, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 22949, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22950, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22951, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22952, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22953, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22954, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22955, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22956, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 22957, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22958, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22959, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22960, Training Loss: -1.000e-07 , Validation Loss: 7.137e-01\n",
      "Iteration: 22961, Training Loss: 1.007e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 22962, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22963, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22964, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 22965, Training Loss: 3.274e+00 , Validation Loss: 4.336e+00\n",
      "Iteration: 22966, Training Loss: 3.526e+00 , Validation Loss: 4.125e+00\n",
      "Iteration: 22967, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22968, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 22969, Training Loss: 2.518e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 22970, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 22971, Training Loss: 5.037e-01 , Validation Loss: 1.270e+00\n",
      "Iteration: 22972, Training Loss: -1.000e-07 , Validation Loss: 1.827e+00\n",
      "Iteration: 22973, Training Loss: 1.259e+00 , Validation Loss: 1.827e+00\n",
      "Iteration: 22974, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22975, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22976, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22977, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22978, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22979, Training Loss: -1.000e-07 , Validation Loss: 4.959e-01\n",
      "Iteration: 22980, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 22981, Training Loss: 1.259e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 22982, Training Loss: 1.511e+00 , Validation Loss: 3.593e+00\n",
      "Iteration: 22983, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 22984, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22985, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 22986, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22987, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 22988, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22989, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 22990, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 22991, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22992, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 22993, Training Loss: 3.526e+00 , Validation Loss: 3.828e+00\n",
      "Iteration: 22994, Training Loss: 4.785e+00 , Validation Loss: 5.594e+00\n",
      "Iteration: 22995, Training Loss: 1.007e+00 , Validation Loss: 5.504e-01\n",
      "Iteration: 22996, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22997, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 22998, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 22999, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23000, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 23001, Training Loss: 1.763e+00 , Validation Loss: 3.532e+00\n",
      "Iteration: 23002, Training Loss: 1.259e+00 , Validation Loss: 5.020e-01\n",
      "Iteration: 23003, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23004, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23005, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23006, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23007, Training Loss: 1.511e+00 , Validation Loss: 2.776e+00\n",
      "Iteration: 23008, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23009, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23010, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23011, Training Loss: 2.267e+00 , Validation Loss: 3.732e+00\n",
      "Iteration: 23012, Training Loss: 1.007e+00 , Validation Loss: 1.494e+00\n",
      "Iteration: 23013, Training Loss: 1.259e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 23014, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 23015, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23016, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 23017, Training Loss: 2.267e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 23018, Training Loss: 7.555e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 23019, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 23020, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23021, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23022, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 23023, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23024, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23025, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 23026, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 23027, Training Loss: 7.555e-01 , Validation Loss: 1.064e+00\n",
      "Iteration: 23028, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23029, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23030, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 23031, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 23032, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23033, Training Loss: 5.037e-01 , Validation Loss: 1.385e+00\n",
      "Iteration: 23034, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23035, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 23036, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23037, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23038, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 23039, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23040, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23041, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23042, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23043, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23044, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23045, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23046, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23047, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 23048, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23049, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23050, Training Loss: 1.259e+00 , Validation Loss: 1.246e+00\n",
      "Iteration: 23051, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23052, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23053, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23054, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23055, Training Loss: 5.037e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 23056, Training Loss: 4.030e+00 , Validation Loss: 4.155e+00\n",
      "Iteration: 23057, Training Loss: 2.770e+00 , Validation Loss: 2.903e+00\n",
      "Iteration: 23058, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 23059, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 23060, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23061, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23062, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23063, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23064, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23065, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23066, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23067, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23068, Training Loss: 1.259e+00 , Validation Loss: 3.635e+00\n",
      "Iteration: 23069, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 23070, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23071, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23072, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23073, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23074, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 23075, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 23076, Training Loss: 1.259e+00 , Validation Loss: 2.879e+00\n",
      "Iteration: 23077, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23078, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23079, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23080, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23081, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23082, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23083, Training Loss: -1.000e-07 , Validation Loss: 6.895e-01\n",
      "Iteration: 23084, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 23085, Training Loss: 5.037e-01 , Validation Loss: 1.524e+00\n",
      "Iteration: 23086, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 23087, Training Loss: 1.259e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 23088, Training Loss: 7.555e-01 , Validation Loss: 1.845e+00\n",
      "Iteration: 23089, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 23090, Training Loss: 2.518e+00 , Validation Loss: 3.786e+00\n",
      "Iteration: 23091, Training Loss: 7.555e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 23092, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 23093, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23094, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23095, Training Loss: 2.518e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 23096, Training Loss: 1.511e+00 , Validation Loss: 1.500e+00\n",
      "Iteration: 23097, Training Loss: 1.259e+00 , Validation Loss: 6.471e-01\n",
      "Iteration: 23098, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23099, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23100, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23101, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23102, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23103, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23104, Training Loss: 3.526e+00 , Validation Loss: 4.282e+00\n",
      "Iteration: 23105, Training Loss: 2.267e+00 , Validation Loss: 2.123e+00\n",
      "Iteration: 23106, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 23107, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 23108, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23109, Training Loss: -1.000e-07 , Validation Loss: 7.983e-01\n",
      "Iteration: 23110, Training Loss: 1.007e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 23111, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23112, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 23113, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23114, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 23115, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 23116, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23117, Training Loss: 1.259e+00 , Validation Loss: 1.802e+00\n",
      "Iteration: 23118, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23119, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23120, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23121, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23122, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23123, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 23124, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 23125, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 23126, Training Loss: 5.037e-01 , Validation Loss: 1.911e+00\n",
      "Iteration: 23127, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23128, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 23129, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 23130, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 23131, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 23132, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 23133, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 23134, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23135, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 23136, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23137, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 23138, Training Loss: 2.518e-01 , Validation Loss: 1.990e+00\n",
      "Iteration: 23139, Training Loss: 3.778e+00 , Validation Loss: 4.203e+00\n",
      "Iteration: 23140, Training Loss: 7.555e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 23141, Training Loss: 1.259e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 23142, Training Loss: 1.007e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 23143, Training Loss: 3.022e+00 , Validation Loss: 3.865e+00\n",
      "Iteration: 23144, Training Loss: 1.007e+00 , Validation Loss: 1.778e+00\n",
      "Iteration: 23145, Training Loss: 1.007e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 23146, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23147, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 23148, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23149, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23150, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23151, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23152, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 23153, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23154, Training Loss: -1.000e-07 , Validation Loss: 1.228e+00\n",
      "Iteration: 23155, Training Loss: 7.555e-01 , Validation Loss: 1.228e+00\n",
      "Iteration: 23156, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23157, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23158, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23159, Training Loss: 5.037e-01 , Validation Loss: 1.270e+00\n",
      "Iteration: 23160, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23161, Training Loss: 2.518e-01 , Validation Loss: 1.494e+00\n",
      "Iteration: 23162, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 23163, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23164, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23165, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23166, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23167, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23168, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23169, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23170, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23171, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23172, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23173, Training Loss: 7.555e-01 , Validation Loss: 1.954e+00\n",
      "Iteration: 23174, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23175, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23176, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23177, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23178, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23179, Training Loss: 7.555e-01 , Validation Loss: 1.524e+00\n",
      "Iteration: 23180, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 23181, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23182, Training Loss: 1.259e+00 , Validation Loss: 4.010e+00\n",
      "Iteration: 23183, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23184, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23185, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23186, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23187, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23188, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23189, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23190, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23191, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23192, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23193, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23194, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23195, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23196, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23197, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 23198, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23199, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23200, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23201, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23202, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23203, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23204, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23205, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23206, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23207, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 23208, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23209, Training Loss: 1.511e+00 , Validation Loss: 3.169e+00\n",
      "Iteration: 23210, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23211, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23212, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23213, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 23214, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 23215, Training Loss: 2.015e+00 , Validation Loss: 3.726e+00\n",
      "Iteration: 23216, Training Loss: 2.518e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 23217, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23218, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23219, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23220, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23221, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23222, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23223, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23224, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23225, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23226, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 23227, Training Loss: 2.770e+00 , Validation Loss: 5.546e+00\n",
      "Iteration: 23228, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 23229, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23230, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 23231, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 23232, Training Loss: 2.518e+00 , Validation Loss: 4.052e+00\n",
      "Iteration: 23233, Training Loss: 1.763e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 23234, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23235, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 23236, Training Loss: 1.259e+00 , Validation Loss: 2.734e+00\n",
      "Iteration: 23237, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 23238, Training Loss: 5.037e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 23239, Training Loss: 7.555e-01 , Validation Loss: 1.494e+00\n",
      "Iteration: 23240, Training Loss: 7.555e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 23241, Training Loss: 1.763e+00 , Validation Loss: 3.635e+00\n",
      "Iteration: 23242, Training Loss: 1.007e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 23243, Training Loss: 1.007e+00 , Validation Loss: 6.653e-01\n",
      "Iteration: 23244, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 23245, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23246, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 23247, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 23248, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 23249, Training Loss: 2.267e+00 , Validation Loss: 3.822e+00\n",
      "Iteration: 23250, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 23251, Training Loss: 7.555e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 23252, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23253, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 23254, Training Loss: -1.000e-07 , Validation Loss: 6.169e-01\n",
      "Iteration: 23255, Training Loss: 5.037e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 23256, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23257, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 23258, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 23259, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23260, Training Loss: -1.000e-07 , Validation Loss: 7.923e-01\n",
      "Iteration: 23261, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 23262, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23263, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 23264, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 23265, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 23266, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23267, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23268, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23269, Training Loss: 3.778e+00 , Validation Loss: 4.433e+00\n",
      "Iteration: 23270, Training Loss: 8.311e+00 , Validation Loss: 1.020e+01\n",
      "Iteration: 23271, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23272, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23273, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23274, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23275, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23276, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23277, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23278, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23279, Training Loss: 1.007e+00 , Validation Loss: 2.661e+00\n",
      "Iteration: 23280, Training Loss: 2.015e+00 , Validation Loss: 3.992e+00\n",
      "Iteration: 23281, Training Loss: 7.555e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 23282, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 23283, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23284, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23285, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23286, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23287, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23288, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23289, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23290, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23291, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 23292, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 23293, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 23294, Training Loss: 5.037e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 23295, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23296, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23297, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 23298, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23299, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23300, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 23301, Training Loss: -1.000e-07 , Validation Loss: 5.262e-01\n",
      "Iteration: 23302, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 23303, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 23304, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 23305, Training Loss: 7.555e-01 , Validation Loss: 2.534e+00\n",
      "Iteration: 23306, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23307, Training Loss: 1.259e+00 , Validation Loss: 2.915e+00\n",
      "Iteration: 23308, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23309, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23310, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 23311, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 23312, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23313, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23314, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23315, Training Loss: 1.007e+00 , Validation Loss: 3.242e+00\n",
      "Iteration: 23316, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23317, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23318, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23319, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23320, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23321, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 23322, Training Loss: 5.037e-01 , Validation Loss: 1.560e+00\n",
      "Iteration: 23323, Training Loss: 1.511e+00 , Validation Loss: 3.018e+00\n",
      "Iteration: 23324, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23325, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23326, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23327, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 23328, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23329, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 23330, Training Loss: 2.770e+00 , Validation Loss: 4.367e+00\n",
      "Iteration: 23331, Training Loss: 3.526e+00 , Validation Loss: 2.443e+00\n",
      "Iteration: 23332, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23333, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23334, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23335, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23336, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23337, Training Loss: 5.037e-01 , Validation Loss: 1.052e+00\n",
      "Iteration: 23338, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23339, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23340, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23341, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23342, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 23343, Training Loss: -1.000e-07 , Validation Loss: 1.040e+00\n",
      "Iteration: 23344, Training Loss: 1.007e+00 , Validation Loss: 1.040e+00\n",
      "Iteration: 23345, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 23346, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23347, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23348, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23349, Training Loss: 2.770e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 23350, Training Loss: 2.267e+00 , Validation Loss: 4.300e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 23351, Training Loss: 1.763e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 23352, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 23353, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 23354, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 23355, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23356, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23357, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23358, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23359, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23360, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 23361, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23362, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23363, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23364, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23365, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23366, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23367, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23368, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23369, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23370, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23371, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23372, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23373, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 23374, Training Loss: 5.037e+00 , Validation Loss: 5.074e+00\n",
      "Iteration: 23375, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 23376, Training Loss: 3.526e+00 , Validation Loss: 4.717e+00\n",
      "Iteration: 23377, Training Loss: 2.015e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 23378, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 23379, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23380, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 23381, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23382, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23383, Training Loss: 1.511e+00 , Validation Loss: 2.177e+00\n",
      "Iteration: 23384, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23385, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 23386, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23387, Training Loss: 1.763e+00 , Validation Loss: 2.298e+00\n",
      "Iteration: 23388, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 23389, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23390, Training Loss: 7.555e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 23391, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23392, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23393, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23394, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23395, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23396, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23397, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23398, Training Loss: 1.511e+00 , Validation Loss: 3.417e+00\n",
      "Iteration: 23399, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 23400, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23401, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23402, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23403, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23404, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23405, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23406, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23407, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23408, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23409, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23410, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23411, Training Loss: 5.037e-01 , Validation Loss: 2.413e+00\n",
      "Iteration: 23412, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23413, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23414, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23415, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 23416, Training Loss: 1.007e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 23417, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23418, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23419, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23420, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23421, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23422, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23423, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23424, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23425, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23426, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23427, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23428, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23429, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23430, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23431, Training Loss: 5.037e-01 , Validation Loss: 1.669e+00\n",
      "Iteration: 23432, Training Loss: 1.007e+00 , Validation Loss: 2.982e+00\n",
      "Iteration: 23433, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 23434, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23435, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23436, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 23437, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 23438, Training Loss: 4.785e+00 , Validation Loss: 5.032e+00\n",
      "Iteration: 23439, Training Loss: 3.022e+00 , Validation Loss: 3.907e+00\n",
      "Iteration: 23440, Training Loss: 2.015e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 23441, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23442, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23443, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 23444, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23445, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23446, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23447, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 23448, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 23449, Training Loss: 1.007e+00 , Validation Loss: 3.459e+00\n",
      "Iteration: 23450, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 23451, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23452, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 23453, Training Loss: 3.022e+00 , Validation Loss: 3.466e+00\n",
      "Iteration: 23454, Training Loss: 1.763e+00 , Validation Loss: 1.064e+00\n",
      "Iteration: 23455, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23456, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23457, Training Loss: 1.259e+00 , Validation Loss: 3.103e+00\n",
      "Iteration: 23458, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23459, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23460, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23461, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23462, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23463, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23464, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 23465, Training Loss: 1.007e+00 , Validation Loss: 1.131e+00\n",
      "Iteration: 23466, Training Loss: 2.770e+00 , Validation Loss: 4.711e+00\n",
      "Iteration: 23467, Training Loss: 1.763e+00 , Validation Loss: 9.979e-01\n",
      "Iteration: 23468, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 23469, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23470, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23471, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23472, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23473, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23474, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 23475, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23476, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23477, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23478, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23479, Training Loss: -1.000e-07 , Validation Loss: 4.597e-01\n",
      "Iteration: 23480, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 23481, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 23482, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23483, Training Loss: 2.518e+00 , Validation Loss: 3.659e+00\n",
      "Iteration: 23484, Training Loss: 1.007e+00 , Validation Loss: 9.012e-01\n",
      "Iteration: 23485, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23486, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 23487, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23488, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23489, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23490, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23491, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 23492, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 23493, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23494, Training Loss: 7.555e-01 , Validation Loss: 1.379e+00\n",
      "Iteration: 23495, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23496, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23497, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23498, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23499, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23500, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23501, Training Loss: 1.007e+00 , Validation Loss: 3.169e+00\n",
      "Iteration: 23502, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23503, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23504, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23505, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23506, Training Loss: 1.259e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 23507, Training Loss: 2.518e+00 , Validation Loss: 3.949e+00\n",
      "Iteration: 23508, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 23509, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23510, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23511, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23512, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23513, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23514, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23515, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23516, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23517, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23518, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23519, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23520, Training Loss: 5.037e-01 , Validation Loss: 2.105e+00\n",
      "Iteration: 23521, Training Loss: 1.259e+00 , Validation Loss: 2.208e+00\n",
      "Iteration: 23522, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23523, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23524, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23525, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23526, Training Loss: 2.518e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 23527, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23528, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23529, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 23530, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23531, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23532, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23533, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 23534, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 23535, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 23536, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 23537, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23538, Training Loss: -1.000e-07 , Validation Loss: 6.713e-01\n",
      "Iteration: 23539, Training Loss: -1.000e-07 , Validation Loss: 6.713e-01\n",
      "Iteration: 23540, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 23541, Training Loss: -1.000e-07 , Validation Loss: 2.552e+00\n",
      "Iteration: 23542, Training Loss: 5.037e-01 , Validation Loss: 2.552e+00\n",
      "Iteration: 23543, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23544, Training Loss: 3.022e+00 , Validation Loss: 3.665e+00\n",
      "Iteration: 23545, Training Loss: 1.511e+00 , Validation Loss: 1.125e+00\n",
      "Iteration: 23546, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23547, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23548, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 23549, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 23550, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23551, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 23552, Training Loss: 5.037e-01 , Validation Loss: 1.572e+00\n",
      "Iteration: 23553, Training Loss: 5.037e-01 , Validation Loss: 1.960e+00\n",
      "Iteration: 23554, Training Loss: 5.037e-01 , Validation Loss: 1.857e+00\n",
      "Iteration: 23555, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 23556, Training Loss: 5.037e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 23557, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23558, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23559, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23560, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 23561, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 23562, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23563, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 23564, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 23565, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 23566, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23567, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23568, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23569, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 23570, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23571, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 23572, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23573, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23574, Training Loss: -1.000e-07 , Validation Loss: 7.802e-01\n",
      "Iteration: 23575, Training Loss: 7.555e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 23576, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23577, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23578, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23579, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23580, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23581, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23582, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23583, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 23584, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23585, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23586, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23587, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23588, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23589, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23590, Training Loss: 1.511e+00 , Validation Loss: 2.365e+00\n",
      "Iteration: 23591, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 23592, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23593, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 23594, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23595, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23596, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23597, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23598, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23599, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23600, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 23601, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 23602, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23603, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23604, Training Loss: 5.037e-01 , Validation Loss: 1.693e+00\n",
      "Iteration: 23605, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 23606, Training Loss: 7.555e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 23607, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 23608, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 23609, Training Loss: 1.007e+00 , Validation Loss: 4.476e-01\n",
      "Iteration: 23610, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23611, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23612, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23613, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23614, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 23615, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 23616, Training Loss: 2.518e-01 , Validation Loss: 1.143e+00\n",
      "Iteration: 23617, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 23618, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 23619, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23620, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 23621, Training Loss: 7.555e-01 , Validation Loss: 1.887e+00\n",
      "Iteration: 23622, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23623, Training Loss: 3.022e+00 , Validation Loss: 3.955e+00\n",
      "Iteration: 23624, Training Loss: 2.770e+00 , Validation Loss: 3.508e+00\n",
      "Iteration: 23625, Training Loss: 5.037e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 23626, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23627, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 23628, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23629, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23630, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23631, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23632, Training Loss: 5.037e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 23633, Training Loss: 4.281e+00 , Validation Loss: 4.972e+00\n",
      "Iteration: 23634, Training Loss: 4.533e+00 , Validation Loss: 4.554e+00\n",
      "Iteration: 23635, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 23636, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23637, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23638, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23639, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23640, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23641, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23642, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23643, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23644, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23645, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23646, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23647, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23648, Training Loss: 2.015e+00 , Validation Loss: 3.853e+00\n",
      "Iteration: 23649, Training Loss: 2.770e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 23650, Training Loss: 2.015e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 23651, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 23652, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23653, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23654, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23655, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23656, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23657, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23658, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 23659, Training Loss: 7.555e-01 , Validation Loss: 2.492e+00\n",
      "Iteration: 23660, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23661, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23662, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23663, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23664, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23665, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23666, Training Loss: 1.259e+00 , Validation Loss: 2.449e+00\n",
      "Iteration: 23667, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23668, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23669, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23670, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23671, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23672, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23673, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 23674, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 23675, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23676, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23677, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23678, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23679, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23680, Training Loss: 1.763e+00 , Validation Loss: 3.786e+00\n",
      "Iteration: 23681, Training Loss: 1.763e+00 , Validation Loss: 8.951e-01\n",
      "Iteration: 23682, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 23683, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23684, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23685, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23686, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23687, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23688, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23689, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23690, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23691, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 23692, Training Loss: 5.037e-01 , Validation Loss: 9.919e-01\n",
      "Iteration: 23693, Training Loss: 2.518e-01 , Validation Loss: 1.385e+00\n",
      "Iteration: 23694, Training Loss: -1.000e-07 , Validation Loss: 4.234e-01\n",
      "Iteration: 23695, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 23696, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23697, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 23698, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23699, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23700, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23701, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23702, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23703, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 23704, Training Loss: 1.763e+00 , Validation Loss: 3.357e+00\n",
      "Iteration: 23705, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23706, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23707, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23708, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23709, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23710, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23711, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23712, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 23713, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23714, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 23715, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 23716, Training Loss: 7.555e-01 , Validation Loss: 2.879e+00\n",
      "Iteration: 23717, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23718, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23719, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23720, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23721, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 23722, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23723, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 23724, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 23725, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23726, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23727, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23728, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23729, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23730, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23731, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23732, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23733, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23734, Training Loss: 5.037e-01 , Validation Loss: 1.518e+00\n",
      "Iteration: 23735, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23736, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 23737, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23738, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 23739, Training Loss: 5.037e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 23740, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23741, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23742, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 23743, Training Loss: 2.267e+00 , Validation Loss: 5.014e+00\n",
      "Iteration: 23744, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 23745, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23746, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23747, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23748, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23749, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 23750, Training Loss: 2.518e-01 , Validation Loss: 3.199e+00\n",
      "Iteration: 23751, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 23752, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 23753, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 23754, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 23755, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23756, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23757, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23758, Training Loss: -1.000e-07 , Validation Loss: 7.318e-01\n",
      "Iteration: 23759, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 23760, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 23761, Training Loss: 5.037e-01 , Validation Loss: 1.452e+00\n",
      "Iteration: 23762, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23763, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23764, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 23765, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 23766, Training Loss: 2.770e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 23767, Training Loss: 2.015e+00 , Validation Loss: 3.109e+00\n",
      "Iteration: 23768, Training Loss: 7.555e-01 , Validation Loss: 8.588e-01\n",
      "Iteration: 23769, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23770, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23771, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 23772, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 23773, Training Loss: 1.007e+00 , Validation Loss: 1.591e+00\n",
      "Iteration: 23774, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 23775, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 23776, Training Loss: 7.555e-01 , Validation Loss: 2.310e+00\n",
      "Iteration: 23777, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23778, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23779, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 23780, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23781, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23782, Training Loss: 1.511e+00 , Validation Loss: 3.671e+00\n",
      "Iteration: 23783, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 23784, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23785, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23786, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23787, Training Loss: -1.000e-07 , Validation Loss: 4.838e-01\n",
      "Iteration: 23788, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 23789, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23790, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23791, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 23792, Training Loss: 2.015e+00 , Validation Loss: 2.498e+00\n",
      "Iteration: 23793, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23794, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23795, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23796, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23797, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23798, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23799, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23800, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23801, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 23802, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 23803, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23804, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23805, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 23806, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 23807, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 23808, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 23809, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23810, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 23811, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 23812, Training Loss: 5.037e-01 , Validation Loss: 1.506e+00\n",
      "Iteration: 23813, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23814, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23815, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23816, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23817, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23818, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23819, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23820, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23821, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 23822, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 23823, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23824, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23825, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23826, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23827, Training Loss: 2.518e-01 , Validation Loss: 1.034e+00\n",
      "Iteration: 23828, Training Loss: -1.000e-07 , Validation Loss: 5.927e-01\n",
      "Iteration: 23829, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 23830, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 23831, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23832, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 23833, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23834, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23835, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23836, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23837, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 23838, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 23839, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 23840, Training Loss: 1.259e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 23841, Training Loss: 1.763e+00 , Validation Loss: 4.270e+00\n",
      "Iteration: 23842, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23843, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23844, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23845, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23846, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23847, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23848, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23849, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23850, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 23851, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 23852, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 23853, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23854, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23855, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23856, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23857, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23858, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 23859, Training Loss: 5.037e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 23860, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23861, Training Loss: 7.555e-01 , Validation Loss: 2.830e+00\n",
      "Iteration: 23862, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23863, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 23864, Training Loss: -1.000e-07 , Validation Loss: 5.322e-01\n",
      "Iteration: 23865, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 23866, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 23867, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 23868, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23869, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 23870, Training Loss: 5.037e-01 , Validation Loss: 1.143e+00\n",
      "Iteration: 23871, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23872, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23873, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23874, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23875, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23876, Training Loss: -1.000e-07 , Validation Loss: 7.681e-01\n",
      "Iteration: 23877, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 23878, Training Loss: 2.518e-01 , Validation Loss: 2.395e+00\n",
      "Iteration: 23879, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 23880, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 23881, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 23882, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23883, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23884, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23885, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 23886, Training Loss: 2.518e+00 , Validation Loss: 4.947e+00\n",
      "Iteration: 23887, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 23888, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 23889, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23890, Training Loss: 1.259e+00 , Validation Loss: 7.802e-01\n",
      "Iteration: 23891, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23892, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23893, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23894, Training Loss: 2.518e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 23895, Training Loss: 1.511e+00 , Validation Loss: 2.111e+00\n",
      "Iteration: 23896, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23897, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23898, Training Loss: 7.555e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 23899, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23900, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23901, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 23902, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 23903, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 23904, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23905, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 23906, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 23907, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23908, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 23909, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 23910, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 23911, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 23912, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 23913, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23914, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 23915, Training Loss: 2.518e+00 , Validation Loss: 3.363e+00\n",
      "Iteration: 23916, Training Loss: 7.555e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 23917, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 23918, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23919, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23920, Training Loss: 1.259e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 23921, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23922, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23923, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 23924, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 23925, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 23926, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 23927, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 23928, Training Loss: 2.015e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 23929, Training Loss: 1.007e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 23930, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23931, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23932, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23933, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23934, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23935, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 23936, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23937, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 23938, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23939, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 23940, Training Loss: 4.533e+00 , Validation Loss: 5.201e+00\n",
      "Iteration: 23941, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 23942, Training Loss: 7.555e-01 , Validation Loss: 3.151e+00\n",
      "Iteration: 23943, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23944, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23945, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23946, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 23947, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23948, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23949, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23950, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23951, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23952, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23953, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23954, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23955, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23956, Training Loss: 5.037e-01 , Validation Loss: 2.522e+00\n",
      "Iteration: 23957, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23958, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23959, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 23960, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 23961, Training Loss: 2.518e-01 , Validation Loss: 1.306e+00\n",
      "Iteration: 23962, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 23963, Training Loss: 2.518e+00 , Validation Loss: 4.433e+00\n",
      "Iteration: 23964, Training Loss: 7.555e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 23965, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23966, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 23967, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23968, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23969, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23970, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23971, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23972, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23973, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 23974, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23975, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23976, Training Loss: -1.000e-07 , Validation Loss: 1.579e+00\n",
      "Iteration: 23977, Training Loss: 2.518e-01 , Validation Loss: 1.579e+00\n",
      "Iteration: 23978, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 23979, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 23980, Training Loss: 1.007e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 23981, Training Loss: 1.511e+00 , Validation Loss: 9.375e-01\n",
      "Iteration: 23982, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23983, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 23984, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23985, Training Loss: 7.555e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 23986, Training Loss: 3.526e+00 , Validation Loss: 3.955e+00\n",
      "Iteration: 23987, Training Loss: 1.007e+00 , Validation Loss: 2.516e+00\n",
      "Iteration: 23988, Training Loss: 2.015e+00 , Validation Loss: 1.639e+00\n",
      "Iteration: 23989, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 23990, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 23991, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23992, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 23993, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 23994, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 23995, Training Loss: 2.267e+00 , Validation Loss: 3.943e+00\n",
      "Iteration: 23996, Training Loss: 1.259e+00 , Validation Loss: 1.240e+00\n",
      "Iteration: 23997, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 23998, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 23999, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24000, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24001, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24002, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24003, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24004, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 24005, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24006, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24007, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24008, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24009, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 24010, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24011, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24012, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24013, Training Loss: 5.037e-01 , Validation Loss: 1.778e+00\n",
      "Iteration: 24014, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24015, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 24016, Training Loss: 3.274e+00 , Validation Loss: 5.117e+00\n",
      "Iteration: 24017, Training Loss: 3.526e+00 , Validation Loss: 3.931e+00\n",
      "Iteration: 24018, Training Loss: 1.511e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 24019, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24020, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 24021, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 24022, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24023, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 24024, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 24025, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24026, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 24027, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 24028, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24029, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24030, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24031, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24032, Training Loss: 2.770e+00 , Validation Loss: 4.929e+00\n",
      "Iteration: 24033, Training Loss: 2.015e+00 , Validation Loss: 1.089e+00\n",
      "Iteration: 24034, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24035, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24036, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24037, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 24038, Training Loss: 1.511e+00 , Validation Loss: 2.268e+00\n",
      "Iteration: 24039, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 24040, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24041, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 24042, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24043, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24044, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24045, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24046, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 24047, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 24048, Training Loss: 2.518e+00 , Validation Loss: 3.925e+00\n",
      "Iteration: 24049, Training Loss: 2.518e-01 , Validation Loss: 1.288e+00\n",
      "Iteration: 24050, Training Loss: 1.511e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 24051, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24052, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 24053, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24054, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24055, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24056, Training Loss: 5.037e-01 , Validation Loss: 1.083e+00\n",
      "Iteration: 24057, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24058, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24059, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 24060, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 24061, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24062, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24063, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24064, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24065, Training Loss: 3.526e+00 , Validation Loss: 3.931e+00\n",
      "Iteration: 24066, Training Loss: 1.763e+00 , Validation Loss: 1.724e+00\n",
      "Iteration: 24067, Training Loss: 1.007e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 24068, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24069, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24070, Training Loss: -1.000e-07 , Validation Loss: 1.403e+00\n",
      "Iteration: 24071, Training Loss: 5.037e-01 , Validation Loss: 1.403e+00\n",
      "Iteration: 24072, Training Loss: 7.555e-01 , Validation Loss: 2.226e+00\n",
      "Iteration: 24073, Training Loss: 3.274e+00 , Validation Loss: 4.645e+00\n",
      "Iteration: 24074, Training Loss: 1.763e+00 , Validation Loss: 1.935e+00\n",
      "Iteration: 24075, Training Loss: 1.259e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 24076, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24077, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 24078, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24079, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24080, Training Loss: 1.259e+00 , Validation Loss: 2.201e+00\n",
      "Iteration: 24081, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24082, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24083, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 24084, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 24085, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 24086, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 24087, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 24088, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24089, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24090, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24091, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24092, Training Loss: 5.037e-01 , Validation Loss: 9.193e-01\n",
      "Iteration: 24093, Training Loss: 1.007e+00 , Validation Loss: 1.415e+00\n",
      "Iteration: 24094, Training Loss: 3.274e+00 , Validation Loss: 4.965e+00\n",
      "Iteration: 24095, Training Loss: 1.259e+00 , Validation Loss: 2.208e+00\n",
      "Iteration: 24096, Training Loss: 1.511e+00 , Validation Loss: 9.737e-01\n",
      "Iteration: 24097, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24098, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 24099, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24100, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 24101, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 24102, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24103, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24104, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 24105, Training Loss: 3.022e+00 , Validation Loss: 3.986e+00\n",
      "Iteration: 24106, Training Loss: 1.511e+00 , Validation Loss: 1.863e+00\n",
      "Iteration: 24107, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 24108, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 24109, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 24110, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24111, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24112, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24113, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24114, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24115, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24116, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 24117, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24118, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24119, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24120, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24121, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24122, Training Loss: 7.555e-01 , Validation Loss: 3.889e+00\n",
      "Iteration: 24123, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24124, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 24125, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24126, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24127, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24128, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24129, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 24130, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 24131, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24132, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24133, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24134, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24135, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 24136, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 24137, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 24138, Training Loss: 2.518e-01 , Validation Loss: 1.464e+00\n",
      "Iteration: 24139, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 24140, Training Loss: 2.518e-01 , Validation Loss: 1.191e+00\n",
      "Iteration: 24141, Training Loss: 3.526e+00 , Validation Loss: 3.871e+00\n",
      "Iteration: 24142, Training Loss: 2.015e+00 , Validation Loss: 3.308e+00\n",
      "Iteration: 24143, Training Loss: 1.511e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 24144, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24145, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 24146, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 24147, Training Loss: 1.007e+00 , Validation Loss: 1.161e+00\n",
      "Iteration: 24148, Training Loss: 1.259e+00 , Validation Loss: 1.972e+00\n",
      "Iteration: 24149, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24150, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 24151, Training Loss: 1.259e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 24152, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 24153, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24154, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24155, Training Loss: 7.555e-01 , Validation Loss: 1.101e+00\n",
      "Iteration: 24156, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 24157, Training Loss: 1.763e+00 , Validation Loss: 2.685e+00\n",
      "Iteration: 24158, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 24159, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 24160, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 24161, Training Loss: 1.259e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 24162, Training Loss: 1.007e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 24163, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24164, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24165, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24166, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24167, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24168, Training Loss: 1.007e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 24169, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 24170, Training Loss: 1.259e+00 , Validation Loss: 2.782e+00\n",
      "Iteration: 24171, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24172, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 24173, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 24174, Training Loss: 3.526e+00 , Validation Loss: 5.153e+00\n",
      "Iteration: 24175, Training Loss: 5.037e-01 , Validation Loss: 1.929e+00\n",
      "Iteration: 24176, Training Loss: 1.007e+00 , Validation Loss: 1.464e+00\n",
      "Iteration: 24177, Training Loss: 5.037e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 24178, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 24179, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24180, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 24181, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 24182, Training Loss: 2.518e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 24183, Training Loss: 2.518e+00 , Validation Loss: 4.095e+00\n",
      "Iteration: 24184, Training Loss: 2.518e-01 , Validation Loss: 1.064e+00\n",
      "Iteration: 24185, Training Loss: 1.259e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 24186, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24187, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24188, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24189, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 24190, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24191, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 24192, Training Loss: 1.511e+00 , Validation Loss: 3.617e+00\n",
      "Iteration: 24193, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24194, Training Loss: 2.518e-01 , Validation Loss: 9.677e-01\n",
      "Iteration: 24195, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 24196, Training Loss: 3.274e+00 , Validation Loss: 3.949e+00\n",
      "Iteration: 24197, Training Loss: 3.022e+00 , Validation Loss: 2.456e+00\n",
      "Iteration: 24198, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24199, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24200, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24201, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 24202, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24203, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24204, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24205, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24206, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24207, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24208, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 24209, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24210, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24211, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24212, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24213, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24214, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24215, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24216, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24217, Training Loss: 3.526e+00 , Validation Loss: 4.663e+00\n",
      "Iteration: 24218, Training Loss: 1.511e+00 , Validation Loss: 1.464e+00\n",
      "Iteration: 24219, Training Loss: 7.555e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 24220, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 24221, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24222, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24223, Training Loss: -1.000e-07 , Validation Loss: 4.778e-01\n",
      "Iteration: 24224, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 24225, Training Loss: -1.000e-07 , Validation Loss: 6.169e-01\n",
      "Iteration: 24226, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 24227, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24228, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24229, Training Loss: 1.763e+00 , Validation Loss: 3.556e+00\n",
      "Iteration: 24230, Training Loss: 1.763e+00 , Validation Loss: 8.891e-01\n",
      "Iteration: 24231, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24232, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24233, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24234, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24235, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 24236, Training Loss: 3.022e+00 , Validation Loss: 4.379e+00\n",
      "Iteration: 24237, Training Loss: 1.511e+00 , Validation Loss: 1.458e+00\n",
      "Iteration: 24238, Training Loss: 1.007e+00 , Validation Loss: 5.080e-01\n",
      "Iteration: 24239, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24240, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24241, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 24242, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24243, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24244, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 24245, Training Loss: 1.007e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 24246, Training Loss: 1.259e+00 , Validation Loss: 1.820e+00\n",
      "Iteration: 24247, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24248, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 24249, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 24250, Training Loss: 7.555e-01 , Validation Loss: 1.095e+00\n",
      "Iteration: 24251, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24252, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24253, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24254, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24255, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24256, Training Loss: 1.007e+00 , Validation Loss: 2.014e+00\n",
      "Iteration: 24257, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24258, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24259, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 24260, Training Loss: 1.259e+00 , Validation Loss: 1.464e+00\n",
      "Iteration: 24261, Training Loss: 2.518e-01 , Validation Loss: 2.020e+00\n",
      "Iteration: 24262, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 24263, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 24264, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 24265, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24266, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 24267, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24268, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24269, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 24270, Training Loss: 5.037e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 24271, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24272, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24273, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24274, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24275, Training Loss: 5.037e-01 , Validation Loss: 2.830e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 24276, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24277, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24278, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24279, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24280, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24281, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24282, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24283, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24284, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24285, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24286, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 24287, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 24288, Training Loss: 1.259e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 24289, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 24290, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 24291, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 24292, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24293, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24294, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24295, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24296, Training Loss: -1.000e-07 , Validation Loss: 6.834e-01\n",
      "Iteration: 24297, Training Loss: 1.007e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 24298, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24299, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24300, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24301, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24302, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24303, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 24304, Training Loss: 7.555e-01 , Validation Loss: 3.072e+00\n",
      "Iteration: 24305, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24306, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24307, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24308, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24309, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24310, Training Loss: 1.259e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 24311, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 24312, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24313, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 24314, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 24315, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 24316, Training Loss: 3.022e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 24317, Training Loss: 2.267e+00 , Validation Loss: 2.329e+00\n",
      "Iteration: 24318, Training Loss: 1.007e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 24319, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24320, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24321, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24322, Training Loss: 1.007e+00 , Validation Loss: 1.524e+00\n",
      "Iteration: 24323, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24324, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24325, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24326, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 24327, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24328, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 24329, Training Loss: 5.037e-01 , Validation Loss: 1.439e+00\n",
      "Iteration: 24330, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24331, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24332, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24333, Training Loss: -1.000e-07 , Validation Loss: 8.407e-01\n",
      "Iteration: 24334, Training Loss: 2.518e-01 , Validation Loss: 8.407e-01\n",
      "Iteration: 24335, Training Loss: -1.000e-07 , Validation Loss: 4.838e-01\n",
      "Iteration: 24336, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 24337, Training Loss: 3.274e+00 , Validation Loss: 5.099e+00\n",
      "Iteration: 24338, Training Loss: 1.511e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 24339, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24340, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 24341, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24342, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24343, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24344, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 24345, Training Loss: 3.022e+00 , Validation Loss: 3.744e+00\n",
      "Iteration: 24346, Training Loss: 7.555e-01 , Validation Loss: 2.353e+00\n",
      "Iteration: 24347, Training Loss: 2.518e-01 , Validation Loss: 1.318e+00\n",
      "Iteration: 24348, Training Loss: 7.555e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 24349, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 24350, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24351, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 24352, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24353, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24354, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 24355, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24356, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24357, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24358, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24359, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 24360, Training Loss: 7.555e-01 , Validation Loss: 1.282e+00\n",
      "Iteration: 24361, Training Loss: 5.037e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 24362, Training Loss: 7.555e-01 , Validation Loss: 2.425e+00\n",
      "Iteration: 24363, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24364, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24365, Training Loss: 2.518e-01 , Validation Loss: 1.415e+00\n",
      "Iteration: 24366, Training Loss: 2.015e+00 , Validation Loss: 4.209e+00\n",
      "Iteration: 24367, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24368, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 24369, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24370, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24371, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 24372, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24373, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24374, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 24375, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24376, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24377, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24378, Training Loss: 2.267e+00 , Validation Loss: 3.623e+00\n",
      "Iteration: 24379, Training Loss: 1.007e+00 , Validation Loss: 1.288e+00\n",
      "Iteration: 24380, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 24381, Training Loss: 1.259e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 24382, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 24383, Training Loss: 2.518e+00 , Validation Loss: 4.101e+00\n",
      "Iteration: 24384, Training Loss: 1.259e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 24385, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 24386, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24387, Training Loss: 2.518e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 24388, Training Loss: -1.000e-07 , Validation Loss: 5.322e-01\n",
      "Iteration: 24389, Training Loss: 1.007e+00 , Validation Loss: 5.322e-01\n",
      "Iteration: 24390, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 24391, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24392, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24393, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24394, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 24395, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24396, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24397, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24398, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24399, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24400, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24401, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 24402, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 24403, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 24404, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 24405, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24406, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24407, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24408, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 24409, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24410, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 24411, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 24412, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24413, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24414, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24415, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24416, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24417, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 24418, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24419, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 24420, Training Loss: 2.518e-01 , Validation Loss: 2.026e+00\n",
      "Iteration: 24421, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 24422, Training Loss: 1.259e+00 , Validation Loss: 1.010e+00\n",
      "Iteration: 24423, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24424, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24425, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24426, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24427, Training Loss: 2.267e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 24428, Training Loss: 1.259e+00 , Validation Loss: 1.560e+00\n",
      "Iteration: 24429, Training Loss: 7.555e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 24430, Training Loss: 1.007e+00 , Validation Loss: 4.657e-01\n",
      "Iteration: 24431, Training Loss: 1.007e+00 , Validation Loss: 1.349e+00\n",
      "Iteration: 24432, Training Loss: 3.526e+00 , Validation Loss: 5.147e+00\n",
      "Iteration: 24433, Training Loss: 1.511e+00 , Validation Loss: 1.566e+00\n",
      "Iteration: 24434, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 24435, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24436, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24437, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 24438, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 24439, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 24440, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 24441, Training Loss: -1.000e-07 , Validation Loss: 5.201e-01\n",
      "Iteration: 24442, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 24443, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24444, Training Loss: 1.511e+00 , Validation Loss: 2.885e+00\n",
      "Iteration: 24445, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24446, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24447, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24448, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24449, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24450, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24451, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24452, Training Loss: 1.511e+00 , Validation Loss: 2.425e+00\n",
      "Iteration: 24453, Training Loss: 1.763e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 24454, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24455, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24456, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 24457, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24458, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 24459, Training Loss: 1.511e+00 , Validation Loss: 2.492e+00\n",
      "Iteration: 24460, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24461, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24462, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 24463, Training Loss: 2.518e-01 , Validation Loss: 1.089e+00\n",
      "Iteration: 24464, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24465, Training Loss: 3.022e+00 , Validation Loss: 3.859e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 24466, Training Loss: 7.555e-01 , Validation Loss: 1.615e+00\n",
      "Iteration: 24467, Training Loss: 7.555e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 24468, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24469, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 24470, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24471, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24472, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 24473, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24474, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24475, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24476, Training Loss: 2.518e-01 , Validation Loss: 9.556e-01\n",
      "Iteration: 24477, Training Loss: 3.022e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 24478, Training Loss: 2.015e+00 , Validation Loss: 1.947e+00\n",
      "Iteration: 24479, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 24480, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24481, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24482, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24483, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24484, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 24485, Training Loss: 2.770e+00 , Validation Loss: 3.538e+00\n",
      "Iteration: 24486, Training Loss: 5.037e-01 , Validation Loss: 2.189e+00\n",
      "Iteration: 24487, Training Loss: 7.555e-01 , Validation Loss: 1.802e+00\n",
      "Iteration: 24488, Training Loss: 1.259e+00 , Validation Loss: 8.528e-01\n",
      "Iteration: 24489, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24490, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 24491, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 24492, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24493, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24494, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24495, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24496, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24497, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24498, Training Loss: 2.015e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 24499, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 24500, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24501, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24502, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24503, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24504, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 24505, Training Loss: 2.518e-01 , Validation Loss: 2.111e+00\n",
      "Iteration: 24506, Training Loss: 2.770e+00 , Validation Loss: 4.367e+00\n",
      "Iteration: 24507, Training Loss: 1.007e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 24508, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24509, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24510, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24511, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24512, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24513, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 24514, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24515, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24516, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24517, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 24518, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24519, Training Loss: 1.763e+00 , Validation Loss: 3.435e+00\n",
      "Iteration: 24520, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 24521, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24522, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 24523, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 24524, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24525, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24526, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24527, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24528, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24529, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24530, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 24531, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 24532, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 24533, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 24534, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24535, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24536, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 24537, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 24538, Training Loss: 1.259e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 24539, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24540, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24541, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24542, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24543, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24544, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24545, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24546, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24547, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 24548, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 24549, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 24550, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24551, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24552, Training Loss: 1.259e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 24553, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24554, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24555, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24556, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24557, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24558, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 24559, Training Loss: 4.030e+00 , Validation Loss: 4.627e+00\n",
      "Iteration: 24560, Training Loss: 1.033e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 24561, Training Loss: 1.511e+00 , Validation Loss: 4.524e+00\n",
      "Iteration: 24562, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 24563, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24564, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24565, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24566, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24567, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 24568, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 24569, Training Loss: 1.511e+00 , Validation Loss: 3.478e+00\n",
      "Iteration: 24570, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24571, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 24572, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24573, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 24574, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24575, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24576, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24577, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24578, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24579, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24580, Training Loss: -1.000e-07 , Validation Loss: 6.532e-01\n",
      "Iteration: 24581, Training Loss: 2.518e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 24582, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24583, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 24584, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24585, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24586, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24587, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24588, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24589, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24590, Training Loss: 1.763e+00 , Validation Loss: 2.250e+00\n",
      "Iteration: 24591, Training Loss: 1.259e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 24592, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24593, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24594, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24595, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24596, Training Loss: 7.555e-01 , Validation Loss: 2.655e+00\n",
      "Iteration: 24597, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24598, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24599, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24600, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24601, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24602, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 24603, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24604, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24605, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24606, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24607, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24608, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24609, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24610, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24611, Training Loss: 1.763e+00 , Validation Loss: 1.530e+00\n",
      "Iteration: 24612, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 24613, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 24614, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24615, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24616, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24617, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24618, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24619, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24620, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24621, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 24622, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24623, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24624, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 24625, Training Loss: 2.518e-01 , Validation Loss: 1.306e+00\n",
      "Iteration: 24626, Training Loss: 4.030e+00 , Validation Loss: 4.125e+00\n",
      "Iteration: 24627, Training Loss: 2.015e+00 , Validation Loss: 2.026e+00\n",
      "Iteration: 24628, Training Loss: 2.015e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 24629, Training Loss: 1.259e+00 , Validation Loss: 1.748e+00\n",
      "Iteration: 24630, Training Loss: 1.007e+00 , Validation Loss: 9.133e-01\n",
      "Iteration: 24631, Training Loss: 1.511e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 24632, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24633, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24634, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24635, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24636, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24637, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 24638, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24639, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 24640, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24641, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24642, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24643, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24644, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 24645, Training Loss: 7.555e-01 , Validation Loss: 1.379e+00\n",
      "Iteration: 24646, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24647, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24648, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24649, Training Loss: 1.511e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 24650, Training Loss: 1.763e+00 , Validation Loss: 2.903e+00\n",
      "Iteration: 24651, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24652, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24653, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24654, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24655, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 24656, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24657, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24658, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24659, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24660, Training Loss: 1.259e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 24661, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24662, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24663, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24664, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24665, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24666, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24667, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24668, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24669, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24670, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24671, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24672, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24673, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24674, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24675, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 24676, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 24677, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 24678, Training Loss: 2.267e+00 , Validation Loss: 3.798e+00\n",
      "Iteration: 24679, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 24680, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 24681, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24682, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24683, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24684, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24685, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 24686, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24687, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24688, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24689, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 24690, Training Loss: 1.259e+00 , Validation Loss: 2.891e+00\n",
      "Iteration: 24691, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 24692, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24693, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24694, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 24695, Training Loss: -1.000e-07 , Validation Loss: 9.979e-01\n",
      "Iteration: 24696, Training Loss: 5.037e-01 , Validation Loss: 9.979e-01\n",
      "Iteration: 24697, Training Loss: 2.518e-01 , Validation Loss: 1.929e+00\n",
      "Iteration: 24698, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 24699, Training Loss: 3.526e+00 , Validation Loss: 4.125e+00\n",
      "Iteration: 24700, Training Loss: 6.800e+00 , Validation Loss: 8.782e+00\n",
      "Iteration: 24701, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24702, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24703, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24704, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24705, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24706, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24707, Training Loss: -1.000e-07 , Validation Loss: 4.838e-01\n",
      "Iteration: 24708, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 24709, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24710, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24711, Training Loss: 1.763e+00 , Validation Loss: 3.387e+00\n",
      "Iteration: 24712, Training Loss: 2.770e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 24713, Training Loss: 2.770e+00 , Validation Loss: 4.197e+00\n",
      "Iteration: 24714, Training Loss: 1.511e+00 , Validation Loss: 1.464e+00\n",
      "Iteration: 24715, Training Loss: 1.007e+00 , Validation Loss: 7.137e-01\n",
      "Iteration: 24716, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 24717, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24718, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24719, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24720, Training Loss: 2.518e-01 , Validation Loss: 1.718e+00\n",
      "Iteration: 24721, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 24722, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 24723, Training Loss: 1.259e+00 , Validation Loss: 1.089e+00\n",
      "Iteration: 24724, Training Loss: 2.518e+00 , Validation Loss: 4.463e+00\n",
      "Iteration: 24725, Training Loss: 1.007e+00 , Validation Loss: 1.064e+00\n",
      "Iteration: 24726, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 24727, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 24728, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 24729, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 24730, Training Loss: 5.037e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 24731, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24732, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24733, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 24734, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 24735, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24736, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24737, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 24738, Training Loss: 2.518e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 24739, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24740, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24741, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24742, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 24743, Training Loss: 2.267e+00 , Validation Loss: 4.784e+00\n",
      "Iteration: 24744, Training Loss: 7.555e-01 , Validation Loss: 1.083e+00\n",
      "Iteration: 24745, Training Loss: 1.259e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 24746, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24747, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24748, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24749, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24750, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 24751, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 24752, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24753, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24754, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24755, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24756, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 24757, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 24758, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24759, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24760, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24761, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24762, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24763, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24764, Training Loss: 7.555e-01 , Validation Loss: 2.147e+00\n",
      "Iteration: 24765, Training Loss: 2.518e+00 , Validation Loss: 5.002e+00\n",
      "Iteration: 24766, Training Loss: 1.007e+00 , Validation Loss: 9.254e-01\n",
      "Iteration: 24767, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24768, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24769, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24770, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24771, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24772, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 24773, Training Loss: 1.511e+00 , Validation Loss: 3.532e+00\n",
      "Iteration: 24774, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24775, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24776, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24777, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 24778, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 24779, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 24780, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 24781, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24782, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24783, Training Loss: 1.259e+00 , Validation Loss: 2.183e+00\n",
      "Iteration: 24784, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24785, Training Loss: 1.259e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 24786, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24787, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24788, Training Loss: 1.511e+00 , Validation Loss: 2.770e+00\n",
      "Iteration: 24789, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24790, Training Loss: 7.555e-01 , Validation Loss: 9.677e-01\n",
      "Iteration: 24791, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24792, Training Loss: 5.037e-01 , Validation Loss: 1.978e+00\n",
      "Iteration: 24793, Training Loss: 2.770e+00 , Validation Loss: 5.183e+00\n",
      "Iteration: 24794, Training Loss: 1.259e+00 , Validation Loss: 1.222e+00\n",
      "Iteration: 24795, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 24796, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 24797, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 24798, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 24799, Training Loss: 2.015e+00 , Validation Loss: 2.220e+00\n",
      "Iteration: 24800, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24801, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 24802, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24803, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24804, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24805, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24806, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24807, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24808, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24809, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24810, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 24811, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24812, Training Loss: 7.555e-01 , Validation Loss: 6.411e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 24813, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 24814, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 24815, Training Loss: 2.015e+00 , Validation Loss: 3.284e+00\n",
      "Iteration: 24816, Training Loss: -1.000e-07 , Validation Loss: 6.713e-01\n",
      "Iteration: 24817, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 24818, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 24819, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24820, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 24821, Training Loss: 1.007e+00 , Validation Loss: 3.689e-01\n",
      "Iteration: 24822, Training Loss: 7.555e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 24823, Training Loss: 1.763e+00 , Validation Loss: 3.647e+00\n",
      "Iteration: 24824, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 24825, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24826, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24827, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24828, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24829, Training Loss: -1.000e-07 , Validation Loss: 5.443e-01\n",
      "Iteration: 24830, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 24831, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24832, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24833, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24834, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24835, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24836, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24837, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24838, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24839, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24840, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24841, Training Loss: 2.267e+00 , Validation Loss: 5.002e+00\n",
      "Iteration: 24842, Training Loss: 5.037e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 24843, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 24844, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 24845, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 24846, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24847, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24848, Training Loss: 1.259e+00 , Validation Loss: 3.496e+00\n",
      "Iteration: 24849, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24850, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24851, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24852, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24853, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24854, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24855, Training Loss: 7.555e-01 , Validation Loss: 1.433e+00\n",
      "Iteration: 24856, Training Loss: 1.763e+00 , Validation Loss: 4.155e+00\n",
      "Iteration: 24857, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24858, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 24859, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24860, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24861, Training Loss: 7.555e-01 , Validation Loss: 1.071e+00\n",
      "Iteration: 24862, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24863, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24864, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24865, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24866, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24867, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24868, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24869, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24870, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24871, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 24872, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 24873, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24874, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 24875, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 24876, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24877, Training Loss: 7.555e-01 , Validation Loss: 1.766e+00\n",
      "Iteration: 24878, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24879, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24880, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24881, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24882, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 24883, Training Loss: 2.770e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 24884, Training Loss: 2.518e+00 , Validation Loss: 2.304e+00\n",
      "Iteration: 24885, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 24886, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 24887, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 24888, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24889, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 24890, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 24891, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24892, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24893, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24894, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 24895, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24896, Training Loss: 1.259e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 24897, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 24898, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 24899, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 24900, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24901, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24902, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24903, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24904, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 24905, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 24906, Training Loss: 5.037e-01 , Validation Loss: 2.401e+00\n",
      "Iteration: 24907, Training Loss: 1.511e+00 , Validation Loss: 2.171e+00\n",
      "Iteration: 24908, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24909, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24910, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 24911, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 24912, Training Loss: 1.007e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 24913, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 24914, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24915, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24916, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 24917, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24918, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24919, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 24920, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24921, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 24922, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 24923, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 24924, Training Loss: 2.015e+00 , Validation Loss: 1.300e+00\n",
      "Iteration: 24925, Training Loss: 1.511e+00 , Validation Loss: 8.225e-01\n",
      "Iteration: 24926, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24927, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24928, Training Loss: 5.037e-01 , Validation Loss: 9.495e-01\n",
      "Iteration: 24929, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 24930, Training Loss: 1.763e+00 , Validation Loss: 1.258e+00\n",
      "Iteration: 24931, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 24932, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 24933, Training Loss: 5.037e-01 , Validation Loss: 1.893e+00\n",
      "Iteration: 24934, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 24935, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24936, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24937, Training Loss: 1.007e+00 , Validation Loss: 1.306e+00\n",
      "Iteration: 24938, Training Loss: 2.518e-01 , Validation Loss: 2.026e+00\n",
      "Iteration: 24939, Training Loss: 7.555e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 24940, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 24941, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24942, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24943, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24944, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 24945, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24946, Training Loss: 5.037e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 24947, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24948, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 24949, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24950, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24951, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 24952, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24953, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24954, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24955, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24956, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24957, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 24958, Training Loss: 1.259e+00 , Validation Loss: 1.046e+00\n",
      "Iteration: 24959, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 24960, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 24961, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24962, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24963, Training Loss: 5.037e-01 , Validation Loss: 8.044e-01\n",
      "Iteration: 24964, Training Loss: 2.015e+00 , Validation Loss: 4.445e+00\n",
      "Iteration: 24965, Training Loss: 5.037e-01 , Validation Loss: 8.407e-01\n",
      "Iteration: 24966, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 24967, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24968, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24969, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24970, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24971, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24972, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24973, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 24974, Training Loss: 7.555e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 24975, Training Loss: 1.259e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 24976, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24977, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24978, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24979, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24980, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24981, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24982, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 24983, Training Loss: 7.555e-01 , Validation Loss: 2.564e+00\n",
      "Iteration: 24984, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24985, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24986, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24987, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 24988, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 24989, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24990, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24991, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24992, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24993, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24994, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 24995, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24996, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 24997, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 24998, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 24999, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 25000, Training Loss: 5.037e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 25001, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25002, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25003, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25004, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25005, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25006, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25007, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 25008, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 25009, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 25010, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25011, Training Loss: 7.555e-01 , Validation Loss: 1.524e+00\n",
      "Iteration: 25012, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 25013, Training Loss: 3.022e+00 , Validation Loss: 5.165e+00\n",
      "Iteration: 25014, Training Loss: 1.007e+00 , Validation Loss: 1.494e+00\n",
      "Iteration: 25015, Training Loss: 5.037e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 25016, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 25017, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 25018, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25019, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 25020, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25021, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 25022, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25023, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25024, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25025, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25026, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25027, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25028, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25029, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 25030, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 25031, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 25032, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25033, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25034, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25035, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25036, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 25037, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 25038, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25039, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 25040, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25041, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25042, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25043, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25044, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25045, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 25046, Training Loss: 3.274e+00 , Validation Loss: 4.578e+00\n",
      "Iteration: 25047, Training Loss: 2.770e+00 , Validation Loss: 4.143e+00\n",
      "Iteration: 25048, Training Loss: 1.259e+00 , Validation Loss: 9.012e-01\n",
      "Iteration: 25049, Training Loss: 1.007e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 25050, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25051, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25052, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25053, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25054, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25055, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25056, Training Loss: 7.555e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 25057, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 25058, Training Loss: 5.037e-01 , Validation Loss: 1.784e+00\n",
      "Iteration: 25059, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25060, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25061, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 25062, Training Loss: 5.037e-01 , Validation Loss: 1.730e+00\n",
      "Iteration: 25063, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25064, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25065, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25066, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25067, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25068, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25069, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25070, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25071, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25072, Training Loss: 7.555e-01 , Validation Loss: 1.718e+00\n",
      "Iteration: 25073, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25074, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 25075, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25076, Training Loss: 2.267e+00 , Validation Loss: 4.252e+00\n",
      "Iteration: 25077, Training Loss: 1.511e+00 , Validation Loss: 1.228e+00\n",
      "Iteration: 25078, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 25079, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25080, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25081, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 25082, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25083, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25084, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25085, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25086, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25087, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 25088, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25089, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25090, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 25091, Training Loss: 7.555e-01 , Validation Loss: 1.391e+00\n",
      "Iteration: 25092, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 25093, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25094, Training Loss: 1.007e+00 , Validation Loss: 1.391e+00\n",
      "Iteration: 25095, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25096, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25097, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25098, Training Loss: -1.000e-07 , Validation Loss: 5.020e-01\n",
      "Iteration: 25099, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 25100, Training Loss: 1.259e+00 , Validation Loss: 3.834e+00\n",
      "Iteration: 25101, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25102, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25103, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25104, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 25105, Training Loss: 2.518e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 25106, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 25107, Training Loss: 1.007e+00 , Validation Loss: 2.383e+00\n",
      "Iteration: 25108, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25109, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25110, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25111, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 25112, Training Loss: 2.015e+00 , Validation Loss: 3.810e+00\n",
      "Iteration: 25113, Training Loss: 1.259e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 25114, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25115, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25116, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25117, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25118, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25119, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25120, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25121, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 25122, Training Loss: 4.785e+00 , Validation Loss: 4.300e+00\n",
      "Iteration: 25123, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 25124, Training Loss: 7.555e-01 , Validation Loss: 1.264e+00\n",
      "Iteration: 25125, Training Loss: 1.007e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 25126, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25127, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25128, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25129, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25130, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25131, Training Loss: 1.259e+00 , Validation Loss: 2.268e+00\n",
      "Iteration: 25132, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25133, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 25134, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25135, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 25136, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25137, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25138, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25139, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25140, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25141, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25142, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25143, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25144, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25145, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25146, Training Loss: 1.007e+00 , Validation Loss: 1.379e+00\n",
      "Iteration: 25147, Training Loss: 3.022e+00 , Validation Loss: 5.044e+00\n",
      "Iteration: 25148, Training Loss: 2.015e+00 , Validation Loss: 1.125e+00\n",
      "Iteration: 25149, Training Loss: 1.511e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 25150, Training Loss: 1.511e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 25151, Training Loss: 3.274e+00 , Validation Loss: 5.365e+00\n",
      "Iteration: 25152, Training Loss: 1.763e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 25153, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25154, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25155, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25156, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25157, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 25158, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 25159, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 25160, Training Loss: 2.518e+00 , Validation Loss: 3.556e+00\n",
      "Iteration: 25161, Training Loss: 1.007e+00 , Validation Loss: 8.588e-01\n",
      "Iteration: 25162, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25163, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25164, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25165, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25166, Training Loss: 1.259e+00 , Validation Loss: 3.157e+00\n",
      "Iteration: 25167, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25168, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25169, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25170, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25171, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25172, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25173, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25174, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25175, Training Loss: 1.511e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 25176, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25177, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25178, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25179, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25180, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25181, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 25182, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25183, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 25184, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 25185, Training Loss: 5.037e+00 , Validation Loss: 5.183e+00\n",
      "Iteration: 25186, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 25187, Training Loss: 3.274e+00 , Validation Loss: 5.014e+00\n",
      "Iteration: 25188, Training Loss: 1.511e+00 , Validation Loss: 6.592e-01\n",
      "Iteration: 25189, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 25190, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 25191, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25192, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25193, Training Loss: 1.007e+00 , Validation Loss: 1.996e+00\n",
      "Iteration: 25194, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 25195, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 25196, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 25197, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 25198, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 25199, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25200, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25201, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25202, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25203, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25204, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25205, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25206, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25207, Training Loss: 1.007e+00 , Validation Loss: 2.220e+00\n",
      "Iteration: 25208, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25209, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25210, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25211, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 25212, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 25213, Training Loss: 3.022e+00 , Validation Loss: 3.907e+00\n",
      "Iteration: 25214, Training Loss: 1.511e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 25215, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25216, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 25217, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25218, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25219, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25220, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25221, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25222, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25223, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25224, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25225, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25226, Training Loss: 7.555e-01 , Validation Loss: 1.808e+00\n",
      "Iteration: 25227, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 25228, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 25229, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25230, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 25231, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25232, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 25233, Training Loss: 2.770e+00 , Validation Loss: 3.266e+00\n",
      "Iteration: 25234, Training Loss: 1.763e+00 , Validation Loss: 1.621e+00\n",
      "Iteration: 25235, Training Loss: 1.259e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 25236, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25237, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25238, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25239, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25240, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25241, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25242, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25243, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25244, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25245, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25246, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25247, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25248, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25249, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 25250, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25251, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 25252, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 25253, Training Loss: 7.555e-01 , Validation Loss: 2.812e+00\n",
      "Iteration: 25254, Training Loss: 1.007e+00 , Validation Loss: 1.591e+00\n",
      "Iteration: 25255, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25256, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 25257, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25258, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25259, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 25260, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25261, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25262, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25263, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25264, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 25265, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 25266, Training Loss: 1.007e+00 , Validation Loss: 6.532e-01\n",
      "Iteration: 25267, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25268, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25269, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25270, Training Loss: 7.555e-01 , Validation Loss: 1.101e+00\n",
      "Iteration: 25271, Training Loss: 3.526e+00 , Validation Loss: 4.064e+00\n",
      "Iteration: 25272, Training Loss: 3.022e+00 , Validation Loss: 2.583e+00\n",
      "Iteration: 25273, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25274, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25275, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25276, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25277, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25278, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25279, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25280, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25281, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 25282, Training Loss: 1.763e+00 , Validation Loss: 4.379e+00\n",
      "Iteration: 25283, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 25284, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 25285, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25286, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25287, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25288, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25289, Training Loss: 5.037e+00 , Validation Loss: 4.439e+00\n",
      "Iteration: 25290, Training Loss: 1.234e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 25291, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 25292, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 25293, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 25294, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 25295, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 25296, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25297, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25298, Training Loss: 5.037e-01 , Validation Loss: 2.002e+00\n",
      "Iteration: 25299, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25300, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 25301, Training Loss: 2.267e+00 , Validation Loss: 3.072e+00\n",
      "Iteration: 25302, Training Loss: 1.007e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 25303, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 25304, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 25305, Training Loss: 1.259e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 25306, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25307, Training Loss: 1.259e+00 , Validation Loss: 2.353e+00\n",
      "Iteration: 25308, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 25309, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 25310, Training Loss: 2.518e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 25311, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25312, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25313, Training Loss: 1.007e+00 , Validation Loss: 2.298e+00\n",
      "Iteration: 25314, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25315, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25316, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 25317, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 25318, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25319, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25320, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25321, Training Loss: 2.015e+00 , Validation Loss: 2.812e+00\n",
      "Iteration: 25322, Training Loss: 2.267e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 25323, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25324, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25325, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25326, Training Loss: 3.526e+00 , Validation Loss: 4.917e+00\n",
      "Iteration: 25327, Training Loss: 3.526e+00 , Validation Loss: 3.865e+00\n",
      "Iteration: 25328, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 25329, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25330, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25331, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25332, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25333, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25334, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25335, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25336, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25337, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25338, Training Loss: 3.022e+00 , Validation Loss: 4.488e+00\n",
      "Iteration: 25339, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 25340, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 25341, Training Loss: 1.007e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 25342, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25343, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25344, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25345, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25346, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25347, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25348, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25349, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25350, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 25351, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 25352, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 25353, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25354, Training Loss: 7.555e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 25355, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25356, Training Loss: 5.037e-01 , Validation Loss: 1.978e+00\n",
      "Iteration: 25357, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 25358, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 25359, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 25360, Training Loss: 7.555e-01 , Validation Loss: 1.651e+00\n",
      "Iteration: 25361, Training Loss: 1.511e+00 , Validation Loss: 4.185e+00\n",
      "Iteration: 25362, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 25363, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25364, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25365, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25366, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25367, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25368, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25369, Training Loss: 2.518e-01 , Validation Loss: 1.119e+00\n",
      "Iteration: 25370, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 25371, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 25372, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 25373, Training Loss: 3.778e+00 , Validation Loss: 5.189e+00\n",
      "Iteration: 25374, Training Loss: 5.289e+00 , Validation Loss: 5.867e+00\n",
      "Iteration: 25375, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25376, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25377, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 25378, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 25379, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 25380, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25381, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25382, Training Loss: 2.518e-01 , Validation Loss: 1.222e+00\n",
      "Iteration: 25383, Training Loss: 1.511e+00 , Validation Loss: 3.351e+00\n",
      "Iteration: 25384, Training Loss: 7.555e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 25385, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25386, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25387, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25388, Training Loss: 2.518e-01 , Validation Loss: 8.165e-01\n",
      "Iteration: 25389, Training Loss: 7.555e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 25390, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25391, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25392, Training Loss: 7.555e-01 , Validation Loss: 1.034e+00\n",
      "Iteration: 25393, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 25394, Training Loss: 1.763e+00 , Validation Loss: 3.387e+00\n",
      "Iteration: 25395, Training Loss: 7.555e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 25396, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25397, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25398, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25399, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 25400, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25401, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25402, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25403, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25404, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25405, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25406, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25407, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 25408, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25409, Training Loss: 3.274e+00 , Validation Loss: 4.349e+00\n",
      "Iteration: 25410, Training Loss: 1.007e+00 , Validation Loss: 1.071e+00\n",
      "Iteration: 25411, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 25412, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25413, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25414, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 25415, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25416, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 25417, Training Loss: 1.007e+00 , Validation Loss: 7.862e-01\n",
      "Iteration: 25418, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 25419, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 25420, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 25421, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25422, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25423, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25424, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25425, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 25426, Training Loss: 1.007e+00 , Validation Loss: 2.824e+00\n",
      "Iteration: 25427, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25428, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25429, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25430, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25431, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25432, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25433, Training Loss: 1.007e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 25434, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 25435, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25436, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25437, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25438, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25439, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 25440, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 25441, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25442, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25443, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25444, Training Loss: -1.000e-07 , Validation Loss: 7.742e-01\n",
      "Iteration: 25445, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 25446, Training Loss: -1.000e-07 , Validation Loss: 2.631e+00\n",
      "Iteration: 25447, Training Loss: 1.259e+00 , Validation Loss: 2.631e+00\n",
      "Iteration: 25448, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 25449, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 25450, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25451, Training Loss: 7.555e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 25452, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 25453, Training Loss: 2.518e-01 , Validation Loss: 1.155e+00\n",
      "Iteration: 25454, Training Loss: 1.259e+00 , Validation Loss: 5.504e-01\n",
      "Iteration: 25455, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25456, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25457, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25458, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 25459, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25460, Training Loss: 5.037e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 25461, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 25462, Training Loss: 7.555e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 25463, Training Loss: 2.015e+00 , Validation Loss: 4.808e+00\n",
      "Iteration: 25464, Training Loss: 7.555e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 25465, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 25466, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25467, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25468, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25469, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25470, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25471, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 25472, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 25473, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 25474, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 25475, Training Loss: 1.259e+00 , Validation Loss: 2.625e+00\n",
      "Iteration: 25476, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25477, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 25478, Training Loss: 1.007e+00 , Validation Loss: 1.161e+00\n",
      "Iteration: 25479, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 25480, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 25481, Training Loss: 5.037e-01 , Validation Loss: 2.365e+00\n",
      "Iteration: 25482, Training Loss: 3.274e+00 , Validation Loss: 5.219e+00\n",
      "Iteration: 25483, Training Loss: 1.511e+00 , Validation Loss: 2.468e+00\n",
      "Iteration: 25484, Training Loss: 1.007e+00 , Validation Loss: 8.649e-01\n",
      "Iteration: 25485, Training Loss: 1.007e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 25486, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25487, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25488, Training Loss: 2.015e+00 , Validation Loss: 2.837e+00\n",
      "Iteration: 25489, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 25490, Training Loss: 1.007e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 25491, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25492, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25493, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25494, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25495, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25496, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25497, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25498, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25499, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25500, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25501, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25502, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25503, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25504, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25505, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 25506, Training Loss: 5.037e-01 , Validation Loss: 1.270e+00\n",
      "Iteration: 25507, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25508, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25509, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 25510, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25511, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25512, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25513, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 25514, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25515, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25516, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25517, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25518, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25519, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25520, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25521, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25522, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 25523, Training Loss: 5.037e+00 , Validation Loss: 4.234e+00\n",
      "Iteration: 25524, Training Loss: 9.318e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 25525, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25526, Training Loss: 1.511e+00 , Validation Loss: 3.937e+00\n",
      "Iteration: 25527, Training Loss: -1.000e-07 , Validation Loss: 4.597e-01\n",
      "Iteration: 25528, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 25529, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25530, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25531, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25532, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25533, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 25534, Training Loss: 3.274e+00 , Validation Loss: 4.470e+00\n",
      "Iteration: 25535, Training Loss: 2.015e+00 , Validation Loss: 1.325e+00\n",
      "Iteration: 25536, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25537, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25538, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25539, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 25540, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 25541, Training Loss: 1.511e+00 , Validation Loss: 3.931e-01\n",
      "Iteration: 25542, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25543, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25544, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25545, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25546, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25547, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25548, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25549, Training Loss: -1.000e-07 , Validation Loss: 1.010e+00\n",
      "Iteration: 25550, Training Loss: 1.007e+00 , Validation Loss: 1.010e+00\n",
      "Iteration: 25551, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25552, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 25553, Training Loss: 3.526e+00 , Validation Loss: 4.524e+00\n",
      "Iteration: 25554, Training Loss: 1.511e+00 , Validation Loss: 1.802e+00\n",
      "Iteration: 25555, Training Loss: 1.259e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 25556, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25557, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 25558, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25559, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25560, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25561, Training Loss: 5.037e-01 , Validation Loss: 3.478e+00\n",
      "Iteration: 25562, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25563, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 25564, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25565, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25566, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25567, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25568, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25569, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25570, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25571, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25572, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25573, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25574, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25575, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25576, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25577, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 25578, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 25579, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 25580, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 25581, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 25582, Training Loss: 3.022e+00 , Validation Loss: 4.216e+00\n",
      "Iteration: 25583, Training Loss: 3.526e+00 , Validation Loss: 2.443e+00\n",
      "Iteration: 25584, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25585, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25586, Training Loss: 2.015e+00 , Validation Loss: 3.417e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 25587, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25588, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 25589, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 25590, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25591, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25592, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25593, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25594, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25595, Training Loss: 7.555e-01 , Validation Loss: 9.193e-01\n",
      "Iteration: 25596, Training Loss: 2.518e+00 , Validation Loss: 4.058e+00\n",
      "Iteration: 25597, Training Loss: 1.511e+00 , Validation Loss: 1.204e+00\n",
      "Iteration: 25598, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 25599, Training Loss: 1.007e+00 , Validation Loss: 4.778e-01\n",
      "Iteration: 25600, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25601, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 25602, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25603, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25604, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25605, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25606, Training Loss: 5.037e-01 , Validation Loss: 1.954e+00\n",
      "Iteration: 25607, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25608, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25609, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25610, Training Loss: 7.555e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 25611, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25612, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25613, Training Loss: 2.518e-01 , Validation Loss: 1.113e+00\n",
      "Iteration: 25614, Training Loss: 7.555e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 25615, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 25616, Training Loss: 7.555e-01 , Validation Loss: 1.288e+00\n",
      "Iteration: 25617, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25618, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25619, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25620, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25621, Training Loss: 1.007e+00 , Validation Loss: 5.504e-01\n",
      "Iteration: 25622, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 25623, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 25624, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25625, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25626, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 25627, Training Loss: 5.037e-01 , Validation Loss: 1.083e+00\n",
      "Iteration: 25628, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25629, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25630, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 25631, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25632, Training Loss: 1.007e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 25633, Training Loss: 1.763e+00 , Validation Loss: 3.066e+00\n",
      "Iteration: 25634, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 25635, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25636, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25637, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25638, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25639, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25640, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25641, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 25642, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25643, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25644, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25645, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25646, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 25647, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 25648, Training Loss: 7.555e-01 , Validation Loss: 1.403e+00\n",
      "Iteration: 25649, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 25650, Training Loss: 7.555e-01 , Validation Loss: 1.766e+00\n",
      "Iteration: 25651, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 25652, Training Loss: 1.007e+00 , Validation Loss: 2.304e+00\n",
      "Iteration: 25653, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25654, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25655, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25656, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25657, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25658, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25659, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25660, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 25661, Training Loss: 1.007e+00 , Validation Loss: 2.625e+00\n",
      "Iteration: 25662, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25663, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25664, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25665, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25666, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25667, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25668, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25669, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 25670, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25671, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25672, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25673, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25674, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25675, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 25676, Training Loss: 1.007e+00 , Validation Loss: 1.101e+00\n",
      "Iteration: 25677, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 25678, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 25679, Training Loss: 2.267e+00 , Validation Loss: 3.103e+00\n",
      "Iteration: 25680, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 25681, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 25682, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25683, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25684, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25685, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25686, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25687, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25688, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25689, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 25690, Training Loss: 2.267e+00 , Validation Loss: 4.451e+00\n",
      "Iteration: 25691, Training Loss: 1.259e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 25692, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25693, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25694, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25695, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25696, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25697, Training Loss: 5.037e-01 , Validation Loss: 1.222e+00\n",
      "Iteration: 25698, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 25699, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25700, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 25701, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25702, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25703, Training Loss: 7.555e-01 , Validation Loss: 8.225e-01\n",
      "Iteration: 25704, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 25705, Training Loss: 1.007e+00 , Validation Loss: 1.125e+00\n",
      "Iteration: 25706, Training Loss: 7.555e-01 , Validation Loss: 1.742e+00\n",
      "Iteration: 25707, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25708, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25709, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25710, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25711, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25712, Training Loss: 1.007e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 25713, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25714, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25715, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25716, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25717, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25718, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25719, Training Loss: 2.518e-01 , Validation Loss: 1.318e+00\n",
      "Iteration: 25720, Training Loss: -1.000e-07 , Validation Loss: 5.988e-01\n",
      "Iteration: 25721, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 25722, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25723, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 25724, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 25725, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 25726, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 25727, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 25728, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25729, Training Loss: 1.007e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 25730, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25731, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25732, Training Loss: -1.000e-07 , Validation Loss: 4.899e-01\n",
      "Iteration: 25733, Training Loss: -1.000e-07 , Validation Loss: 4.899e-01\n",
      "Iteration: 25734, Training Loss: 2.518e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 25735, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25736, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25737, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25738, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25739, Training Loss: 5.037e-01 , Validation Loss: 1.228e+00\n",
      "Iteration: 25740, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25741, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 25742, Training Loss: 2.770e+00 , Validation Loss: 4.125e+00\n",
      "Iteration: 25743, Training Loss: 2.267e+00 , Validation Loss: 1.845e+00\n",
      "Iteration: 25744, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25745, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 25746, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 25747, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 25748, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25749, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25750, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25751, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 25752, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25753, Training Loss: 1.007e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 25754, Training Loss: 3.274e+00 , Validation Loss: 4.784e+00\n",
      "Iteration: 25755, Training Loss: 5.037e-01 , Validation Loss: 1.458e+00\n",
      "Iteration: 25756, Training Loss: 1.511e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 25757, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 25758, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25759, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25760, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25761, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25762, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25763, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25764, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25765, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25766, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25767, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25768, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25769, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 25770, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 25771, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25772, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 25773, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 25774, Training Loss: 2.015e+00 , Validation Loss: 3.490e+00\n",
      "Iteration: 25775, Training Loss: 7.555e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 25776, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 25777, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 25778, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25779, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 25780, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 25781, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25782, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 25783, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25784, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25785, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25786, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25787, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25788, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25789, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25790, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25791, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25792, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25793, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25794, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25795, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 25796, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25797, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25798, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 25799, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25800, Training Loss: 5.037e-01 , Validation Loss: 2.951e+00\n",
      "Iteration: 25801, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 25802, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 25803, Training Loss: -1.000e-07 , Validation Loss: 7.197e-01\n",
      "Iteration: 25804, Training Loss: -1.000e-07 , Validation Loss: 7.197e-01\n",
      "Iteration: 25805, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 25806, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 25807, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25808, Training Loss: 1.259e+00 , Validation Loss: 1.579e+00\n",
      "Iteration: 25809, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25810, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 25811, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 25812, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25813, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25814, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 25815, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25816, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25817, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25818, Training Loss: 5.037e-01 , Validation Loss: 1.845e+00\n",
      "Iteration: 25819, Training Loss: 1.763e+00 , Validation Loss: 2.679e+00\n",
      "Iteration: 25820, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25821, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25822, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25823, Training Loss: 1.259e+00 , Validation Loss: 2.117e+00\n",
      "Iteration: 25824, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25825, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25826, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25827, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25828, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 25829, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 25830, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 25831, Training Loss: 1.007e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 25832, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25833, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25834, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25835, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 25836, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 25837, Training Loss: 1.007e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 25838, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 25839, Training Loss: 2.518e-01 , Validation Loss: 1.331e+00\n",
      "Iteration: 25840, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 25841, Training Loss: 4.533e+00 , Validation Loss: 5.092e+00\n",
      "Iteration: 25842, Training Loss: 2.518e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 25843, Training Loss: 7.555e-01 , Validation Loss: 9.979e-01\n",
      "Iteration: 25844, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 25845, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25846, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25847, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25848, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25849, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25850, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25851, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25852, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25853, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25854, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25855, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25856, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25857, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25858, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25859, Training Loss: 7.555e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 25860, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 25861, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 25862, Training Loss: 7.555e-01 , Validation Loss: 1.191e+00\n",
      "Iteration: 25863, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25864, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 25865, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25866, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 25867, Training Loss: 2.015e+00 , Validation Loss: 3.768e+00\n",
      "Iteration: 25868, Training Loss: 1.007e+00 , Validation Loss: 9.979e-01\n",
      "Iteration: 25869, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 25870, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25871, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25872, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25873, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25874, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25875, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 25876, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25877, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25878, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 25879, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 25880, Training Loss: 7.555e-01 , Validation Loss: 2.613e+00\n",
      "Iteration: 25881, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25882, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25883, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25884, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25885, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25886, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25887, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25888, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25889, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25890, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25891, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25892, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25893, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 25894, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25895, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25896, Training Loss: 7.555e-01 , Validation Loss: 2.419e+00\n",
      "Iteration: 25897, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 25898, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25899, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25900, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 25901, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25902, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25903, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25904, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 25905, Training Loss: 3.274e+00 , Validation Loss: 4.107e+00\n",
      "Iteration: 25906, Training Loss: 1.007e+00 , Validation Loss: 1.881e+00\n",
      "Iteration: 25907, Training Loss: 7.555e-01 , Validation Loss: 9.133e-01\n",
      "Iteration: 25908, Training Loss: 1.259e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 25909, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25910, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25911, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25912, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25913, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25914, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25915, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25916, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25917, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25918, Training Loss: 2.015e+00 , Validation Loss: 3.786e+00\n",
      "Iteration: 25919, Training Loss: 1.007e+00 , Validation Loss: 9.495e-01\n",
      "Iteration: 25920, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 25921, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25922, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 25923, Training Loss: 5.037e-01 , Validation Loss: 1.046e+00\n",
      "Iteration: 25924, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25925, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25926, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25927, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25928, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25929, Training Loss: 1.007e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 25930, Training Loss: 4.533e+00 , Validation Loss: 4.881e+00\n",
      "Iteration: 25931, Training Loss: 1.335e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 25932, Training Loss: 2.518e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 25933, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25934, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 25935, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25936, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 25937, Training Loss: 2.015e+00 , Validation Loss: 3.072e+00\n",
      "Iteration: 25938, Training Loss: 1.763e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 25939, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 25940, Training Loss: 1.007e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 25941, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25942, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25943, Training Loss: 5.037e-01 , Validation Loss: 1.034e+00\n",
      "Iteration: 25944, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25945, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25946, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25947, Training Loss: 1.763e+00 , Validation Loss: 3.968e+00\n",
      "Iteration: 25948, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25949, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25950, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25951, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25952, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25953, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25954, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 25955, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25956, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25957, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25958, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25959, Training Loss: 2.770e+00 , Validation Loss: 4.276e+00\n",
      "Iteration: 25960, Training Loss: 5.037e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 25961, Training Loss: 1.007e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 25962, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 25963, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25964, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25965, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25966, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25967, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25968, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25969, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25970, Training Loss: 3.022e+00 , Validation Loss: 4.228e+00\n",
      "Iteration: 25971, Training Loss: 3.778e+00 , Validation Loss: 2.764e+00\n",
      "Iteration: 25972, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25973, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25974, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25975, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25976, Training Loss: 1.511e+00 , Validation Loss: 2.613e+00\n",
      "Iteration: 25977, Training Loss: 1.511e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 25978, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25979, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25980, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25981, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25982, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25983, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25984, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 25985, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25986, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25987, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25988, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25989, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 25990, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 25991, Training Loss: 7.555e-01 , Validation Loss: 2.595e+00\n",
      "Iteration: 25992, Training Loss: 3.022e+00 , Validation Loss: 4.155e+00\n",
      "Iteration: 25993, Training Loss: 1.511e+00 , Validation Loss: 8.951e-01\n",
      "Iteration: 25994, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 25995, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 25996, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 25997, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25998, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 25999, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26000, Training Loss: 2.267e+00 , Validation Loss: 3.538e+00\n",
      "Iteration: 26001, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26002, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 26003, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 26004, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26005, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26006, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 26007, Training Loss: 1.259e+00 , Validation Loss: 3.078e+00\n",
      "Iteration: 26008, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26009, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26010, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26011, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26012, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 26013, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 26014, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 26015, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26016, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26017, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26018, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26019, Training Loss: -1.000e-07 , Validation Loss: 6.290e-01\n",
      "Iteration: 26020, Training Loss: 5.037e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 26021, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 26022, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 26023, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 26024, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 26025, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 26026, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 26027, Training Loss: 7.555e-01 , Validation Loss: 2.099e+00\n",
      "Iteration: 26028, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26029, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 26030, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 26031, Training Loss: 2.518e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 26032, Training Loss: 7.555e-01 , Validation Loss: 9.375e-01\n",
      "Iteration: 26033, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26034, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26035, Training Loss: 2.518e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 26036, Training Loss: -1.000e-07 , Validation Loss: 1.645e+00\n",
      "Iteration: 26037, Training Loss: -1.000e-07 , Validation Loss: 1.645e+00\n",
      "Iteration: 26038, Training Loss: 2.518e-01 , Validation Loss: 1.645e+00\n",
      "Iteration: 26039, Training Loss: 1.007e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 26040, Training Loss: 5.289e+00 , Validation Loss: 5.564e+00\n",
      "Iteration: 26041, Training Loss: 1.310e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 26042, Training Loss: 3.274e+00 , Validation Loss: 4.851e+00\n",
      "Iteration: 26043, Training Loss: 1.511e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 26044, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 26045, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26046, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26047, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 26048, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26049, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26050, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26051, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26052, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 26053, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 26054, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26055, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26056, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26057, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26058, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26059, Training Loss: 2.518e+00 , Validation Loss: 4.131e+00\n",
      "Iteration: 26060, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 26061, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26062, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26063, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26064, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26065, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 26066, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26067, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26068, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26069, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26070, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26071, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26072, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 26073, Training Loss: 1.259e+00 , Validation Loss: 2.897e+00\n",
      "Iteration: 26074, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26075, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26076, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26077, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26078, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26079, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26080, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26081, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26082, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26083, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26084, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26085, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26086, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 26087, Training Loss: 1.763e+00 , Validation Loss: 3.254e+00\n",
      "Iteration: 26088, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 26089, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26090, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 26091, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26092, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26093, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 26094, Training Loss: 2.518e-01 , Validation Loss: 1.361e+00\n",
      "Iteration: 26095, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 26096, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 26097, Training Loss: 2.267e+00 , Validation Loss: 4.300e+00\n",
      "Iteration: 26098, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 26099, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 26100, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26101, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26102, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26103, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26104, Training Loss: 1.511e+00 , Validation Loss: 3.006e+00\n",
      "Iteration: 26105, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26106, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 26107, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26108, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26109, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 26110, Training Loss: 7.555e-01 , Validation Loss: 8.951e-01\n",
      "Iteration: 26111, Training Loss: 2.015e+00 , Validation Loss: 4.149e+00\n",
      "Iteration: 26112, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 26113, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 26114, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 26115, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26116, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26117, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26118, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26119, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26120, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26121, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26122, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26123, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26124, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26125, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26126, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26127, Training Loss: 1.007e+00 , Validation Loss: 9.798e-01\n",
      "Iteration: 26128, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26129, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26130, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 26131, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26132, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26133, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26134, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26135, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26136, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 26137, Training Loss: -1.000e-07 , Validation Loss: 9.375e-01\n",
      "Iteration: 26138, Training Loss: 5.037e-01 , Validation Loss: 9.375e-01\n",
      "Iteration: 26139, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26140, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26141, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 26142, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26143, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26144, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26145, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26146, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26147, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26148, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26149, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26150, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26151, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26152, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26153, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26154, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26155, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26156, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26157, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26158, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26159, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26160, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 26161, Training Loss: 1.511e+00 , Validation Loss: 3.623e+00\n",
      "Iteration: 26162, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26163, Training Loss: 1.259e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 26164, Training Loss: 1.259e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 26165, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 26166, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26167, Training Loss: 7.555e-01 , Validation Loss: 2.341e+00\n",
      "Iteration: 26168, Training Loss: 2.518e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 26169, Training Loss: 2.770e+00 , Validation Loss: 4.076e+00\n",
      "Iteration: 26170, Training Loss: 1.007e+00 , Validation Loss: 1.058e+00\n",
      "Iteration: 26171, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 26172, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26173, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26174, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 26175, Training Loss: 7.555e-01 , Validation Loss: 2.873e+00\n",
      "Iteration: 26176, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26177, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 26178, Training Loss: 7.555e-01 , Validation Loss: 3.490e+00\n",
      "Iteration: 26179, Training Loss: -1.000e-07 , Validation Loss: 2.062e+00\n",
      "Iteration: 26180, Training Loss: 1.763e+00 , Validation Loss: 2.062e+00\n",
      "Iteration: 26181, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26182, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26183, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26184, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 26185, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 26186, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 26187, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26188, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26189, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 26190, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 26191, Training Loss: -1.000e-07 , Validation Loss: 5.867e-01\n",
      "Iteration: 26192, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 26193, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26194, Training Loss: 7.555e-01 , Validation Loss: 1.361e+00\n",
      "Iteration: 26195, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26196, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26197, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26198, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26199, Training Loss: -1.000e-07 , Validation Loss: 9.254e-01\n",
      "Iteration: 26200, Training Loss: 7.555e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 26201, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 26202, Training Loss: 1.259e+00 , Validation Loss: 3.883e+00\n",
      "Iteration: 26203, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 26204, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 26205, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 26206, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26207, Training Loss: 1.259e+00 , Validation Loss: 1.941e+00\n",
      "Iteration: 26208, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26209, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26210, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26211, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 26212, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26213, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26214, Training Loss: 5.037e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 26215, Training Loss: 2.015e+00 , Validation Loss: 4.657e+00\n",
      "Iteration: 26216, Training Loss: 1.259e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 26217, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26218, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 26219, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26220, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26221, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26222, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26223, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 26224, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26225, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 26226, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26227, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 26228, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26229, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26230, Training Loss: 5.037e-01 , Validation Loss: 9.858e-01\n",
      "Iteration: 26231, Training Loss: 2.015e+00 , Validation Loss: 1.609e+00\n",
      "Iteration: 26232, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 26233, Training Loss: 5.037e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 26234, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26235, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 26236, Training Loss: 1.007e+00 , Validation Loss: 2.970e+00\n",
      "Iteration: 26237, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26238, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 26239, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 26240, Training Loss: 2.518e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 26241, Training Loss: 1.007e+00 , Validation Loss: 1.185e+00\n",
      "Iteration: 26242, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26243, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26244, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 26245, Training Loss: 1.259e+00 , Validation Loss: 2.867e+00\n",
      "Iteration: 26246, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26247, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 26248, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26249, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26250, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26251, Training Loss: 3.022e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 26252, Training Loss: 1.259e+00 , Validation Loss: 1.264e+00\n",
      "Iteration: 26253, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 26254, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26255, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26256, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26257, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 26258, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 26259, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26260, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26261, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26262, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26263, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 26264, Training Loss: 3.526e+00 , Validation Loss: 5.183e+00\n",
      "Iteration: 26265, Training Loss: 3.526e+00 , Validation Loss: 2.879e+00\n",
      "Iteration: 26266, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 26267, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26268, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26269, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26270, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26271, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26272, Training Loss: 2.518e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 26273, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26274, Training Loss: -1.000e-07 , Validation Loss: 1.766e+00\n",
      "Iteration: 26275, Training Loss: 7.555e-01 , Validation Loss: 1.766e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 26276, Training Loss: 2.267e+00 , Validation Loss: 3.683e+00\n",
      "Iteration: 26277, Training Loss: 1.259e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 26278, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26279, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26280, Training Loss: 2.518e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 26281, Training Loss: 5.037e-01 , Validation Loss: 1.899e+00\n",
      "Iteration: 26282, Training Loss: 1.007e+00 , Validation Loss: 3.018e+00\n",
      "Iteration: 26283, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26284, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26285, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26286, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26287, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26288, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26289, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26290, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26291, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26292, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26293, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26294, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 26295, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26296, Training Loss: -1.000e-07 , Validation Loss: 6.895e-01\n",
      "Iteration: 26297, Training Loss: -1.000e-07 , Validation Loss: 6.895e-01\n",
      "Iteration: 26298, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 26299, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26300, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26301, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26302, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26303, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 26304, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26305, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 26306, Training Loss: 1.259e+00 , Validation Loss: 3.411e+00\n",
      "Iteration: 26307, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26308, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 26309, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26310, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26311, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 26312, Training Loss: 4.030e+00 , Validation Loss: 4.421e+00\n",
      "Iteration: 26313, Training Loss: 1.033e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 26314, Training Loss: 2.518e+00 , Validation Loss: 5.014e+00\n",
      "Iteration: 26315, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26316, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26317, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 26318, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26319, Training Loss: 1.511e+00 , Validation Loss: 3.834e+00\n",
      "Iteration: 26320, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26321, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26322, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26323, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 26324, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 26325, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26326, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 26327, Training Loss: 1.259e+00 , Validation Loss: 6.532e-01\n",
      "Iteration: 26328, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26329, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26330, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26331, Training Loss: 1.007e+00 , Validation Loss: 2.752e+00\n",
      "Iteration: 26332, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 26333, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26334, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26335, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26336, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26337, Training Loss: 1.007e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 26338, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 26339, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 26340, Training Loss: 5.037e-01 , Validation Loss: 2.522e+00\n",
      "Iteration: 26341, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26342, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 26343, Training Loss: -1.000e-07 , Validation Loss: 6.713e-01\n",
      "Iteration: 26344, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 26345, Training Loss: 2.267e+00 , Validation Loss: 3.883e+00\n",
      "Iteration: 26346, Training Loss: 1.511e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 26347, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26348, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26349, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26350, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26351, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26352, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26353, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26354, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26355, Training Loss: 2.015e+00 , Validation Loss: 3.955e+00\n",
      "Iteration: 26356, Training Loss: 1.007e+00 , Validation Loss: 1.046e+00\n",
      "Iteration: 26357, Training Loss: 1.259e+00 , Validation Loss: 6.169e-01\n",
      "Iteration: 26358, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26359, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26360, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26361, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26362, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26363, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26364, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26365, Training Loss: 1.007e+00 , Validation Loss: 9.798e-01\n",
      "Iteration: 26366, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26367, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26368, Training Loss: 2.518e-01 , Validation Loss: 1.016e+00\n",
      "Iteration: 26369, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 26370, Training Loss: 1.259e+00 , Validation Loss: 1.566e+00\n",
      "Iteration: 26371, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26372, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26373, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 26374, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 26375, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 26376, Training Loss: 1.259e+00 , Validation Loss: 3.103e+00\n",
      "Iteration: 26377, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 26378, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 26379, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 26380, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26381, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 26382, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26383, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 26384, Training Loss: 5.037e-01 , Validation Loss: 8.709e-01\n",
      "Iteration: 26385, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26386, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 26387, Training Loss: 1.511e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 26388, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26389, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26390, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26391, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 26392, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26393, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26394, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26395, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 26396, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 26397, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 26398, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26399, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26400, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26401, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26402, Training Loss: -1.000e-07 , Validation Loss: 2.583e+00\n",
      "Iteration: 26403, Training Loss: 2.015e+00 , Validation Loss: 2.583e+00\n",
      "Iteration: 26404, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26405, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26406, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26407, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26408, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26409, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 26410, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 26411, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 26412, Training Loss: 7.555e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 26413, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26414, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26415, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26416, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 26417, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26418, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26419, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26420, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26421, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 26422, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 26423, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 26424, Training Loss: 1.511e+00 , Validation Loss: 3.532e+00\n",
      "Iteration: 26425, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26426, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 26427, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26428, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26429, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26430, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26431, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 26432, Training Loss: 5.037e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 26433, Training Loss: 4.030e+00 , Validation Loss: 4.409e+00\n",
      "Iteration: 26434, Training Loss: 1.209e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 26435, Training Loss: 3.526e+00 , Validation Loss: 4.929e+00\n",
      "Iteration: 26436, Training Loss: 2.267e+00 , Validation Loss: 2.014e+00\n",
      "Iteration: 26437, Training Loss: 1.259e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 26438, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 26439, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26440, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 26441, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26442, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26443, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26444, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26445, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26446, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26447, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 26448, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26449, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26450, Training Loss: 2.015e+00 , Validation Loss: 3.060e+00\n",
      "Iteration: 26451, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26452, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26453, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26454, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26455, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26456, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26457, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26458, Training Loss: 7.555e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 26459, Training Loss: 2.267e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 26460, Training Loss: 1.259e+00 , Validation Loss: 2.068e+00\n",
      "Iteration: 26461, Training Loss: 1.511e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 26462, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 26463, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26464, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26465, Training Loss: -1.000e-07 , Validation Loss: 4.959e-01\n",
      "Iteration: 26466, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 26467, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26468, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26469, Training Loss: 7.555e-01 , Validation Loss: 3.490e+00\n",
      "Iteration: 26470, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26471, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26472, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26473, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26474, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26475, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26476, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26477, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26478, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26479, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26480, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26481, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26482, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26483, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26484, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26485, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 26486, Training Loss: 2.015e+00 , Validation Loss: 3.738e+00\n",
      "Iteration: 26487, Training Loss: 1.007e+00 , Validation Loss: 8.104e-01\n",
      "Iteration: 26488, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 26489, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26490, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26491, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 26492, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26493, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26494, Training Loss: 1.007e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 26495, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26496, Training Loss: 2.518e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 26497, Training Loss: 2.267e+00 , Validation Loss: 3.459e+00\n",
      "Iteration: 26498, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 26499, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 26500, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26501, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 26502, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26503, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26504, Training Loss: -1.000e-07 , Validation Loss: 2.988e+00\n",
      "Iteration: 26505, Training Loss: 1.007e+00 , Validation Loss: 2.988e+00\n",
      "Iteration: 26506, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 26507, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 26508, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 26509, Training Loss: 1.007e+00 , Validation Loss: 1.560e+00\n",
      "Iteration: 26510, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26511, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 26512, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26513, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26514, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26515, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26516, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26517, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26518, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26519, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 26520, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 26521, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 26522, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 26523, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26524, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 26525, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26526, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 26527, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 26528, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 26529, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 26530, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 26531, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26532, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26533, Training Loss: 7.555e-01 , Validation Loss: 2.008e+00\n",
      "Iteration: 26534, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26535, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26536, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 26537, Training Loss: 1.259e+00 , Validation Loss: 2.558e+00\n",
      "Iteration: 26538, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 26539, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 26540, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 26541, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 26542, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26543, Training Loss: 1.007e+00 , Validation Loss: 4.778e-01\n",
      "Iteration: 26544, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26545, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 26546, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 26547, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 26548, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26549, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26550, Training Loss: -1.000e-07 , Validation Loss: 6.471e-01\n",
      "Iteration: 26551, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 26552, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 26553, Training Loss: 1.763e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 26554, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26555, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 26556, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26557, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26558, Training Loss: 7.555e-01 , Validation Loss: 1.318e+00\n",
      "Iteration: 26559, Training Loss: 2.518e+00 , Validation Loss: 4.560e+00\n",
      "Iteration: 26560, Training Loss: 2.015e+00 , Validation Loss: 1.228e+00\n",
      "Iteration: 26561, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26562, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26563, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26564, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26565, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 26566, Training Loss: 1.007e+00 , Validation Loss: 8.225e-01\n",
      "Iteration: 26567, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26568, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26569, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26570, Training Loss: 5.037e-01 , Validation Loss: 1.579e+00\n",
      "Iteration: 26571, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26572, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26573, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 26574, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 26575, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26576, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26577, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26578, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 26579, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26580, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26581, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26582, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26583, Training Loss: 2.518e-01 , Validation Loss: 1.506e+00\n",
      "Iteration: 26584, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 26585, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 26586, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26587, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26588, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26589, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 26590, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26591, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26592, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26593, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 26594, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 26595, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 26596, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 26597, Training Loss: 2.518e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 26598, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 26599, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 26600, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26601, Training Loss: 5.037e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 26602, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26603, Training Loss: 1.007e+00 , Validation Loss: 1.784e+00\n",
      "Iteration: 26604, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 26605, Training Loss: 1.511e+00 , Validation Loss: 1.506e+00\n",
      "Iteration: 26606, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26607, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26608, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26609, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26610, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26611, Training Loss: 7.555e-01 , Validation Loss: 2.740e+00\n",
      "Iteration: 26612, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26613, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26614, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26615, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 26616, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 26617, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26618, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 26619, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26620, Training Loss: 5.037e-01 , Validation Loss: 1.929e+00\n",
      "Iteration: 26621, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26622, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 26623, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26624, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26625, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 26626, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 26627, Training Loss: 1.007e+00 , Validation Loss: 1.615e+00\n",
      "Iteration: 26628, Training Loss: 2.518e-01 , Validation Loss: 2.335e+00\n",
      "Iteration: 26629, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 26630, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 26631, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26632, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26633, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26634, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26635, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26636, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26637, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26638, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26639, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26640, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26641, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26642, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 26643, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 26644, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26645, Training Loss: 2.518e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 26646, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 26647, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 26648, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26649, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26650, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26651, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26652, Training Loss: 1.007e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 26653, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26654, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26655, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26656, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26657, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 26658, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26659, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 26660, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26661, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26662, Training Loss: 3.022e+00 , Validation Loss: 3.883e+00\n",
      "Iteration: 26663, Training Loss: 1.007e+00 , Validation Loss: 1.349e+00\n",
      "Iteration: 26664, Training Loss: 5.037e-01 , Validation Loss: 8.286e-01\n",
      "Iteration: 26665, Training Loss: 1.007e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 26666, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 26667, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 26668, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26669, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26670, Training Loss: 2.015e+00 , Validation Loss: 2.710e+00\n",
      "Iteration: 26671, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 26672, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26673, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26674, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26675, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26676, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26677, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26678, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26679, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26680, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26681, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26682, Training Loss: 1.007e+00 , Validation Loss: 5.080e-01\n",
      "Iteration: 26683, Training Loss: -1.000e-07 , Validation Loss: 5.383e-01\n",
      "Iteration: 26684, Training Loss: 7.555e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 26685, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26686, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26687, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26688, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 26689, Training Loss: -1.000e-07 , Validation Loss: 4.959e-01\n",
      "Iteration: 26690, Training Loss: -1.000e-07 , Validation Loss: 4.959e-01\n",
      "Iteration: 26691, Training Loss: 7.555e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 26692, Training Loss: 1.007e+00 , Validation Loss: 2.649e+00\n",
      "Iteration: 26693, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26694, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26695, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 26696, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 26697, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 26698, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26699, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 26700, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 26701, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 26702, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 26703, Training Loss: 2.518e-01 , Validation Loss: 1.966e+00\n",
      "Iteration: 26704, Training Loss: 5.037e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 26705, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26706, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26707, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26708, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26709, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26710, Training Loss: -1.000e-07 , Validation Loss: 7.621e-01\n",
      "Iteration: 26711, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 26712, Training Loss: 7.555e-01 , Validation Loss: 1.482e+00\n",
      "Iteration: 26713, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26714, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26715, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 26716, Training Loss: 7.555e-01 , Validation Loss: 1.766e+00\n",
      "Iteration: 26717, Training Loss: 1.511e+00 , Validation Loss: 4.433e+00\n",
      "Iteration: 26718, Training Loss: 1.511e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 26719, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 26720, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26721, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26722, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26723, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26724, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26725, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26726, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26727, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26728, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 26729, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 26730, Training Loss: 2.518e-01 , Validation Loss: 1.621e+00\n",
      "Iteration: 26731, Training Loss: 2.770e+00 , Validation Loss: 4.228e+00\n",
      "Iteration: 26732, Training Loss: 5.037e-01 , Validation Loss: 1.077e+00\n",
      "Iteration: 26733, Training Loss: 2.518e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 26734, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 26735, Training Loss: 1.259e+00 , Validation Loss: 4.415e-01\n",
      "Iteration: 26736, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26737, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 26738, Training Loss: 1.259e+00 , Validation Loss: 3.629e-01\n",
      "Iteration: 26739, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26740, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26741, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26742, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26743, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26744, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26745, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 26746, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 26747, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26748, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26749, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26750, Training Loss: 7.555e-01 , Validation Loss: 1.282e+00\n",
      "Iteration: 26751, Training Loss: 1.259e+00 , Validation Loss: 4.288e+00\n",
      "Iteration: 26752, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26753, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 26754, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 26755, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26756, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 26757, Training Loss: 2.015e+00 , Validation Loss: 4.391e+00\n",
      "Iteration: 26758, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 26759, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26760, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26761, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 26762, Training Loss: 1.763e+00 , Validation Loss: 3.526e+00\n",
      "Iteration: 26763, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26764, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26765, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 26766, Training Loss: 1.511e+00 , Validation Loss: 3.405e+00\n",
      "Iteration: 26767, Training Loss: 1.511e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 26768, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26769, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26770, Training Loss: 2.015e+00 , Validation Loss: 3.617e+00\n",
      "Iteration: 26771, Training Loss: 1.259e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 26772, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26773, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26774, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26775, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26776, Training Loss: 7.555e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 26777, Training Loss: 3.274e+00 , Validation Loss: 3.877e+00\n",
      "Iteration: 26778, Training Loss: 2.518e+00 , Validation Loss: 2.322e+00\n",
      "Iteration: 26779, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26780, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26781, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26782, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26783, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 26784, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26785, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26786, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 26787, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 26788, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26789, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26790, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26791, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26792, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 26793, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26794, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26795, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 26796, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26797, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26798, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26799, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26800, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26801, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 26802, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26803, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26804, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26805, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26806, Training Loss: 1.007e+00 , Validation Loss: 4.294e-01\n",
      "Iteration: 26807, Training Loss: 1.763e+00 , Validation Loss: 3.998e+00\n",
      "Iteration: 26808, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 26809, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26810, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 26811, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26812, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26813, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 26814, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 26815, Training Loss: 1.007e+00 , Validation Loss: 1.954e+00\n",
      "Iteration: 26816, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26817, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26818, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26819, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 26820, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26821, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 26822, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 26823, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26824, Training Loss: 1.007e+00 , Validation Loss: 1.258e+00\n",
      "Iteration: 26825, Training Loss: 7.555e-01 , Validation Loss: 2.238e+00\n",
      "Iteration: 26826, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26827, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 26828, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 26829, Training Loss: 3.274e+00 , Validation Loss: 4.675e+00\n",
      "Iteration: 26830, Training Loss: 2.770e+00 , Validation Loss: 2.716e+00\n",
      "Iteration: 26831, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 26832, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26833, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26834, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26835, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26836, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26837, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26838, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26839, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26840, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26841, Training Loss: 1.007e+00 , Validation Loss: 8.891e-01\n",
      "Iteration: 26842, Training Loss: 1.511e+00 , Validation Loss: 3.544e+00\n",
      "Iteration: 26843, Training Loss: 7.555e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 26844, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26845, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26846, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26847, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 26848, Training Loss: 2.518e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 26849, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 26850, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 26851, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 26852, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 26853, Training Loss: 2.518e-01 , Validation Loss: 1.052e+00\n",
      "Iteration: 26854, Training Loss: 2.267e+00 , Validation Loss: 3.798e+00\n",
      "Iteration: 26855, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 26856, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 26857, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26858, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26859, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 26860, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26861, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26862, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26863, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26864, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26865, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26866, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26867, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26868, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26869, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26870, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26871, Training Loss: -1.000e-07 , Validation Loss: 6.895e-01\n",
      "Iteration: 26872, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 26873, Training Loss: 3.526e+00 , Validation Loss: 3.901e+00\n",
      "Iteration: 26874, Training Loss: 2.267e+00 , Validation Loss: 3.883e+00\n",
      "Iteration: 26875, Training Loss: 5.037e-01 , Validation Loss: 9.919e-01\n",
      "Iteration: 26876, Training Loss: 1.007e+00 , Validation Loss: 8.286e-01\n",
      "Iteration: 26877, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 26878, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26879, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26880, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26881, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26882, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26883, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26884, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26885, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 26886, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 26887, Training Loss: 7.555e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 26888, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 26889, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 26890, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 26891, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 26892, Training Loss: 7.555e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 26893, Training Loss: 1.259e+00 , Validation Loss: 1.718e+00\n",
      "Iteration: 26894, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 26895, Training Loss: 1.259e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 26896, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 26897, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26898, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26899, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26900, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26901, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26902, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26903, Training Loss: 7.555e-01 , Validation Loss: 1.742e+00\n",
      "Iteration: 26904, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 26905, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 26906, Training Loss: 2.518e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 26907, Training Loss: 2.518e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 26908, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26909, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26910, Training Loss: 5.037e-01 , Validation Loss: 1.935e+00\n",
      "Iteration: 26911, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 26912, Training Loss: 7.555e-01 , Validation Loss: 9.737e-01\n",
      "Iteration: 26913, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26914, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26915, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26916, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26917, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 26918, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26919, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26920, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26921, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26922, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26923, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 26924, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 26925, Training Loss: 1.763e+00 , Validation Loss: 3.804e+00\n",
      "Iteration: 26926, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 26927, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26928, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26929, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 26930, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 26931, Training Loss: 1.259e+00 , Validation Loss: 3.012e+00\n",
      "Iteration: 26932, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26933, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26934, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26935, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26936, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 26937, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26938, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26939, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26940, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 26941, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26942, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 26943, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 26944, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26945, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26946, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26947, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26948, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26949, Training Loss: -1.000e-07 , Validation Loss: 6.048e-01\n",
      "Iteration: 26950, Training Loss: 7.555e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 26951, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 26952, Training Loss: 2.518e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 26953, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 26954, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26955, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26956, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26957, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 26958, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 26959, Training Loss: -1.000e-07 , Validation Loss: 2.165e+00\n",
      "Iteration: 26960, Training Loss: 5.037e-01 , Validation Loss: 2.165e+00\n",
      "Iteration: 26961, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26962, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26963, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26964, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26965, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26966, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26967, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26968, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26969, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 26970, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26971, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26972, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 26973, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26974, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26975, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26976, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26977, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 26978, Training Loss: -1.000e-07 , Validation Loss: 6.592e-01\n",
      "Iteration: 26979, Training Loss: 5.037e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 26980, Training Loss: 5.037e-01 , Validation Loss: 8.830e-01\n",
      "Iteration: 26981, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26982, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26983, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26984, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 26985, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 26986, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26987, Training Loss: 2.518e-01 , Validation Loss: 1.240e+00\n",
      "Iteration: 26988, Training Loss: -1.000e-07 , Validation Loss: 4.476e-01\n",
      "Iteration: 26989, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 26990, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26991, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 26992, Training Loss: 7.555e-01 , Validation Loss: 1.657e+00\n",
      "Iteration: 26993, Training Loss: 3.526e+00 , Validation Loss: 4.228e+00\n",
      "Iteration: 26994, Training Loss: 3.526e+00 , Validation Loss: 2.806e+00\n",
      "Iteration: 26995, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 26996, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 26997, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 26998, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 26999, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27000, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27001, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27002, Training Loss: 4.533e+00 , Validation Loss: 4.427e+00\n",
      "Iteration: 27003, Training Loss: 8.059e+00 , Validation Loss: 7.748e+00\n",
      "Iteration: 27004, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27005, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27006, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27007, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27008, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27009, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27010, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27011, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27012, Training Loss: 7.555e-01 , Validation Loss: 1.470e+00\n",
      "Iteration: 27013, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27014, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27015, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27016, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27017, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27018, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27019, Training Loss: 2.518e-01 , Validation Loss: 2.891e+00\n",
      "Iteration: 27020, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27021, Training Loss: 1.763e+00 , Validation Loss: 2.691e+00\n",
      "Iteration: 27022, Training Loss: 5.037e-01 , Validation Loss: 8.528e-01\n",
      "Iteration: 27023, Training Loss: 5.037e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 27024, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 27025, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27026, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27027, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27028, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27029, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27030, Training Loss: 5.037e-01 , Validation Loss: 1.947e+00\n",
      "Iteration: 27031, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27032, Training Loss: 2.518e-01 , Validation Loss: 2.770e+00\n",
      "Iteration: 27033, Training Loss: -1.000e-07 , Validation Loss: 6.411e-01\n",
      "Iteration: 27034, Training Loss: 5.037e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 27035, Training Loss: 2.770e+00 , Validation Loss: 4.391e+00\n",
      "Iteration: 27036, Training Loss: 7.555e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 27037, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 27038, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27039, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27040, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 27041, Training Loss: 3.274e+00 , Validation Loss: 5.201e+00\n",
      "Iteration: 27042, Training Loss: 1.007e+00 , Validation Loss: 1.433e+00\n",
      "Iteration: 27043, Training Loss: 2.015e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 27044, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27045, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27046, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27047, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27048, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27049, Training Loss: 2.518e-01 , Validation Loss: 1.500e+00\n",
      "Iteration: 27050, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 27051, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27052, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27053, Training Loss: 5.037e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 27054, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 27055, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 27056, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 27057, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 27058, Training Loss: 5.037e-01 , Validation Loss: 1.693e+00\n",
      "Iteration: 27059, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27060, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27061, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27062, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 27063, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27064, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 27065, Training Loss: 5.037e-01 , Validation Loss: 2.008e+00\n",
      "Iteration: 27066, Training Loss: 1.259e+00 , Validation Loss: 2.510e+00\n",
      "Iteration: 27067, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 27068, Training Loss: -1.000e-07 , Validation Loss: 7.802e-01\n",
      "Iteration: 27069, Training Loss: -1.000e-07 , Validation Loss: 7.802e-01\n",
      "Iteration: 27070, Training Loss: 1.007e+00 , Validation Loss: 7.802e-01\n",
      "Iteration: 27071, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27072, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27073, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 27074, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 27075, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 27076, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27077, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27078, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27079, Training Loss: 7.555e-01 , Validation Loss: 9.979e-01\n",
      "Iteration: 27080, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27081, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27082, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27083, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27084, Training Loss: 2.267e+00 , Validation Loss: 4.040e+00\n",
      "Iteration: 27085, Training Loss: 1.007e+00 , Validation Loss: 1.222e+00\n",
      "Iteration: 27086, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27087, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 27088, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27089, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27090, Training Loss: 2.518e-01 , Validation Loss: 9.919e-01\n",
      "Iteration: 27091, Training Loss: 2.518e+00 , Validation Loss: 3.786e+00\n",
      "Iteration: 27092, Training Loss: 2.518e+00 , Validation Loss: 1.790e+00\n",
      "Iteration: 27093, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27094, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 27095, Training Loss: 5.037e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 27096, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27097, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 27098, Training Loss: 7.555e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 27099, Training Loss: 2.518e+00 , Validation Loss: 3.919e+00\n",
      "Iteration: 27100, Training Loss: 1.259e+00 , Validation Loss: 1.712e+00\n",
      "Iteration: 27101, Training Loss: 1.007e+00 , Validation Loss: 6.230e-01\n",
      "Iteration: 27102, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27103, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27104, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27105, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27106, Training Loss: 5.037e-01 , Validation Loss: 1.318e+00\n",
      "Iteration: 27107, Training Loss: 7.555e-01 , Validation Loss: 3.205e+00\n",
      "Iteration: 27108, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27109, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27110, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27111, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 27112, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 27113, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27114, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 27115, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27116, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27117, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27118, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27119, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 27120, Training Loss: 1.007e+00 , Validation Loss: 1.742e+00\n",
      "Iteration: 27121, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27122, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27123, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27124, Training Loss: 2.518e-01 , Validation Loss: 1.966e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 27125, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 27126, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27127, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 27128, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27129, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 27130, Training Loss: 5.037e-01 , Validation Loss: 1.058e+00\n",
      "Iteration: 27131, Training Loss: 7.555e-01 , Validation Loss: 2.595e+00\n",
      "Iteration: 27132, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27133, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27134, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 27135, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 27136, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 27137, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27138, Training Loss: 2.518e+00 , Validation Loss: 4.076e+00\n",
      "Iteration: 27139, Training Loss: 1.007e+00 , Validation Loss: 2.437e+00\n",
      "Iteration: 27140, Training Loss: 1.259e+00 , Validation Loss: 1.300e+00\n",
      "Iteration: 27141, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 27142, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 27143, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27144, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 27145, Training Loss: 2.518e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 27146, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 27147, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27148, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27149, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27150, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27151, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 27152, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 27153, Training Loss: 1.007e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 27154, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27155, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27156, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27157, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 27158, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27159, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 27160, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 27161, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27162, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27163, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27164, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 27165, Training Loss: -1.000e-07 , Validation Loss: 5.141e-01\n",
      "Iteration: 27166, Training Loss: -1.000e-07 , Validation Loss: 5.141e-01\n",
      "Iteration: 27167, Training Loss: -1.000e-07 , Validation Loss: 5.141e-01\n",
      "Iteration: 27168, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 27169, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27170, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27171, Training Loss: -1.000e-07 , Validation Loss: 6.774e-01\n",
      "Iteration: 27172, Training Loss: 5.037e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 27173, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27174, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27175, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 27176, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27177, Training Loss: 7.555e-01 , Validation Loss: 1.917e+00\n",
      "Iteration: 27178, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27179, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27180, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 27181, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 27182, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27183, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27184, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27185, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27186, Training Loss: -1.000e-07 , Validation Loss: 1.107e+00\n",
      "Iteration: 27187, Training Loss: 7.555e-01 , Validation Loss: 1.107e+00\n",
      "Iteration: 27188, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 27189, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27190, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27191, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27192, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27193, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27194, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27195, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27196, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27197, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27198, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27199, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27200, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27201, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 27202, Training Loss: 7.555e-01 , Validation Loss: 1.312e+00\n",
      "Iteration: 27203, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27204, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27205, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27206, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27207, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27208, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27209, Training Loss: 2.518e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 27210, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27211, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27212, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27213, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27214, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27215, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27216, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27217, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27218, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27219, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27220, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27221, Training Loss: -1.000e-07 , Validation Loss: 5.806e-01\n",
      "Iteration: 27222, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 27223, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27224, Training Loss: 2.518e-01 , Validation Loss: 9.072e-01\n",
      "Iteration: 27225, Training Loss: 1.007e+00 , Validation Loss: 5.141e-01\n",
      "Iteration: 27226, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 27227, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27228, Training Loss: 2.267e+00 , Validation Loss: 1.252e+00\n",
      "Iteration: 27229, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27230, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27231, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27232, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27233, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27234, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27235, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27236, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27237, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27238, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27239, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27240, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27241, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27242, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27243, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27244, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 27245, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27246, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27247, Training Loss: 5.037e-01 , Validation Loss: 1.893e+00\n",
      "Iteration: 27248, Training Loss: 2.267e+00 , Validation Loss: 3.381e+00\n",
      "Iteration: 27249, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 27250, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 27251, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27252, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 27253, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27254, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27255, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 27256, Training Loss: 1.007e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 27257, Training Loss: 4.533e+00 , Validation Loss: 5.117e+00\n",
      "Iteration: 27258, Training Loss: 3.022e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 27259, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27260, Training Loss: 1.511e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 27261, Training Loss: 1.511e+00 , Validation Loss: 1.766e+00\n",
      "Iteration: 27262, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27263, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27264, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27265, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27266, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27267, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27268, Training Loss: 5.037e-01 , Validation Loss: 2.232e+00\n",
      "Iteration: 27269, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27270, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 27271, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27272, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27273, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27274, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27275, Training Loss: 1.007e+00 , Validation Loss: 2.570e+00\n",
      "Iteration: 27276, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27277, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27278, Training Loss: 1.259e+00 , Validation Loss: 6.653e-01\n",
      "Iteration: 27279, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27280, Training Loss: 5.037e-01 , Validation Loss: 1.724e+00\n",
      "Iteration: 27281, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27282, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 27283, Training Loss: 1.259e+00 , Validation Loss: 1.331e+00\n",
      "Iteration: 27284, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27285, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27286, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27287, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27288, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27289, Training Loss: 5.037e-01 , Validation Loss: 1.349e+00\n",
      "Iteration: 27290, Training Loss: 1.259e+00 , Validation Loss: 1.373e+00\n",
      "Iteration: 27291, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27292, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27293, Training Loss: 7.555e-01 , Validation Loss: 2.183e+00\n",
      "Iteration: 27294, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27295, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27296, Training Loss: 2.267e+00 , Validation Loss: 3.695e+00\n",
      "Iteration: 27297, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 27298, Training Loss: 5.037e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 27299, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 27300, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 27301, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 27302, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27303, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27304, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27305, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27306, Training Loss: 7.555e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 27307, Training Loss: 7.555e-01 , Validation Loss: 2.746e+00\n",
      "Iteration: 27308, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 27309, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 27310, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27311, Training Loss: 1.259e+00 , Validation Loss: 3.484e+00\n",
      "Iteration: 27312, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 27313, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27314, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27315, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 27316, Training Loss: 1.007e+00 , Validation Loss: 1.935e+00\n",
      "Iteration: 27317, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27318, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27319, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27320, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27321, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27322, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27323, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 27324, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27325, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27326, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27327, Training Loss: 3.526e+00 , Validation Loss: 4.240e+00\n",
      "Iteration: 27328, Training Loss: 1.007e+00 , Validation Loss: 2.244e+00\n",
      "Iteration: 27329, Training Loss: 2.518e+00 , Validation Loss: 9.979e-01\n",
      "Iteration: 27330, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27331, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27332, Training Loss: 7.555e-01 , Validation Loss: 1.355e+00\n",
      "Iteration: 27333, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27334, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27335, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27336, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27337, Training Loss: 1.259e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 27338, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 27339, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27340, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27341, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27342, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27343, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27344, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27345, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27346, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27347, Training Loss: 1.007e+00 , Validation Loss: 4.899e-01\n",
      "Iteration: 27348, Training Loss: 2.015e+00 , Validation Loss: 2.341e+00\n",
      "Iteration: 27349, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27350, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27351, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27352, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27353, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27354, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27355, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27356, Training Loss: 1.007e+00 , Validation Loss: 2.474e+00\n",
      "Iteration: 27357, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27358, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27359, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 27360, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27361, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27362, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27363, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27364, Training Loss: 2.770e+00 , Validation Loss: 4.143e+00\n",
      "Iteration: 27365, Training Loss: 3.526e+00 , Validation Loss: 2.310e+00\n",
      "Iteration: 27366, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 27367, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27368, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27369, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27370, Training Loss: 2.015e+00 , Validation Loss: 3.792e+00\n",
      "Iteration: 27371, Training Loss: 1.007e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 27372, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 27373, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27374, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27375, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27376, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27377, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27378, Training Loss: 5.037e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 27379, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27380, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27381, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 27382, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 27383, Training Loss: 1.763e+00 , Validation Loss: 3.689e+00\n",
      "Iteration: 27384, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 27385, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27386, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27387, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 27388, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 27389, Training Loss: 2.518e+00 , Validation Loss: 3.859e+00\n",
      "Iteration: 27390, Training Loss: 1.259e+00 , Validation Loss: 8.830e-01\n",
      "Iteration: 27391, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27392, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27393, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27394, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27395, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 27396, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27397, Training Loss: -1.000e-07 , Validation Loss: 6.955e-01\n",
      "Iteration: 27398, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 27399, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 27400, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 27401, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 27402, Training Loss: 5.037e-01 , Validation Loss: 1.645e+00\n",
      "Iteration: 27403, Training Loss: 1.007e+00 , Validation Loss: 2.552e+00\n",
      "Iteration: 27404, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27405, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 27406, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27407, Training Loss: 2.015e+00 , Validation Loss: 4.070e+00\n",
      "Iteration: 27408, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27409, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 27410, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 27411, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27412, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27413, Training Loss: 7.555e-01 , Validation Loss: 1.802e+00\n",
      "Iteration: 27414, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27415, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27416, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27417, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27418, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 27419, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27420, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27421, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27422, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27423, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27424, Training Loss: 1.763e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 27425, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 27426, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 27427, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27428, Training Loss: -1.000e-07 , Validation Loss: 1.554e+00\n",
      "Iteration: 27429, Training Loss: 2.518e-01 , Validation Loss: 1.554e+00\n",
      "Iteration: 27430, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 27431, Training Loss: 2.518e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 27432, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 27433, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27434, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 27435, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27436, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27437, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27438, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27439, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27440, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27441, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 27442, Training Loss: 3.022e+00 , Validation Loss: 3.599e+00\n",
      "Iteration: 27443, Training Loss: 3.022e+00 , Validation Loss: 1.857e+00\n",
      "Iteration: 27444, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 27445, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27446, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27447, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27448, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27449, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27450, Training Loss: -1.000e-07 , Validation Loss: 4.415e-01\n",
      "Iteration: 27451, Training Loss: 1.007e+00 , Validation Loss: 4.415e-01\n",
      "Iteration: 27452, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27453, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27454, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 27455, Training Loss: 2.518e-01 , Validation Loss: 3.411e+00\n",
      "Iteration: 27456, Training Loss: 2.518e-01 , Validation Loss: 9.254e-01\n",
      "Iteration: 27457, Training Loss: 1.259e+00 , Validation Loss: 5.564e-01\n",
      "Iteration: 27458, Training Loss: 3.022e+00 , Validation Loss: 5.189e+00\n",
      "Iteration: 27459, Training Loss: 5.037e-01 , Validation Loss: 9.435e-01\n",
      "Iteration: 27460, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 27461, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27462, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27463, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27464, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27465, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27466, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27467, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27468, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27469, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 27470, Training Loss: 2.518e-01 , Validation Loss: 1.911e+00\n",
      "Iteration: 27471, Training Loss: 2.518e+00 , Validation Loss: 4.312e+00\n",
      "Iteration: 27472, Training Loss: 1.007e+00 , Validation Loss: 1.149e+00\n",
      "Iteration: 27473, Training Loss: 1.007e+00 , Validation Loss: 6.592e-01\n",
      "Iteration: 27474, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27475, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27476, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27477, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27478, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 27479, Training Loss: 1.763e+00 , Validation Loss: 3.992e+00\n",
      "Iteration: 27480, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 27481, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27482, Training Loss: 1.511e+00 , Validation Loss: 3.677e+00\n",
      "Iteration: 27483, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 27484, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 27485, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27486, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27487, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27488, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27489, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27490, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27491, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27492, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 27493, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27494, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27495, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 27496, Training Loss: 2.267e+00 , Validation Loss: 3.562e+00\n",
      "Iteration: 27497, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 27498, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27499, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27500, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27501, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27502, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27503, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 27504, Training Loss: 1.259e+00 , Validation Loss: 6.290e-01\n",
      "Iteration: 27505, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27506, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27507, Training Loss: 5.037e-01 , Validation Loss: 1.742e+00\n",
      "Iteration: 27508, Training Loss: 2.267e+00 , Validation Loss: 3.435e+00\n",
      "Iteration: 27509, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27510, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 27511, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27512, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27513, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27514, Training Loss: 1.007e+00 , Validation Loss: 3.750e-01\n",
      "Iteration: 27515, Training Loss: 3.526e+00 , Validation Loss: 4.512e+00\n",
      "Iteration: 27516, Training Loss: 3.526e+00 , Validation Loss: 2.401e+00\n",
      "Iteration: 27517, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 27518, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27519, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27520, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27521, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27522, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27523, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 27524, Training Loss: 2.518e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 27525, Training Loss: 7.555e-01 , Validation Loss: 2.528e+00\n",
      "Iteration: 27526, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27527, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 27528, Training Loss: -1.000e-07 , Validation Loss: 5.141e-01\n",
      "Iteration: 27529, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 27530, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 27531, Training Loss: 2.518e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 27532, Training Loss: 1.259e+00 , Validation Loss: 2.413e+00\n",
      "Iteration: 27533, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 27534, Training Loss: 7.555e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 27535, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27536, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27537, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27538, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 27539, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27540, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27541, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27542, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27543, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27544, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27545, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27546, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27547, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27548, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27549, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27550, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27551, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 27552, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27553, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27554, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27555, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27556, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 27557, Training Loss: 2.015e+00 , Validation Loss: 2.256e+00\n",
      "Iteration: 27558, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 27559, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27560, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27561, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27562, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27563, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27564, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 27565, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 27566, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27567, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27568, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 27569, Training Loss: 7.555e-01 , Validation Loss: 1.058e+00\n",
      "Iteration: 27570, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 27571, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 27572, Training Loss: 7.555e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 27573, Training Loss: 4.281e+00 , Validation Loss: 4.004e+00\n",
      "Iteration: 27574, Training Loss: 3.274e+00 , Validation Loss: 4.953e+00\n",
      "Iteration: 27575, Training Loss: 1.511e+00 , Validation Loss: 8.588e-01\n",
      "Iteration: 27576, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27577, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27578, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27579, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27580, Training Loss: -1.000e-07 , Validation Loss: 5.564e-01\n",
      "Iteration: 27581, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 27582, Training Loss: 7.555e-01 , Validation Loss: 2.044e+00\n",
      "Iteration: 27583, Training Loss: 1.259e+00 , Validation Loss: 1.222e+00\n",
      "Iteration: 27584, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27585, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 27586, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27587, Training Loss: 1.259e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 27588, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27589, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27590, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27591, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27592, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27593, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27594, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27595, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27596, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27597, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27598, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27599, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27600, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27601, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27602, Training Loss: 1.511e+00 , Validation Loss: 1.536e+00\n",
      "Iteration: 27603, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27604, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27605, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27606, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27607, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27608, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27609, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27610, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27611, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27612, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 27613, Training Loss: 7.555e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 27614, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27615, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27616, Training Loss: 7.555e-01 , Validation Loss: 5.988e-01\n",
      "Iteration: 27617, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 27618, Training Loss: 3.274e+00 , Validation Loss: 3.968e+00\n",
      "Iteration: 27619, Training Loss: 1.511e+00 , Validation Loss: 1.712e+00\n",
      "Iteration: 27620, Training Loss: 1.007e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 27621, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27622, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27623, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27624, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27625, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27626, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27627, Training Loss: 2.518e-01 , Validation Loss: 1.415e+00\n",
      "Iteration: 27628, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 27629, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 27630, Training Loss: 3.274e+00 , Validation Loss: 5.092e+00\n",
      "Iteration: 27631, Training Loss: 1.511e+00 , Validation Loss: 1.899e+00\n",
      "Iteration: 27632, Training Loss: 1.259e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 27633, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27634, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27635, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27636, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27637, Training Loss: 1.259e+00 , Validation Loss: 3.218e+00\n",
      "Iteration: 27638, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27639, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 27640, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27641, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 27642, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27643, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 27644, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27645, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27646, Training Loss: 1.007e+00 , Validation Loss: 1.802e+00\n",
      "Iteration: 27647, Training Loss: 1.007e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 27648, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27649, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27650, Training Loss: 3.274e+00 , Validation Loss: 3.568e+00\n",
      "Iteration: 27651, Training Loss: 3.526e+00 , Validation Loss: 1.899e+00\n",
      "Iteration: 27652, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27653, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27654, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27655, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27656, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27657, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27658, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27659, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 27660, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27661, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27662, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27663, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27664, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27665, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27666, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27667, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27668, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27669, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27670, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27671, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27672, Training Loss: 5.037e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 27673, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27674, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27675, Training Loss: 2.518e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 27676, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27677, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27678, Training Loss: -1.000e-07 , Validation Loss: 7.862e-01\n",
      "Iteration: 27679, Training Loss: 5.037e-01 , Validation Loss: 7.862e-01\n",
      "Iteration: 27680, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27681, Training Loss: 5.037e-01 , Validation Loss: 2.468e+00\n",
      "Iteration: 27682, Training Loss: 5.037e-01 , Validation Loss: 1.488e+00\n",
      "Iteration: 27683, Training Loss: 3.022e+00 , Validation Loss: 4.778e+00\n",
      "Iteration: 27684, Training Loss: 1.511e+00 , Validation Loss: 1.071e+00\n",
      "Iteration: 27685, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 27686, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27687, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27688, Training Loss: -1.000e-07 , Validation Loss: 7.076e-01\n",
      "Iteration: 27689, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 27690, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27691, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27692, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27693, Training Loss: 1.259e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 27694, Training Loss: -1.000e-07 , Validation Loss: 6.169e-01\n",
      "Iteration: 27695, Training Loss: -1.000e-07 , Validation Loss: 6.169e-01\n",
      "Iteration: 27696, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 27697, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27698, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27699, Training Loss: 2.518e-01 , Validation Loss: 2.897e+00\n",
      "Iteration: 27700, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 27701, Training Loss: 4.030e+00 , Validation Loss: 5.147e+00\n",
      "Iteration: 27702, Training Loss: 2.267e+00 , Validation Loss: 3.556e+00\n",
      "Iteration: 27703, Training Loss: 1.259e+00 , Validation Loss: 8.951e-01\n",
      "Iteration: 27704, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27705, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27706, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27707, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27708, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27709, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27710, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 27711, Training Loss: 7.555e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 27712, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27713, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 27714, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 27715, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 27716, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27717, Training Loss: 1.007e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 27718, Training Loss: 4.281e+00 , Validation Loss: 4.361e+00\n",
      "Iteration: 27719, Training Loss: 8.815e+00 , Validation Loss: 1.003e+01\n",
      "Iteration: 27720, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27721, Training Loss: 2.015e+00 , Validation Loss: 4.064e+00\n",
      "Iteration: 27722, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 27723, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 27724, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27725, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27726, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27727, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27728, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27729, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27730, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27731, Training Loss: 2.518e-01 , Validation Loss: 2.631e+00\n",
      "Iteration: 27732, Training Loss: 2.518e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 27733, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27734, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27735, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27736, Training Loss: 2.518e-01 , Validation Loss: 1.095e+00\n",
      "Iteration: 27737, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27738, Training Loss: 2.267e+00 , Validation Loss: 3.992e+00\n",
      "Iteration: 27739, Training Loss: 7.555e-01 , Validation Loss: 8.467e-01\n",
      "Iteration: 27740, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 27741, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27742, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27743, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27744, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27745, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27746, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27747, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27748, Training Loss: -1.000e-07 , Validation Loss: 5.020e-01\n",
      "Iteration: 27749, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 27750, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27751, Training Loss: 7.555e-01 , Validation Loss: 1.415e+00\n",
      "Iteration: 27752, Training Loss: 2.518e+00 , Validation Loss: 4.355e+00\n",
      "Iteration: 27753, Training Loss: 7.555e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 27754, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27755, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27756, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 27757, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27758, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27759, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 27760, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27761, Training Loss: 1.007e+00 , Validation Loss: 5.383e-01\n",
      "Iteration: 27762, Training Loss: 2.518e-01 , Validation Loss: 1.530e+00\n",
      "Iteration: 27763, Training Loss: 2.518e+00 , Validation Loss: 3.943e+00\n",
      "Iteration: 27764, Training Loss: 2.518e+00 , Validation Loss: 1.004e+00\n",
      "Iteration: 27765, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 27766, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27767, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27768, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27769, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27770, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27771, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27772, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27773, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27774, Training Loss: -1.000e-07 , Validation Loss: 5.685e-01\n",
      "Iteration: 27775, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 27776, Training Loss: 2.015e+00 , Validation Loss: 1.917e+00\n",
      "Iteration: 27777, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 27778, Training Loss: 1.763e+00 , Validation Loss: 4.899e-01\n",
      "Iteration: 27779, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 27780, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27781, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27782, Training Loss: 2.015e+00 , Validation Loss: 3.617e+00\n",
      "Iteration: 27783, Training Loss: 1.763e+00 , Validation Loss: 1.125e+00\n",
      "Iteration: 27784, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 27785, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27786, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27787, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27788, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27789, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27790, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27791, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27792, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27793, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27794, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27795, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27796, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27797, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27798, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27799, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27800, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 27801, Training Loss: 2.518e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 27802, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27803, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27804, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27805, Training Loss: -1.000e-07 , Validation Loss: 2.111e+00\n",
      "Iteration: 27806, Training Loss: 1.007e+00 , Validation Loss: 2.111e+00\n",
      "Iteration: 27807, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27808, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27809, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27810, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27811, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27812, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27813, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27814, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27815, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27816, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27817, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27818, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27819, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27820, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27821, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27822, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 27823, Training Loss: 5.037e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 27824, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27825, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27826, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27827, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27828, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 27829, Training Loss: 1.259e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 27830, Training Loss: 2.015e+00 , Validation Loss: 3.889e+00\n",
      "Iteration: 27831, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 27832, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27833, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27834, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27835, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27836, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27837, Training Loss: 5.037e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 27838, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 27839, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27840, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27841, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 27842, Training Loss: 5.037e-01 , Validation Loss: 2.038e+00\n",
      "Iteration: 27843, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 27844, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 27845, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27846, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27847, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27848, Training Loss: 2.518e-01 , Validation Loss: 1.820e+00\n",
      "Iteration: 27849, Training Loss: 1.007e+00 , Validation Loss: 7.742e-01\n",
      "Iteration: 27850, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 27851, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 27852, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27853, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27854, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27855, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27856, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27857, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27858, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 27859, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27860, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27861, Training Loss: 1.511e+00 , Validation Loss: 2.492e+00\n",
      "Iteration: 27862, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 27863, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27864, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27865, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27866, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27867, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27868, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27869, Training Loss: 5.037e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 27870, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27871, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27872, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27873, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27874, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27875, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27876, Training Loss: 7.555e-01 , Validation Loss: 2.074e+00\n",
      "Iteration: 27877, Training Loss: 2.518e-01 , Validation Loss: 1.137e+00\n",
      "Iteration: 27878, Training Loss: 1.259e+00 , Validation Loss: 3.714e+00\n",
      "Iteration: 27879, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27880, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27881, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27882, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 27883, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 27884, Training Loss: 2.518e-01 , Validation Loss: 9.858e-01\n",
      "Iteration: 27885, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 27886, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27887, Training Loss: 3.778e+00 , Validation Loss: 3.980e+00\n",
      "Iteration: 27888, Training Loss: 3.274e+00 , Validation Loss: 4.615e+00\n",
      "Iteration: 27889, Training Loss: 1.511e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 27890, Training Loss: 2.015e+00 , Validation Loss: 4.899e-01\n",
      "Iteration: 27891, Training Loss: -1.000e-07 , Validation Loss: 5.080e-01\n",
      "Iteration: 27892, Training Loss: 2.518e-01 , Validation Loss: 5.080e-01\n",
      "Iteration: 27893, Training Loss: 1.511e+00 , Validation Loss: 2.123e+00\n",
      "Iteration: 27894, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 27895, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 27896, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27897, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27898, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27899, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27900, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27901, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27902, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27903, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27904, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 27905, Training Loss: 5.037e-01 , Validation Loss: 1.572e+00\n",
      "Iteration: 27906, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27907, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 27908, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27909, Training Loss: 2.518e-01 , Validation Loss: 2.068e+00\n",
      "Iteration: 27910, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 27911, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 27912, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27913, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27914, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27915, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27916, Training Loss: 1.259e+00 , Validation Loss: 6.955e-01\n",
      "Iteration: 27917, Training Loss: 2.770e+00 , Validation Loss: 4.101e+00\n",
      "Iteration: 27918, Training Loss: 1.511e+00 , Validation Loss: 3.556e+00\n",
      "Iteration: 27919, Training Loss: 1.259e+00 , Validation Loss: 1.071e+00\n",
      "Iteration: 27920, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27921, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27922, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27923, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27924, Training Loss: 1.007e+00 , Validation Loss: 6.532e-01\n",
      "Iteration: 27925, Training Loss: 5.037e-01 , Validation Loss: 1.554e+00\n",
      "Iteration: 27926, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27927, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27928, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 27929, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 27930, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 27931, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27932, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 27933, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 27934, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27935, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27936, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27937, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27938, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27939, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27940, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27941, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 27942, Training Loss: 1.259e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 27943, Training Loss: 1.763e+00 , Validation Loss: 5.159e+00\n",
      "Iteration: 27944, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 27945, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27946, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27947, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27948, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27949, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27950, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 27951, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27952, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27953, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27954, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27955, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27956, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27957, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27958, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27959, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27960, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27961, Training Loss: 5.037e-01 , Validation Loss: 1.881e+00\n",
      "Iteration: 27962, Training Loss: -1.000e-07 , Validation Loss: 1.724e+00\n",
      "Iteration: 27963, Training Loss: 7.555e-01 , Validation Loss: 1.724e+00\n",
      "Iteration: 27964, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 27965, Training Loss: 2.518e-01 , Validation Loss: 1.929e+00\n",
      "Iteration: 27966, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 27967, Training Loss: 2.015e+00 , Validation Loss: 3.078e+00\n",
      "Iteration: 27968, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27969, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27970, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27971, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27972, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27973, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 27974, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 27975, Training Loss: 7.555e-01 , Validation Loss: 3.689e+00\n",
      "Iteration: 27976, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 27977, Training Loss: 2.518e+00 , Validation Loss: 4.554e+00\n",
      "Iteration: 27978, Training Loss: 1.007e+00 , Validation Loss: 7.621e-01\n",
      "Iteration: 27979, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 27980, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27981, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 27982, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27983, Training Loss: 5.037e-01 , Validation Loss: 7.500e-01\n",
      "Iteration: 27984, Training Loss: 1.511e+00 , Validation Loss: 1.452e+00\n",
      "Iteration: 27985, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27986, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 27987, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27988, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 27989, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27990, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 27991, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 27992, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 27993, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 27994, Training Loss: 3.778e+00 , Validation Loss: 4.451e+00\n",
      "Iteration: 27995, Training Loss: 3.274e+00 , Validation Loss: 4.754e+00\n",
      "Iteration: 27996, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 27997, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27998, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 27999, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28000, Training Loss: 7.555e-01 , Validation Loss: 1.899e+00\n",
      "Iteration: 28001, Training Loss: 2.518e-01 , Validation Loss: 9.798e-01\n",
      "Iteration: 28002, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 28003, Training Loss: 5.037e-01 , Validation Loss: 1.984e+00\n",
      "Iteration: 28004, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28005, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28006, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28007, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 28008, Training Loss: 1.007e+00 , Validation Loss: 2.105e+00\n",
      "Iteration: 28009, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28010, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 28011, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 28012, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 28013, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28014, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28015, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28016, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 28017, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 28018, Training Loss: 2.015e+00 , Validation Loss: 3.750e+00\n",
      "Iteration: 28019, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 28020, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 28021, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28022, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28023, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28024, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28025, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28026, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 28027, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 28028, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 28029, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 28030, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28031, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 28032, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28033, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28034, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28035, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 28036, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28037, Training Loss: 7.555e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 28038, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28039, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28040, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 28041, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28042, Training Loss: 2.518e-01 , Validation Loss: 1.796e+00\n",
      "Iteration: 28043, Training Loss: 2.518e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 28044, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 28045, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28046, Training Loss: 1.007e+00 , Validation Loss: 1.191e+00\n",
      "Iteration: 28047, Training Loss: 2.015e+00 , Validation Loss: 2.050e+00\n",
      "Iteration: 28048, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 28049, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 28050, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 28051, Training Loss: 3.778e+00 , Validation Loss: 4.935e+00\n",
      "Iteration: 28052, Training Loss: 1.259e+00 , Validation Loss: 1.633e+00\n",
      "Iteration: 28053, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 28054, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28055, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 28056, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 28057, Training Loss: 7.555e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 28058, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28059, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28060, Training Loss: 5.037e-01 , Validation Loss: 7.076e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 28061, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28062, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28063, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28064, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28065, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28066, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28067, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28068, Training Loss: 7.555e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28069, Training Loss: 1.259e+00 , Validation Loss: 8.649e-01\n",
      "Iteration: 28070, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28071, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28072, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28073, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 28074, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28075, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28076, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28077, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28078, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28079, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28080, Training Loss: 2.518e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 28081, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28082, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28083, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28084, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28085, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28086, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28087, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28088, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28089, Training Loss: 2.518e+00 , Validation Loss: 4.016e+00\n",
      "Iteration: 28090, Training Loss: 1.007e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 28091, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 28092, Training Loss: 7.555e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 28093, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28094, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28095, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28096, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28097, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28098, Training Loss: 2.518e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 28099, Training Loss: 7.555e-01 , Validation Loss: 1.941e+00\n",
      "Iteration: 28100, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28101, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28102, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28103, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28104, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28105, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28106, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28107, Training Loss: 7.555e-01 , Validation Loss: 1.016e+00\n",
      "Iteration: 28108, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 28109, Training Loss: 4.533e+00 , Validation Loss: 4.627e+00\n",
      "Iteration: 28110, Training Loss: 1.385e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 28111, Training Loss: 1.763e+00 , Validation Loss: 5.473e+00\n",
      "Iteration: 28112, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 28113, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28114, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28115, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 28116, Training Loss: 1.763e+00 , Validation Loss: 3.018e+00\n",
      "Iteration: 28117, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 28118, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28119, Training Loss: -1.000e-07 , Validation Loss: 4.536e-01\n",
      "Iteration: 28120, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 28121, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28122, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 28123, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28124, Training Loss: 1.007e+00 , Validation Loss: 2.371e+00\n",
      "Iteration: 28125, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28126, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28127, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28128, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28129, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28130, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28131, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28132, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28133, Training Loss: 1.763e+00 , Validation Loss: 2.643e+00\n",
      "Iteration: 28134, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28135, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28136, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28137, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28138, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28139, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28140, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 28141, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 28142, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 28143, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28144, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28145, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 28146, Training Loss: 1.007e+00 , Validation Loss: 3.810e-01\n",
      "Iteration: 28147, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28148, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28149, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28150, Training Loss: 7.555e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 28151, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 28152, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28153, Training Loss: 7.555e-01 , Validation Loss: 1.609e+00\n",
      "Iteration: 28154, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28155, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28156, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28157, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28158, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 28159, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 28160, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28161, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28162, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28163, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28164, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28165, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28166, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28167, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28168, Training Loss: 1.511e+00 , Validation Loss: 2.141e+00\n",
      "Iteration: 28169, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28170, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28171, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28172, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28173, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 28174, Training Loss: 7.555e-01 , Validation Loss: 7.742e-01\n",
      "Iteration: 28175, Training Loss: 7.555e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 28176, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 28177, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28178, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 28179, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28180, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 28181, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28182, Training Loss: 7.555e-01 , Validation Loss: 1.282e+00\n",
      "Iteration: 28183, Training Loss: -1.000e-07 , Validation Loss: 7.681e-01\n",
      "Iteration: 28184, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 28185, Training Loss: 2.770e+00 , Validation Loss: 2.189e+00\n",
      "Iteration: 28186, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28187, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 28188, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28189, Training Loss: 5.037e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 28190, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28191, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28192, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28193, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 28194, Training Loss: 5.037e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 28195, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28196, Training Loss: 7.555e-01 , Validation Loss: 1.161e+00\n",
      "Iteration: 28197, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28198, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28199, Training Loss: 1.511e+00 , Validation Loss: 2.238e+00\n",
      "Iteration: 28200, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 28201, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28202, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28203, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28204, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 28205, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28206, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28207, Training Loss: -1.000e-07 , Validation Loss: 6.350e-01\n",
      "Iteration: 28208, Training Loss: 5.037e-01 , Validation Loss: 6.350e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 28209, Training Loss: 1.007e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 28210, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 28211, Training Loss: 7.555e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 28212, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28213, Training Loss: 5.037e-01 , Validation Loss: 2.607e+00\n",
      "Iteration: 28214, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 28215, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 28216, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28217, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28218, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28219, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28220, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28221, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28222, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28223, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28224, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28225, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28226, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28227, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 28228, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 28229, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28230, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28231, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28232, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28233, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28234, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 28235, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 28236, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 28237, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 28238, Training Loss: 2.518e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 28239, Training Loss: 1.511e+00 , Validation Loss: 1.482e+00\n",
      "Iteration: 28240, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 28241, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28242, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28243, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28244, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28245, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28246, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28247, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28248, Training Loss: 2.518e+00 , Validation Loss: 4.016e+00\n",
      "Iteration: 28249, Training Loss: 1.511e+00 , Validation Loss: 9.858e-01\n",
      "Iteration: 28250, Training Loss: 1.259e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 28251, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28252, Training Loss: 5.037e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 28253, Training Loss: 1.007e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 28254, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28255, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28256, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28257, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28258, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28259, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28260, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28261, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28262, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28263, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28264, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28265, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28266, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28267, Training Loss: 1.007e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 28268, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28269, Training Loss: 7.555e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 28270, Training Loss: 1.511e+00 , Validation Loss: 1.947e+00\n",
      "Iteration: 28271, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 28272, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 28273, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 28274, Training Loss: 2.267e+00 , Validation Loss: 3.907e+00\n",
      "Iteration: 28275, Training Loss: 2.267e+00 , Validation Loss: 1.500e+00\n",
      "Iteration: 28276, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28277, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28278, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28279, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28280, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28281, Training Loss: 1.007e+00 , Validation Loss: 6.834e-01\n",
      "Iteration: 28282, Training Loss: 2.267e+00 , Validation Loss: 3.986e+00\n",
      "Iteration: 28283, Training Loss: 1.259e+00 , Validation Loss: 7.983e-01\n",
      "Iteration: 28284, Training Loss: 2.518e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 28285, Training Loss: -1.000e-07 , Validation Loss: 3.689e-01\n",
      "Iteration: 28286, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 28287, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 28288, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 28289, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 28290, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28291, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28292, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 28293, Training Loss: 2.518e-01 , Validation Loss: 1.016e+00\n",
      "Iteration: 28294, Training Loss: 2.267e+00 , Validation Loss: 3.617e+00\n",
      "Iteration: 28295, Training Loss: 1.259e+00 , Validation Loss: 6.653e-01\n",
      "Iteration: 28296, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28297, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28298, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28299, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28300, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28301, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 28302, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 28303, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 28304, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 28305, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 28306, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28307, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28308, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 28309, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28310, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28311, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28312, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28313, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28314, Training Loss: 1.007e+00 , Validation Loss: 5.262e-01\n",
      "Iteration: 28315, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 28316, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28317, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28318, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 28319, Training Loss: 2.770e+00 , Validation Loss: 4.234e+00\n",
      "Iteration: 28320, Training Loss: 1.259e+00 , Validation Loss: 1.198e+00\n",
      "Iteration: 28321, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 28322, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28323, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 28324, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28325, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28326, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28327, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28328, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28329, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28330, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28331, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 28332, Training Loss: 2.518e-01 , Validation Loss: 1.748e+00\n",
      "Iteration: 28333, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 28334, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 28335, Training Loss: 2.267e+00 , Validation Loss: 3.714e+00\n",
      "Iteration: 28336, Training Loss: 7.555e-01 , Validation Loss: 8.104e-01\n",
      "Iteration: 28337, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28338, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28339, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28340, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 28341, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28342, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28343, Training Loss: 2.518e+00 , Validation Loss: 3.871e+00\n",
      "Iteration: 28344, Training Loss: 1.007e+00 , Validation Loss: 1.464e+00\n",
      "Iteration: 28345, Training Loss: 7.555e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 28346, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28347, Training Loss: 7.555e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28348, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 28349, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28350, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28351, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 28352, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28353, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28354, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28355, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 28356, Training Loss: 4.533e+00 , Validation Loss: 4.379e+00\n",
      "Iteration: 28357, Training Loss: 1.259e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 28358, Training Loss: 5.037e-01 , Validation Loss: 1.343e+00\n",
      "Iteration: 28359, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28360, Training Loss: 3.778e+00 , Validation Loss: 2.776e+00\n",
      "Iteration: 28361, Training Loss: 2.267e+00 , Validation Loss: 4.143e+00\n",
      "Iteration: 28362, Training Loss: 5.037e-01 , Validation Loss: 1.077e+00\n",
      "Iteration: 28363, Training Loss: 1.007e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 28364, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 28365, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28366, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28367, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28368, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28369, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28370, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28371, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 28372, Training Loss: 2.518e+00 , Validation Loss: 3.986e+00\n",
      "Iteration: 28373, Training Loss: 2.015e+00 , Validation Loss: 6.592e-01\n",
      "Iteration: 28374, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 28375, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28376, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28377, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28378, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28379, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28380, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28381, Training Loss: 1.259e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 28382, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28383, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28384, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28385, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28386, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28387, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28388, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28389, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28390, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28391, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28392, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28393, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28394, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28395, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28396, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28397, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28398, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28399, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28400, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28401, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28402, Training Loss: 3.022e+00 , Validation Loss: 4.276e+00\n",
      "Iteration: 28403, Training Loss: 1.007e+00 , Validation Loss: 8.709e-01\n",
      "Iteration: 28404, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28405, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28406, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28407, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28408, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28409, Training Loss: 2.770e+00 , Validation Loss: 3.744e+00\n",
      "Iteration: 28410, Training Loss: 2.015e+00 , Validation Loss: 1.494e+00\n",
      "Iteration: 28411, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 28412, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28413, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28414, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28415, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 28416, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 28417, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28418, Training Loss: -1.000e-07 , Validation Loss: 7.197e-01\n",
      "Iteration: 28419, Training Loss: 5.037e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 28420, Training Loss: 3.526e+00 , Validation Loss: 4.705e+00\n",
      "Iteration: 28421, Training Loss: 1.763e+00 , Validation Loss: 2.335e+00\n",
      "Iteration: 28422, Training Loss: 1.007e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 28423, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28424, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28425, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28426, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28427, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28428, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28429, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28430, Training Loss: 5.037e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 28431, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28432, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 28433, Training Loss: 2.770e+00 , Validation Loss: 4.222e+00\n",
      "Iteration: 28434, Training Loss: 2.267e+00 , Validation Loss: 1.790e+00\n",
      "Iteration: 28435, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28436, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28437, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28438, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28439, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28440, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28441, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28442, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 28443, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 28444, Training Loss: 1.763e+00 , Validation Loss: 3.955e+00\n",
      "Iteration: 28445, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28446, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28447, Training Loss: 1.007e+00 , Validation Loss: 1.046e+00\n",
      "Iteration: 28448, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 28449, Training Loss: 2.518e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 28450, Training Loss: -1.000e-07 , Validation Loss: 4.294e-01\n",
      "Iteration: 28451, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 28452, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 28453, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 28454, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28455, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28456, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28457, Training Loss: 1.007e+00 , Validation Loss: 5.988e-01\n",
      "Iteration: 28458, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 28459, Training Loss: 1.511e+00 , Validation Loss: 7.258e-01\n",
      "Iteration: 28460, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28461, Training Loss: -1.000e-07 , Validation Loss: 7.318e-01\n",
      "Iteration: 28462, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 28463, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28464, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28465, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28466, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28467, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28468, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28469, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28470, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28471, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28472, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 28473, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28474, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28475, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28476, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 28477, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28478, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28479, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28480, Training Loss: 5.037e-01 , Validation Loss: 7.621e-01\n",
      "Iteration: 28481, Training Loss: 7.555e-01 , Validation Loss: 1.300e+00\n",
      "Iteration: 28482, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28483, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28484, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28485, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 28486, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28487, Training Loss: 2.518e-01 , Validation Loss: 7.802e-01\n",
      "Iteration: 28488, Training Loss: 2.518e-01 , Validation Loss: 2.117e+00\n",
      "Iteration: 28489, Training Loss: -1.000e-07 , Validation Loss: 7.560e-01\n",
      "Iteration: 28490, Training Loss: 1.763e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 28491, Training Loss: 5.037e-01 , Validation Loss: 1.802e+00\n",
      "Iteration: 28492, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 28493, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28494, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28495, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28496, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 28497, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28498, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28499, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28500, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28501, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28502, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 28503, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 28504, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28505, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28506, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28507, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28508, Training Loss: 5.037e-01 , Validation Loss: 1.089e+00\n",
      "Iteration: 28509, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 28510, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 28511, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28512, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28513, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28514, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 28515, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28516, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28517, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28518, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28519, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28520, Training Loss: -1.000e-07 , Validation Loss: 6.169e-01\n",
      "Iteration: 28521, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 28522, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28523, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 28524, Training Loss: 1.763e+00 , Validation Loss: 2.238e+00\n",
      "Iteration: 28525, Training Loss: 5.037e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 28526, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 28527, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28528, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28529, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 28530, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 28531, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 28532, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28533, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28534, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 28535, Training Loss: 7.555e-01 , Validation Loss: 1.548e+00\n",
      "Iteration: 28536, Training Loss: 2.770e+00 , Validation Loss: 4.101e+00\n",
      "Iteration: 28537, Training Loss: 7.555e-01 , Validation Loss: 1.185e+00\n",
      "Iteration: 28538, Training Loss: 7.555e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 28539, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28540, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28541, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28542, Training Loss: -1.000e-07 , Validation Loss: 4.173e-01\n",
      "Iteration: 28543, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 28544, Training Loss: 1.259e+00 , Validation Loss: 5.746e-01\n",
      "Iteration: 28545, Training Loss: 4.030e+00 , Validation Loss: 4.300e+00\n",
      "Iteration: 28546, Training Loss: 2.770e+00 , Validation Loss: 3.248e+00\n",
      "Iteration: 28547, Training Loss: 1.763e+00 , Validation Loss: 6.411e-01\n",
      "Iteration: 28548, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28549, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28550, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28551, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28552, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28553, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28554, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28555, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 28556, Training Loss: 1.259e+00 , Validation Loss: 3.205e+00\n",
      "Iteration: 28557, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 28558, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28559, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28560, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28561, Training Loss: 1.007e+00 , Validation Loss: 4.173e-01\n",
      "Iteration: 28562, Training Loss: 4.030e+00 , Validation Loss: 4.355e+00\n",
      "Iteration: 28563, Training Loss: 6.800e+00 , Validation Loss: 7.088e+00\n",
      "Iteration: 28564, Training Loss: 1.511e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 28565, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28566, Training Loss: 1.259e+00 , Validation Loss: 2.734e+00\n",
      "Iteration: 28567, Training Loss: 1.259e+00 , Validation Loss: 7.076e-01\n",
      "Iteration: 28568, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28569, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28570, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28571, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28572, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28573, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28574, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28575, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28576, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28577, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28578, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28579, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28580, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28581, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28582, Training Loss: 1.259e+00 , Validation Loss: 2.716e+00\n",
      "Iteration: 28583, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28584, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28585, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28586, Training Loss: 1.259e+00 , Validation Loss: 2.800e+00\n",
      "Iteration: 28587, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 28588, Training Loss: -1.000e-07 , Validation Loss: 3.750e-01\n",
      "Iteration: 28589, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28590, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28591, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28592, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28593, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 28594, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28595, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28596, Training Loss: -1.000e-07 , Validation Loss: 4.899e-01\n",
      "Iteration: 28597, Training Loss: 1.007e+00 , Validation Loss: 4.899e-01\n",
      "Iteration: 28598, Training Loss: 7.555e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 28599, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28600, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 28601, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28602, Training Loss: 1.763e+00 , Validation Loss: 3.647e+00\n",
      "Iteration: 28603, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 28604, Training Loss: 1.259e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 28605, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28606, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28607, Training Loss: 1.259e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 28608, Training Loss: 7.555e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 28609, Training Loss: 7.555e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 28610, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 28611, Training Loss: 2.518e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 28612, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 28613, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28614, Training Loss: 2.518e-01 , Validation Loss: 6.471e-01\n",
      "Iteration: 28615, Training Loss: 1.007e+00 , Validation Loss: 1.790e+00\n",
      "Iteration: 28616, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28617, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28618, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28619, Training Loss: 5.037e-01 , Validation Loss: 1.185e+00\n",
      "Iteration: 28620, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28621, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 28622, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28623, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 28624, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28625, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28626, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28627, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28628, Training Loss: 1.259e+00 , Validation Loss: 3.538e+00\n",
      "Iteration: 28629, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 28630, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28631, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28632, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28633, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28634, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28635, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28636, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28637, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28638, Training Loss: 2.518e-01 , Validation Loss: 7.983e-01\n",
      "Iteration: 28639, Training Loss: 1.007e+00 , Validation Loss: 2.359e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 28640, Training Loss: 1.511e+00 , Validation Loss: 3.714e+00\n",
      "Iteration: 28641, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28642, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28643, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 28644, Training Loss: 4.281e+00 , Validation Loss: 5.026e+00\n",
      "Iteration: 28645, Training Loss: 7.304e+00 , Validation Loss: 7.439e+00\n",
      "Iteration: 28646, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28647, Training Loss: -1.000e-07 , Validation Loss: 6.713e-01\n",
      "Iteration: 28648, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 28649, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28650, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28651, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28652, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28653, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28654, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 28655, Training Loss: -1.000e-07 , Validation Loss: 1.058e+00\n",
      "Iteration: 28656, Training Loss: 7.555e-01 , Validation Loss: 1.058e+00\n",
      "Iteration: 28657, Training Loss: 3.526e+00 , Validation Loss: 4.040e+00\n",
      "Iteration: 28658, Training Loss: 2.015e+00 , Validation Loss: 3.254e+00\n",
      "Iteration: 28659, Training Loss: 5.037e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 28660, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28661, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 28662, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28663, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28664, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28665, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 28666, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 28667, Training Loss: 2.518e-01 , Validation Loss: 1.706e+00\n",
      "Iteration: 28668, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 28669, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28670, Training Loss: 2.518e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 28671, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 28672, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28673, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28674, Training Loss: 2.267e+00 , Validation Loss: 3.587e+00\n",
      "Iteration: 28675, Training Loss: 1.007e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 28676, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28677, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28678, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28679, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 28680, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28681, Training Loss: 1.259e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 28682, Training Loss: 5.037e+00 , Validation Loss: 5.486e+00\n",
      "Iteration: 28683, Training Loss: 1.184e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 28684, Training Loss: 2.518e+00 , Validation Loss: 3.018e+00\n",
      "Iteration: 28685, Training Loss: 1.763e+00 , Validation Loss: 9.616e-01\n",
      "Iteration: 28686, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28687, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28688, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28689, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28690, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28691, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28692, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28693, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28694, Training Loss: 1.259e+00 , Validation Loss: 2.431e+00\n",
      "Iteration: 28695, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28696, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28697, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28698, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28699, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28700, Training Loss: 2.770e+00 , Validation Loss: 3.085e+00\n",
      "Iteration: 28701, Training Loss: 2.267e+00 , Validation Loss: 1.754e+00\n",
      "Iteration: 28702, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28703, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28704, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28705, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28706, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28707, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28708, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28709, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28710, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28711, Training Loss: 2.267e+00 , Validation Loss: 3.701e+00\n",
      "Iteration: 28712, Training Loss: -1.000e-07 , Validation Loss: 4.657e-01\n",
      "Iteration: 28713, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 28714, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 28715, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 28716, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28717, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28718, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28719, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28720, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28721, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28722, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 28723, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28724, Training Loss: 2.518e-01 , Validation Loss: 1.204e+00\n",
      "Iteration: 28725, Training Loss: 1.259e+00 , Validation Loss: 3.284e+00\n",
      "Iteration: 28726, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 28727, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28728, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28729, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28730, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28731, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 28732, Training Loss: 5.037e-01 , Validation Loss: 1.724e+00\n",
      "Iteration: 28733, Training Loss: 1.763e+00 , Validation Loss: 3.326e+00\n",
      "Iteration: 28734, Training Loss: 5.037e-01 , Validation Loss: 8.649e-01\n",
      "Iteration: 28735, Training Loss: 1.259e+00 , Validation Loss: 6.774e-01\n",
      "Iteration: 28736, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 28737, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28738, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 28739, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 28740, Training Loss: -1.000e-07 , Validation Loss: 1.010e+00\n",
      "Iteration: 28741, Training Loss: 7.555e-01 , Validation Loss: 1.010e+00\n",
      "Iteration: 28742, Training Loss: 4.030e+00 , Validation Loss: 5.219e+00\n",
      "Iteration: 28743, Training Loss: 2.518e+00 , Validation Loss: 2.038e+00\n",
      "Iteration: 28744, Training Loss: 1.259e+00 , Validation Loss: 5.625e-01\n",
      "Iteration: 28745, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28746, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28747, Training Loss: 1.259e+00 , Validation Loss: 4.058e+00\n",
      "Iteration: 28748, Training Loss: -1.000e-07 , Validation Loss: 4.052e-01\n",
      "Iteration: 28749, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 28750, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28751, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28752, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28753, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28754, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28755, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28756, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28757, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 28758, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28759, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 28760, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 28761, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 28762, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 28763, Training Loss: 7.555e-01 , Validation Loss: 4.899e-01\n",
      "Iteration: 28764, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28765, Training Loss: -1.000e-07 , Validation Loss: 6.955e-01\n",
      "Iteration: 28766, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 28767, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28768, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28769, Training Loss: 2.518e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 28770, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28771, Training Loss: 2.770e+00 , Validation Loss: 3.810e+00\n",
      "Iteration: 28772, Training Loss: 1.259e+00 , Validation Loss: 1.058e+00\n",
      "Iteration: 28773, Training Loss: 7.555e-01 , Validation Loss: 5.262e-01\n",
      "Iteration: 28774, Training Loss: 7.555e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 28775, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28776, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28777, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28778, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28779, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28780, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 28781, Training Loss: 2.518e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 28782, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28783, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28784, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 28785, Training Loss: -1.000e-07 , Validation Loss: 5.504e-01\n",
      "Iteration: 28786, Training Loss: 2.518e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 28787, Training Loss: 5.037e-01 , Validation Loss: 7.681e-01\n",
      "Iteration: 28788, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28789, Training Loss: -1.000e-07 , Validation Loss: 6.955e-01\n",
      "Iteration: 28790, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 28791, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28792, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28793, Training Loss: 1.511e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 28794, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 28795, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28796, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28797, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 28798, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28799, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28800, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28801, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28802, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 28803, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28804, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 28805, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28806, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28807, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 28808, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28809, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28810, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28811, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 28812, Training Loss: 1.259e+00 , Validation Loss: 2.208e+00\n",
      "Iteration: 28813, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28814, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 28815, Training Loss: -1.000e-07 , Validation Loss: 7.197e-01\n",
      "Iteration: 28816, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 28817, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 28818, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28819, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28820, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28821, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28822, Training Loss: 5.037e-01 , Validation Loss: 2.226e+00\n",
      "Iteration: 28823, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28824, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28825, Training Loss: 1.007e+00 , Validation Loss: 1.161e+00\n",
      "Iteration: 28826, Training Loss: 1.007e+00 , Validation Loss: 1.845e+00\n",
      "Iteration: 28827, Training Loss: 2.518e-01 , Validation Loss: 5.443e-01\n",
      "Iteration: 28828, Training Loss: 5.037e-01 , Validation Loss: 1.071e+00\n",
      "Iteration: 28829, Training Loss: -1.000e-07 , Validation Loss: 1.288e+00\n",
      "Iteration: 28830, Training Loss: 5.037e-01 , Validation Loss: 1.288e+00\n",
      "Iteration: 28831, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 28832, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28833, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28834, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28835, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28836, Training Loss: 2.518e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 28837, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 28838, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28839, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28840, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28841, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28842, Training Loss: 7.555e-01 , Validation Loss: 9.193e-01\n",
      "Iteration: 28843, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28844, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28845, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28846, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28847, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28848, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28849, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28850, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28851, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28852, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28853, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28854, Training Loss: 2.518e+00 , Validation Loss: 3.937e+00\n",
      "Iteration: 28855, Training Loss: 1.007e+00 , Validation Loss: 1.458e+00\n",
      "Iteration: 28856, Training Loss: 1.259e+00 , Validation Loss: 7.923e-01\n",
      "Iteration: 28857, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28858, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28859, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28860, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28861, Training Loss: 5.037e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 28862, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28863, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28864, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28865, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28866, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28867, Training Loss: 5.037e-01 , Validation Loss: 2.014e+00\n",
      "Iteration: 28868, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28869, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28870, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28871, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 28872, Training Loss: 7.555e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 28873, Training Loss: 2.518e-01 , Validation Loss: 2.371e+00\n",
      "Iteration: 28874, Training Loss: 2.518e-01 , Validation Loss: 9.193e-01\n",
      "Iteration: 28875, Training Loss: 1.259e+00 , Validation Loss: 3.671e+00\n",
      "Iteration: 28876, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28877, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28878, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28879, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28880, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28881, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28882, Training Loss: 1.511e+00 , Validation Loss: 2.516e+00\n",
      "Iteration: 28883, Training Loss: 1.511e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 28884, Training Loss: -1.000e-07 , Validation Loss: 7.258e-01\n",
      "Iteration: 28885, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 28886, Training Loss: 7.555e-01 , Validation Loss: 3.345e+00\n",
      "Iteration: 28887, Training Loss: -1.000e-07 , Validation Loss: 7.439e-01\n",
      "Iteration: 28888, Training Loss: -1.000e-07 , Validation Loss: 7.439e-01\n",
      "Iteration: 28889, Training Loss: 5.037e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 28890, Training Loss: 3.526e+00 , Validation Loss: 5.038e+00\n",
      "Iteration: 28891, Training Loss: 3.274e+00 , Validation Loss: 3.732e+00\n",
      "Iteration: 28892, Training Loss: 1.511e+00 , Validation Loss: 6.109e-01\n",
      "Iteration: 28893, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28894, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28895, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28896, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28897, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28898, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28899, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28900, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28901, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28902, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 28903, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 28904, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28905, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28906, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28907, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28908, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28909, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28910, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28911, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28912, Training Loss: 5.037e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 28913, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28914, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28915, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 28916, Training Loss: 7.555e-01 , Validation Loss: 1.312e+00\n",
      "Iteration: 28917, Training Loss: 2.267e+00 , Validation Loss: 3.961e+00\n",
      "Iteration: 28918, Training Loss: 1.259e+00 , Validation Loss: 9.435e-01\n",
      "Iteration: 28919, Training Loss: 1.007e+00 , Validation Loss: 4.778e-01\n",
      "Iteration: 28920, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28921, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28922, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28923, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28924, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28925, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28926, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28927, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28928, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28929, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28930, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28931, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28932, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28933, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28934, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28935, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 28936, Training Loss: 2.518e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 28937, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28938, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 28939, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28940, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28941, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 28942, Training Loss: 1.007e+00 , Validation Loss: 4.597e-01\n",
      "Iteration: 28943, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28944, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28945, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28946, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28947, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28948, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28949, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28950, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 28951, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28952, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28953, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28954, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28955, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28956, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28957, Training Loss: 7.555e-01 , Validation Loss: 1.276e+00\n",
      "Iteration: 28958, Training Loss: 5.037e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 28959, Training Loss: 5.037e-01 , Validation Loss: 1.252e+00\n",
      "Iteration: 28960, Training Loss: 2.518e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 28961, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28962, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28963, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28964, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28965, Training Loss: 2.518e+00 , Validation Loss: 3.623e+00\n",
      "Iteration: 28966, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28967, Training Loss: 1.259e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 28968, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28969, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28970, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28971, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28972, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28973, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28974, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28975, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 28976, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28977, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28978, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28979, Training Loss: 2.518e-01 , Validation Loss: 1.464e+00\n",
      "Iteration: 28980, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 28981, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 28982, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28983, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28984, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28985, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 28986, Training Loss: 3.022e+00 , Validation Loss: 5.014e+00\n",
      "Iteration: 28987, Training Loss: 1.007e+00 , Validation Loss: 1.064e+00\n",
      "Iteration: 28988, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 28989, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28990, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 28991, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 28992, Training Loss: 1.259e+00 , Validation Loss: 3.018e+00\n",
      "Iteration: 28993, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28994, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 28995, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28996, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28997, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 28998, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 28999, Training Loss: 7.555e-01 , Validation Loss: 1.857e+00\n",
      "Iteration: 29000, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29001, Training Loss: 2.518e-01 , Validation Loss: 1.814e+00\n",
      "Iteration: 29002, Training Loss: 3.274e+00 , Validation Loss: 4.209e+00\n",
      "Iteration: 29003, Training Loss: 1.259e+00 , Validation Loss: 1.766e+00\n",
      "Iteration: 29004, Training Loss: 1.511e+00 , Validation Loss: 7.439e-01\n",
      "Iteration: 29005, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 29006, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29007, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29008, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29009, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29010, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 29011, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29012, Training Loss: 1.763e+00 , Validation Loss: 4.034e+00\n",
      "Iteration: 29013, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 29014, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29015, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29016, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29017, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29018, Training Loss: 1.007e+00 , Validation Loss: 1.494e+00\n",
      "Iteration: 29019, Training Loss: 2.770e+00 , Validation Loss: 3.653e+00\n",
      "Iteration: 29020, Training Loss: 1.259e+00 , Validation Loss: 1.506e+00\n",
      "Iteration: 29021, Training Loss: 1.007e+00 , Validation Loss: 6.713e-01\n",
      "Iteration: 29022, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 29023, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29024, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29025, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 29026, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29027, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 29028, Training Loss: 3.022e+00 , Validation Loss: 4.947e+00\n",
      "Iteration: 29029, Training Loss: 2.267e+00 , Validation Loss: 2.395e+00\n",
      "Iteration: 29030, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29031, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29032, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29033, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29034, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29035, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29036, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29037, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29038, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29039, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29040, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29041, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29042, Training Loss: 7.555e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 29043, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29044, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 29045, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 29046, Training Loss: 1.511e+00 , Validation Loss: 2.812e+00\n",
      "Iteration: 29047, Training Loss: 1.511e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 29048, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29049, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29050, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29051, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29052, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29053, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29054, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 29055, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29056, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29057, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29058, Training Loss: 2.518e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 29059, Training Loss: 5.037e-01 , Validation Loss: 3.006e+00\n",
      "Iteration: 29060, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 29061, Training Loss: 1.259e+00 , Validation Loss: 1.542e+00\n",
      "Iteration: 29062, Training Loss: 7.555e-01 , Validation Loss: 7.258e-01\n",
      "Iteration: 29063, Training Loss: 2.518e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 29064, Training Loss: 2.518e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 29065, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 29066, Training Loss: 5.037e-01 , Validation Loss: 9.858e-01\n",
      "Iteration: 29067, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29068, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29069, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29070, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29071, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29072, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29073, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29074, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29075, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 29076, Training Loss: 1.763e+00 , Validation Loss: 3.677e+00\n",
      "Iteration: 29077, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29078, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29079, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29080, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29081, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29082, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29083, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29084, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29085, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29086, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29087, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 29088, Training Loss: 7.555e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 29089, Training Loss: -1.000e-07 , Validation Loss: 3.992e-01\n",
      "Iteration: 29090, Training Loss: 5.037e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 29091, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29092, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29093, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29094, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29095, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 29096, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 29097, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 29098, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 29099, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 29100, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29101, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29102, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29103, Training Loss: 7.555e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 29104, Training Loss: 1.007e+00 , Validation Loss: 2.722e+00\n",
      "Iteration: 29105, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29106, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29107, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29108, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29109, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29110, Training Loss: -1.000e-07 , Validation Loss: 3.629e-01\n",
      "Iteration: 29111, Training Loss: 2.518e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 29112, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29113, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29114, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29115, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29116, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 29117, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 29118, Training Loss: 7.555e-01 , Validation Loss: 7.379e-01\n",
      "Iteration: 29119, Training Loss: 1.511e+00 , Validation Loss: 2.903e+00\n",
      "Iteration: 29120, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29121, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 29122, Training Loss: 1.007e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 29123, Training Loss: -1.000e-07 , Validation Loss: 6.774e-01\n",
      "Iteration: 29124, Training Loss: 7.555e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 29125, Training Loss: 1.259e+00 , Validation Loss: 6.350e-01\n",
      "Iteration: 29126, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29127, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29128, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 29129, Training Loss: 1.511e+00 , Validation Loss: 4.179e+00\n",
      "Iteration: 29130, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 29131, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29132, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29133, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29134, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 29135, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29136, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 29137, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 29138, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 29139, Training Loss: 5.037e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 29140, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29141, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29142, Training Loss: 2.518e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 29143, Training Loss: 5.037e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 29144, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 29145, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29146, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 29147, Training Loss: 5.037e-01 , Validation Loss: 1.579e+00\n",
      "Iteration: 29148, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 29149, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 29150, Training Loss: 2.518e-01 , Validation Loss: 9.375e-01\n",
      "Iteration: 29151, Training Loss: 5.037e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 29152, Training Loss: 5.037e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 29153, Training Loss: 5.037e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 29154, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29155, Training Loss: 1.259e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 29156, Training Loss: 2.267e+00 , Validation Loss: 4.101e+00\n",
      "Iteration: 29157, Training Loss: 1.259e+00 , Validation Loss: 8.165e-01\n",
      "Iteration: 29158, Training Loss: 5.037e-01 , Validation Loss: 4.838e-01\n",
      "Iteration: 29159, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 29160, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29161, Training Loss: -1.000e-07 , Validation Loss: 6.834e-01\n",
      "Iteration: 29162, Training Loss: 2.518e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 29163, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29164, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29165, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 29166, Training Loss: 5.037e-01 , Validation Loss: 1.046e+00\n",
      "Iteration: 29167, Training Loss: 7.555e-01 , Validation Loss: 1.857e+00\n",
      "Iteration: 29168, Training Loss: 1.007e+00 , Validation Loss: 1.385e+00\n",
      "Iteration: 29169, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29170, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29171, Training Loss: 1.007e+00 , Validation Loss: 5.927e-01\n",
      "Iteration: 29172, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29173, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29174, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29175, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29176, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29177, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29178, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29179, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29180, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29181, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29182, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29183, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29184, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29185, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29186, Training Loss: 2.518e-01 , Validation Loss: 1.234e+00\n",
      "Iteration: 29187, Training Loss: 7.555e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 29188, Training Loss: 2.015e+00 , Validation Loss: 3.526e+00\n",
      "Iteration: 29189, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 29190, Training Loss: 7.555e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 29191, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29192, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29193, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 29194, Training Loss: 2.518e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 29195, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 29196, Training Loss: 5.792e+00 , Validation Loss: 5.189e+00\n",
      "Iteration: 29197, Training Loss: 1.360e+01 , Validation Loss: 1.024e+01\n",
      "Iteration: 29198, Training Loss: 7.555e-01 , Validation Loss: 6.532e-01\n",
      "Iteration: 29199, Training Loss: 5.037e-01 , Validation Loss: 3.387e-01\n",
      "Iteration: 29200, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29201, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29202, Training Loss: 1.259e+00 , Validation Loss: 1.597e+00\n",
      "Iteration: 29203, Training Loss: 2.518e-01 , Validation Loss: 1.464e+00\n",
      "Iteration: 29204, Training Loss: 3.022e+00 , Validation Loss: 2.873e+00\n",
      "Iteration: 29205, Training Loss: 1.511e+00 , Validation Loss: 1.820e+00\n",
      "Iteration: 29206, Training Loss: 5.037e-01 , Validation Loss: 7.560e-01\n",
      "Iteration: 29207, Training Loss: 1.007e+00 , Validation Loss: 5.685e-01\n",
      "Iteration: 29208, Training Loss: -1.000e-07 , Validation Loss: 3.871e-01\n",
      "Iteration: 29209, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 29210, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 29211, Training Loss: -1.000e-07 , Validation Loss: 3.387e-01\n",
      "Iteration: 29212, Training Loss: 1.007e+00 , Validation Loss: 3.387e-01\n",
      "Iteration: 29213, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29214, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29215, Training Loss: 5.037e-01 , Validation Loss: 8.770e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 29216, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29217, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29218, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29219, Training Loss: 2.015e+00 , Validation Loss: 3.514e+00\n",
      "Iteration: 29220, Training Loss: 1.259e+00 , Validation Loss: 5.141e-01\n",
      "Iteration: 29221, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29222, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29223, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29224, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29225, Training Loss: 2.518e+00 , Validation Loss: 1.712e+00\n",
      "Iteration: 29226, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29227, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 29228, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29229, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29230, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 29231, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29232, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29233, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 29234, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29235, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29236, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29237, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29238, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29239, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29240, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29241, Training Loss: 2.518e-01 , Validation Loss: 6.774e-01\n",
      "Iteration: 29242, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 29243, Training Loss: 1.007e+00 , Validation Loss: 4.113e-01\n",
      "Iteration: 29244, Training Loss: 7.555e-01 , Validation Loss: 1.052e+00\n",
      "Iteration: 29245, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29246, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29247, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29248, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29249, Training Loss: 5.037e-01 , Validation Loss: 1.754e+00\n",
      "Iteration: 29250, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29251, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29252, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29253, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29254, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29255, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29256, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29257, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29258, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29259, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29260, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29261, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29262, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29263, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29264, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 29265, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 29266, Training Loss: 1.007e+00 , Validation Loss: 3.199e+00\n",
      "Iteration: 29267, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29268, Training Loss: 2.518e-01 , Validation Loss: 6.290e-01\n",
      "Iteration: 29269, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29270, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 29271, Training Loss: 2.518e+00 , Validation Loss: 4.282e+00\n",
      "Iteration: 29272, Training Loss: 2.518e+00 , Validation Loss: 9.979e-01\n",
      "Iteration: 29273, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29274, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29275, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29276, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29277, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29278, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29279, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29280, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29281, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29282, Training Loss: -1.000e-07 , Validation Loss: 5.746e-01\n",
      "Iteration: 29283, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 29284, Training Loss: 1.259e+00 , Validation Loss: 3.266e+00\n",
      "Iteration: 29285, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29286, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29287, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29288, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29289, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29290, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29291, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29292, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29293, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29294, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 29295, Training Loss: 2.518e-01 , Validation Loss: 1.349e+00\n",
      "Iteration: 29296, Training Loss: 5.037e-01 , Validation Loss: 6.048e-01\n",
      "Iteration: 29297, Training Loss: 2.518e-01 , Validation Loss: 6.592e-01\n",
      "Iteration: 29298, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 29299, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 29300, Training Loss: 5.037e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 29301, Training Loss: 5.037e-01 , Validation Loss: 5.201e-01\n",
      "Iteration: 29302, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29303, Training Loss: 1.259e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 29304, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29305, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29306, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29307, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29308, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29309, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29310, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29311, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29312, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29313, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 29314, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29315, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29316, Training Loss: -1.000e-07 , Validation Loss: 4.959e-01\n",
      "Iteration: 29317, Training Loss: 2.518e-01 , Validation Loss: 4.959e-01\n",
      "Iteration: 29318, Training Loss: 5.037e-01 , Validation Loss: 1.796e+00\n",
      "Iteration: 29319, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29320, Training Loss: 2.015e+00 , Validation Loss: 2.897e+00\n",
      "Iteration: 29321, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29322, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 29323, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 29324, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29325, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29326, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29327, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29328, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29329, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29330, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29331, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29332, Training Loss: 5.037e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 29333, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29334, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29335, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29336, Training Loss: 5.037e-01 , Validation Loss: 3.520e+00\n",
      "Iteration: 29337, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29338, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 29339, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29340, Training Loss: 5.037e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 29341, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29342, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 29343, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29344, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29345, Training Loss: 7.555e-01 , Validation Loss: 1.675e+00\n",
      "Iteration: 29346, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29347, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29348, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29349, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29350, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29351, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29352, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 29353, Training Loss: 1.007e+00 , Validation Loss: 1.300e+00\n",
      "Iteration: 29354, Training Loss: 1.007e+00 , Validation Loss: 1.893e+00\n",
      "Iteration: 29355, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29356, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29357, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29358, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29359, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29360, Training Loss: 1.511e+00 , Validation Loss: 3.048e+00\n",
      "Iteration: 29361, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29362, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29363, Training Loss: 1.007e+00 , Validation Loss: 5.988e-01\n",
      "Iteration: 29364, Training Loss: -1.000e-07 , Validation Loss: 7.016e-01\n",
      "Iteration: 29365, Training Loss: 5.037e-01 , Validation Loss: 7.016e-01\n",
      "Iteration: 29366, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29367, Training Loss: 7.555e-01 , Validation Loss: 5.746e-01\n",
      "Iteration: 29368, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29369, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29370, Training Loss: 2.518e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 29371, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29372, Training Loss: 5.037e-01 , Validation Loss: 4.476e-01\n",
      "Iteration: 29373, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 29374, Training Loss: 5.037e-01 , Validation Loss: 1.839e+00\n",
      "Iteration: 29375, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29376, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29377, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29378, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29379, Training Loss: -1.000e-07 , Validation Loss: 3.931e-01\n",
      "Iteration: 29380, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 29381, Training Loss: 5.037e-01 , Validation Loss: 7.318e-01\n",
      "Iteration: 29382, Training Loss: 2.770e+00 , Validation Loss: 4.349e+00\n",
      "Iteration: 29383, Training Loss: 3.022e+00 , Validation Loss: 2.395e+00\n",
      "Iteration: 29384, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 29385, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 29386, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29387, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29388, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29389, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29390, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29391, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29392, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29393, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29394, Training Loss: 3.022e+00 , Validation Loss: 3.822e+00\n",
      "Iteration: 29395, Training Loss: 1.763e+00 , Validation Loss: 1.718e+00\n",
      "Iteration: 29396, Training Loss: 2.518e-01 , Validation Loss: 6.350e-01\n",
      "Iteration: 29397, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 29398, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 29399, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29400, Training Loss: 2.518e-01 , Validation Loss: 3.992e-01\n",
      "Iteration: 29401, Training Loss: 7.555e-01 , Validation Loss: 1.179e+00\n",
      "Iteration: 29402, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29403, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29404, Training Loss: 2.518e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 29405, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29406, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29407, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29408, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29409, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 29410, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29411, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 29412, Training Loss: 1.259e+00 , Validation Loss: 2.492e+00\n",
      "Iteration: 29413, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29414, Training Loss: 2.518e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 29415, Training Loss: -1.000e-07 , Validation Loss: 8.891e-01\n",
      "Iteration: 29416, Training Loss: 5.037e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 29417, Training Loss: 7.555e-01 , Validation Loss: 1.700e+00\n",
      "Iteration: 29418, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29419, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 29420, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29421, Training Loss: 5.037e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 29422, Training Loss: 3.022e+00 , Validation Loss: 4.101e+00\n",
      "Iteration: 29423, Training Loss: 7.555e-01 , Validation Loss: 1.802e+00\n",
      "Iteration: 29424, Training Loss: 1.007e+00 , Validation Loss: 1.216e+00\n",
      "Iteration: 29425, Training Loss: 1.007e+00 , Validation Loss: 6.653e-01\n",
      "Iteration: 29426, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29427, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29428, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29429, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29430, Training Loss: 2.518e-01 , Validation Loss: 1.149e+00\n",
      "Iteration: 29431, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 29432, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29433, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29434, Training Loss: 4.281e+00 , Validation Loss: 4.965e+00\n",
      "Iteration: 29435, Training Loss: 1.511e+00 , Validation Loss: 2.316e+00\n",
      "Iteration: 29436, Training Loss: 1.259e+00 , Validation Loss: 6.592e-01\n",
      "Iteration: 29437, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29438, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29439, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29440, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29441, Training Loss: 3.526e+00 , Validation Loss: 3.780e+00\n",
      "Iteration: 29442, Training Loss: 3.022e+00 , Validation Loss: 2.443e+00\n",
      "Iteration: 29443, Training Loss: 1.007e+00 , Validation Loss: 5.806e-01\n",
      "Iteration: 29444, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29445, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29446, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29447, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29448, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29449, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29450, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29451, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29452, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29453, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29454, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29455, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29456, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29457, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 29458, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29459, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29460, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29461, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29462, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29463, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29464, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29465, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29466, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29467, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29468, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29469, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29470, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29471, Training Loss: 3.274e+00 , Validation Loss: 4.070e+00\n",
      "Iteration: 29472, Training Loss: 1.007e+00 , Validation Loss: 1.796e+00\n",
      "Iteration: 29473, Training Loss: 1.007e+00 , Validation Loss: 9.072e-01\n",
      "Iteration: 29474, Training Loss: 1.511e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 29475, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29476, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29477, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29478, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29479, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29480, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 29481, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29482, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29483, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 29484, Training Loss: 5.037e-01 , Validation Loss: 4.173e-01\n",
      "Iteration: 29485, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29486, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29487, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29488, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29489, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29490, Training Loss: 1.511e+00 , Validation Loss: 2.480e+00\n",
      "Iteration: 29491, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29492, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29493, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29494, Training Loss: 7.555e-01 , Validation Loss: 1.754e+00\n",
      "Iteration: 29495, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29496, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29497, Training Loss: 7.555e-01 , Validation Loss: 6.834e-01\n",
      "Iteration: 29498, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 29499, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29500, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 29501, Training Loss: 5.037e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 29502, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29503, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29504, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29505, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29506, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29507, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29508, Training Loss: 5.037e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 29509, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 29510, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 29511, Training Loss: 1.763e+00 , Validation Loss: 2.014e+00\n",
      "Iteration: 29512, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29513, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29514, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29515, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29516, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29517, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29518, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29519, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29520, Training Loss: -1.000e-07 , Validation Loss: 3.810e-01\n",
      "Iteration: 29521, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 29522, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29523, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29524, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29525, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29526, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29527, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29528, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29529, Training Loss: 7.555e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 29530, Training Loss: 1.007e+00 , Validation Loss: 2.371e+00\n",
      "Iteration: 29531, Training Loss: -1.000e-07 , Validation Loss: 4.778e-01\n",
      "Iteration: 29532, Training Loss: 7.555e-01 , Validation Loss: 4.778e-01\n",
      "Iteration: 29533, Training Loss: 5.037e-01 , Validation Loss: 1.603e+00\n",
      "Iteration: 29534, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29535, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29536, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29537, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29538, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29539, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29540, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29541, Training Loss: 2.518e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 29542, Training Loss: 7.555e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 29543, Training Loss: 5.037e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 29544, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29545, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29546, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29547, Training Loss: 2.518e-01 , Validation Loss: 6.230e-01\n",
      "Iteration: 29548, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 29549, Training Loss: 5.037e-01 , Validation Loss: 2.383e+00\n",
      "Iteration: 29550, Training Loss: 1.007e+00 , Validation Loss: 2.818e+00\n",
      "Iteration: 29551, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29552, Training Loss: 7.555e-01 , Validation Loss: 1.766e+00\n",
      "Iteration: 29553, Training Loss: 3.526e+00 , Validation Loss: 4.355e+00\n",
      "Iteration: 29554, Training Loss: 1.763e+00 , Validation Loss: 3.472e+00\n",
      "Iteration: 29555, Training Loss: 1.511e+00 , Validation Loss: 1.373e+00\n",
      "Iteration: 29556, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29557, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29558, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29559, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29560, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29561, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 29562, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 29563, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29564, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29565, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29566, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29567, Training Loss: 1.259e+00 , Validation Loss: 1.143e+00\n",
      "Iteration: 29568, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29569, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29570, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29571, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29572, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29573, Training Loss: 7.555e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 29574, Training Loss: 4.030e+00 , Validation Loss: 4.984e+00\n",
      "Iteration: 29575, Training Loss: 6.548e+00 , Validation Loss: 6.731e+00\n",
      "Iteration: 29576, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29577, Training Loss: 1.259e+00 , Validation Loss: 4.476e-01\n",
      "Iteration: 29578, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29579, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29580, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29581, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29582, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29583, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29584, Training Loss: 2.518e-01 , Validation Loss: 8.588e-01\n",
      "Iteration: 29585, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29586, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29587, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29588, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29589, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29590, Training Loss: 2.770e+00 , Validation Loss: 4.191e+00\n",
      "Iteration: 29591, Training Loss: 1.007e+00 , Validation Loss: 1.325e+00\n",
      "Iteration: 29592, Training Loss: 1.511e+00 , Validation Loss: 7.379e-01\n",
      "Iteration: 29593, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29594, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29595, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29596, Training Loss: 7.555e-01 , Validation Loss: 6.653e-01\n",
      "Iteration: 29597, Training Loss: 5.037e-01 , Validation Loss: 4.415e-01\n",
      "Iteration: 29598, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29599, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29600, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29601, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29602, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29603, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29604, Training Loss: 1.007e+00 , Validation Loss: 3.568e-01\n",
      "Iteration: 29605, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29606, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29607, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29608, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29609, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29610, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 29611, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29612, Training Loss: 5.037e-01 , Validation Loss: 2.589e+00\n",
      "Iteration: 29613, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29614, Training Loss: 1.007e+00 , Validation Loss: 2.516e+00\n",
      "Iteration: 29615, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29616, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29617, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29618, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29619, Training Loss: 2.518e-01 , Validation Loss: 5.927e-01\n",
      "Iteration: 29620, Training Loss: 7.555e-01 , Validation Loss: 1.851e+00\n",
      "Iteration: 29621, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29622, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29623, Training Loss: 3.274e+00 , Validation Loss: 4.095e+00\n",
      "Iteration: 29624, Training Loss: 1.007e+00 , Validation Loss: 1.125e+00\n",
      "Iteration: 29625, Training Loss: 1.007e+00 , Validation Loss: 5.201e-01\n",
      "Iteration: 29626, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29627, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29628, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29629, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29630, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29631, Training Loss: 2.518e-01 , Validation Loss: 6.109e-01\n",
      "Iteration: 29632, Training Loss: 1.259e+00 , Validation Loss: 3.641e+00\n",
      "Iteration: 29633, Training Loss: 5.037e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 29634, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29635, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29636, Training Loss: 7.555e-01 , Validation Loss: 7.076e-01\n",
      "Iteration: 29637, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29638, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29639, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29640, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29641, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 29642, Training Loss: 5.037e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 29643, Training Loss: 2.518e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 29644, Training Loss: 2.770e+00 , Validation Loss: 3.121e+00\n",
      "Iteration: 29645, Training Loss: 1.763e+00 , Validation Loss: 6.895e-01\n",
      "Iteration: 29646, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29647, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29648, Training Loss: 1.259e+00 , Validation Loss: 1.131e+00\n",
      "Iteration: 29649, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29650, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29651, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29652, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29653, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29654, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29655, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 29656, Training Loss: 3.274e+00 , Validation Loss: 5.425e+00\n",
      "Iteration: 29657, Training Loss: 1.763e+00 , Validation Loss: 1.367e+00\n",
      "Iteration: 29658, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29659, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 29660, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29661, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29662, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29663, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29664, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29665, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29666, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29667, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29668, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29669, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 29670, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 29671, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 29672, Training Loss: -1.000e-07 , Validation Loss: 4.113e-01\n",
      "Iteration: 29673, Training Loss: 2.518e-01 , Validation Loss: 4.113e-01\n",
      "Iteration: 29674, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29675, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29676, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29677, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29678, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29679, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29680, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29681, Training Loss: 2.518e-01 , Validation Loss: 3.689e-01\n",
      "Iteration: 29682, Training Loss: 7.555e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 29683, Training Loss: 1.763e+00 , Validation Loss: 3.720e+00\n",
      "Iteration: 29684, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 29685, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29686, Training Loss: 2.518e-01 , Validation Loss: 3.750e-01\n",
      "Iteration: 29687, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 29688, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29689, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29690, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29691, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29692, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29693, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29694, Training Loss: 2.518e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 29695, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29696, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29697, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29698, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29699, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 29700, Training Loss: 1.007e+00 , Validation Loss: 1.312e+00\n",
      "Iteration: 29701, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29702, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29703, Training Loss: 1.007e+00 , Validation Loss: 3.871e-01\n",
      "Iteration: 29704, Training Loss: 7.555e-01 , Validation Loss: 5.685e-01\n",
      "Iteration: 29705, Training Loss: 5.037e-01 , Validation Loss: 2.716e+00\n",
      "Iteration: 29706, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29707, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 29708, Training Loss: -1.000e-07 , Validation Loss: 7.439e-01\n",
      "Iteration: 29709, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 29710, Training Loss: 7.555e-01 , Validation Loss: 5.322e-01\n",
      "Iteration: 29711, Training Loss: 7.555e-01 , Validation Loss: 2.129e+00\n",
      "Iteration: 29712, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29713, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29714, Training Loss: 7.555e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 29715, Training Loss: -1.000e-07 , Validation Loss: 4.355e-01\n",
      "Iteration: 29716, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 29717, Training Loss: 1.007e+00 , Validation Loss: 5.988e-01\n",
      "Iteration: 29718, Training Loss: 7.555e-01 , Validation Loss: 1.228e+00\n",
      "Iteration: 29719, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29720, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29721, Training Loss: 1.007e+00 , Validation Loss: 3.992e-01\n",
      "Iteration: 29722, Training Loss: 1.511e+00 , Validation Loss: 2.081e+00\n",
      "Iteration: 29723, Training Loss: 5.037e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 29724, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29725, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29726, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29727, Training Loss: 7.555e-01 , Validation Loss: 7.137e-01\n",
      "Iteration: 29728, Training Loss: 1.259e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 29729, Training Loss: 7.555e-01 , Validation Loss: 2.002e+00\n",
      "Iteration: 29730, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29731, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29732, Training Loss: 1.007e+00 , Validation Loss: 7.500e-01\n",
      "Iteration: 29733, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29734, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 29735, Training Loss: 2.267e+00 , Validation Loss: 3.659e+00\n",
      "Iteration: 29736, Training Loss: 1.007e+00 , Validation Loss: 7.318e-01\n",
      "Iteration: 29737, Training Loss: 2.518e-01 , Validation Loss: 4.294e-01\n",
      "Iteration: 29738, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29739, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29740, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29741, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29742, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29743, Training Loss: 1.007e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 29744, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29745, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29746, Training Loss: 1.763e+00 , Validation Loss: 3.375e+00\n",
      "Iteration: 29747, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29748, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 29749, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29750, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 29751, Training Loss: -1.000e-07 , Validation Loss: 5.625e-01\n",
      "Iteration: 29752, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 29753, Training Loss: -1.000e-07 , Validation Loss: 2.026e+00\n",
      "Iteration: 29754, Training Loss: -1.000e-07 , Validation Loss: 2.026e+00\n",
      "Iteration: 29755, Training Loss: 1.007e+00 , Validation Loss: 2.026e+00\n",
      "Iteration: 29756, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29757, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29758, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29759, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29760, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 29761, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29762, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29763, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29764, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29765, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 29766, Training Loss: 2.267e+00 , Validation Loss: 4.070e+00\n",
      "Iteration: 29767, Training Loss: 5.037e-01 , Validation Loss: 6.713e-01\n",
      "Iteration: 29768, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29769, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 29770, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29771, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29772, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29773, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29774, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29775, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29776, Training Loss: 7.555e-01 , Validation Loss: 1.022e+00\n",
      "Iteration: 29777, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29778, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29779, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29780, Training Loss: 7.555e-01 , Validation Loss: 1.107e+00\n",
      "Iteration: 29781, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29782, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29783, Training Loss: 7.555e-01 , Validation Loss: 5.867e-01\n",
      "Iteration: 29784, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29785, Training Loss: 7.555e-01 , Validation Loss: 1.095e+00\n",
      "Iteration: 29786, Training Loss: 2.518e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 29787, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29788, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29789, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29790, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29791, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29792, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29793, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29794, Training Loss: 7.555e-01 , Validation Loss: 6.169e-01\n",
      "Iteration: 29795, Training Loss: 5.037e-01 , Validation Loss: 3.871e-01\n",
      "Iteration: 29796, Training Loss: 1.763e+00 , Validation Loss: 3.816e+00\n",
      "Iteration: 29797, Training Loss: 1.007e+00 , Validation Loss: 5.867e-01\n",
      "Iteration: 29798, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29799, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29800, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29801, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29802, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29803, Training Loss: 1.007e+00 , Validation Loss: 7.016e-01\n",
      "Iteration: 29804, Training Loss: 5.037e-01 , Validation Loss: 3.278e+00\n",
      "Iteration: 29805, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29806, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29807, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29808, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29809, Training Loss: 5.037e-01 , Validation Loss: 5.141e-01\n",
      "Iteration: 29810, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29811, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29812, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29813, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29814, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 29815, Training Loss: 1.007e+00 , Validation Loss: 2.068e+00\n",
      "Iteration: 29816, Training Loss: 2.015e+00 , Validation Loss: 3.066e+00\n",
      "Iteration: 29817, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29818, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29819, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29820, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29821, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29822, Training Loss: -1.000e-07 , Validation Loss: 5.020e-01\n",
      "Iteration: 29823, Training Loss: 5.037e-01 , Validation Loss: 5.020e-01\n",
      "Iteration: 29824, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29825, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29826, Training Loss: 2.518e-01 , Validation Loss: 5.383e-01\n",
      "Iteration: 29827, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29828, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29829, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 29830, Training Loss: 5.037e-01 , Validation Loss: 6.955e-01\n",
      "Iteration: 29831, Training Loss: 7.555e-01 , Validation Loss: 1.016e+00\n",
      "Iteration: 29832, Training Loss: 4.030e+00 , Validation Loss: 4.318e+00\n",
      "Iteration: 29833, Training Loss: 6.296e+00 , Validation Loss: 7.155e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 29834, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29835, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29836, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29837, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29838, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29839, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29840, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29841, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29842, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29843, Training Loss: 1.007e+00 , Validation Loss: 3.447e-01\n",
      "Iteration: 29844, Training Loss: 2.770e+00 , Validation Loss: 4.324e+00\n",
      "Iteration: 29845, Training Loss: 1.259e+00 , Validation Loss: 8.770e-01\n",
      "Iteration: 29846, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29847, Training Loss: 1.007e+00 , Validation Loss: 4.717e-01\n",
      "Iteration: 29848, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29849, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29850, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29851, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 29852, Training Loss: 2.770e+00 , Validation Loss: 3.550e+00\n",
      "Iteration: 29853, Training Loss: 7.555e-01 , Validation Loss: 1.173e+00\n",
      "Iteration: 29854, Training Loss: 1.259e+00 , Validation Loss: 7.560e-01\n",
      "Iteration: 29855, Training Loss: 1.511e+00 , Validation Loss: 4.052e-01\n",
      "Iteration: 29856, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29857, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29858, Training Loss: 2.518e-01 , Validation Loss: 9.858e-01\n",
      "Iteration: 29859, Training Loss: -1.000e-07 , Validation Loss: 4.717e-01\n",
      "Iteration: 29860, Training Loss: 5.037e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29861, Training Loss: -1.000e-07 , Validation Loss: 6.895e-01\n",
      "Iteration: 29862, Training Loss: -1.000e-07 , Validation Loss: 6.895e-01\n",
      "Iteration: 29863, Training Loss: 5.037e-01 , Validation Loss: 6.895e-01\n",
      "Iteration: 29864, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29865, Training Loss: -1.000e-07 , Validation Loss: 8.891e-01\n",
      "Iteration: 29866, Training Loss: 7.555e-01 , Validation Loss: 8.891e-01\n",
      "Iteration: 29867, Training Loss: 1.259e+00 , Validation Loss: 3.810e+00\n",
      "Iteration: 29868, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 29869, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 29870, Training Loss: 2.518e-01 , Validation Loss: 5.625e-01\n",
      "Iteration: 29871, Training Loss: 1.511e+00 , Validation Loss: 2.081e+00\n",
      "Iteration: 29872, Training Loss: 3.778e+00 , Validation Loss: 5.177e+00\n",
      "Iteration: 29873, Training Loss: 2.770e+00 , Validation Loss: 3.974e+00\n",
      "Iteration: 29874, Training Loss: 1.007e+00 , Validation Loss: 7.197e-01\n",
      "Iteration: 29875, Training Loss: 2.518e-01 , Validation Loss: 4.052e-01\n",
      "Iteration: 29876, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29877, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29878, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 29879, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 29880, Training Loss: -1.000e-07 , Validation Loss: 3.568e-01\n",
      "Iteration: 29881, Training Loss: 7.555e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 29882, Training Loss: 2.518e+00 , Validation Loss: 4.349e+00\n",
      "Iteration: 29883, Training Loss: 1.511e+00 , Validation Loss: 9.737e-01\n",
      "Iteration: 29884, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29885, Training Loss: 7.555e-01 , Validation Loss: 4.536e-01\n",
      "Iteration: 29886, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29887, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29888, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29889, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29890, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29891, Training Loss: 7.555e-01 , Validation Loss: 5.504e-01\n",
      "Iteration: 29892, Training Loss: -1.000e-07 , Validation Loss: 8.770e-01\n",
      "Iteration: 29893, Training Loss: -1.000e-07 , Validation Loss: 8.770e-01\n",
      "Iteration: 29894, Training Loss: 5.037e-01 , Validation Loss: 8.770e-01\n",
      "Iteration: 29895, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29896, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29897, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29898, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29899, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29900, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29901, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29902, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29903, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29904, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29905, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29906, Training Loss: 2.518e-01 , Validation Loss: 5.564e-01\n",
      "Iteration: 29907, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29908, Training Loss: 5.037e-01 , Validation Loss: 5.806e-01\n",
      "Iteration: 29909, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 29910, Training Loss: 1.259e+00 , Validation Loss: 3.091e+00\n",
      "Iteration: 29911, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29912, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29913, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29914, Training Loss: 2.518e-01 , Validation Loss: 7.439e-01\n",
      "Iteration: 29915, Training Loss: 2.518e-01 , Validation Loss: 3.931e-01\n",
      "Iteration: 29916, Training Loss: 2.518e-01 , Validation Loss: 7.197e-01\n",
      "Iteration: 29917, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 29918, Training Loss: 1.007e+00 , Validation Loss: 2.631e+00\n",
      "Iteration: 29919, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29920, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29921, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29922, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29923, Training Loss: 1.007e+00 , Validation Loss: 3.508e-01\n",
      "Iteration: 29924, Training Loss: 5.037e-01 , Validation Loss: 1.167e+00\n",
      "Iteration: 29925, Training Loss: 5.037e-01 , Validation Loss: 1.887e+00\n",
      "Iteration: 29926, Training Loss: 7.555e-01 , Validation Loss: 2.310e+00\n",
      "Iteration: 29927, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29928, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29929, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29930, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29931, Training Loss: 5.037e-01 , Validation Loss: 3.568e-01\n",
      "Iteration: 29932, Training Loss: 5.037e-01 , Validation Loss: 1.149e+00\n",
      "Iteration: 29933, Training Loss: 1.511e+00 , Validation Loss: 2.171e+00\n",
      "Iteration: 29934, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29935, Training Loss: 2.518e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 29936, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29937, Training Loss: 2.518e-01 , Validation Loss: 6.411e-01\n",
      "Iteration: 29938, Training Loss: 2.518e-01 , Validation Loss: 3.810e-01\n",
      "Iteration: 29939, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29940, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29941, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29942, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29943, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29944, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29945, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29946, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29947, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29948, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29949, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29950, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29951, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29952, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29953, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29954, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29955, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29956, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29957, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 29958, Training Loss: 5.037e-01 , Validation Loss: 4.657e-01\n",
      "Iteration: 29959, Training Loss: 1.007e+00 , Validation Loss: 2.056e+00\n",
      "Iteration: 29960, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29961, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29962, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29963, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29964, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29965, Training Loss: 2.518e-01 , Validation Loss: 1.337e+00\n",
      "Iteration: 29966, Training Loss: 2.518e-01 , Validation Loss: 7.923e-01\n",
      "Iteration: 29967, Training Loss: 7.555e-01 , Validation Loss: 1.947e+00\n",
      "Iteration: 29968, Training Loss: 4.030e+00 , Validation Loss: 4.373e+00\n",
      "Iteration: 29969, Training Loss: 9.570e+00 , Validation Loss: 1.024e+01\n",
      "Iteration: 29970, Training Loss: 3.778e+00 , Validation Loss: 3.968e+00\n",
      "Iteration: 29971, Training Loss: 4.030e+00 , Validation Loss: 3.998e+00\n",
      "Iteration: 29972, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29973, Training Loss: 7.555e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29974, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29975, Training Loss: 5.037e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29976, Training Loss: 5.037e-01 , Validation Loss: 4.234e-01\n",
      "Iteration: 29977, Training Loss: 7.555e-01 , Validation Loss: 3.508e-01\n",
      "Iteration: 29978, Training Loss: 1.763e+00 , Validation Loss: 2.915e+00\n",
      "Iteration: 29979, Training Loss: 7.555e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29980, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29981, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29982, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29983, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29984, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29985, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29986, Training Loss: 1.763e+00 , Validation Loss: 3.574e+00\n",
      "Iteration: 29987, Training Loss: 2.518e-01 , Validation Loss: 4.717e-01\n",
      "Iteration: 29988, Training Loss: 1.763e+00 , Validation Loss: 4.536e-01\n",
      "Iteration: 29989, Training Loss: -1.000e-07 , Validation Loss: 3.508e-01\n",
      "Iteration: 29990, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 29991, Training Loss: 5.037e-01 , Validation Loss: 3.629e-01\n",
      "Iteration: 29992, Training Loss: 5.037e-01 , Validation Loss: 4.355e-01\n",
      "Iteration: 29993, Training Loss: 2.518e-01 , Validation Loss: 4.597e-01\n",
      "Iteration: 29994, Training Loss: 5.037e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29995, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29996, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29997, Training Loss: -1.000e-07 , Validation Loss: 3.447e-01\n",
      "Iteration: 29998, Training Loss: 2.518e-01 , Validation Loss: 3.447e-01\n",
      "Iteration: 29999, Training Loss: 2.518e-01 , Validation Loss: 3.508e-01\n",
      "Classification accuracy test set: 0.992%\n",
      "Classification accuracy validation set: 0.979%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    data_train = read_csv('data_training.txt', header=0, index_col=1, parse_dates=True, squeeze=True)\n",
    "    data_test1 = read_csv('data_test.txt', header=0, index_col=1, parse_dates=True, squeeze=True)\n",
    "    data_validation = read_csv('data_validation.txt', header=0, index_col=1, parse_dates=True, squeeze=True)    \n",
    "    \n",
    "    N = data_train.values.shape[0]\n",
    "    features = data_train.values[:,1:-1]\n",
    "    labels = data_train.values[:,-1]\n",
    "    \n",
    "    # Training data \n",
    "    X_train = features.astype('float32')\n",
    "    y_train = labels[:,None].astype('int8')\n",
    "    \n",
    "    # Test data\n",
    "    features_test1 = data_test1.values[:,1:-1]\n",
    "    labels_test1 = data_test1.values[:,-1]\n",
    "    \n",
    "    X_test1 = features_test1.astype('float32')\n",
    "    y_test1 = labels_test1[:,None].astype('int8')\n",
    "\n",
    "    # Validation data\n",
    "    features_validation = data_validation.values[:,1:-1]\n",
    "    labels_validation = data_validation.values[:,-1]\n",
    "    \n",
    "    X_val = features_validation.astype('float32')\n",
    "    y_val = labels_validation[:,None].astype('int8')\n",
    "    # Define model\n",
    "    model = IRLS(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Train the model\n",
    "    w = model.IRLS_train(30000, batch_size = 64)\n",
    "    \n",
    "    # Prediction\n",
    "    test_labels = model.predict(X_test1, w)\n",
    "    \n",
    "    ACC1 = model.accuracy(y_test1, test_labels)\n",
    "    print('Classification accuracy test set: %.3f%%' % (ACC1.mean(0)))\n",
    "    plt.figure(1, figsize=(14,5))\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plot_confusion_matrix(model.M, classes=np.arange(2), normalize=True, title=\"Confusion Matrix Test Data\")\n",
    "\n",
    "    test_labels = model.predict(X_val, w)\n",
    "    \n",
    "    ACC2 = model.accuracy(y_val, test_labels)\n",
    "    print('Classification accuracy validation set: %.3f%%' % (ACC2.mean(0)))\n",
    "    plt.subplot(1,3,2)\n",
    "    plot_confusion_matrix(model.M, classes=np.arange(2), normalize=True,title=\"Confusion Matrix Validation Data\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(model.loss_log_t, label = 'Training loss')\n",
    "    plt.plot(model.loss_log_v, label = 'Validation loss')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteration #')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
